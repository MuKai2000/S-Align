2023-07-27 14:34:20 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:15770
2023-07-27 14:34:20 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:15770
2023-07-27 14:34:21 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:15770
2023-07-27 14:34:21 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:15770
2023-07-27 14:34:21 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:15770
2023-07-27 14:34:21 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:15770
2023-07-27 14:34:21 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:15770
2023-07-27 14:34:21 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:15770
2023-07-27 14:34:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-07-27 14:34:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-07-27 14:34:21 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 14:34:22 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-07-27 14:34:22 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-07-27 14:34:22 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 14:34:22 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 14:34:22 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 14:34:22 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 14:34:22 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-07-27 14:34:22 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-07-27 14:34:22 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-07-27 14:34:26 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15770', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr,train_mt', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_rate=0.5, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr,train_mt', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-07-27 14:34:26 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-07-27 14:34:26 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-07-27 14:34:26 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-07-27 14:34:26 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 1.0
2023-07-27 14:34:26 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-27 14:34:31 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-07-27 14:34:31 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-07-27 14:34:31 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-07-27 14:34:33 | INFO | root | load pretrained hubert
2023-07-27 14:34:35 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline
2023-07-27 14:34:37 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-27 14:34:40 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
2023-07-27 14:34:40 | INFO | root | share the sematic adapter and textual encoder
2023-07-27 14:34:40 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=1024, bias=True)
          (fc2): Linear(in_features=1024, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=1024, bias=True)
        (fc2): Linear(in_features=1024, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-07-27 14:34:40 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-07-27 14:34:40 | INFO | fairseq_cli.train | model: S2TJoint
2023-07-27 14:34:40 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-07-27 14:34:40 | INFO | fairseq_cli.train | num. shared model params: 134,449,280 (num. trained: 134,449,280)
2023-07-27 14:34:40 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-07-27 14:34:40 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-27 14:34:40 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 14:34:40 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 14:34:40 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 14:34:41 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-07-27 14:34:42 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-07-27 14:34:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-07-27 14:34:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-27 14:34:42 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 14:34:42 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 14:34:42 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 14:34:42 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 14:34:42 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 14:34:42 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 14:34:42 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 14:34:42 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-07-27 14:34:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-07-27 14:34:42 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-07-27 14:34:42 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-07-27 14:34:42 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-27 14:34:42 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-27 14:34:42 | INFO | fairseq.trainer | loading train data for epoch 1
2023-07-27 14:34:42 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-07-27 14:34:42 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 14:34:42 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-07-27 14:34:43 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_mt", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 14:34:45 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 14:34:47 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-07-27 14:36:02 | INFO | fairseq.optim.adam | using FusedAdam
2023-07-27 14:36:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 14:36:02 | INFO | fairseq.trainer | begin training epoch 1
2023-07-27 14:36:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 14:37:20 | INFO | train_inner | epoch 001:    100 / 1474 loss=19.137, trans_loss=5.598, nll_loss=4.163, w2v_ctc_loss=22.485, task_loss=1.749, contrastive_loss=3.325, total=4207.04, n_correct=209.3, ppl=17.91, accuracy=4.975, wps=19191.3, ups=1.53, wpb=12551.1, bsz=471.4, num_updates=100, lr=4.098e-06, gnorm=0.891, clip=0, loss_scale=128, train_wall=68, gb_free=19.5, wall=158
2023-07-27 14:38:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 14:38:25 | INFO | train_inner | epoch 001:    201 / 1474 loss=16.991, trans_loss=5.478, nll_loss=4.066, w2v_ctc_loss=19.358, task_loss=1.706, contrastive_loss=3.278, total=4124.14, n_correct=223.32, ppl=16.75, accuracy=5.415, wps=18862.8, ups=1.53, wpb=12313.4, bsz=461, num_updates=200, lr=8.096e-06, gnorm=3.625, clip=0, loss_scale=64, train_wall=65, gb_free=19.2, wall=223
2023-07-27 14:39:29 | INFO | train_inner | epoch 001:    301 / 1474 loss=10.093, trans_loss=5.49, nll_loss=4.135, w2v_ctc_loss=8.784, task_loss=1.706, contrastive_loss=3.203, total=4079.62, n_correct=205.48, ppl=17.57, accuracy=5.037, wps=19045, ups=1.56, wpb=12186.7, bsz=438.2, num_updates=300, lr=1.2094e-05, gnorm=4.607, clip=0, loss_scale=64, train_wall=63, gb_free=19.9, wall=287
2023-07-27 14:40:33 | INFO | train_inner | epoch 001:    401 / 1474 loss=8.848, trans_loss=5.516, nll_loss=4.19, w2v_ctc_loss=6.815, task_loss=1.496, contrastive_loss=3.237, total=4174.14, n_correct=193.73, ppl=18.25, accuracy=4.641, wps=19401.2, ups=1.56, wpb=12463.5, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=2.947, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=351
2023-07-27 14:41:38 | INFO | train_inner | epoch 001:    501 / 1474 loss=8.415, trans_loss=5.494, nll_loss=4.178, w2v_ctc_loss=6.177, task_loss=1.368, contrastive_loss=3.233, total=4176.18, n_correct=189.11, ppl=18.1, accuracy=4.528, wps=19269.2, ups=1.54, wpb=12479.7, bsz=477.4, num_updates=500, lr=2.009e-05, gnorm=1.417, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=416
2023-07-27 14:42:43 | INFO | train_inner | epoch 001:    601 / 1474 loss=8.165, trans_loss=5.524, nll_loss=4.214, w2v_ctc_loss=5.809, task_loss=1.274, contrastive_loss=3.288, total=4147.79, n_correct=183.95, ppl=18.56, accuracy=4.435, wps=19114.3, ups=1.55, wpb=12371.6, bsz=484.2, num_updates=600, lr=2.4088e-05, gnorm=0.727, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=481
2023-07-27 14:43:46 | INFO | train_inner | epoch 001:    701 / 1474 loss=8.008, trans_loss=5.524, nll_loss=4.22, w2v_ctc_loss=5.69, task_loss=1.325, contrastive_loss=3.039, total=4152.1, n_correct=192.9, ppl=18.63, accuracy=4.646, wps=19503.1, ups=1.57, wpb=12395.5, bsz=456.2, num_updates=700, lr=2.8086e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=63, gb_free=19.5, wall=544
2023-07-27 14:44:51 | INFO | train_inner | epoch 001:    801 / 1474 loss=7.733, trans_loss=5.462, nll_loss=4.153, w2v_ctc_loss=5.464, task_loss=1.281, contrastive_loss=2.947, total=4123.83, n_correct=237.59, ppl=17.79, accuracy=5.761, wps=19137.9, ups=1.56, wpb=12306.1, bsz=464.1, num_updates=800, lr=3.2084e-05, gnorm=0.819, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=609
2023-07-27 14:45:56 | INFO | train_inner | epoch 001:    901 / 1474 loss=7.472, trans_loss=5.427, nll_loss=4.122, w2v_ctc_loss=5.281, task_loss=1.302, contrastive_loss=2.707, total=4163.61, n_correct=263.59, ppl=17.41, accuracy=6.331, wps=19075.4, ups=1.53, wpb=12433.9, bsz=457.1, num_updates=900, lr=3.6082e-05, gnorm=1.335, clip=0, loss_scale=64, train_wall=64, gb_free=18.9, wall=674
2023-07-27 14:47:00 | INFO | train_inner | epoch 001:   1001 / 1474 loss=7.212, trans_loss=5.402, nll_loss=4.1, w2v_ctc_loss=5.07, task_loss=1.311, contrastive_loss=2.557, total=4135.34, n_correct=285.42, ppl=17.15, accuracy=6.902, wps=19139.2, ups=1.55, wpb=12353.2, bsz=456.8, num_updates=1000, lr=4.008e-05, gnorm=1.418, clip=0, loss_scale=64, train_wall=64, gb_free=19, wall=738
2023-07-27 14:48:04 | INFO | train_inner | epoch 001:   1101 / 1474 loss=6.943, trans_loss=5.389, nll_loss=4.087, w2v_ctc_loss=4.872, task_loss=1.322, contrastive_loss=2.334, total=4147.38, n_correct=309.58, ppl=17, accuracy=7.464, wps=19460.6, ups=1.57, wpb=12367, bsz=454.9, num_updates=1100, lr=4.4078e-05, gnorm=1.657, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=802
2023-07-27 14:49:08 | INFO | train_inner | epoch 001:   1201 / 1474 loss=6.723, trans_loss=5.369, nll_loss=4.07, w2v_ctc_loss=4.706, task_loss=1.377, contrastive_loss=2.131, total=4139.9, n_correct=316.5, ppl=16.8, accuracy=7.645, wps=19348.5, ups=1.56, wpb=12366.5, bsz=440.1, num_updates=1200, lr=4.8076e-05, gnorm=1.755, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=866
2023-07-27 14:50:13 | INFO | train_inner | epoch 001:   1301 / 1474 loss=6.501, trans_loss=5.367, nll_loss=4.069, w2v_ctc_loss=4.511, task_loss=1.324, contrastive_loss=1.941, total=4046.58, n_correct=319.46, ppl=16.79, accuracy=7.895, wps=18660.4, ups=1.54, wpb=12081.6, bsz=439.3, num_updates=1300, lr=5.2074e-05, gnorm=1.735, clip=0, loss_scale=64, train_wall=64, gb_free=19.7, wall=931
2023-07-27 14:51:17 | INFO | train_inner | epoch 001:   1401 / 1474 loss=6.295, trans_loss=5.356, nll_loss=4.059, w2v_ctc_loss=4.31, task_loss=1.308, contrastive_loss=2.009, total=4133.18, n_correct=331.81, ppl=16.67, accuracy=8.028, wps=19243.4, ups=1.56, wpb=12350, bsz=455, num_updates=1400, lr=5.6072e-05, gnorm=1.628, clip=0, loss_scale=64, train_wall=64, gb_free=19.9, wall=995
2023-07-27 14:52:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 14:52:44 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 9.567 | trans_loss 10.917 | nll_loss 9.9 | w2v_ctc_loss 5.605 | task_loss 7.547 | contrastive_loss 2.366 | total 4003.4 | n_correct 385.3 | ppl 955.56 | accuracy 9.624 | uer 71.919 | wer 69.848 | raw_wer 69.848 | bleu 0.03 | wps 1156.2 | wpb 4003.4 | bsz 141.8 | num_updates 1473
2023-07-27 14:52:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1473 updates
2023-07-27 14:52:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 14:52:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 14:52:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 1 @ 1473 updates, score 0.03) (writing took 5.425720511004329 seconds)
2023-07-27 14:52:49 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-07-27 14:52:49 | INFO | train | epoch 001 | loss 9.042 | trans_loss 5.452 | nll_loss 4.127 | w2v_ctc_loss 7.644 | task_loss 1.41 | contrastive_loss 2.762 | total 4138.55 | n_correct 251.685 | ppl 17.47 | accuracy 6.081 | wps 18293.1 | ups 1.48 | wpb 12355.5 | bsz 458.4 | num_updates 1473 | lr 5.89905e-05 | gnorm 1.788 | clip 0 | loss_scale 64 | train_wall 943 | gb_free 19.2 | wall 1087
2023-07-27 14:52:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 14:52:49 | INFO | fairseq.trainer | begin training epoch 2
2023-07-27 14:52:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 14:53:16 | INFO | train_inner | epoch 002:     27 / 1474 loss=6.106, trans_loss=5.349, nll_loss=4.047, w2v_ctc_loss=4.117, task_loss=1.246, contrastive_loss=1.855, total=4162.95, n_correct=338.83, ppl=16.53, accuracy=8.139, wps=10417.1, ups=0.84, wpb=12416.8, bsz=470.8, num_updates=1500, lr=6.007e-05, gnorm=1.654, clip=0, loss_scale=64, train_wall=64, gb_free=19.6, wall=1114
2023-07-27 14:54:20 | INFO | train_inner | epoch 002:    127 / 1474 loss=5.946, trans_loss=5.345, nll_loss=4.041, w2v_ctc_loss=4.001, task_loss=1.33, contrastive_loss=1.651, total=4155.98, n_correct=339.37, ppl=16.46, accuracy=8.166, wps=19443, ups=1.57, wpb=12394.8, bsz=451.6, num_updates=1600, lr=6.4068e-05, gnorm=1.717, clip=0, loss_scale=64, train_wall=63, gb_free=18.9, wall=1178
2023-07-27 14:55:26 | INFO | train_inner | epoch 002:    227 / 1474 loss=5.782, trans_loss=5.325, nll_loss=4.021, w2v_ctc_loss=3.805, task_loss=1.153, contrastive_loss=1.682, total=4179.21, n_correct=348.6, ppl=16.23, accuracy=8.341, wps=18892.8, ups=1.51, wpb=12484.6, bsz=488.9, num_updates=1700, lr=6.8066e-05, gnorm=1.509, clip=0, loss_scale=64, train_wall=65, gb_free=19, wall=1244
2023-07-27 14:56:30 | INFO | train_inner | epoch 002:    327 / 1474 loss=5.621, trans_loss=5.323, nll_loss=4.015, w2v_ctc_loss=3.715, task_loss=1.325, contrastive_loss=1.391, total=4146.1, n_correct=353.18, ppl=16.17, accuracy=8.518, wps=19304.4, ups=1.56, wpb=12374.1, bsz=447.8, num_updates=1800, lr=7.2064e-05, gnorm=1.394, clip=0, loss_scale=64, train_wall=63, gb_free=18.8, wall=1308
2023-07-27 14:57:33 | INFO | train_inner | epoch 002:    427 / 1474 loss=5.481, trans_loss=5.315, nll_loss=4.009, w2v_ctc_loss=3.618, task_loss=1.456, contrastive_loss=1.214, total=4037.99, n_correct=343.36, ppl=16.1, accuracy=8.503, wps=19026, ups=1.58, wpb=12069.2, bsz=415.4, num_updates=1900, lr=7.6062e-05, gnorm=1.442, clip=0, loss_scale=64, train_wall=63, gb_free=19, wall=1371
2023-07-27 14:58:37 | INFO | train_inner | epoch 002:    527 / 1474 loss=5.371, trans_loss=5.303, nll_loss=3.991, w2v_ctc_loss=3.457, task_loss=1.266, contrastive_loss=1.31, total=4176.97, n_correct=360.76, ppl=15.9, accuracy=8.637, wps=19550.4, ups=1.57, wpb=12463.5, bsz=468.6, num_updates=2000, lr=8.006e-05, gnorm=1.283, clip=0, loss_scale=64, train_wall=63, gb_free=19.6, wall=1435
2023-07-27 14:58:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 14:59:18 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.955 | trans_loss 10.759 | nll_loss 9.685 | w2v_ctc_loss 4.515 | task_loss 7.546 | contrastive_loss 1.644 | total 4003.4 | n_correct 413.9 | ppl 822.89 | accuracy 10.339 | uer 61.413 | wer 59.282 | raw_wer 59.282 | bleu 0.05 | wps 1141.4 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0.05
2023-07-27 14:59:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-07-27 14:59:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_2_2000.pt
2023-07-27 14:59:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_2_2000.pt
2023-07-27 14:59:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.05) (writing took 26.49063969962299 seconds)
2023-07-27 15:00:49 | INFO | train_inner | epoch 002:    627 / 1474 loss=5.239, trans_loss=5.296, nll_loss=3.981, w2v_ctc_loss=3.352, task_loss=1.309, contrastive_loss=1.111, total=4126.49, n_correct=367.21, ppl=15.79, accuracy=8.899, wps=9316.2, ups=0.76, wpb=12314.9, bsz=445.5, num_updates=2100, lr=8.4058e-05, gnorm=1.173, clip=0, loss_scale=64, train_wall=64, gb_free=19.2, wall=1567
2023-07-27 15:01:53 | INFO | train_inner | epoch 002:    727 / 1474 loss=5.166, trans_loss=5.28, nll_loss=3.965, w2v_ctc_loss=3.267, task_loss=1.283, contrastive_loss=1.211, total=4149.06, n_correct=375.53, ppl=15.62, accuracy=9.051, wps=19492.8, ups=1.57, wpb=12386.4, bsz=465.4, num_updates=2200, lr=8.8056e-05, gnorm=1.138, clip=0, loss_scale=64, train_wall=63, gb_free=19.2, wall=1631
2023-07-27 15:02:56 | INFO | train_inner | epoch 002:    827 / 1474 loss=5.077, trans_loss=5.264, nll_loss=3.947, w2v_ctc_loss=3.197, task_loss=1.317, contrastive_loss=1.161, total=4175.4, n_correct=384.89, ppl=15.42, accuracy=9.218, wps=19616.8, ups=1.57, wpb=12471.9, bsz=460.9, num_updates=2300, lr=9.2054e-05, gnorm=1.033, clip=0, loss_scale=128, train_wall=63, gb_free=19.8, wall=1694
2023-07-27 15:04:01 | INFO | train_inner | epoch 002:    927 / 1474 loss=4.985, trans_loss=5.253, nll_loss=3.931, w2v_ctc_loss=3.104, task_loss=1.344, contrastive_loss=1.143, total=4104.2, n_correct=381.13, ppl=15.25, accuracy=9.286, wps=19043.2, ups=1.55, wpb=12253.1, bsz=445.9, num_updates=2400, lr=9.6052e-05, gnorm=1.027, clip=0, loss_scale=128, train_wall=64, gb_free=19, wall=1759
2023-07-27 15:05:06 | INFO | train_inner | epoch 002:   1027 / 1474 loss=4.9, trans_loss=5.246, nll_loss=3.925, w2v_ctc_loss=3.034, task_loss=1.305, contrastive_loss=0.995, total=4102.5, n_correct=387.12, ppl=15.19, accuracy=9.436, wps=18874.1, ups=1.54, wpb=12251.2, bsz=456.3, num_updates=2500, lr=0.00010005, gnorm=0.894, clip=0, loss_scale=128, train_wall=64, gb_free=19.2, wall=1824
2023-07-27 15:06:10 | INFO | train_inner | epoch 002:   1127 / 1474 loss=4.857, trans_loss=5.24, nll_loss=3.916, w2v_ctc_loss=2.943, task_loss=1.187, contrastive_loss=1.207, total=4187.61, n_correct=399.96, ppl=15.09, accuracy=9.551, wps=19501.8, ups=1.56, wpb=12495.7, bsz=487.1, num_updates=2600, lr=0.000104048, gnorm=0.904, clip=0, loss_scale=128, train_wall=63, gb_free=19.5, wall=1888
2023-07-27 15:07:14 | INFO | train_inner | epoch 002:   1227 / 1474 loss=4.799, trans_loss=5.227, nll_loss=3.901, w2v_ctc_loss=2.899, task_loss=1.194, contrastive_loss=1.13, total=4221.06, n_correct=417.84, ppl=14.94, accuracy=9.899, wps=19652.5, ups=1.56, wpb=12596.1, bsz=492.8, num_updates=2700, lr=0.000108046, gnorm=0.817, clip=0, loss_scale=128, train_wall=63, gb_free=19.5, wall=1952
2023-07-27 15:08:18 | INFO | train_inner | epoch 002:   1327 / 1474 loss=4.707, trans_loss=5.219, nll_loss=3.895, w2v_ctc_loss=2.864, task_loss=1.259, contrastive_loss=0.838, total=4157.86, n_correct=414.62, ppl=14.88, accuracy=9.972, wps=19395.3, ups=1.56, wpb=12425.5, bsz=460.7, num_updates=2800, lr=0.000112044, gnorm=0.779, clip=0, loss_scale=128, train_wall=63, gb_free=19.5, wall=2016
2023-07-27 15:09:23 | INFO | train_inner | epoch 002:   1427 / 1474 loss=4.67, trans_loss=5.225, nll_loss=3.903, w2v_ctc_loss=2.823, task_loss=1.414, contrastive_loss=0.926, total=4054.34, n_correct=400.8, ppl=14.96, accuracy=9.886, wps=18560.8, ups=1.53, wpb=12107, bsz=438.8, num_updates=2900, lr=0.000116042, gnorm=0.73, clip=0, loss_scale=128, train_wall=65, gb_free=19.4, wall=2081
2023-07-27 15:09:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 15:10:33 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 8.185 | trans_loss 10.242 | nll_loss 9.057 | w2v_ctc_loss 3.615 | task_loss 7.547 | contrastive_loss 0.989 | total 4003.4 | n_correct 500.9 | ppl 532.76 | accuracy 12.512 | uer 51.923 | wer 50.695 | raw_wer 50.695 | bleu 0.12 | wps 1127.2 | wpb 4003.4 | bsz 141.8 | num_updates 2947 | best_bleu 0.12
2023-07-27 15:10:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2947 updates
2023-07-27 15:10:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 15:10:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 15:10:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 2 @ 2947 updates, score 0.12) (writing took 19.063901744782925 seconds)
2023-07-27 15:10:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-07-27 15:10:53 | INFO | train | epoch 002 | loss 5.184 | trans_loss 5.275 | nll_loss 3.959 | w2v_ctc_loss 3.29 | task_loss 1.292 | contrastive_loss 1.214 | total 4138.65 | n_correct 377.022 | ppl 15.55 | accuracy 9.11 | wps 16812.4 | ups 1.36 | wpb 12355.8 | bsz 458.5 | num_updates 2947 | lr 0.000117921 | gnorm 1.126 | clip 0 | loss_scale 128 | train_wall 937 | gb_free 19.3 | wall 2171
2023-07-27 15:10:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 15:10:53 | INFO | fairseq.trainer | begin training epoch 3
2023-07-27 15:10:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 15:11:37 | INFO | train_inner | epoch 003:     53 / 1474 loss=4.595, trans_loss=5.197, nll_loss=3.867, w2v_ctc_loss=2.762, task_loss=1.324, contrastive_loss=0.827, total=4071.2, n_correct=416.3, ppl=14.59, accuracy=10.225, wps=9108.5, ups=0.75, wpb=12154.1, bsz=442.6, num_updates=3000, lr=0.00012004, gnorm=0.719, clip=0, loss_scale=128, train_wall=63, gb_free=19.1, wall=2215
2023-07-27 15:11:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 15:11:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-27 15:11:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-07-27 15:11:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-27 15:11:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-07-27 15:13:13 | INFO | train_inner | epoch 003:    158 / 1474 loss=3.845, trans_loss=4.481, nll_loss=2.933, w2v_ctc_loss=2.411, task_loss=0.903, contrastive_loss=0.767, total=4144.18, n_correct=1059.49, ppl=7.64, accuracy=25.566, wps=12868.3, ups=1.04, wpb=12374.6, bsz=458.6, num_updates=3100, lr=0.000124038, gnorm=1.671, clip=1, loss_scale=4, train_wall=95, gb_free=16.5, wall=2311
2023-07-27 15:14:47 | INFO | train_inner | epoch 003:    258 / 1474 loss=3.396, trans_loss=4.2, nll_loss=2.563, w2v_ctc_loss=2.138, task_loss=0.915, contrastive_loss=0.648, total=4161.13, n_correct=1368.05, ppl=5.91, accuracy=32.877, wps=13144.1, ups=1.06, wpb=12431.5, bsz=467, num_updates=3200, lr=0.000128036, gnorm=1.142, clip=0, loss_scale=4, train_wall=94, gb_free=17.1, wall=2405
2023-07-27 15:16:20 | INFO | train_inner | epoch 003:    358 / 1474 loss=3.247, trans_loss=4.112, nll_loss=2.444, w2v_ctc_loss=2.031, task_loss=0.919, contrastive_loss=0.672, total=4150.02, n_correct=1482.84, ppl=5.44, accuracy=35.731, wps=13365.8, ups=1.08, wpb=12384.9, bsz=461.6, num_updates=3300, lr=0.000132034, gnorm=1.129, clip=0, loss_scale=4, train_wall=92, gb_free=17.1, wall=2498
2023-07-27 15:17:53 | INFO | train_inner | epoch 003:    458 / 1474 loss=3.108, trans_loss=4.04, nll_loss=2.349, w2v_ctc_loss=1.943, task_loss=0.891, contrastive_loss=0.526, total=4209.57, n_correct=1613.08, ppl=5.1, accuracy=38.319, wps=13467.2, ups=1.07, wpb=12566, bsz=476.8, num_updates=3400, lr=0.000136032, gnorm=1.045, clip=0, loss_scale=4, train_wall=93, gb_free=16.1, wall=2591
2023-07-27 15:19:26 | INFO | train_inner | epoch 003:    558 / 1474 loss=3.002, trans_loss=4, nll_loss=2.298, w2v_ctc_loss=1.868, task_loss=0.98, contrastive_loss=0.487, total=4088.48, n_correct=1626.44, ppl=4.92, accuracy=39.781, wps=13173.1, ups=1.08, wpb=12212.5, bsz=439.7, num_updates=3500, lr=0.00014003, gnorm=0.992, clip=0, loss_scale=4, train_wall=92, gb_free=17.7, wall=2684
2023-07-27 15:21:01 | INFO | train_inner | epoch 003:    658 / 1474 loss=2.936, trans_loss=3.957, nll_loss=2.237, w2v_ctc_loss=1.793, task_loss=0.879, contrastive_loss=0.595, total=4221.58, n_correct=1752.87, ppl=4.71, accuracy=41.522, wps=13238.2, ups=1.05, wpb=12587.8, bsz=481.9, num_updates=3600, lr=0.000144028, gnorm=0.951, clip=0, loss_scale=4, train_wall=94, gb_free=16.4, wall=2779
2023-07-27 15:22:34 | INFO | train_inner | epoch 003:    758 / 1474 loss=2.857, trans_loss=3.921, nll_loss=2.195, w2v_ctc_loss=1.765, task_loss=0.878, contrastive_loss=0.362, total=4167.41, n_correct=1778.17, ppl=4.58, accuracy=42.668, wps=13448.7, ups=1.08, wpb=12447.6, bsz=472.6, num_updates=3700, lr=0.000148026, gnorm=0.944, clip=0, loss_scale=4, train_wall=92, gb_free=16.3, wall=2872
2023-07-27 15:24:07 | INFO | train_inner | epoch 003:    858 / 1474 loss=2.799, trans_loss=3.905, nll_loss=2.172, w2v_ctc_loss=1.721, task_loss=0.929, contrastive_loss=0.318, total=4165.53, n_correct=1812.86, ppl=4.51, accuracy=43.521, wps=13408, ups=1.08, wpb=12437.8, bsz=456.1, num_updates=3800, lr=0.000152024, gnorm=0.921, clip=0, loss_scale=4, train_wall=92, gb_free=17.1, wall=2964
2023-07-27 15:25:41 | INFO | train_inner | epoch 003:    958 / 1474 loss=2.768, trans_loss=3.882, nll_loss=2.142, w2v_ctc_loss=1.694, task_loss=0.894, contrastive_loss=0.347, total=4162.3, n_correct=1859.61, ppl=4.41, accuracy=44.677, wps=13190, ups=1.06, wpb=12417, bsz=469.2, num_updates=3900, lr=0.000156022, gnorm=0.905, clip=0, loss_scale=4, train_wall=93, gb_free=16.8, wall=3059
2023-07-27 15:27:13 | INFO | train_inner | epoch 003:   1058 / 1474 loss=2.743, trans_loss=3.865, nll_loss=2.121, w2v_ctc_loss=1.688, task_loss=0.98, contrastive_loss=0.306, total=4069.95, n_correct=1831.95, ppl=4.35, accuracy=45.012, wps=13195.3, ups=1.09, wpb=12153.7, bsz=443.6, num_updates=4000, lr=0.00016002, gnorm=0.916, clip=0, loss_scale=4, train_wall=91, gb_free=16.3, wall=3151
2023-07-27 15:27:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 15:27:44 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 4.943 | trans_loss 6.337 | nll_loss 3.858 | w2v_ctc_loss 1.984 | task_loss 4.362 | contrastive_loss 0.411 | total 4003.4 | n_correct 2007.4 | ppl 14.5 | accuracy 50.142 | uer 28.936 | wer 30.047 | raw_wer 30.047 | bleu 12.17 | wps 1441.9 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 12.17
2023-07-27 15:27:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-07-27 15:27:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_3_4000.pt
2023-07-27 15:27:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_3_4000.pt
2023-07-27 15:28:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 12.17) (writing took 20.20249709300697 seconds)
2023-07-27 15:29:36 | INFO | train_inner | epoch 003:   1158 / 1474 loss=2.699, trans_loss=3.856, nll_loss=2.109, w2v_ctc_loss=1.645, task_loss=0.998, contrastive_loss=0.282, total=4038.49, n_correct=1839.45, ppl=4.31, accuracy=45.548, wps=8403.1, ups=0.7, wpb=12054.8, bsz=432.5, num_updates=4100, lr=0.000164018, gnorm=0.898, clip=0, loss_scale=4, train_wall=91, gb_free=16.4, wall=3294
2023-07-27 15:31:10 | INFO | train_inner | epoch 003:   1258 / 1474 loss=2.656, trans_loss=3.836, nll_loss=2.084, w2v_ctc_loss=1.613, task_loss=0.977, contrastive_loss=0.263, total=4064.31, n_correct=1881.77, ppl=4.24, accuracy=46.3, wps=12995.4, ups=1.07, wpb=12136.8, bsz=433.9, num_updates=4200, lr=0.000168016, gnorm=0.859, clip=0, loss_scale=4, train_wall=92, gb_free=17.3, wall=3388
2023-07-27 15:32:43 | INFO | train_inner | epoch 003:   1358 / 1474 loss=2.644, trans_loss=3.82, nll_loss=2.062, w2v_ctc_loss=1.58, task_loss=0.932, contrastive_loss=0.376, total=4134.58, n_correct=1942.52, ppl=4.18, accuracy=46.982, wps=13294.6, ups=1.08, wpb=12343.8, bsz=460.7, num_updates=4300, lr=0.000172014, gnorm=0.888, clip=0, loss_scale=4, train_wall=92, gb_free=17.8, wall=3480
2023-07-27 15:34:16 | INFO | train_inner | epoch 003:   1458 / 1474 loss=2.619, trans_loss=3.809, nll_loss=2.05, w2v_ctc_loss=1.563, task_loss=0.879, contrastive_loss=0.357, total=4209.94, n_correct=2000.49, ppl=4.14, accuracy=47.518, wps=13468.2, ups=1.07, wpb=12573.5, bsz=477.4, num_updates=4400, lr=0.000176012, gnorm=0.867, clip=0, loss_scale=4, train_wall=93, gb_free=17, wall=3574
2023-07-27 15:34:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 15:35:01 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 4.825 | trans_loss 6.244 | nll_loss 3.741 | w2v_ctc_loss 1.809 | task_loss 4.195 | contrastive_loss 0.402 | total 4003.4 | n_correct 2070.9 | ppl 13.37 | accuracy 51.729 | uer 28.493 | wer 29.119 | raw_wer 29.119 | bleu 13.42 | wps 1511.4 | wpb 4003.4 | bsz 141.8 | num_updates 4416 | best_bleu 13.42
2023-07-27 15:35:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4416 updates
2023-07-27 15:35:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 15:35:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 15:35:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 3 @ 4416 updates, score 13.42) (writing took 19.736654264852405 seconds)
2023-07-27 15:35:20 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-07-27 15:35:20 | INFO | train | epoch 003 | loss 3.007 | trans_loss 4.02 | nll_loss 2.324 | w2v_ctc_loss 1.849 | task_loss 0.938 | contrastive_loss 0.467 | total 4140.05 | n_correct 1659.41 | ppl 5.01 | accuracy 40.082 | wps 12369.4 | ups 1 | wpb 12360.1 | bsz 459 | num_updates 4416 | lr 0.000176652 | gnorm 0.997 | clip 0.1 | loss_scale 4 | train_wall 1344 | gb_free 16.4 | wall 3638
2023-07-27 15:35:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 15:35:21 | INFO | fairseq.trainer | begin training epoch 4
2023-07-27 15:35:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 15:36:48 | INFO | train_inner | epoch 004:     84 / 1474 loss=2.542, trans_loss=3.778, nll_loss=2.006, w2v_ctc_loss=1.513, task_loss=0.955, contrastive_loss=0.21, total=4099.41, n_correct=1978.44, ppl=4.02, accuracy=48.262, wps=8021.7, ups=0.66, wpb=12237, bsz=439.5, num_updates=4500, lr=0.00018001, gnorm=0.843, clip=0, loss_scale=4, train_wall=92, gb_free=16.3, wall=3726
2023-07-27 15:38:20 | INFO | train_inner | epoch 004:    184 / 1474 loss=2.527, trans_loss=3.76, nll_loss=1.983, w2v_ctc_loss=1.499, task_loss=0.882, contrastive_loss=0.239, total=4175.15, n_correct=2044.72, ppl=3.95, accuracy=48.974, wps=13568.5, ups=1.09, wpb=12464.9, bsz=468.3, num_updates=4600, lr=0.000184008, gnorm=0.852, clip=0, loss_scale=4, train_wall=91, gb_free=16.6, wall=3818
2023-07-27 15:39:53 | INFO | train_inner | epoch 004:    284 / 1474 loss=2.541, trans_loss=3.763, nll_loss=1.99, w2v_ctc_loss=1.496, task_loss=0.925, contrastive_loss=0.362, total=4145.23, n_correct=2028.28, ppl=3.97, accuracy=48.93, wps=13293, ups=1.07, wpb=12382.4, bsz=463, num_updates=4700, lr=0.000188006, gnorm=0.833, clip=0, loss_scale=4, train_wall=92, gb_free=15.9, wall=3911
2023-07-27 15:41:26 | INFO | train_inner | epoch 004:    384 / 1474 loss=2.504, trans_loss=3.763, nll_loss=1.987, w2v_ctc_loss=1.48, task_loss=0.965, contrastive_loss=0.206, total=4127.66, n_correct=2024.67, ppl=3.96, accuracy=49.051, wps=13291.2, ups=1.08, wpb=12314.6, bsz=443.5, num_updates=4800, lr=0.000192004, gnorm=0.827, clip=0, loss_scale=4, train_wall=92, gb_free=17.4, wall=4004
2023-07-27 15:43:00 | INFO | train_inner | epoch 004:    484 / 1474 loss=2.531, trans_loss=3.745, nll_loss=1.966, w2v_ctc_loss=1.444, task_loss=0.839, contrastive_loss=0.6, total=4218.78, n_correct=2096.81, ppl=3.91, accuracy=49.702, wps=13468.9, ups=1.07, wpb=12592.4, bsz=497.8, num_updates=4900, lr=0.000196002, gnorm=0.826, clip=0, loss_scale=4, train_wall=93, gb_free=16.5, wall=4098
2023-07-27 15:44:33 | INFO | train_inner | epoch 004:    584 / 1474 loss=2.496, trans_loss=3.741, nll_loss=1.961, w2v_ctc_loss=1.468, task_loss=0.873, contrastive_loss=0.283, total=4217.52, n_correct=2106.16, ppl=3.89, accuracy=49.938, wps=13420.1, ups=1.07, wpb=12591.1, bsz=485.9, num_updates=5000, lr=0.0002, gnorm=0.828, clip=0, loss_scale=4, train_wall=93, gb_free=16, wall=4191
Mixup rate:0.5, token after shrink shape:torch.Size([24, 56]), X shape:torch.Size([24, 56, 512])
CTC Tokens:tensor([  0,  25,   0, 150,   0], device='cuda:0'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:0'), New Tokens:tensor([  0,  25,   0, 150,   0], device='cuda:0')
Org X:tensor([[ 0.9941,  0.3176,  0.0688,  ...,  0.1757,  1.2793, -0.7656],
        [ 0.8423,  0.2211, -0.0565,  ...,  0.3455,  0.3655, -1.2168],
        [ 1.2461,  0.2932,  0.5220,  ...,  0.3611, -1.1729, -1.7021],
        [ 0.3511,  0.2339, -0.3359,  ..., -0.3462, -0.5635, -2.2773],
        [ 1.1172,  0.7949,  1.1533,  ..., -2.0762, -0.1271, -1.6514]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:0'), Mixup Sent Mask:tensor([False, False,  True, False, False, False,  True,  True, False,  True],
       device='cuda:0'), New X:tensor([[ 0.9941,  0.3176,  0.0688,  ...,  0.1757,  1.2793, -0.7656],
        [ 0.8423,  0.2211, -0.0565,  ...,  0.3455,  0.3655, -1.2168],
        [ 1.2461,  0.2932,  0.5220,  ...,  0.3611, -1.1729, -1.7021],
        [ 0.3511,  0.2339, -0.3359,  ..., -0.3462, -0.5635, -2.2773],
        [ 1.1172,  0.7949,  1.1533,  ..., -2.0762, -0.1271, -1.6514]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.4306, device='cuda:0')
2023-07-27 15:46:09 | INFO | train_inner | epoch 004:    684 / 1474 loss=2.464, trans_loss=3.742, nll_loss=1.958, w2v_ctc_loss=1.429, task_loss=0.952, contrastive_loss=0.326, total=4176.39, n_correct=2101.14, ppl=3.89, accuracy=50.31, wps=13032.7, ups=1.05, wpb=12448.9, bsz=455.8, num_updates=5100, lr=0.00019803, gnorm=0.505, clip=0, loss_scale=8, train_wall=95, gb_free=17.1, wall=4287
2023-07-27 15:47:42 | INFO | train_inner | epoch 004:    784 / 1474 loss=2.45, trans_loss=3.735, nll_loss=1.954, w2v_ctc_loss=1.443, task_loss=1.023, contrastive_loss=0.194, total=4026.63, n_correct=2027.23, ppl=3.88, accuracy=50.346, wps=12901.4, ups=1.07, wpb=12025, bsz=420.6, num_updates=5200, lr=0.000196116, gnorm=0.515, clip=0, loss_scale=8, train_wall=92, gb_free=13.2, wall=4380
2023-07-27 15:49:15 | INFO | train_inner | epoch 004:    884 / 1474 loss=2.471, trans_loss=3.723, nll_loss=1.94, w2v_ctc_loss=1.439, task_loss=0.927, contrastive_loss=0.375, total=4186.04, n_correct=2121.89, ppl=3.84, accuracy=50.69, wps=13530.6, ups=1.08, wpb=12501.4, bsz=466.3, num_updates=5300, lr=0.000194257, gnorm=0.515, clip=0, loss_scale=8, train_wall=92, gb_free=17.7, wall=4473
2023-07-27 15:50:48 | INFO | train_inner | epoch 004:    984 / 1474 loss=2.431, trans_loss=3.714, nll_loss=1.929, w2v_ctc_loss=1.419, task_loss=0.941, contrastive_loss=0.244, total=4125.02, n_correct=2111.76, ppl=3.81, accuracy=51.194, wps=13247.3, ups=1.08, wpb=12321, bsz=457.1, num_updates=5400, lr=0.00019245, gnorm=0.504, clip=0, loss_scale=8, train_wall=92, gb_free=12.7, wall=4566
2023-07-27 15:52:22 | INFO | train_inner | epoch 004:   1084 / 1474 loss=2.433, trans_loss=3.725, nll_loss=1.941, w2v_ctc_loss=1.424, task_loss=1.003, contrastive_loss=0.219, total=4075.6, n_correct=2080.29, ppl=3.84, accuracy=51.043, wps=12909, ups=1.06, wpb=12163.4, bsz=435.7, num_updates=5500, lr=0.000190693, gnorm=0.507, clip=0, loss_scale=8, train_wall=94, gb_free=16, wall=4660
2023-07-27 15:53:54 | INFO | train_inner | epoch 004:   1184 / 1474 loss=2.438, trans_loss=3.712, nll_loss=1.928, w2v_ctc_loss=1.414, task_loss=0.873, contrastive_loss=0.333, total=4161.18, n_correct=2140.43, ppl=3.81, accuracy=51.438, wps=13486.5, ups=1.08, wpb=12431.8, bsz=483.4, num_updates=5600, lr=0.000188982, gnorm=0.5, clip=0, loss_scale=8, train_wall=92, gb_free=16.8, wall=4752
2023-07-27 15:55:26 | INFO | train_inner | epoch 004:   1284 / 1474 loss=2.411, trans_loss=3.701, nll_loss=1.913, w2v_ctc_loss=1.395, task_loss=0.886, contrastive_loss=0.296, total=4156.53, n_correct=2155.33, ppl=3.77, accuracy=51.854, wps=13495.9, ups=1.09, wpb=12411.4, bsz=472.7, num_updates=5700, lr=0.000187317, gnorm=0.499, clip=0, loss_scale=8, train_wall=91, gb_free=15.8, wall=4844
2023-07-27 15:56:58 | INFO | train_inner | epoch 004:   1384 / 1474 loss=2.389, trans_loss=3.702, nll_loss=1.914, w2v_ctc_loss=1.397, task_loss=0.954, contrastive_loss=0.173, total=4101.23, n_correct=2127.55, ppl=3.77, accuracy=51.876, wps=13367.1, ups=1.09, wpb=12249, bsz=437.6, num_updates=5800, lr=0.000185695, gnorm=0.496, clip=0, loss_scale=8, train_wall=91, gb_free=15.6, wall=4936
2023-07-27 15:58:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 90]), X shape:torch.Size([8, 90, 512])
CTC Tokens:tensor([ 67, 103, 103,  25,  25], device='cuda:1'), Shrink Mask:tensor([ True,  True, False,  True, False], device='cuda:1'), New Tokens:tensor([ 67, 103,  25,   0, 305], device='cuda:1')
Org X:tensor([[-0.5034,  0.3101, -1.0547,  ...,  0.4739,  0.5000, -2.9453],
        [-0.7109, -0.1624, -1.9414,  ...,  0.4695,  2.1699, -2.5527],
        [ 0.3271, -0.0914, -2.8281,  ..., -0.1462,  2.3809, -1.1494],
        [ 0.7114,  0.0887,  0.2047,  ...,  0.3223,  0.9937, -0.7173],
        [-0.6636,  0.0079,  0.3555,  ...,  0.2158,  0.6592, -1.2188]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]]], device='cuda:1'), Mixup Sent Mask:tensor([False, False,  True, False, False, False,  True,  True],
       device='cuda:1'), New X:tensor([[-0.5034,  0.3101, -1.0547,  ...,  0.4739,  0.5000, -2.9453],
        [-0.7109, -0.1624, -1.9414,  ...,  0.4695,  2.1699, -2.5527],
        [ 0.3271, -0.0914, -2.8281,  ..., -0.1462,  2.3809, -1.1494],
        [ 0.7114,  0.0887,  0.2047,  ...,  0.3223,  0.9937, -0.7173],
        [-0.6636,  0.0079,  0.3555,  ...,  0.2158,  0.6592, -1.2188]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.4306, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 108]), X shape:torch.Size([8, 108, 512])
CTC Tokens:tensor([25, 25, 73, 73,  0], device='cuda:7'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:7'), New Tokens:tensor([ 25,  73,   0, 598,   0], device='cuda:7')
Org X:tensor([[ 0.5093,  0.3765, -1.4932,  ..., -0.4983,  1.7900, -0.3826],
        [ 0.4146, -0.2722, -0.0023,  ...,  0.4509,  0.9580,  0.4148],
        [ 0.8638,  1.4727,  0.1003,  ...,  0.5273,  1.2090,  0.8413],
        [ 1.5254,  1.5059,  0.1094,  ...,  0.2421,  0.7070,  0.2896],
        [ 1.7754,  0.8711,  0.7759,  ..., -0.4790, -0.3250,  0.4551]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:7'), Mixup Sent Mask:tensor([False, False,  True, False, False, False,  True,  True],
       device='cuda:7'), New X:tensor([[ 0.5093,  0.3765, -1.4932,  ..., -0.4983,  1.7900, -0.3826],
        [ 0.4146, -0.2722, -0.0023,  ...,  0.4509,  0.9580,  0.4148],
        [ 0.8638,  1.4727,  0.1003,  ...,  0.5273,  1.2090,  0.8413],
        [ 1.5254,  1.5059,  0.1094,  ...,  0.2421,  0.7070,  0.2896],
        [ 1.7754,  0.8711,  0.7759,  ..., -0.4790, -0.3250,  0.4551]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.4306, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([64, 27]), X shape:torch.Size([64, 27, 512])
CTC Tokens:tensor([ 8,  8,  0,  0, 19], device='cuda:5'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:5'), New Tokens:tensor([  8,   0,  19,   0, 192], device='cuda:5')
Org X:tensor([[ 0.2083, -0.0213, -0.0716,  ...,  0.4402,  0.3726, -0.9795],
        [ 0.4099,  0.7642,  1.0449,  ...,  0.0474,  1.2939, -0.9175],
        [ 0.2554,  1.2861, -0.2764,  ..., -0.0544,  0.5024, -1.1426],
        [ 1.0684,  1.3457,  1.0381,  ...,  0.2095,  0.7402, -1.6484],
        [ 0.7432, -0.0664, -0.0529,  ...,  0.4111, -1.3105, -0.6855]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([False, False,  True, False, False, False,  True,  True, False,  True],
       device='cuda:5'), New X:tensor([[ 0.2083, -0.0213, -0.0716,  ...,  0.4402,  0.3726, -0.9795],
        [ 0.4099,  0.7642,  1.0449,  ...,  0.0474,  1.2939, -0.9175],
        [ 0.2554,  1.2861, -0.2764,  ..., -0.0544,  0.5024, -1.1426],
        [ 1.0684,  1.3457,  1.0381,  ...,  0.2095,  0.7402, -1.6484],
        [ 0.7432, -0.0664, -0.0529,  ...,  0.4111, -1.3105, -0.6855]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.4306, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 48]), X shape:torch.Size([32, 48, 512])
CTC Tokens:tensor([  29,   29,    0,    0, 3296], device='cuda:4'), Shrink Mask:tensor([ True, False,  True, False,  True], device='cuda:4'), New Tokens:tensor([  29,    0, 3296,    0,    4], device='cuda:4')
Org X:tensor([[ 0.0593,  0.3079, -0.1750,  ...,  0.6562, -1.2969, -0.2137],
        [ 0.0778,  0.1775,  0.2074,  ..., -0.1512,  0.0404, -0.9634],
        [-0.0474,  0.3667, -0.0536,  ...,  0.1378,  1.1338,  1.0068],
        [ 0.3384,  0.4458,  0.8188,  ..., -0.3496,  2.0117,  0.5483],
        [ 0.4287,  0.7915,  0.4077,  ...,  0.2357,  1.3535, -0.6426]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:4'), Mixup Sent Mask:tensor([False, False,  True, False, False, False,  True,  True, False,  True],
       device='cuda:4'), New X:tensor([[ 0.0593,  0.3079, -0.1750,  ...,  0.6562, -1.2969, -0.2137],
        [ 0.0778,  0.1775,  0.2074,  ..., -0.1512,  0.0404, -0.9634],
        [-0.0474,  0.3667, -0.0536,  ...,  0.1378,  1.1338,  1.0068],
        [ 0.3384,  0.4458,  0.8188,  ..., -0.3496,  2.0117,  0.5483],
        [ 0.4287,  0.7915,  0.4077,  ...,  0.2357,  1.3535, -0.6426]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.4306, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 61]), X shape:torch.Size([24, 61, 512])
CTC Tokens:tensor([   0,  120,   21,    6, 4736], device='cuda:6'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:6'), New Tokens:tensor([   0,  120,   21,    6, 4736], device='cuda:6')
Org X:tensor([[-0.5391,  0.1929,  0.2095,  ...,  0.2539, -0.1022, -0.3362],
        [ 0.1313, -0.3872, -1.7754,  ...,  0.3137, -0.6665, -1.9648],
        [ 0.0186, -0.4739, -2.3516,  ..., -0.5688,  0.8169, -0.8545],
        [-0.5332, -0.0753, -0.5630,  ..., -1.8867,  1.3652, -1.0869],
        [-1.2246,  0.2194,  1.5059,  ...,  0.0659, -0.4468, -0.3750]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([False, False,  True, False, False, False,  True,  True, False,  True],
       device='cuda:6'), New X:tensor([[-0.5391,  0.1929,  0.2095,  ...,  0.2539, -0.1022, -0.3362],
        [ 0.1313, -0.3872, -1.7754,  ...,  0.3137, -0.6665, -1.9648],
        [ 0.0186, -0.4739, -2.3516,  ..., -0.5688,  0.8169, -0.8545],
        [-0.5332, -0.0753, -0.5630,  ..., -1.8867,  1.3652, -1.0869],
        [-1.2246,  0.2194,  1.5059,  ...,  0.0659, -0.4468, -0.3750]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.4306, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([40, 33]), X shape:torch.Size([40, 33, 512])
CTC Tokens:tensor([ 24,  24,   0, 135,   0], device='cuda:3'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:3'), New Tokens:tensor([ 24,   0, 135,   0, 117], device='cuda:3')
Org X:tensor([[-0.7656,  0.6050,  0.3516,  ..., -0.1936,  1.3652, -0.7173],
        [ 0.7451,  0.8955,  0.8340,  ...,  0.4114,  0.5742, -0.1520],
        [ 0.5376,  1.0176,  0.5229,  ...,  0.1759, -0.8296, -0.4023],
        [ 0.6382,  1.7246,  0.1150,  ..., -1.3193, -1.3311, -0.7974],
        [-1.0645,  1.4336, -1.6855,  ..., -0.7573, -0.0485, -0.4946]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:3'), Mixup Sent Mask:tensor([False, False,  True, False, False, False,  True,  True, False,  True],
       device='cuda:3'), New X:tensor([[-0.7656,  0.6050,  0.3516,  ..., -0.1936,  1.3652, -0.7173],
        [ 0.7451,  0.8955,  0.8340,  ...,  0.4114,  0.5742, -0.1520],
        [ 0.5376,  1.0176,  0.5229,  ...,  0.1759, -0.8296, -0.4023],
        [ 0.6382,  1.7246,  0.1150,  ..., -1.3193, -1.3311, -0.7974],
        [-1.0645,  1.4336, -1.6855,  ..., -0.7573, -0.0485, -0.4946]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.4306, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 64]), X shape:torch.Size([16, 64, 512])
CTC Tokens:tensor([   0,    0, 6101,    0,    0], device='cuda:2'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:2'), New Tokens:tensor([   0, 6101,    0, 1313,    0], device='cuda:2')
Org X:tensor([[ 0.5503,  0.0735,  1.7617,  ..., -2.0586,  2.2871, -0.5166],
        [-1.0723, -0.1691,  1.6748,  ..., -1.2627,  0.5723, -0.7266],
        [-0.2505,  0.2571,  2.0312,  ..., -1.2939,  0.6558, -1.0186],
        [ 1.3535, -1.1338, -0.5752,  ...,  0.2421, -2.6172,  0.4395],
        [ 0.9229, -0.4790, -0.2832,  ..., -0.0578, -0.1503,  0.0044]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:2'), Mixup Sent Mask:tensor([False, False,  True, False, False, False,  True,  True, False,  True],
       device='cuda:2'), New X:tensor([[ 0.5503,  0.0735,  1.7617,  ..., -2.0586,  2.2871, -0.5166],
        [-1.0723, -0.1691,  1.6748,  ..., -1.2627,  0.5723, -0.7266],
        [-0.2505,  0.2571,  2.0312,  ..., -1.2939,  0.6558, -1.0186],
        [ 1.3535, -1.1338, -0.5752,  ...,  0.2421, -2.6172,  0.4395],
        [ 0.9229, -0.4790, -0.2832,  ..., -0.0578, -0.1503,  0.0044]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.4306, device='cuda:2')
2023-07-27 15:58:44 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.51 | trans_loss 5.926 | nll_loss 3.307 | w2v_ctc_loss 1.533 | task_loss 4.447 | contrastive_loss 0.314 | total 4003.4 | n_correct 2260.3 | ppl 9.9 | accuracy 56.46 | uer 22.876 | wer 24.384 | raw_wer 24.384 | bleu 16.37 | wps 2078.4 | wpb 4003.4 | bsz 141.8 | num_updates 5890 | best_bleu 16.37
2023-07-27 15:58:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5890 updates
2023-07-27 15:58:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 15:58:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 15:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 4 @ 5890 updates, score 16.37) (writing took 21.09058322943747 seconds)
2023-07-27 15:59:06 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-07-27 15:59:06 | INFO | train | epoch 004 | loss 2.467 | trans_loss 3.733 | nll_loss 1.951 | w2v_ctc_loss 1.442 | task_loss 0.928 | contrastive_loss 0.289 | total 4138.65 | n_correct 2085.71 | ppl 3.87 | accuracy 50.396 | wps 12777.9 | ups 1.03 | wpb 12355.8 | bsz 458.5 | num_updates 5890 | lr 0.000184271 | gnorm 0.635 | clip 0 | loss_scale 8 | train_wall 1359 | gb_free 14.8 | wall 5064
2023-07-27 15:59:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 15:59:06 | INFO | fairseq.trainer | begin training epoch 5
2023-07-27 15:59:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 15:59:22 | INFO | train_inner | epoch 005:     10 / 1474 loss=2.371, trans_loss=3.693, nll_loss=1.902, w2v_ctc_loss=1.371, task_loss=0.965, contrastive_loss=0.193, total=4037.7, n_correct=2109.46, ppl=3.74, accuracy=52.244, wps=8324.6, ups=0.69, wpb=12055.9, bsz=439.3, num_updates=5900, lr=0.000184115, gnorm=0.498, clip=0, loss_scale=8, train_wall=91, gb_free=16.9, wall=5080
2023-07-27 16:00:55 | INFO | train_inner | epoch 005:    110 / 1474 loss=2.296, trans_loss=3.634, nll_loss=1.826, w2v_ctc_loss=1.293, task_loss=0.839, contrastive_loss=0.202, total=4247.37, n_correct=2290.43, ppl=3.54, accuracy=53.926, wps=13719.6, ups=1.08, wpb=12683.4, bsz=495.1, num_updates=6000, lr=0.000182574, gnorm=0.477, clip=0, loss_scale=8, train_wall=92, gb_free=16.7, wall=5173
2023-07-27 16:00:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 16:01:20 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.509 | trans_loss 5.92 | nll_loss 3.296 | w2v_ctc_loss 1.539 | task_loss 4.465 | contrastive_loss 0.319 | total 4003.4 | n_correct 2256.4 | ppl 9.82 | accuracy 56.362 | uer 22.924 | wer 24.712 | raw_wer 24.712 | bleu 16.14 | wps 1983 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 16.37
2023-07-27 16:01:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-07-27 16:01:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_5_6000.pt
2023-07-27 16:01:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_5_6000.pt
2023-07-27 16:01:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 16.14) (writing took 25.61953780055046 seconds)
2023-07-27 16:03:18 | INFO | train_inner | epoch 005:    210 / 1474 loss=2.336, trans_loss=3.648, nll_loss=1.841, w2v_ctc_loss=1.309, task_loss=0.86, contrastive_loss=0.418, total=4189.85, n_correct=2242.34, ppl=3.58, accuracy=53.518, wps=8730.8, ups=0.7, wpb=12500.5, bsz=488.2, num_updates=6100, lr=0.000181071, gnorm=0.478, clip=0, loss_scale=8, train_wall=92, gb_free=17.8, wall=5316
2023-07-27 16:04:50 | INFO | train_inner | epoch 005:    310 / 1474 loss=2.321, trans_loss=3.642, nll_loss=1.837, w2v_ctc_loss=1.324, task_loss=0.955, contrastive_loss=0.264, total=4090.1, n_correct=2186.34, ppl=3.57, accuracy=53.454, wps=13343.9, ups=1.09, wpb=12228.1, bsz=443.9, num_updates=6200, lr=0.000179605, gnorm=0.489, clip=0, loss_scale=8, train_wall=91, gb_free=16.2, wall=5408
2023-07-27 16:06:22 | INFO | train_inner | epoch 005:    410 / 1474 loss=2.316, trans_loss=3.635, nll_loss=1.831, w2v_ctc_loss=1.293, task_loss=0.898, contrastive_loss=0.359, total=4147.17, n_correct=2233.52, ppl=3.56, accuracy=53.856, wps=13366.4, ups=1.08, wpb=12395.1, bsz=472.5, num_updates=6300, lr=0.000178174, gnorm=0.496, clip=0, loss_scale=8, train_wall=92, gb_free=14.8, wall=5500
2023-07-27 16:07:54 | INFO | train_inner | epoch 005:    510 / 1474 loss=2.289, trans_loss=3.646, nll_loss=1.841, w2v_ctc_loss=1.306, task_loss=1.046, contrastive_loss=0.143, total=4026.81, n_correct=2159.63, ppl=3.58, accuracy=53.631, wps=13101.5, ups=1.09, wpb=12029.7, bsz=416.6, num_updates=6400, lr=0.000176777, gnorm=0.479, clip=0, loss_scale=8, train_wall=91, gb_free=17.4, wall=5592
2023-07-27 16:09:27 | INFO | train_inner | epoch 005:    610 / 1474 loss=2.307, trans_loss=3.651, nll_loss=1.845, w2v_ctc_loss=1.292, task_loss=0.956, contrastive_loss=0.316, total=4107.75, n_correct=2205.58, ppl=3.59, accuracy=53.693, wps=13186.4, ups=1.08, wpb=12253.8, bsz=451.2, num_updates=6500, lr=0.000175412, gnorm=0.487, clip=0, loss_scale=8, train_wall=92, gb_free=16.1, wall=5685
2023-07-27 16:11:00 | INFO | train_inner | epoch 005:    710 / 1474 loss=2.307, trans_loss=3.645, nll_loss=1.84, w2v_ctc_loss=1.291, task_loss=0.88, contrastive_loss=0.298, total=4178.85, n_correct=2255.31, ppl=3.58, accuracy=53.97, wps=13487, ups=1.08, wpb=12473.1, bsz=480.9, num_updates=6600, lr=0.000174078, gnorm=0.479, clip=0, loss_scale=8, train_wall=92, gb_free=17.7, wall=5778
2023-07-27 16:12:32 | INFO | train_inner | epoch 005:    810 / 1474 loss=2.289, trans_loss=3.644, nll_loss=1.838, w2v_ctc_loss=1.287, task_loss=0.956, contrastive_loss=0.221, total=4127.73, n_correct=2228.11, ppl=3.58, accuracy=53.979, wps=13304, ups=1.08, wpb=12320.4, bsz=449.2, num_updates=6700, lr=0.000172774, gnorm=0.481, clip=0, loss_scale=8, train_wall=92, gb_free=15.1, wall=5870
2023-07-27 16:14:05 | INFO | train_inner | epoch 005:    910 / 1474 loss=2.268, trans_loss=3.636, nll_loss=1.829, w2v_ctc_loss=1.277, task_loss=0.963, contrastive_loss=0.183, total=4095.48, n_correct=2223.26, ppl=3.55, accuracy=54.286, wps=13210.2, ups=1.08, wpb=12229.5, bsz=445.3, num_updates=6800, lr=0.000171499, gnorm=0.483, clip=0, loss_scale=8, train_wall=92, gb_free=15.6, wall=5963
2023-07-27 16:15:37 | INFO | train_inner | epoch 005:   1010 / 1474 loss=2.282, trans_loss=3.641, nll_loss=1.836, w2v_ctc_loss=1.28, task_loss=0.919, contrastive_loss=0.262, total=4165.12, n_correct=2256.44, ppl=3.57, accuracy=54.175, wps=13477.7, ups=1.08, wpb=12433.6, bsz=463.5, num_updates=6900, lr=0.000170251, gnorm=0.471, clip=0, loss_scale=8, train_wall=91, gb_free=15.6, wall=6055
2023-07-27 16:17:11 | INFO | train_inner | epoch 005:   1110 / 1474 loss=2.294, trans_loss=3.639, nll_loss=1.832, w2v_ctc_loss=1.289, task_loss=0.92, contrastive_loss=0.27, total=4176.72, n_correct=2273.46, ppl=3.56, accuracy=54.432, wps=13248.5, ups=1.06, wpb=12459.2, bsz=466.1, num_updates=7000, lr=0.000169031, gnorm=0.48, clip=0, loss_scale=8, train_wall=93, gb_free=16.7, wall=6149
2023-07-27 16:18:45 | INFO | train_inner | epoch 005:   1210 / 1474 loss=2.254, trans_loss=3.636, nll_loss=1.828, w2v_ctc_loss=1.262, task_loss=0.947, contrastive_loss=0.17, total=4164.13, n_correct=2269.88, ppl=3.55, accuracy=54.51, wps=13307.6, ups=1.07, wpb=12420.9, bsz=453.8, num_updates=7100, lr=0.000167836, gnorm=0.477, clip=0, loss_scale=16, train_wall=93, gb_free=16.9, wall=6243
2023-07-27 16:20:17 | INFO | train_inner | epoch 005:   1310 / 1474 loss=2.242, trans_loss=3.635, nll_loss=1.828, w2v_ctc_loss=1.254, task_loss=0.945, contrastive_loss=0.137, total=4134.91, n_correct=2255.68, ppl=3.55, accuracy=54.552, wps=13375.5, ups=1.08, wpb=12341.4, bsz=445.6, num_updates=7200, lr=0.000166667, gnorm=0.466, clip=0, loss_scale=16, train_wall=92, gb_free=16.3, wall=6335
2023-07-27 16:21:49 | INFO | train_inner | epoch 005:   1410 / 1474 loss=2.251, trans_loss=3.634, nll_loss=1.829, w2v_ctc_loss=1.25, task_loss=0.939, contrastive_loss=0.205, total=4134.37, n_correct=2257.57, ppl=3.55, accuracy=54.605, wps=13370.1, ups=1.08, wpb=12347.5, bsz=458.5, num_updates=7300, lr=0.000165521, gnorm=0.476, clip=0, loss_scale=16, train_wall=92, gb_free=17.8, wall=6427
2023-07-27 16:22:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 16:23:12 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.452 | trans_loss 5.882 | nll_loss 3.253 | w2v_ctc_loss 1.423 | task_loss 4.472 | contrastive_loss 0.33 | total 4003.4 | n_correct 2292.4 | ppl 9.53 | accuracy 57.261 | uer 22.767 | wer 24.492 | raw_wer 24.492 | bleu 16.68 | wps 2148.5 | wpb 4003.4 | bsz 141.8 | num_updates 7364 | best_bleu 16.68
2023-07-27 16:23:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7364 updates
2023-07-27 16:23:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 16:23:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 16:23:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 5 @ 7364 updates, score 16.68) (writing took 19.649491788819432 seconds)
2023-07-27 16:23:32 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-07-27 16:23:32 | INFO | train | epoch 005 | loss 2.289 | trans_loss 3.64 | nll_loss 1.834 | w2v_ctc_loss 1.286 | task_loss 0.929 | contrastive_loss 0.247 | total 4138.65 | n_correct 2237.54 | ppl 3.56 | accuracy 54.064 | wps 12420.5 | ups 1.01 | wpb 12355.8 | bsz 458.5 | num_updates 7364 | lr 0.0001648 | gnorm 0.481 | clip 0 | loss_scale 16 | train_wall 1353 | gb_free 16.2 | wall 6530
2023-07-27 16:23:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 16:23:32 | INFO | fairseq.trainer | begin training epoch 6
2023-07-27 16:23:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 16:24:15 | INFO | train_inner | epoch 006:     36 / 1474 loss=2.238, trans_loss=3.61, nll_loss=1.796, w2v_ctc_loss=1.247, task_loss=0.955, contrastive_loss=0.199, total=4115.45, n_correct=2269.85, ppl=3.47, accuracy=55.154, wps=8404.4, ups=0.68, wpb=12281.2, bsz=447.9, num_updates=7400, lr=0.000164399, gnorm=0.485, clip=0, loss_scale=16, train_wall=93, gb_free=16.4, wall=6573
2023-07-27 16:25:47 | INFO | train_inner | epoch 006:    136 / 1474 loss=2.192, trans_loss=3.577, nll_loss=1.753, w2v_ctc_loss=1.197, task_loss=0.929, contrastive_loss=0.242, total=4154.25, n_correct=2323.3, ppl=3.37, accuracy=55.926, wps=13473.8, ups=1.09, wpb=12407.4, bsz=456.1, num_updates=7500, lr=0.000163299, gnorm=0.47, clip=0, loss_scale=16, train_wall=91, gb_free=15.5, wall=6665
2023-07-27 16:27:20 | INFO | train_inner | epoch 006:    236 / 1474 loss=2.206, trans_loss=3.59, nll_loss=1.77, w2v_ctc_loss=1.229, task_loss=0.997, contrastive_loss=0.151, total=4112.66, n_correct=2284.83, ppl=3.41, accuracy=55.556, wps=13269.5, ups=1.08, wpb=12287.2, bsz=437.3, num_updates=7600, lr=0.000162221, gnorm=0.471, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=6758
2023-07-27 16:28:55 | INFO | train_inner | epoch 006:    336 / 1474 loss=2.216, trans_loss=3.578, nll_loss=1.755, w2v_ctc_loss=1.177, task_loss=0.864, contrastive_loss=0.458, total=4177.51, n_correct=2344.07, ppl=3.38, accuracy=56.112, wps=13104.9, ups=1.05, wpb=12473.8, bsz=491.3, num_updates=7700, lr=0.000161165, gnorm=0.477, clip=0, loss_scale=16, train_wall=94, gb_free=16, wall=6853
2023-07-27 16:30:28 | INFO | train_inner | epoch 006:    436 / 1474 loss=2.175, trans_loss=3.58, nll_loss=1.758, w2v_ctc_loss=1.188, task_loss=0.895, contrastive_loss=0.168, total=4154.57, n_correct=2338.02, ppl=3.38, accuracy=56.276, wps=13379.2, ups=1.08, wpb=12405.5, bsz=470.2, num_updates=7800, lr=0.000160128, gnorm=0.469, clip=0, loss_scale=16, train_wall=92, gb_free=16.1, wall=6946
2023-07-27 16:32:01 | INFO | train_inner | epoch 006:    536 / 1474 loss=2.186, trans_loss=3.588, nll_loss=1.767, w2v_ctc_loss=1.205, task_loss=0.936, contrastive_loss=0.154, total=4167.79, n_correct=2336.19, ppl=3.4, accuracy=56.053, wps=13411.7, ups=1.08, wpb=12438.5, bsz=455.2, num_updates=7900, lr=0.000159111, gnorm=0.469, clip=0, loss_scale=16, train_wall=92, gb_free=15.7, wall=7039
2023-07-27 16:33:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-07-27 16:33:34 | INFO | train_inner | epoch 006:    637 / 1474 loss=2.172, trans_loss=3.587, nll_loss=1.768, w2v_ctc_loss=1.184, task_loss=0.887, contrastive_loss=0.152, total=4144.56, n_correct=2323.49, ppl=3.4, accuracy=56.061, wps=13274.1, ups=1.07, wpb=12371.7, bsz=469.2, num_updates=8000, lr=0.000158114, gnorm=0.467, clip=0, loss_scale=8, train_wall=93, gb_free=15.8, wall=7132
2023-07-27 16:33:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 16:33:57 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.378 | trans_loss 5.784 | nll_loss 3.118 | w2v_ctc_loss 1.426 | task_loss 4.542 | contrastive_loss 0.291 | total 4003.4 | n_correct 2340.9 | ppl 8.68 | accuracy 58.473 | uer 20.558 | wer 22.359 | raw_wer 22.359 | bleu 17.63 | wps 2190.6 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 17.63
2023-07-27 16:33:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-07-27 16:33:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_6_8000.pt
2023-07-27 16:34:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_6_8000.pt
2023-07-27 16:34:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 17.63) (writing took 21.126465402543545 seconds)
2023-07-27 16:35:52 | INFO | train_inner | epoch 006:    737 / 1474 loss=2.188, trans_loss=3.593, nll_loss=1.775, w2v_ctc_loss=1.202, task_loss=0.949, contrastive_loss=0.163, total=4151.01, n_correct=2321.93, ppl=3.42, accuracy=55.937, wps=8967.3, ups=0.72, wpb=12393.7, bsz=454.2, num_updates=8100, lr=0.000157135, gnorm=0.467, clip=0, loss_scale=8, train_wall=93, gb_free=12.9, wall=7270
2023-07-27 16:37:25 | INFO | train_inner | epoch 006:    837 / 1474 loss=2.18, trans_loss=3.601, nll_loss=1.786, w2v_ctc_loss=1.194, task_loss=0.984, contrastive_loss=0.144, total=4108.83, n_correct=2289.17, ppl=3.45, accuracy=55.713, wps=13212.9, ups=1.08, wpb=12267.1, bsz=439.4, num_updates=8200, lr=0.000156174, gnorm=0.467, clip=0, loss_scale=8, train_wall=92, gb_free=17.1, wall=7363
2023-07-27 16:38:58 | INFO | train_inner | epoch 006:    937 / 1474 loss=2.198, trans_loss=3.599, nll_loss=1.783, w2v_ctc_loss=1.198, task_loss=0.978, contrastive_loss=0.244, total=4076.46, n_correct=2274.91, ppl=3.44, accuracy=55.806, wps=13110.4, ups=1.08, wpb=12166, bsz=443, num_updates=8300, lr=0.00015523, gnorm=0.476, clip=0, loss_scale=8, train_wall=92, gb_free=12.5, wall=7456
2023-07-27 16:40:32 | INFO | train_inner | epoch 006:   1037 / 1474 loss=2.192, trans_loss=3.585, nll_loss=1.766, w2v_ctc_loss=1.18, task_loss=0.874, contrastive_loss=0.323, total=4175.9, n_correct=2349.23, ppl=3.4, accuracy=56.257, wps=13296.8, ups=1.07, wpb=12465, bsz=480.4, num_updates=8400, lr=0.000154303, gnorm=0.484, clip=0, loss_scale=8, train_wall=93, gb_free=14.1, wall=7549
2023-07-27 16:42:04 | INFO | train_inner | epoch 006:   1137 / 1474 loss=2.177, trans_loss=3.59, nll_loss=1.772, w2v_ctc_loss=1.192, task_loss=1.023, contrastive_loss=0.15, total=4077.2, n_correct=2285.23, ppl=3.42, accuracy=56.049, wps=13218.7, ups=1.09, wpb=12172.7, bsz=430.6, num_updates=8500, lr=0.000153393, gnorm=0.471, clip=0, loss_scale=8, train_wall=91, gb_free=16.2, wall=7642
2023-07-27 16:43:37 | INFO | train_inner | epoch 006:   1237 / 1474 loss=2.209, trans_loss=3.586, nll_loss=1.769, w2v_ctc_loss=1.178, task_loss=0.915, contrastive_loss=0.465, total=4133.46, n_correct=2319.75, ppl=3.41, accuracy=56.121, wps=13225.1, ups=1.07, wpb=12346.4, bsz=470.3, num_updates=8600, lr=0.000152499, gnorm=0.469, clip=0, loss_scale=8, train_wall=93, gb_free=12.2, wall=7735
2023-07-27 16:45:10 | INFO | train_inner | epoch 006:   1337 / 1474 loss=2.156, trans_loss=3.589, nll_loss=1.769, w2v_ctc_loss=1.174, task_loss=0.925, contrastive_loss=0.133, total=4127.77, n_correct=2328.13, ppl=3.41, accuracy=56.402, wps=13276.2, ups=1.08, wpb=12312.7, bsz=454.1, num_updates=8700, lr=0.00015162, gnorm=0.463, clip=0, loss_scale=8, train_wall=92, gb_free=17, wall=7828
2023-07-27 16:46:43 | INFO | train_inner | epoch 006:   1437 / 1474 loss=2.156, trans_loss=3.582, nll_loss=1.762, w2v_ctc_loss=1.174, task_loss=0.934, contrastive_loss=0.14, total=4190.32, n_correct=2370.23, ppl=3.39, accuracy=56.564, wps=13419.7, ups=1.07, wpb=12507.6, bsz=460.5, num_updates=8800, lr=0.000150756, gnorm=0.459, clip=0, loss_scale=8, train_wall=93, gb_free=16.8, wall=7921
2023-07-27 16:47:17 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 16:47:40 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.354 | trans_loss 5.766 | nll_loss 3.096 | w2v_ctc_loss 1.387 | task_loss 4.551 | contrastive_loss 0.291 | total 4003.4 | n_correct 2354.2 | ppl 8.55 | accuracy 58.805 | uer 19.898 | wer 21.662 | raw_wer 21.662 | bleu 17.94 | wps 2081.8 | wpb 4003.4 | bsz 141.8 | num_updates 8837 | best_bleu 17.94
2023-07-27 16:47:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8837 updates
2023-07-27 16:47:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 16:47:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 16:47:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 6 @ 8837 updates, score 17.94) (writing took 18.968557942658663 seconds)
2023-07-27 16:47:59 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-07-27 16:47:59 | INFO | train | epoch 006 | loss 2.185 | trans_loss 3.587 | nll_loss 1.768 | w2v_ctc_loss 1.19 | task_loss 0.932 | contrastive_loss 0.22 | total 4138.46 | n_correct 2320.78 | ppl 3.41 | accuracy 56.078 | wps 12405.4 | ups 1 | wpb 12355.3 | bsz 458.3 | num_updates 8837 | lr 0.00015044 | gnorm 0.47 | clip 0 | loss_scale 8 | train_wall 1359 | gb_free 15.1 | wall 7997
2023-07-27 16:48:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 16:48:00 | INFO | fairseq.trainer | begin training epoch 7
2023-07-27 16:48:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 16:49:06 | INFO | train_inner | epoch 007:     63 / 1474 loss=2.125, trans_loss=3.558, nll_loss=1.732, w2v_ctc_loss=1.144, task_loss=0.909, contrastive_loss=0.159, total=4110.43, n_correct=2348.94, ppl=3.32, accuracy=57.146, wps=8550.5, ups=0.7, wpb=12272.9, bsz=462.8, num_updates=8900, lr=0.000149906, gnorm=0.465, clip=0, loss_scale=8, train_wall=92, gb_free=17.3, wall=8064
2023-07-27 16:50:40 | INFO | train_inner | epoch 007:    163 / 1474 loss=2.122, trans_loss=3.547, nll_loss=1.716, w2v_ctc_loss=1.128, task_loss=0.947, contrastive_loss=0.226, total=4109.53, n_correct=2353.46, ppl=3.29, accuracy=57.268, wps=13165.3, ups=1.07, wpb=12269.6, bsz=454.1, num_updates=9000, lr=0.000149071, gnorm=0.466, clip=0, loss_scale=8, train_wall=92, gb_free=13.5, wall=8158
2023-07-27 16:52:12 | INFO | train_inner | epoch 007:    263 / 1474 loss=2.108, trans_loss=3.543, nll_loss=1.709, w2v_ctc_loss=1.132, task_loss=0.932, contrastive_loss=0.136, total=4133.29, n_correct=2379.06, ppl=3.27, accuracy=57.559, wps=13342.8, ups=1.08, wpb=12335.8, bsz=455.5, num_updates=9100, lr=0.00014825, gnorm=0.462, clip=0, loss_scale=8, train_wall=92, gb_free=15.2, wall=8250
2023-07-27 16:53:45 | INFO | train_inner | epoch 007:    363 / 1474 loss=2.142, trans_loss=3.552, nll_loss=1.722, w2v_ctc_loss=1.124, task_loss=0.905, contrastive_loss=0.4, total=4194.76, n_correct=2400.93, ppl=3.3, accuracy=57.236, wps=13451.6, ups=1.07, wpb=12518.2, bsz=477.6, num_updates=9200, lr=0.000147442, gnorm=0.46, clip=0, loss_scale=8, train_wall=92, gb_free=12.9, wall=8343
2023-07-27 16:55:19 | INFO | train_inner | epoch 007:    463 / 1474 loss=2.13, trans_loss=3.552, nll_loss=1.724, w2v_ctc_loss=1.12, task_loss=0.921, contrastive_loss=0.317, total=4153.22, n_correct=2375.69, ppl=3.3, accuracy=57.201, wps=13290.1, ups=1.07, wpb=12403.3, bsz=463, num_updates=9300, lr=0.000146647, gnorm=0.459, clip=0, loss_scale=8, train_wall=92, gb_free=16.8, wall=8436
2023-07-27 16:56:51 | INFO | train_inner | epoch 007:    563 / 1474 loss=2.107, trans_loss=3.552, nll_loss=1.721, w2v_ctc_loss=1.124, task_loss=0.911, contrastive_loss=0.144, total=4168.14, n_correct=2394.72, ppl=3.3, accuracy=57.453, wps=13396.4, ups=1.08, wpb=12434.7, bsz=459.8, num_updates=9400, lr=0.000145865, gnorm=0.46, clip=0, loss_scale=8, train_wall=92, gb_free=16.8, wall=8529
2023-07-27 16:58:24 | INFO | train_inner | epoch 007:    663 / 1474 loss=2.098, trans_loss=3.549, nll_loss=1.718, w2v_ctc_loss=1.118, task_loss=0.925, contrastive_loss=0.132, total=4157.82, n_correct=2395.24, ppl=3.29, accuracy=57.608, wps=13367.7, ups=1.08, wpb=12406.4, bsz=455.4, num_updates=9500, lr=0.000145095, gnorm=0.462, clip=0, loss_scale=8, train_wall=92, gb_free=15.5, wall=8622
2023-07-27 16:59:57 | INFO | train_inner | epoch 007:    763 / 1474 loss=2.101, trans_loss=3.548, nll_loss=1.718, w2v_ctc_loss=1.121, task_loss=0.976, contrastive_loss=0.127, total=4122.1, n_correct=2366.64, ppl=3.29, accuracy=57.413, wps=13289.5, ups=1.08, wpb=12308.4, bsz=446.2, num_updates=9600, lr=0.000144338, gnorm=0.464, clip=0, loss_scale=8, train_wall=92, gb_free=15.6, wall=8715
2023-07-27 17:01:31 | INFO | train_inner | epoch 007:    863 / 1474 loss=2.104, trans_loss=3.555, nll_loss=1.727, w2v_ctc_loss=1.12, task_loss=0.939, contrastive_loss=0.151, total=4147.23, n_correct=2382.51, ppl=3.31, accuracy=57.448, wps=13182, ups=1.06, wpb=12377.6, bsz=460.9, num_updates=9700, lr=0.000143592, gnorm=0.463, clip=0, loss_scale=8, train_wall=93, gb_free=17.5, wall=8809
2023-07-27 17:03:05 | INFO | train_inner | epoch 007:    963 / 1474 loss=2.111, trans_loss=3.55, nll_loss=1.722, w2v_ctc_loss=1.109, task_loss=0.885, contrastive_loss=0.248, total=4140.14, n_correct=2381.88, ppl=3.3, accuracy=57.531, wps=13169.4, ups=1.07, wpb=12359.1, bsz=474.6, num_updates=9800, lr=0.000142857, gnorm=0.467, clip=0, loss_scale=8, train_wall=93, gb_free=15.9, wall=8903
2023-07-27 17:04:37 | INFO | train_inner | epoch 007:   1063 / 1474 loss=2.102, trans_loss=3.562, nll_loss=1.738, w2v_ctc_loss=1.124, task_loss=0.977, contrastive_loss=0.111, total=4103.51, n_correct=2349.41, ppl=3.33, accuracy=57.254, wps=13266.9, ups=1.08, wpb=12251.1, bsz=437.5, num_updates=9900, lr=0.000142134, gnorm=0.461, clip=0, loss_scale=8, train_wall=92, gb_free=16.8, wall=8995
2023-07-27 17:06:10 | INFO | train_inner | epoch 007:   1163 / 1474 loss=2.137, trans_loss=3.548, nll_loss=1.723, w2v_ctc_loss=1.114, task_loss=0.906, contrastive_loss=0.385, total=4137.04, n_correct=2380.39, ppl=3.3, accuracy=57.538, wps=13217.9, ups=1.07, wpb=12361.6, bsz=470.9, num_updates=10000, lr=0.000141421, gnorm=0.483, clip=0, loss_scale=8, train_wall=93, gb_free=15.9, wall=9088
2023-07-27 17:06:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 17:06:34 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.304 | trans_loss 5.717 | nll_loss 3.039 | w2v_ctc_loss 1.334 | task_loss 4.575 | contrastive_loss 0.284 | total 4003.4 | n_correct 2380.1 | ppl 8.22 | accuracy 59.452 | uer 19.101 | wer 20.79 | raw_wer 20.79 | bleu 18.51 | wps 2262.8 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 18.51
2023-07-27 17:06:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-07-27 17:06:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_7_10000.pt
2023-07-27 17:06:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_7_10000.pt
2023-07-27 17:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 18.51) (writing took 22.146511539816856 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([8, 92]), X shape:torch.Size([8, 92, 512])
CTC Tokens:tensor([84, 11,  0, 39, 39], device='cuda:0'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:0'), New Tokens:tensor([  84,   11,    0,   39, 1141], device='cuda:0')
Org X:tensor([[-0.0999,  0.1276, -0.2064,  ..., -0.5239, -0.5225, -0.0075],
        [-0.2362,  0.3572, -0.5410,  ..., -0.3484, -1.0332, -0.0980],
        [ 0.2612,  0.2798, -0.4260,  ..., -2.1602, -1.3994,  0.2610],
        [-0.2744,  0.5479, -1.2578,  ..., -0.7471, -0.8325,  0.4724],
        [-0.6074,  0.2502,  1.8184,  ..., -0.0757,  0.6304, -0.5381]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:0'), Mixup Sent Mask:tensor([False, False, False, False,  True,  True,  True, False],
       device='cuda:0'), New X:tensor([[-0.0999,  0.1276, -0.2064,  ..., -0.5239, -0.5225, -0.0075],
        [-0.2362,  0.3572, -0.5410,  ..., -0.3484, -1.0332, -0.0980],
        [ 0.2612,  0.2798, -0.4260,  ..., -2.1602, -1.3994,  0.2610],
        [-0.2744,  0.5479, -1.2578,  ..., -0.7471, -0.8325,  0.4724],
        [-0.6074,  0.2502,  1.8184,  ..., -0.0757,  0.6304, -0.5381]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:0')
2023-07-27 17:08:29 | INFO | train_inner | epoch 007:   1263 / 1474 loss=2.095, trans_loss=3.555, nll_loss=1.73, w2v_ctc_loss=1.111, task_loss=0.943, contrastive_loss=0.141, total=4129.52, n_correct=2372.22, ppl=3.32, accuracy=57.445, wps=8892.9, ups=0.72, wpb=12331.4, bsz=450.2, num_updates=10100, lr=0.00014072, gnorm=0.389, clip=0, loss_scale=16, train_wall=92, gb_free=16.7, wall=9227
2023-07-27 17:10:02 | INFO | train_inner | epoch 007:   1363 / 1474 loss=2.106, trans_loss=3.549, nll_loss=1.722, w2v_ctc_loss=1.118, task_loss=0.877, contrastive_loss=0.179, total=4172.87, n_correct=2405.56, ppl=3.3, accuracy=57.648, wps=13399.1, ups=1.08, wpb=12458.1, bsz=476.2, num_updates=10200, lr=0.000140028, gnorm=0.39, clip=0, loss_scale=16, train_wall=92, gb_free=17.2, wall=9320
2023-07-27 17:11:37 | INFO | train_inner | epoch 007:   1463 / 1474 loss=2.117, trans_loss=3.554, nll_loss=1.73, w2v_ctc_loss=1.121, task_loss=1.004, contrastive_loss=0.244, total=4109.42, n_correct=2358.28, ppl=3.32, accuracy=57.387, wps=12984.1, ups=1.06, wpb=12278.1, bsz=443.8, num_updates=10300, lr=0.000139347, gnorm=0.396, clip=0, loss_scale=16, train_wall=94, gb_free=16.4, wall=9415
2023-07-27 17:11:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([32, 48]), X shape:torch.Size([32, 48, 512])
CTC Tokens:tensor([  8,   0,   0, 432,   0], device='cuda:6'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:6'), New Tokens:tensor([   8,    0,  432,    0, 1111], device='cuda:6')
Org X:tensor([[ 0.3252, -0.7378,  0.4480,  ...,  0.6680,  0.4880, -0.9126],
        [ 0.4006, -0.5972,  1.3125,  ..., -0.0598,  1.9805, -0.1099],
        [-0.7896,  0.2185, -0.0114,  ...,  0.2539,  1.0469, -0.4297],
        [ 0.0893,  0.5054,  1.4785,  ...,  0.4846,  0.0379, -1.7822],
        [-1.2646,  1.2676, -0.4102,  ...,  0.2144,  0.4836,  0.6680]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([False, False, False, False,  True,  True,  True, False, False, False],
       device='cuda:6'), New X:tensor([[ 0.3252, -0.7378,  0.4480,  ...,  0.6680,  0.4880, -0.9126],
        [ 0.4006, -0.5972,  1.3125,  ..., -0.0598,  1.9805, -0.1099],
        [-0.7896,  0.2185, -0.0114,  ...,  0.2539,  1.0469, -0.4297],
        [ 0.0893,  0.5054,  1.4785,  ...,  0.4846,  0.0379, -1.7822],
        [-1.2646,  1.2676, -0.4102,  ...,  0.2144,  0.4836,  0.6680]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 37]), X shape:torch.Size([32, 37, 512])
CTC Tokens:tensor([ 0,  0, 33,  0,  0], device='cuda:1'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:1'), New Tokens:tensor([ 0, 33,  0, 26,  0], device='cuda:1')
Org X:tensor([[ 0.4966, -0.0443,  1.0146,  ..., -0.8584,  0.5068,  0.1643],
        [-0.2001,  0.7163, -0.5439,  ..., -0.9727, -1.3262,  0.5381],
        [ 0.2585,  0.7598,  0.1964,  ..., -3.6406, -0.2734,  0.8379],
        [ 0.2122,  0.0966,  0.4958,  ..., -0.0855,  0.3767, -0.5557],
        [ 0.0416, -0.1147,  2.1562,  ..., -0.4336, -0.0693, -0.0493]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:1'), Mixup Sent Mask:tensor([False, False, False, False,  True,  True,  True, False, False, False],
       device='cuda:1'), New X:tensor([[ 0.4966, -0.0443,  1.0146,  ..., -0.8584,  0.5068,  0.1643],
        [-0.2001,  0.7163, -0.5439,  ..., -0.9727, -1.3262,  0.5381],
        [ 0.2585,  0.7598,  0.1964,  ..., -3.6406, -0.2734,  0.8379],
        [ 0.2122,  0.0966,  0.4958,  ..., -0.0855,  0.3767, -0.5557],
        [ 0.0416, -0.1147,  2.1562,  ..., -0.4336, -0.0693, -0.0493]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 158]), X shape:torch.Size([8, 158, 512])
CTC Tokens:tensor([19, 19,  0,  0,  0], device='cuda:2'), Shrink Mask:tensor([ True, False,  True, False, False], device='cuda:2'), New Tokens:tensor([  19,    0, 2613,    0,    7], device='cuda:2')
Org X:tensor([[ 0.0059,  0.4548, -0.1954,  ...,  0.2145,  0.4116, -1.4453],
        [ 1.0283,  1.4453,  1.6602,  ..., -1.9580,  1.2148, -1.3096],
        [ 1.7627,  0.4277,  2.0742,  ...,  0.2881, -1.7910,  0.4644],
        [ 1.3613, -0.0592,  3.1348,  ..., -0.7598, -0.7417,  0.4041],
        [ 0.1225,  0.4133, -1.5830,  ...,  0.0631, -0.4075, -1.6904]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:2'), Mixup Sent Mask:tensor([False, False, False, False,  True,  True,  True, False],
       device='cuda:2'), New X:tensor([[ 0.0059,  0.4548, -0.1954,  ...,  0.2145,  0.4116, -1.4453],
        [ 1.0283,  1.4453,  1.6602,  ..., -1.9580,  1.2148, -1.3096],
        [ 1.7627,  0.4277,  2.0742,  ...,  0.2881, -1.7910,  0.4644],
        [ 1.3613, -0.0592,  3.1348,  ..., -0.7598, -0.7417,  0.4041],
        [ 0.1225,  0.4133, -1.5830,  ...,  0.0631, -0.4075, -1.6904]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 83]), X shape:torch.Size([8, 83, 512])
CTC Tokens:tensor([  8, 101, 246, 289,  38], device='cuda:3'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:3'), New Tokens:tensor([  8, 101, 246, 289,  38], device='cuda:3')
Org X:tensor([[ 0.0306,  0.0358, -0.3606,  ...,  0.3594,  0.0239, -0.3772],
        [ 0.2384,  0.2186, -0.1008,  ..., -0.5181,  1.2598, -0.0610],
        [ 2.8809,  0.2793, -0.4263,  ..., -0.7241, -0.6333, -1.3916],
        [ 1.0049,  0.2527, -2.9395,  ..., -1.3184, -1.3750, -0.8691],
        [ 0.0279,  0.3376, -2.3750,  ..., -1.2383,  0.9277, -1.4971]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:3'), Mixup Sent Mask:tensor([False, False, False, False,  True,  True,  True, False],
       device='cuda:3'), New X:tensor([[ 0.0306,  0.0358, -0.3606,  ...,  0.3594,  0.0239, -0.3772],
        [ 0.2384,  0.2186, -0.1008,  ..., -0.5181,  1.2598, -0.0610],
        [ 2.8809,  0.2793, -0.4263,  ..., -0.7241, -0.6333, -1.3916],
        [ 1.0049,  0.2527, -2.9395,  ..., -1.3184, -1.3750, -0.8691],
        [ 0.0279,  0.3376, -2.3750,  ..., -1.2383,  0.9277, -1.4971]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([32, 38]), X shape:torch.Size([32, 38, 512])
CTC Tokens:tensor([ 17,  17,   0,  11, 276], device='cuda:7'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:7'), New Tokens:tensor([ 17,   0,  11, 276,   0], device='cuda:7')
Org X:tensor([[ 0.0613,  0.0723, -1.1191,  ..., -1.1514,  0.4226, -1.5820],
        [-0.5557,  0.1592,  0.2373,  ..., -0.3040,  1.8008, -1.3125],
        [-0.6128,  0.5991,  1.0049,  ..., -0.2258,  0.9741, -0.3286],
        [-1.5107,  0.4426,  0.1632,  ...,  0.1338,  0.3801,  0.5337],
        [-0.8188,  0.5005,  1.7734,  ...,  0.1048, -0.3716,  0.5728]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:7'), Mixup Sent Mask:tensor([False, False, False, False,  True,  True,  True, False, False, False],
       device='cuda:7'), New X:tensor([[ 0.0613,  0.0723, -1.1191,  ..., -1.1514,  0.4226, -1.5820],
        [-0.5557,  0.1592,  0.2373,  ..., -0.3040,  1.8008, -1.3125],
        [-0.6128,  0.5991,  1.0049,  ..., -0.2258,  0.9741, -0.3286],
        [-1.5107,  0.4426,  0.1632,  ...,  0.1338,  0.3801,  0.5337],
        [-0.8188,  0.5005,  1.7734,  ...,  0.1048, -0.3716,  0.5728]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 95]), X shape:torch.Size([8, 95, 512])
CTC Tokens:tensor([ 8,  0, 24,  0, 73], device='cuda:5'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:5'), New Tokens:tensor([ 8,  0, 24,  0, 73], device='cuda:5')
Org X:tensor([[ 0.0711, -0.3276, -0.0032,  ...,  0.3591,  0.3391, -0.5063],
        [-0.2827,  0.1804,  0.0418,  ..., -1.6582,  1.0898, -0.3347],
        [-1.6670,  0.5103, -1.0928,  ..., -0.3340,  1.0137, -1.0537],
        [-0.4155, -0.4749,  1.3418,  ..., -0.2849,  0.9429, -0.0707],
        [ 0.2426, -0.9673,  0.1173,  ...,  0.3408, -0.7534, -0.0481]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([False, False, False, False,  True,  True,  True, False],
       device='cuda:5'), New X:tensor([[ 0.0711, -0.3276, -0.0032,  ...,  0.3591,  0.3391, -0.5063],
        [-0.2827,  0.1804,  0.0418,  ..., -1.6582,  1.0898, -0.3347],
        [-1.6670,  0.5103, -1.0928,  ..., -0.3340,  1.0137, -1.0537],
        [-0.4155, -0.4749,  1.3418,  ..., -0.2849,  0.9429, -0.0707],
        [ 0.2426, -0.9673,  0.1173,  ...,  0.3408, -0.7534, -0.0481]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 67]), X shape:torch.Size([16, 67, 512])
CTC Tokens:tensor([   8,    8,    0, 1136,    0], device='cuda:4'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:4'), New Tokens:tensor([   8,    0, 1136,    0, 6294], device='cuda:4')
Org X:tensor([[-3.6353e-01, -5.9570e-01, -1.9836e-01,  ...,  2.9370e-01,
         -1.7749e-01, -3.2300e-01],
        [ 1.6632e-03,  3.2690e-01,  1.8008e+00,  ..., -1.3662e+00,
          8.4180e-01, -5.5817e-02],
        [-1.0918e+00,  2.2485e-01,  1.4854e-02,  ..., -5.4297e-01,
          2.0300e-01, -5.7190e-02],
        [ 1.5259e-01,  3.1201e-01,  2.8711e-01,  ..., -4.0924e-02,
          2.0859e+00, -9.3231e-03],
        [-1.0195e+00,  1.6479e-01,  6.6406e-01,  ..., -2.5665e-02,
          7.8467e-01,  3.7280e-01]], device='cuda:4', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:4'), Mixup Sent Mask:tensor([False, False, False, False,  True,  True,  True, False, False, False],
       device='cuda:4'), New X:tensor([[-3.6353e-01, -5.9570e-01, -1.9836e-01,  ...,  2.9370e-01,
         -1.7749e-01, -3.2300e-01],
        [ 1.6632e-03,  3.2690e-01,  1.8008e+00,  ..., -1.3662e+00,
          8.4180e-01, -5.5817e-02],
        [-1.0918e+00,  2.2485e-01,  1.4854e-02,  ..., -5.4297e-01,
          2.0300e-01, -5.7190e-02],
        [ 1.5259e-01,  3.1201e-01,  2.8711e-01,  ..., -4.0924e-02,
          2.0859e+00, -9.3231e-03],
        [-1.0195e+00,  1.6479e-01,  6.6406e-01,  ..., -2.5665e-02,
          7.8467e-01,  3.7280e-01]], device='cuda:4', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:4')
2023-07-27 17:12:09 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.299 | trans_loss 5.706 | nll_loss 3.024 | w2v_ctc_loss 1.345 | task_loss 4.549 | contrastive_loss 0.279 | total 4003.4 | n_correct 2378.3 | ppl 8.14 | accuracy 59.407 | uer 19.311 | wer 20.935 | raw_wer 20.935 | bleu 18.55 | wps 2190.8 | wpb 4003.4 | bsz 141.8 | num_updates 10311 | best_bleu 18.55
2023-07-27 17:12:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10311 updates
2023-07-27 17:12:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 17:12:20 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 17:12:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 7 @ 10311 updates, score 18.55) (writing took 20.781355986371636 seconds)
2023-07-27 17:12:30 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-07-27 17:12:30 | INFO | train | epoch 007 | loss 2.113 | trans_loss 3.551 | nll_loss 1.722 | w2v_ctc_loss 1.121 | task_loss 0.932 | contrastive_loss 0.208 | total 4138.65 | n_correct 2376.78 | ppl 3.3 | accuracy 57.429 | wps 12379.2 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 10311 | lr 0.000139272 | gnorm 0.449 | clip 0 | loss_scale 16 | train_wall 1360 | gb_free 13.1 | wall 9468
2023-07-27 17:12:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 17:12:31 | INFO | fairseq.trainer | begin training epoch 8
2023-07-27 17:12:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 17:14:03 | INFO | train_inner | epoch 008:     89 / 1474 loss=2.062, trans_loss=3.528, nll_loss=1.69, w2v_ctc_loss=1.08, task_loss=0.982, contrastive_loss=0.138, total=4116.25, n_correct=2400.35, ppl=3.23, accuracy=58.314, wps=8387, ups=0.68, wpb=12273, bsz=443.3, num_updates=10400, lr=0.000138675, gnorm=0.391, clip=0, loss_scale=16, train_wall=92, gb_free=16.9, wall=9561
2023-07-27 17:15:36 | INFO | train_inner | epoch 008:    189 / 1474 loss=2.062, trans_loss=3.52, nll_loss=1.679, w2v_ctc_loss=1.078, task_loss=1.009, contrastive_loss=0.159, total=4037.23, n_correct=2360.97, ppl=3.2, accuracy=58.48, wps=12962.6, ups=1.08, wpb=12041.5, bsz=428.6, num_updates=10500, lr=0.000138013, gnorm=0.398, clip=0, loss_scale=16, train_wall=92, gb_free=12.6, wall=9654
2023-07-27 17:17:09 | INFO | train_inner | epoch 008:    289 / 1474 loss=2.057, trans_loss=3.516, nll_loss=1.677, w2v_ctc_loss=1.074, task_loss=0.875, contrastive_loss=0.159, total=4207.78, n_correct=2464.58, ppl=3.2, accuracy=58.572, wps=13455.5, ups=1.07, wpb=12556.5, bsz=488.1, num_updates=10600, lr=0.000137361, gnorm=0.392, clip=0, loss_scale=16, train_wall=92, gb_free=12.8, wall=9747
2023-07-27 17:18:43 | INFO | train_inner | epoch 008:    389 / 1474 loss=2.078, trans_loss=3.527, nll_loss=1.69, w2v_ctc_loss=1.094, task_loss=0.994, contrastive_loss=0.179, total=4127.24, n_correct=2400.89, ppl=3.23, accuracy=58.172, wps=13157.5, ups=1.07, wpb=12316.2, bsz=441.4, num_updates=10700, lr=0.000136717, gnorm=0.396, clip=0, loss_scale=16, train_wall=93, gb_free=11.6, wall=9841
2023-07-27 17:20:17 | INFO | train_inner | epoch 008:    489 / 1474 loss=2.104, trans_loss=3.52, nll_loss=1.684, w2v_ctc_loss=1.07, task_loss=0.835, contrastive_loss=0.444, total=4203.76, n_correct=2456.07, ppl=3.21, accuracy=58.426, wps=13367.6, ups=1.07, wpb=12548.2, bsz=504.5, num_updates=10800, lr=0.000136083, gnorm=0.391, clip=0, loss_scale=16, train_wall=93, gb_free=14.5, wall=9935
2023-07-27 17:21:50 | INFO | train_inner | epoch 008:    589 / 1474 loss=2.067, trans_loss=3.524, nll_loss=1.691, w2v_ctc_loss=1.095, task_loss=1.019, contrastive_loss=0.112, total=4062.5, n_correct=2360.7, ppl=3.23, accuracy=58.11, wps=12959.6, ups=1.07, wpb=12145.4, bsz=427.9, num_updates=10900, lr=0.000135457, gnorm=0.397, clip=0, loss_scale=16, train_wall=93, gb_free=11.1, wall=10028
2023-07-27 17:23:24 | INFO | train_inner | epoch 008:    689 / 1474 loss=2.056, trans_loss=3.518, nll_loss=1.68, w2v_ctc_loss=1.085, task_loss=0.961, contrastive_loss=0.123, total=4142.78, n_correct=2425.21, ppl=3.2, accuracy=58.541, wps=13168.1, ups=1.07, wpb=12364.4, bsz=448.6, num_updates=11000, lr=0.00013484, gnorm=0.387, clip=0, loss_scale=16, train_wall=93, gb_free=15.8, wall=10122
2023-07-27 17:24:58 | INFO | train_inner | epoch 008:    789 / 1474 loss=2.067, trans_loss=3.52, nll_loss=1.686, w2v_ctc_loss=1.08, task_loss=0.956, contrastive_loss=0.207, total=4118.9, n_correct=2404.69, ppl=3.22, accuracy=58.382, wps=13109.6, ups=1.06, wpb=12310.9, bsz=447.8, num_updates=11100, lr=0.000134231, gnorm=0.392, clip=0, loss_scale=16, train_wall=93, gb_free=15.1, wall=10216
2023-07-27 17:26:32 | INFO | train_inner | epoch 008:    889 / 1474 loss=2.063, trans_loss=3.52, nll_loss=1.687, w2v_ctc_loss=1.069, task_loss=0.896, contrastive_loss=0.221, total=4169.01, n_correct=2442.28, ppl=3.22, accuracy=58.582, wps=13273.7, ups=1.07, wpb=12452.5, bsz=473.7, num_updates=11200, lr=0.000133631, gnorm=0.391, clip=0, loss_scale=16, train_wall=93, gb_free=16, wall=10310
2023-07-27 17:28:05 | INFO | train_inner | epoch 008:    989 / 1474 loss=2.046, trans_loss=3.522, nll_loss=1.687, w2v_ctc_loss=1.068, task_loss=0.893, contrastive_loss=0.121, total=4154.69, n_correct=2436.29, ppl=3.22, accuracy=58.64, wps=13322.2, ups=1.07, wpb=12403.4, bsz=464.9, num_updates=11300, lr=0.000133038, gnorm=0.389, clip=0, loss_scale=16, train_wall=92, gb_free=17.7, wall=10403
2023-07-27 17:29:40 | INFO | train_inner | epoch 008:   1089 / 1474 loss=2.078, trans_loss=3.53, nll_loss=1.697, w2v_ctc_loss=1.069, task_loss=0.925, contrastive_loss=0.344, total=4199.1, n_correct=2446.97, ppl=3.24, accuracy=58.274, wps=13249.9, ups=1.06, wpb=12534.3, bsz=465.3, num_updates=11400, lr=0.000132453, gnorm=0.394, clip=0, loss_scale=16, train_wall=94, gb_free=12.5, wall=10498
2023-07-27 17:31:13 | INFO | train_inner | epoch 008:   1189 / 1474 loss=2.051, trans_loss=3.522, nll_loss=1.688, w2v_ctc_loss=1.071, task_loss=0.881, contrastive_loss=0.13, total=4177.31, n_correct=2446.72, ppl=3.22, accuracy=58.572, wps=13425.6, ups=1.08, wpb=12476.3, bsz=472.6, num_updates=11500, lr=0.000131876, gnorm=0.389, clip=0, loss_scale=16, train_wall=92, gb_free=14.8, wall=10591
2023-07-27 17:32:46 | INFO | train_inner | epoch 008:   1289 / 1474 loss=2.065, trans_loss=3.53, nll_loss=1.699, w2v_ctc_loss=1.085, task_loss=0.973, contrastive_loss=0.153, total=4063.85, n_correct=2363.46, ppl=3.25, accuracy=58.158, wps=13024.1, ups=1.07, wpb=12140.6, bsz=438.4, num_updates=11600, lr=0.000131306, gnorm=0.398, clip=0, loss_scale=16, train_wall=92, gb_free=16.7, wall=10684
2023-07-27 17:34:19 | INFO | train_inner | epoch 008:   1389 / 1474 loss=2.066, trans_loss=3.529, nll_loss=1.698, w2v_ctc_loss=1.074, task_loss=0.922, contrastive_loss=0.205, total=4141.5, n_correct=2418.52, ppl=3.24, accuracy=58.397, wps=13260.2, ups=1.07, wpb=12367.2, bsz=461.5, num_updates=11700, lr=0.000130744, gnorm=0.392, clip=0, loss_scale=16, train_wall=92, gb_free=16.3, wall=10777
2023-07-27 17:35:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 17:36:03 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.284 | trans_loss 5.683 | nll_loss 2.987 | w2v_ctc_loss 1.348 | task_loss 4.58 | contrastive_loss 0.273 | total 4003.4 | n_correct 2398.1 | ppl 7.93 | accuracy 59.902 | uer 18.756 | wer 20.588 | raw_wer 20.588 | bleu 18.59 | wps 2075.5 | wpb 4003.4 | bsz 141.8 | num_updates 11785 | best_bleu 18.59
2023-07-27 17:36:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11785 updates
2023-07-27 17:36:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 17:36:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 17:36:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 8 @ 11785 updates, score 18.59) (writing took 20.245754895731807 seconds)
2023-07-27 17:36:24 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-07-27 17:36:24 | INFO | train | epoch 008 | loss 2.066 | trans_loss 3.524 | nll_loss 1.688 | w2v_ctc_loss 1.077 | task_loss 0.933 | contrastive_loss 0.199 | total 4138.65 | n_correct 2417.34 | ppl 3.22 | accuracy 58.409 | wps 12708.2 | ups 1.03 | wpb 12355.8 | bsz 458.5 | num_updates 11785 | lr 0.000130272 | gnorm 0.393 | clip 0 | loss_scale 16 | train_wall 1366 | gb_free 16.8 | wall 10902
2023-07-27 17:36:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 17:36:24 | INFO | fairseq.trainer | begin training epoch 9
2023-07-27 17:36:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 17:36:47 | INFO | train_inner | epoch 009:     15 / 1474 loss=2.067, trans_loss=3.523, nll_loss=1.689, w2v_ctc_loss=1.059, task_loss=0.899, contrastive_loss=0.328, total=4139.35, n_correct=2424.15, ppl=3.22, accuracy=58.564, wps=8363, ups=0.68, wpb=12350.9, bsz=472.9, num_updates=11800, lr=0.000130189, gnorm=0.395, clip=0, loss_scale=16, train_wall=93, gb_free=15.4, wall=10925
2023-07-27 17:38:20 | INFO | train_inner | epoch 009:    115 / 1474 loss=2.012, trans_loss=3.485, nll_loss=1.639, w2v_ctc_loss=1.032, task_loss=0.886, contrastive_loss=0.152, total=4181.9, n_correct=2488.74, ppl=3.11, accuracy=59.512, wps=13343, ups=1.07, wpb=12488.1, bsz=475.9, num_updates=11900, lr=0.000129641, gnorm=0.385, clip=0, loss_scale=16, train_wall=93, gb_free=16.2, wall=11018
2023-07-27 17:39:55 | INFO | train_inner | epoch 009:    215 / 1474 loss=2.011, trans_loss=3.493, nll_loss=1.649, w2v_ctc_loss=1.036, task_loss=1.006, contrastive_loss=0.107, total=4062.07, n_correct=2408.9, ppl=3.14, accuracy=59.302, wps=12806.9, ups=1.06, wpb=12129.1, bsz=431.6, num_updates=12000, lr=0.000129099, gnorm=0.394, clip=0, loss_scale=16, train_wall=94, gb_free=15.5, wall=11113
2023-07-27 17:39:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 17:40:18 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.27 | trans_loss 5.683 | nll_loss 2.991 | w2v_ctc_loss 1.301 | task_loss 4.568 | contrastive_loss 0.279 | total 4003.4 | n_correct 2397.2 | ppl 7.95 | accuracy 59.879 | uer 18.544 | wer 20.38 | raw_wer 20.38 | bleu 18.82 | wps 2212.7 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 18.82
2023-07-27 17:40:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-07-27 17:40:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_9_12000.pt
2023-07-27 17:40:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_9_12000.pt
2023-07-27 17:40:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 18.82) (writing took 20.65030766837299 seconds)
2023-07-27 17:42:13 | INFO | train_inner | epoch 009:    315 / 1474 loss=2.002, trans_loss=3.479, nll_loss=1.634, w2v_ctc_loss=1.02, task_loss=0.872, contrastive_loss=0.158, total=4152.1, n_correct=2482.23, ppl=3.1, accuracy=59.783, wps=9004.6, ups=0.73, wpb=12407.3, bsz=476.6, num_updates=12100, lr=0.000128565, gnorm=0.39, clip=0, loss_scale=32, train_wall=92, gb_free=16.2, wall=11251
2023-07-27 17:43:48 | INFO | train_inner | epoch 009:    415 / 1474 loss=2.011, trans_loss=3.496, nll_loss=1.654, w2v_ctc_loss=1.033, task_loss=0.912, contrastive_loss=0.125, total=4203.78, n_correct=2492.75, ppl=3.15, accuracy=59.298, wps=13246.5, ups=1.06, wpb=12551.8, bsz=469.8, num_updates=12200, lr=0.000128037, gnorm=0.389, clip=0, loss_scale=32, train_wall=94, gb_free=17.1, wall=11346
2023-07-27 17:45:22 | INFO | train_inner | epoch 009:    515 / 1474 loss=2.039, trans_loss=3.502, nll_loss=1.66, w2v_ctc_loss=1.057, task_loss=0.981, contrastive_loss=0.176, total=4112.78, n_correct=2429.19, ppl=3.16, accuracy=59.064, wps=13090.6, ups=1.07, wpb=12275.6, bsz=437.7, num_updates=12300, lr=0.000127515, gnorm=0.396, clip=0, loss_scale=32, train_wall=93, gb_free=16.1, wall=11439
2023-07-27 17:46:56 | INFO | train_inner | epoch 009:    615 / 1474 loss=2.006, trans_loss=3.492, nll_loss=1.651, w2v_ctc_loss=1.028, task_loss=0.951, contrastive_loss=0.137, total=4131.32, n_correct=2455.47, ppl=3.14, accuracy=59.435, wps=13063.2, ups=1.06, wpb=12347.4, bsz=455, num_updates=12400, lr=0.000127, gnorm=0.389, clip=0, loss_scale=32, train_wall=94, gb_free=17.8, wall=11534
2023-07-27 17:48:29 | INFO | train_inner | epoch 009:    715 / 1474 loss=2.041, trans_loss=3.504, nll_loss=1.666, w2v_ctc_loss=1.053, task_loss=0.954, contrastive_loss=0.219, total=4082.11, n_correct=2406.68, ppl=3.17, accuracy=58.957, wps=13123.9, ups=1.08, wpb=12194.8, bsz=449.7, num_updates=12500, lr=0.000126491, gnorm=0.402, clip=0, loss_scale=32, train_wall=92, gb_free=16.9, wall=11627
2023-07-27 17:50:04 | INFO | train_inner | epoch 009:    815 / 1474 loss=2.06, trans_loss=3.498, nll_loss=1.66, w2v_ctc_loss=1.043, task_loss=0.841, contrastive_loss=0.362, total=4221.08, n_correct=2499.07, ppl=3.16, accuracy=59.205, wps=13287.1, ups=1.05, wpb=12613.9, bsz=501.6, num_updates=12600, lr=0.000125988, gnorm=0.393, clip=0, loss_scale=32, train_wall=94, gb_free=17.6, wall=11722
2023-07-27 17:51:39 | INFO | train_inner | epoch 009:    915 / 1474 loss=2.044, trans_loss=3.504, nll_loss=1.663, w2v_ctc_loss=1.041, task_loss=0.965, contrastive_loss=0.34, total=4142.34, n_correct=2448.99, ppl=3.17, accuracy=59.121, wps=13027.6, ups=1.05, wpb=12359.9, bsz=448.9, num_updates=12700, lr=0.000125491, gnorm=0.393, clip=0, loss_scale=32, train_wall=94, gb_free=17.2, wall=11817
2023-07-27 17:53:13 | INFO | train_inner | epoch 009:   1015 / 1474 loss=2.026, trans_loss=3.511, nll_loss=1.671, w2v_ctc_loss=1.049, task_loss=1.047, contrastive_loss=0.124, total=4097.15, n_correct=2414.54, ppl=3.18, accuracy=58.932, wps=13050.4, ups=1.07, wpb=12228.7, bsz=422.6, num_updates=12800, lr=0.000125, gnorm=0.395, clip=0, loss_scale=32, train_wall=93, gb_free=16.8, wall=11910
2023-07-27 17:54:46 | INFO | train_inner | epoch 009:   1115 / 1474 loss=2.021, trans_loss=3.506, nll_loss=1.664, w2v_ctc_loss=1.037, task_loss=0.871, contrastive_loss=0.15, total=4182.29, n_correct=2481.93, ppl=3.17, accuracy=59.344, wps=13292.7, ups=1.07, wpb=12467.1, bsz=477.6, num_updates=12900, lr=0.000124515, gnorm=0.391, clip=0, loss_scale=32, train_wall=93, gb_free=17.2, wall=12004
2023-07-27 17:56:21 | INFO | train_inner | epoch 009:   1215 / 1474 loss=2.03, trans_loss=3.508, nll_loss=1.67, w2v_ctc_loss=1.055, task_loss=0.99, contrastive_loss=0.13, total=4141.43, n_correct=2446, ppl=3.18, accuracy=59.062, wps=13094.2, ups=1.06, wpb=12364.7, bsz=446.6, num_updates=13000, lr=0.000124035, gnorm=0.393, clip=0, loss_scale=32, train_wall=94, gb_free=17.5, wall=12099
2023-07-27 17:57:55 | INFO | train_inner | epoch 009:   1315 / 1474 loss=2.04, trans_loss=3.502, nll_loss=1.663, w2v_ctc_loss=1.031, task_loss=0.848, contrastive_loss=0.322, total=4203.91, n_correct=2495.19, ppl=3.17, accuracy=59.354, wps=13360, ups=1.06, wpb=12545.2, bsz=492.3, num_updates=13100, lr=0.00012356, gnorm=0.392, clip=0, loss_scale=32, train_wall=93, gb_free=17.3, wall=12193
2023-07-27 17:59:28 | INFO | train_inner | epoch 009:   1415 / 1474 loss=2.022, trans_loss=3.515, nll_loss=1.678, w2v_ctc_loss=1.048, task_loss=1.006, contrastive_loss=0.105, total=4077.08, n_correct=2406.46, ppl=3.2, accuracy=59.024, wps=13044.5, ups=1.07, wpb=12165, bsz=429.1, num_updates=13200, lr=0.000123091, gnorm=0.395, clip=0, loss_scale=32, train_wall=92, gb_free=17.3, wall=12286
2023-07-27 18:00:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 18:00:46 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.256 | trans_loss 5.655 | nll_loss 2.961 | w2v_ctc_loss 1.326 | task_loss 4.599 | contrastive_loss 0.267 | total 4003.4 | n_correct 2409.6 | ppl 7.79 | accuracy 60.189 | uer 18.281 | wer 20.025 | raw_wer 20.025 | bleu 19.07 | wps 2153.5 | wpb 4003.4 | bsz 141.8 | num_updates 13259 | best_bleu 19.07
2023-07-27 18:00:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13259 updates
2023-07-27 18:00:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 18:00:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 18:01:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 9 @ 13259 updates, score 19.07) (writing took 20.113159580156207 seconds)
2023-07-27 18:01:06 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-07-27 18:01:06 | INFO | train | epoch 009 | loss 2.027 | trans_loss 3.5 | nll_loss 1.659 | w2v_ctc_loss 1.04 | task_loss 0.933 | contrastive_loss 0.192 | total 4138.65 | n_correct 2452.02 | ppl 3.16 | accuracy 59.247 | wps 12283.9 | ups 0.99 | wpb 12355.8 | bsz 458.5 | num_updates 13259 | lr 0.000122817 | gnorm 0.393 | clip 0 | loss_scale 32 | train_wall 1372 | gb_free 11.4 | wall 12384
2023-07-27 18:01:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 18:01:07 | INFO | fairseq.trainer | begin training epoch 10
2023-07-27 18:01:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 18:01:55 | INFO | train_inner | epoch 010:     41 / 1474 loss=2.014, trans_loss=3.494, nll_loss=1.651, w2v_ctc_loss=1.022, task_loss=0.89, contrastive_loss=0.207, total=4100.86, n_correct=2445.96, ppl=3.14, accuracy=59.645, wps=8323.6, ups=0.68, wpb=12239.9, bsz=470.2, num_updates=13300, lr=0.000122628, gnorm=0.392, clip=0, loss_scale=32, train_wall=93, gb_free=16.4, wall=12433
2023-07-27 18:03:29 | INFO | train_inner | epoch 010:    141 / 1474 loss=1.97, trans_loss=3.466, nll_loss=1.615, w2v_ctc_loss=0.994, task_loss=0.882, contrastive_loss=0.13, total=4240.18, n_correct=2555.06, ppl=3.06, accuracy=60.258, wps=13500.4, ups=1.07, wpb=12664, bsz=479.1, num_updates=13400, lr=0.000122169, gnorm=0.386, clip=0, loss_scale=32, train_wall=93, gb_free=14.8, wall=12527
2023-07-27 18:05:03 | INFO | train_inner | epoch 010:    241 / 1474 loss=1.997, trans_loss=3.469, nll_loss=1.618, w2v_ctc_loss=1.004, task_loss=0.925, contrastive_loss=0.252, total=4126.3, n_correct=2488.42, ppl=3.07, accuracy=60.306, wps=13098.7, ups=1.06, wpb=12312.3, bsz=460.9, num_updates=13500, lr=0.000121716, gnorm=0.387, clip=0, loss_scale=32, train_wall=93, gb_free=15.4, wall=12621
2023-07-27 18:06:38 | INFO | train_inner | epoch 010:    341 / 1474 loss=1.978, trans_loss=3.469, nll_loss=1.623, w2v_ctc_loss=0.999, task_loss=0.949, contrastive_loss=0.16, total=4132.25, n_correct=2483.38, ppl=3.08, accuracy=60.098, wps=13038.8, ups=1.06, wpb=12352, bsz=452.8, num_updates=13600, lr=0.000121268, gnorm=0.392, clip=0, loss_scale=32, train_wall=94, gb_free=14.6, wall=12715
2023-07-27 18:07:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-07-27 18:08:14 | INFO | train_inner | epoch 010:    442 / 1474 loss=1.972, trans_loss=3.475, nll_loss=1.627, w2v_ctc_loss=0.995, task_loss=0.926, contrastive_loss=0.121, total=4175.72, n_correct=2509.78, ppl=3.09, accuracy=60.104, wps=12968.3, ups=1.04, wpb=12467.5, bsz=465.6, num_updates=13700, lr=0.000120824, gnorm=0.386, clip=0, loss_scale=16, train_wall=95, gb_free=16, wall=12812
2023-07-27 18:09:48 | INFO | train_inner | epoch 010:    542 / 1474 loss=1.997, trans_loss=3.488, nll_loss=1.64, w2v_ctc_loss=1.024, task_loss=1.004, contrastive_loss=0.117, total=4102.8, n_correct=2452.16, ppl=3.12, accuracy=59.768, wps=12953.3, ups=1.06, wpb=12234.1, bsz=437.8, num_updates=13800, lr=0.000120386, gnorm=0.398, clip=0, loss_scale=16, train_wall=93, gb_free=16.9, wall=12906
2023-07-27 18:11:24 | INFO | train_inner | epoch 010:    642 / 1474 loss=2.007, trans_loss=3.485, nll_loss=1.64, w2v_ctc_loss=1.013, task_loss=0.888, contrastive_loss=0.235, total=4176.56, n_correct=2500.39, ppl=3.12, accuracy=59.867, wps=13071.1, ups=1.05, wpb=12464, bsz=477.2, num_updates=13900, lr=0.000119952, gnorm=0.393, clip=0, loss_scale=16, train_wall=94, gb_free=16.1, wall=13001
2023-07-27 18:12:57 | INFO | train_inner | epoch 010:    742 / 1474 loss=1.999, trans_loss=3.485, nll_loss=1.64, w2v_ctc_loss=1.029, task_loss=0.935, contrastive_loss=0.113, total=4125.87, n_correct=2464.75, ppl=3.12, accuracy=59.739, wps=13203.9, ups=1.07, wpb=12315.3, bsz=454.4, num_updates=14000, lr=0.000119523, gnorm=0.397, clip=0, loss_scale=16, train_wall=92, gb_free=14.4, wall=13095
2023-07-27 18:12:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 18:13:20 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.263 | trans_loss 5.659 | nll_loss 2.959 | w2v_ctc_loss 1.331 | task_loss 4.611 | contrastive_loss 0.281 | total 4003.4 | n_correct 2419.6 | ppl 7.77 | accuracy 60.439 | uer 18.785 | wer 20.435 | raw_wer 20.435 | bleu 19.21 | wps 2244.3 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 19.21
2023-07-27 18:13:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-07-27 18:13:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_10_14000.pt
2023-07-27 18:13:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_10_14000.pt
2023-07-27 18:13:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 19.21) (writing took 20.57087710686028 seconds)
2023-07-27 18:15:15 | INFO | train_inner | epoch 010:    842 / 1474 loss=1.978, trans_loss=3.48, nll_loss=1.635, w2v_ctc_loss=1.002, task_loss=0.924, contrastive_loss=0.118, total=4128.44, n_correct=2475.76, ppl=3.11, accuracy=59.968, wps=8890.2, ups=0.72, wpb=12327.8, bsz=456.3, num_updates=14100, lr=0.000119098, gnorm=0.391, clip=0, loss_scale=16, train_wall=93, gb_free=14.6, wall=13233
2023-07-27 18:16:50 | INFO | train_inner | epoch 010:    942 / 1474 loss=1.994, trans_loss=3.482, nll_loss=1.635, w2v_ctc_loss=1.011, task_loss=0.896, contrastive_loss=0.159, total=4160.94, n_correct=2494.15, ppl=3.11, accuracy=59.942, wps=13198.2, ups=1.06, wpb=12411.1, bsz=468.1, num_updates=14200, lr=0.000118678, gnorm=0.393, clip=0, loss_scale=16, train_wall=93, gb_free=15.3, wall=13327
2023-07-27 18:18:24 | INFO | train_inner | epoch 010:   1042 / 1474 loss=1.991, trans_loss=3.486, nll_loss=1.642, w2v_ctc_loss=1.014, task_loss=1.009, contrastive_loss=0.13, total=4067.53, n_correct=2428.85, ppl=3.12, accuracy=59.713, wps=12900.4, ups=1.06, wpb=12145, bsz=434.3, num_updates=14300, lr=0.000118262, gnorm=0.396, clip=0, loss_scale=16, train_wall=93, gb_free=16.9, wall=13422
2023-07-27 18:19:57 | INFO | train_inner | epoch 010:   1142 / 1474 loss=2, trans_loss=3.493, nll_loss=1.651, w2v_ctc_loss=1.029, task_loss=1.042, contrastive_loss=0.11, total=4044.03, n_correct=2407.81, ppl=3.14, accuracy=59.54, wps=12940.8, ups=1.07, wpb=12074.4, bsz=422.3, num_updates=14400, lr=0.000117851, gnorm=0.403, clip=0, loss_scale=16, train_wall=92, gb_free=17.3, wall=13515
2023-07-27 18:21:31 | INFO | train_inner | epoch 010:   1242 / 1474 loss=1.99, trans_loss=3.481, nll_loss=1.64, w2v_ctc_loss=1.021, task_loss=0.955, contrastive_loss=0.106, total=4110.41, n_correct=2458.5, ppl=3.12, accuracy=59.812, wps=13064.7, ups=1.06, wpb=12291.6, bsz=445.2, num_updates=14500, lr=0.000117444, gnorm=0.395, clip=0, loss_scale=16, train_wall=93, gb_free=16.4, wall=13609
2023-07-27 18:23:06 | INFO | train_inner | epoch 010:   1342 / 1474 loss=1.99, trans_loss=3.489, nll_loss=1.648, w2v_ctc_loss=1.015, task_loss=0.953, contrastive_loss=0.121, total=4121.38, n_correct=2467.64, ppl=3.13, accuracy=59.874, wps=12947.3, ups=1.05, wpb=12308.4, bsz=451, num_updates=14600, lr=0.000117041, gnorm=0.395, clip=0, loss_scale=16, train_wall=94, gb_free=13.9, wall=13704
2023-07-27 18:24:41 | INFO | train_inner | epoch 010:   1442 / 1474 loss=2.025, trans_loss=3.494, nll_loss=1.651, w2v_ctc_loss=0.999, task_loss=0.879, contrastive_loss=0.378, total=4192.39, n_correct=2504.14, ppl=3.14, accuracy=59.731, wps=13226.4, ups=1.06, wpb=12506.1, bsz=482, num_updates=14700, lr=0.000116642, gnorm=0.396, clip=0, loss_scale=16, train_wall=94, gb_free=17, wall=13799
2023-07-27 18:25:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 18:25:35 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.249 | trans_loss 5.634 | nll_loss 2.929 | w2v_ctc_loss 1.345 | task_loss 4.605 | contrastive_loss 0.277 | total 4003.4 | n_correct 2431.4 | ppl 7.61 | accuracy 60.733 | uer 18.09 | wer 19.641 | raw_wer 19.641 | bleu 19.2 | wps 2002.6 | wpb 4003.4 | bsz 141.8 | num_updates 14732 | best_bleu 19.21
2023-07-27 18:25:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14732 updates
2023-07-27 18:25:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.2001.pt
2023-07-27 18:25:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.2001.pt
2023-07-27 18:25:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.2001.pt (epoch 10 @ 14732 updates, score 19.2) (writing took 12.397090895101428 seconds)
2023-07-27 18:25:49 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-07-27 18:25:49 | INFO | train | epoch 010 | loss 1.992 | trans_loss 3.481 | nll_loss 1.636 | w2v_ctc_loss 1.009 | task_loss 0.936 | contrastive_loss 0.172 | total 4136.93 | n_correct 2478.81 | ppl 3.11 | accuracy 59.919 | wps 12268.6 | ups 0.99 | wpb 12350.7 | bsz 457.4 | num_updates 14732 | lr 0.000116516 | gnorm 0.393 | clip 0 | loss_scale 16 | train_wall 1376 | gb_free 17.2 | wall 13867
2023-07-27 18:25:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 18:25:50 | INFO | fairseq.trainer | begin training epoch 11
2023-07-27 18:25:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 18:27:02 | INFO | train_inner | epoch 011:     68 / 1474 loss=1.965, trans_loss=3.456, nll_loss=1.604, w2v_ctc_loss=0.983, task_loss=0.864, contrastive_loss=0.192, total=4175.24, n_correct=2534.09, ppl=3.04, accuracy=60.693, wps=8823.3, ups=0.71, wpb=12463.5, bsz=478.8, num_updates=14800, lr=0.000116248, gnorm=0.387, clip=0, loss_scale=16, train_wall=93, gb_free=16.7, wall=13940
2023-07-27 18:28:36 | INFO | train_inner | epoch 011:    168 / 1474 loss=1.955, trans_loss=3.456, nll_loss=1.605, w2v_ctc_loss=0.985, task_loss=0.961, contrastive_loss=0.113, total=4087.78, n_correct=2479.04, ppl=3.04, accuracy=60.645, wps=12922.3, ups=1.06, wpb=12214.2, bsz=445.9, num_updates=14900, lr=0.000115857, gnorm=0.394, clip=0, loss_scale=16, train_wall=93, gb_free=16.4, wall=14034
2023-07-27 18:30:10 | INFO | train_inner | epoch 011:    268 / 1474 loss=1.947, trans_loss=3.457, nll_loss=1.605, w2v_ctc_loss=0.976, task_loss=0.962, contrastive_loss=0.108, total=4118.77, n_correct=2501.38, ppl=3.04, accuracy=60.731, wps=13147, ups=1.07, wpb=12299.1, bsz=446.5, num_updates=15000, lr=0.00011547, gnorm=0.388, clip=0, loss_scale=16, train_wall=93, gb_free=12.2, wall=14128
Mixup rate:0.5, token after shrink shape:torch.Size([8, 93]), X shape:torch.Size([8, 93, 512])
CTC Tokens:tensor([67, 67,  0,  9,  9], device='cuda:0'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:0'), New Tokens:tensor([ 67,   0,   9,   0, 140], device='cuda:0')
Org X:tensor([[ 0.1818, -0.2915,  0.5845,  ..., -0.1031,  0.3193, -2.2148],
        [ 0.1199,  0.2367,  1.6826,  ..., -0.2583, -0.3376, -1.4043],
        [ 0.5000, -0.7510, -0.0663,  ..., -0.0108,  0.1045, -0.0954],
        [ 0.2732,  0.6226,  1.3867,  ...,  0.0897, -0.5811,  0.4617],
        [-0.3911, -0.1602,  3.2578,  ..., -0.2456, -0.3418,  0.9556]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:0'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True],
       device='cuda:0'), New X:tensor([[ 0.1818, -0.2915,  0.5845,  ..., -0.1031,  0.3193, -2.2148],
        [ 0.1199,  0.2367,  1.6826,  ..., -0.2583, -0.3376, -1.4043],
        [ 0.5000, -0.7510, -0.0663,  ..., -0.0108,  0.1045, -0.0954],
        [ 0.2732,  0.6226,  1.3867,  ...,  0.0897, -0.5811,  0.4617],
        [-0.3911, -0.1602,  3.2578,  ..., -0.2456, -0.3418,  0.9556]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:0')
2023-07-27 18:31:21 | INFO | train_inner | epoch 011:    368 / 1474 loss=2.112, trans_loss=5.138, nll_loss=2.389, w2v_ctc_loss=0.734, task_loss=1.428, contrastive_loss=0.089, total=4097.83, n_correct=2483.46, ppl=5.24, accuracy=60.604, wps=11639.9, ups=1.41, wpb=8240.8, bsz=298, num_updates=15100, lr=0.000115087, gnorm=0.528, clip=0, loss_scale=16, train_wall=70, gb_free=15.6, wall=14199
2023-07-27 18:32:33 | INFO | train_inner | epoch 011:    468 / 1474 loss=2.129, trans_loss=5.177, nll_loss=2.416, w2v_ctc_loss=0.728, task_loss=1.465, contrastive_loss=0.208, total=4110.64, n_correct=2477.92, ppl=5.34, accuracy=60.281, wps=11457.9, ups=1.39, wpb=8221.3, bsz=300.4, num_updates=15200, lr=0.000114708, gnorm=0.535, clip=0, loss_scale=16, train_wall=71, gb_free=16.3, wall=14271
2023-07-27 18:33:43 | INFO | train_inner | epoch 011:    568 / 1474 loss=2.131, trans_loss=5.176, nll_loss=2.416, w2v_ctc_loss=0.741, task_loss=1.503, contrastive_loss=0.206, total=4071.69, n_correct=2453.29, ppl=5.34, accuracy=60.252, wps=11488.9, ups=1.41, wpb=8143.4, bsz=293.7, num_updates=15300, lr=0.000114332, gnorm=0.536, clip=0, loss_scale=16, train_wall=70, gb_free=16, wall=14341
2023-07-27 18:34:54 | INFO | train_inner | epoch 011:    668 / 1474 loss=2.134, trans_loss=5.179, nll_loss=2.42, w2v_ctc_loss=0.734, task_loss=1.374, contrastive_loss=0.261, total=4157.2, n_correct=2501.75, ppl=5.35, accuracy=60.179, wps=11726.3, ups=1.41, wpb=8314.4, bsz=309.6, num_updates=15400, lr=0.000113961, gnorm=0.527, clip=0, loss_scale=16, train_wall=70, gb_free=16.7, wall=14412
2023-07-27 18:36:06 | INFO | train_inner | epoch 011:    768 / 1474 loss=2.127, trans_loss=5.186, nll_loss=2.429, w2v_ctc_loss=0.749, task_loss=1.408, contrastive_loss=0.088, total=4174.91, n_correct=2519.62, ppl=5.39, accuracy=60.351, wps=11724, ups=1.4, wpb=8349.8, bsz=306.9, num_updates=15500, lr=0.000113592, gnorm=0.53, clip=0, loss_scale=16, train_wall=70, gb_free=17, wall=14484
2023-07-27 18:37:16 | INFO | train_inner | epoch 011:    868 / 1474 loss=2.127, trans_loss=5.188, nll_loss=2.43, w2v_ctc_loss=0.742, task_loss=1.464, contrastive_loss=0.074, total=4118.44, n_correct=2474.58, ppl=5.39, accuracy=60.085, wps=11657.8, ups=1.42, wpb=8236.9, bsz=293.7, num_updates=15600, lr=0.000113228, gnorm=0.524, clip=0, loss_scale=16, train_wall=70, gb_free=10.6, wall=14554
2023-07-27 18:38:27 | INFO | train_inner | epoch 011:    968 / 1474 loss=2.126, trans_loss=5.186, nll_loss=2.429, w2v_ctc_loss=0.745, task_loss=1.433, contrastive_loss=0.088, total=4140.92, n_correct=2492.42, ppl=5.39, accuracy=60.19, wps=11720.3, ups=1.42, wpb=8281.8, bsz=301.9, num_updates=15700, lr=0.000112867, gnorm=0.529, clip=0, loss_scale=32, train_wall=70, gb_free=15.6, wall=14625
2023-07-27 18:39:38 | INFO | train_inner | epoch 011:   1068 / 1474 loss=2.122, trans_loss=5.18, nll_loss=2.422, w2v_ctc_loss=0.742, task_loss=1.373, contrastive_loss=0.11, total=4136.99, n_correct=2502.35, ppl=5.36, accuracy=60.487, wps=11715.2, ups=1.42, wpb=8274, bsz=308.8, num_updates=15800, lr=0.000112509, gnorm=0.532, clip=0, loss_scale=32, train_wall=70, gb_free=17.6, wall=14696
2023-07-27 18:40:48 | INFO | train_inner | epoch 011:   1168 / 1474 loss=2.126, trans_loss=5.188, nll_loss=2.433, w2v_ctc_loss=0.745, task_loss=1.393, contrastive_loss=0.095, total=4185.65, n_correct=2519.78, ppl=5.4, accuracy=60.2, wps=11887.4, ups=1.42, wpb=8371.3, bsz=309.8, num_updates=15900, lr=0.000112154, gnorm=0.525, clip=0, loss_scale=32, train_wall=70, gb_free=13.9, wall=14766
2023-07-27 18:41:58 | INFO | train_inner | epoch 011:   1268 / 1474 loss=2.129, trans_loss=5.183, nll_loss=2.427, w2v_ctc_loss=0.745, task_loss=1.346, contrastive_loss=0.166, total=4171.89, n_correct=2518.9, ppl=5.38, accuracy=60.378, wps=11877, ups=1.42, wpb=8343.8, bsz=314.1, num_updates=16000, lr=0.000111803, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=14836
2023-07-27 18:41:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([24, 46]), X shape:torch.Size([24, 46, 512])
CTC Tokens:tensor([   8,    0,    7, 1043, 1043], device='cuda:6'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:6'), New Tokens:tensor([   8,    0,    7, 1043,    0], device='cuda:6')
Org X:tensor([[ 0.2896, -0.3887,  0.5713,  ...,  0.7124,  0.0783, -0.8574],
        [ 0.0501,  0.0778,  0.1351,  ...,  0.3706, -0.1610, -1.2031],
        [-0.3848,  0.2739, -0.5371,  ...,  0.2983, -0.0244, -1.7324],
        [-0.4785,  0.1880,  1.6484,  ...,  0.3311,  1.0068, -0.1824],
        [ 0.4158,  1.0303,  2.4707,  ...,  0.0248, -0.2177, -0.6030]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:6'), New X:tensor([[ 0.2896, -0.3887,  0.5713,  ...,  0.7124,  0.0783, -0.8574],
        [ 0.0501,  0.0778,  0.1351,  ...,  0.3706, -0.1610, -1.2031],
        [-0.3848,  0.2739, -0.5371,  ...,  0.2983, -0.0244, -1.7324],
        [-0.4785,  0.1880,  1.6484,  ...,  0.3311,  1.0068, -0.1824],
        [ 0.4158,  1.0303,  2.4707,  ...,  0.0248, -0.2177, -0.6030]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 57]), X shape:torch.Size([24, 57, 512])
CTC Tokens:tensor([  0,   0, 305,   0,   0], device='cuda:2'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:2'), New Tokens:tensor([  0, 305,   0,   7,   0], device='cuda:2')
Org X:tensor([[ 6.7529e-01, -1.0469e+00,  1.5840e+00,  ..., -3.4692e-01,
          7.4170e-01, -8.2178e-01],
        [ 1.1902e-01,  3.3154e-01, -7.6294e-04,  ...,  1.9397e-01,
          3.7354e-01, -8.0371e-01],
        [ 3.6499e-01,  6.5820e-01,  1.1084e+00,  ..., -3.8721e-01,
          1.8945e-01, -1.0004e-01],
        [ 3.0615e-01,  5.5908e-01, -1.4473e+00,  ...,  1.4111e-01,
         -1.1298e-01, -1.8809e+00],
        [ 8.2812e-01, -7.2949e-01,  5.8301e-01,  ..., -3.8599e-01,
          6.1768e-02, -2.0874e-01]], device='cuda:2', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:2'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:2'), New X:tensor([[ 6.7529e-01, -1.0469e+00,  1.5840e+00,  ..., -3.4692e-01,
          7.4170e-01, -8.2178e-01],
        [ 1.1902e-01,  3.3154e-01, -7.6294e-04,  ...,  1.9397e-01,
          3.7354e-01, -8.0371e-01],
        [ 3.6499e-01,  6.5820e-01,  1.1084e+00,  ..., -3.8721e-01,
          1.8945e-01, -1.0004e-01],
        [ 3.0615e-01,  5.5908e-01, -1.4473e+00,  ...,  1.4111e-01,
         -1.1298e-01, -1.8809e+00],
        [ 8.2812e-01, -7.2949e-01,  5.8301e-01,  ..., -3.8599e-01,
          6.1768e-02, -2.0874e-01]], device='cuda:2', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 69]), X shape:torch.Size([16, 69, 512])
CTC Tokens:tensor([ 70,  11,   0,   0, 347], device='cuda:3'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:3'), New Tokens:tensor([ 70,  11,   0, 347,   0], device='cuda:3')
Org X:tensor([[-0.0931, -0.0581, -0.5044,  ..., -0.2815, -0.0054,  0.1131],
        [ 0.0211,  0.3408, -0.1925,  ..., -0.6733, -0.6714,  0.1548],
        [ 0.0355,  0.3120, -0.0362,  ..., -1.5957, -1.5312,  0.4390],
        [-1.0088,  0.5215,  0.6035,  ...,  0.2654, -0.6860, -0.5840],
        [-0.9248,  0.9922,  1.9971,  ..., -0.6030, -0.5190, -1.1309]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:3'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:3'), New X:tensor([[-0.0931, -0.0581, -0.5044,  ..., -0.2815, -0.0054,  0.1131],
        [ 0.0211,  0.3408, -0.1925,  ..., -0.6733, -0.6714,  0.1548],
        [ 0.0355,  0.3120, -0.0362,  ..., -1.5957, -1.5312,  0.4390],
        [-1.0088,  0.5215,  0.6035,  ...,  0.2654, -0.6860, -0.5840],
        [-0.9248,  0.9922,  1.9971,  ..., -0.6030, -0.5190, -1.1309]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([6, 188]), X shape:torch.Size([6, 188, 512])
CTC Tokens:tensor([   0, 1313,    0,    0,   29], device='cuda:7'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:7'), New Tokens:tensor([   0, 1313,    0,   29,    0], device='cuda:7')
Org X:tensor([[ 0.7593, -1.2373,  1.4443,  ..., -0.1602, -1.0264,  1.0498],
        [ 0.0418, -0.6382,  0.4395,  ...,  0.1262, -1.7783,  0.3118],
        [-0.2494, -0.7983,  1.2061,  ...,  0.3301, -1.5127,  0.3689],
        [-0.8364,  0.6587,  2.1602,  ...,  0.0092,  0.1545, -0.6357],
        [-1.7402,  0.8594,  2.2188,  ..., -1.1191,  1.0752, -0.5991]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:7'), Mixup Sent Mask:tensor([False, False,  True, False, False, False], device='cuda:7'), New X:tensor([[ 0.7593, -1.2373,  1.4443,  ..., -0.1602, -1.0264,  1.0498],
        [ 0.0418, -0.6382,  0.4395,  ...,  0.1262, -1.7783,  0.3118],
        [-0.2494, -0.7983,  1.2061,  ...,  0.3301, -1.5127,  0.3689],
        [-0.8364,  0.6587,  2.1602,  ...,  0.0092,  0.1545, -0.6357],
        [-1.7402,  0.8594,  2.2188,  ..., -1.1191,  1.0752, -0.5991]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([72, 24]), X shape:torch.Size([72, 24, 512])
CTC Tokens:tensor([ 53,  53, 162,   0,   0], device='cuda:1'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:1'), New Tokens:tensor([  53,  162,    0, 7345,    0], device='cuda:1')
Org X:tensor([[ 0.1580,  0.0725, -0.6187,  ..., -1.5928,  0.9507, -1.8525],
        [ 0.2249,  0.5557,  0.3547,  ..., -0.8828,  0.2668, -1.5869],
        [-0.8872,  0.8247,  1.1689,  ..., -0.2996, -0.6152, -0.6328],
        [-1.1123,  0.2097,  0.2900,  ...,  0.1587,  0.4199,  2.0195],
        [-0.2922, -0.5610,  0.8276,  ...,  0.3992,  0.7119,  0.9878]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:1'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:1'), New X:tensor([[ 0.1580,  0.0725, -0.6187,  ..., -1.5928,  0.9507, -1.8525],
        [ 0.2249,  0.5557,  0.3547,  ..., -0.8828,  0.2668, -1.5869],
        [-0.8872,  0.8247,  1.1689,  ..., -0.2996, -0.6152, -0.6328],
        [-1.1123,  0.2097,  0.2900,  ...,  0.1587,  0.4199,  2.0195],
        [-0.2922, -0.5610,  0.8276,  ...,  0.3992,  0.7119,  0.9878]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 65]), X shape:torch.Size([16, 65, 512])
CTC Tokens:tensor([9, 0, 0, 0, 0], device='cuda:5'), Shrink Mask:tensor([ True,  True, False, False, False], device='cuda:5'), New Tokens:tensor([   9,    0,  250,    0, 5201], device='cuda:5')
Org X:tensor([[-0.1276, -0.6646, -0.3838,  ...,  0.3250,  1.6309,  0.1987],
        [-0.0282, -0.4951,  0.4626,  ...,  0.3474,  1.6572,  0.3562],
        [-0.8926,  0.4585, -0.6602,  ...,  0.1631, -1.3281,  0.4255],
        [-0.1113,  0.5908,  0.0040,  ..., -0.5010,  0.3794,  0.7095],
        [-1.1338,  0.0635, -0.2754,  ..., -0.0997, -0.9883,  1.2412]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:5'), New X:tensor([[-0.1276, -0.6646, -0.3838,  ...,  0.3250,  1.6309,  0.1987],
        [-0.0282, -0.4951,  0.4626,  ...,  0.3474,  1.6572,  0.3562],
        [-0.8926,  0.4585, -0.6602,  ...,  0.1631, -1.3281,  0.4255],
        [-0.1113,  0.5908,  0.0040,  ..., -0.5010,  0.3794,  0.7095],
        [-1.1338,  0.0635, -0.2754,  ..., -0.0997, -0.9883,  1.2412]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 55]), X shape:torch.Size([24, 55, 512])
CTC Tokens:tensor([  0, 440, 440,   4,   0], device='cuda:4'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:4'), New Tokens:tensor([  0, 440,   4,   0, 214], device='cuda:4')
Org X:tensor([[ 1.1816, -0.3604,  1.6123,  ..., -0.9790,  0.1790, -0.0102],
        [ 0.3291,  0.6694,  0.4004,  ..., -0.0817,  0.0095,  0.1667],
        [ 0.3538,  1.0527,  0.2976,  ..., -2.6777,  1.1982, -0.3201],
        [ 0.5400,  1.0088,  0.5947,  ..., -1.7422,  1.1201,  0.3313],
        [-0.1644,  1.0654, -0.1970,  ..., -0.9590,  1.0352,  0.7471]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:4'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:4'), New X:tensor([[ 1.1816, -0.3604,  1.6123,  ..., -0.9790,  0.1790, -0.0102],
        [ 0.3291,  0.6694,  0.4004,  ..., -0.0817,  0.0095,  0.1667],
        [ 0.3538,  1.0527,  0.2976,  ..., -2.6777,  1.1982, -0.3201],
        [ 0.5400,  1.0088,  0.5947,  ..., -1.7422,  1.1201,  0.3313],
        [-0.1644,  1.0654, -0.1970,  ..., -0.9590,  1.0352,  0.7471]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:4')
2023-07-27 18:42:23 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.256 | trans_loss 5.645 | nll_loss 2.943 | w2v_ctc_loss 1.346 | task_loss 4.613 | contrastive_loss 0.275 | total 4003.4 | n_correct 2427.2 | ppl 7.69 | accuracy 60.628 | uer 17.957 | wer 19.626 | raw_wer 19.626 | bleu 18.92 | wps 2073.1 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 19.21
2023-07-27 18:42:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-07-27 18:42:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_11_16000.pt
2023-07-27 18:42:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_11_16000.pt
2023-07-27 18:42:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 18.92) (writing took 11.773879067972302 seconds)
2023-07-27 18:43:47 | INFO | train_inner | epoch 011:   1368 / 1474 loss=2.135, trans_loss=5.184, nll_loss=2.43, w2v_ctc_loss=0.733, task_loss=1.293, contrastive_loss=0.329, total=4190.34, n_correct=2523.14, ppl=5.39, accuracy=60.213, wps=7681.7, ups=0.92, wpb=8380.7, bsz=327.9, num_updates=16100, lr=0.000111456, gnorm=0.526, clip=0, loss_scale=32, train_wall=71, gb_free=17, wall=14945
2023-07-27 18:44:58 | INFO | train_inner | epoch 011:   1468 / 1474 loss=2.122, trans_loss=5.188, nll_loss=2.434, w2v_ctc_loss=0.739, task_loss=1.354, contrastive_loss=0.098, total=4158.39, n_correct=2507.15, ppl=5.4, accuracy=60.291, wps=11760.9, ups=1.41, wpb=8316.8, bsz=312, num_updates=16200, lr=0.000111111, gnorm=0.526, clip=0, loss_scale=32, train_wall=70, gb_free=16.8, wall=15016
2023-07-27 18:45:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 18:45:27 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.243 | trans_loss 5.633 | nll_loss 2.931 | w2v_ctc_loss 1.328 | task_loss 4.573 | contrastive_loss 0.27 | total 4003.4 | n_correct 2431.5 | ppl 7.63 | accuracy 60.736 | uer 18.016 | wer 19.671 | raw_wer 19.671 | bleu 19.49 | wps 2050.8 | wpb 4003.4 | bsz 141.8 | num_updates 16206 | best_bleu 19.49
2023-07-27 18:45:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16206 updates
2023-07-27 18:45:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 18:45:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 18:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 11 @ 16206 updates, score 19.49) (writing took 19.965768547728658 seconds)
2023-07-27 18:45:47 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-07-27 18:45:47 | INFO | train | epoch 011 | loss 2.083 | trans_loss 4.751 | nll_loss 2.219 | w2v_ctc_loss 0.799 | task_loss 1.284 | contrastive_loss 0.141 | total 4138.65 | n_correct 2498.7 | ppl 4.65 | accuracy 60.375 | wps 11097.3 | ups 1.23 | wpb 9021 | bsz 333.4 | num_updates 16206 | lr 0.000111091 | gnorm 0.504 | clip 0 | loss_scale 32 | train_wall 1095 | gb_free 17.2 | wall 15065
2023-07-27 18:45:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 18:45:48 | INFO | fairseq.trainer | begin training epoch 12
2023-07-27 18:45:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 18:47:02 | INFO | train_inner | epoch 012:     94 / 1474 loss=2.097, trans_loss=5.129, nll_loss=2.356, w2v_ctc_loss=0.721, task_loss=1.338, contrastive_loss=0.129, total=4146.82, n_correct=2539.18, ppl=5.12, accuracy=61.232, wps=6675.8, ups=0.8, wpb=8293.6, bsz=313.9, num_updates=16300, lr=0.00011077, gnorm=0.525, clip=0, loss_scale=32, train_wall=69, gb_free=15.7, wall=15140
2023-07-27 18:48:13 | INFO | train_inner | epoch 012:    194 / 1474 loss=2.103, trans_loss=5.139, nll_loss=2.367, w2v_ctc_loss=0.73, task_loss=1.445, contrastive_loss=0.078, total=4120.68, n_correct=2513.4, ppl=5.16, accuracy=60.995, wps=11608, ups=1.41, wpb=8241.4, bsz=294.7, num_updates=16400, lr=0.000110432, gnorm=0.532, clip=0, loss_scale=32, train_wall=70, gb_free=15.6, wall=15211
2023-07-27 18:49:25 | INFO | train_inner | epoch 012:    294 / 1474 loss=2.098, trans_loss=5.139, nll_loss=2.368, w2v_ctc_loss=0.717, task_loss=1.318, contrastive_loss=0.114, total=4199.46, n_correct=2563.8, ppl=5.16, accuracy=61.051, wps=11772.4, ups=1.4, wpb=8398.9, bsz=320.3, num_updates=16500, lr=0.000110096, gnorm=0.521, clip=0, loss_scale=32, train_wall=71, gb_free=16.5, wall=15283
2023-07-27 18:50:35 | INFO | train_inner | epoch 012:    394 / 1474 loss=2.103, trans_loss=5.146, nll_loss=2.377, w2v_ctc_loss=0.727, task_loss=1.373, contrastive_loss=0.097, total=4151.14, n_correct=2529.84, ppl=5.19, accuracy=60.943, wps=11851.3, ups=1.43, wpb=8302.3, bsz=307.8, num_updates=16600, lr=0.000109764, gnorm=0.528, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=15353
2023-07-27 18:51:45 | INFO | train_inner | epoch 012:    494 / 1474 loss=2.114, trans_loss=5.161, nll_loss=2.398, w2v_ctc_loss=0.736, task_loss=1.408, contrastive_loss=0.103, total=4110.49, n_correct=2498.55, ppl=5.27, accuracy=60.785, wps=11698.8, ups=1.42, wpb=8221, bsz=302.2, num_updates=16700, lr=0.000109435, gnorm=0.53, clip=0, loss_scale=32, train_wall=70, gb_free=13.7, wall=15423
2023-07-27 18:52:56 | INFO | train_inner | epoch 012:    594 / 1474 loss=2.109, trans_loss=5.151, nll_loss=2.385, w2v_ctc_loss=0.727, task_loss=1.343, contrastive_loss=0.168, total=4189.92, n_correct=2554.99, ppl=5.22, accuracy=60.979, wps=11731.6, ups=1.4, wpb=8379.8, bsz=315.1, num_updates=16800, lr=0.000109109, gnorm=0.527, clip=0, loss_scale=32, train_wall=71, gb_free=14.8, wall=15494
2023-07-27 18:54:06 | INFO | train_inner | epoch 012:    694 / 1474 loss=2.103, trans_loss=5.146, nll_loss=2.379, w2v_ctc_loss=0.709, task_loss=1.283, contrastive_loss=0.258, total=4206.3, n_correct=2572.78, ppl=5.2, accuracy=61.165, wps=12085.3, ups=1.44, wpb=8412.6, bsz=325.7, num_updates=16900, lr=0.000108786, gnorm=0.518, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=15564
2023-07-27 18:55:16 | INFO | train_inner | epoch 012:    794 / 1474 loss=2.106, trans_loss=5.149, nll_loss=2.381, w2v_ctc_loss=0.732, task_loss=1.432, contrastive_loss=0.092, total=4085.96, n_correct=2492.04, ppl=5.21, accuracy=60.99, wps=11660, ups=1.43, wpb=8171.9, bsz=297.1, num_updates=17000, lr=0.000108465, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=15634
2023-07-27 18:56:27 | INFO | train_inner | epoch 012:    894 / 1474 loss=2.111, trans_loss=5.156, nll_loss=2.391, w2v_ctc_loss=0.727, task_loss=1.431, contrastive_loss=0.148, total=4169.74, n_correct=2537.56, ppl=5.24, accuracy=60.857, wps=11715.4, ups=1.4, wpb=8339.5, bsz=306.4, num_updates=17100, lr=0.000108148, gnorm=0.531, clip=0, loss_scale=32, train_wall=70, gb_free=16, wall=15705
2023-07-27 18:57:38 | INFO | train_inner | epoch 012:    994 / 1474 loss=2.117, trans_loss=5.166, nll_loss=2.405, w2v_ctc_loss=0.733, task_loss=1.432, contrastive_loss=0.152, total=4117.67, n_correct=2496.69, ppl=5.3, accuracy=60.634, wps=11689.7, ups=1.42, wpb=8235.3, bsz=301.4, num_updates=17200, lr=0.000107833, gnorm=0.534, clip=0, loss_scale=32, train_wall=70, gb_free=17.6, wall=15776
2023-07-27 18:58:48 | INFO | train_inner | epoch 012:   1094 / 1474 loss=2.125, trans_loss=5.171, nll_loss=2.41, w2v_ctc_loss=0.737, task_loss=1.47, contrastive_loss=0.196, total=4047.61, n_correct=2451.39, ppl=5.32, accuracy=60.564, wps=11551.6, ups=1.43, wpb=8095.2, bsz=290.4, num_updates=17300, lr=0.000107521, gnorm=0.544, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=15846
2023-07-27 19:00:00 | INFO | train_inner | epoch 012:   1194 / 1474 loss=2.126, trans_loss=5.18, nll_loss=2.424, w2v_ctc_loss=0.744, task_loss=1.388, contrastive_loss=0.152, total=4184.55, n_correct=2526.81, ppl=5.37, accuracy=60.384, wps=11664.4, ups=1.39, wpb=8369.1, bsz=314.3, num_updates=17400, lr=0.000107211, gnorm=0.526, clip=0, loss_scale=32, train_wall=71, gb_free=16.6, wall=15918
2023-07-27 19:01:10 | INFO | train_inner | epoch 012:   1294 / 1474 loss=2.124, trans_loss=5.173, nll_loss=2.414, w2v_ctc_loss=0.751, task_loss=1.526, contrastive_loss=0.097, total=4086.33, n_correct=2474.33, ppl=5.33, accuracy=60.551, wps=11596.5, ups=1.42, wpb=8172.7, bsz=291.4, num_updates=17500, lr=0.000106904, gnorm=0.536, clip=0, loss_scale=32, train_wall=70, gb_free=16.6, wall=15988
2023-07-27 19:02:20 | INFO | train_inner | epoch 012:   1394 / 1474 loss=2.117, trans_loss=5.172, nll_loss=2.414, w2v_ctc_loss=0.725, task_loss=1.423, contrastive_loss=0.183, total=4134.89, n_correct=2505.82, ppl=5.33, accuracy=60.602, wps=11824.7, ups=1.43, wpb=8269.8, bsz=304.4, num_updates=17600, lr=0.0001066, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=16058
2023-07-27 19:03:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 19:03:42 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.251 | trans_loss 5.626 | nll_loss 2.918 | w2v_ctc_loss 1.372 | task_loss 4.599 | contrastive_loss 0.274 | total 4003.4 | n_correct 2441.6 | ppl 7.56 | accuracy 60.988 | uer 18.002 | wer 19.619 | raw_wer 19.619 | bleu 19.29 | wps 1832.7 | wpb 4003.4 | bsz 141.8 | num_updates 17680 | best_bleu 19.49
2023-07-27 19:03:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17680 updates
2023-07-27 19:03:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.2905.pt
2023-07-27 19:03:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.2905.pt
2023-07-27 19:03:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.2905.pt (epoch 12 @ 17680 updates, score 19.29) (writing took 11.012404492124915 seconds)
2023-07-27 19:03:53 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-07-27 19:03:53 | INFO | train | epoch 012 | loss 2.111 | trans_loss 5.156 | nll_loss 2.392 | w2v_ctc_loss 0.73 | task_loss 1.401 | contrastive_loss 0.138 | total 4138.65 | n_correct 2517.68 | ppl 5.25 | accuracy 60.833 | wps 11237.5 | ups 1.36 | wpb 8277.3 | bsz 305.7 | num_updates 17680 | lr 0.000106359 | gnorm 0.53 | clip 0 | loss_scale 32 | train_wall 1028 | gb_free 12.7 | wall 16151
2023-07-27 19:03:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 19:03:53 | INFO | fairseq.trainer | begin training epoch 13
2023-07-27 19:03:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 19:04:16 | INFO | train_inner | epoch 013:     20 / 1474 loss=2.114, trans_loss=5.169, nll_loss=2.409, w2v_ctc_loss=0.737, task_loss=1.449, contrastive_loss=0.088, total=4104.86, n_correct=2492.94, ppl=5.31, accuracy=60.731, wps=7062, ups=0.86, wpb=8209.7, bsz=296.8, num_updates=17700, lr=0.000106299, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=14.7, wall=16174
2023-07-27 19:05:27 | INFO | train_inner | epoch 013:    120 / 1474 loss=2.089, trans_loss=5.12, nll_loss=2.343, w2v_ctc_loss=0.715, task_loss=1.4, contrastive_loss=0.098, total=4161.2, n_correct=2560.44, ppl=5.08, accuracy=61.531, wps=11803.5, ups=1.42, wpb=8322.4, bsz=302.9, num_updates=17800, lr=0.000106, gnorm=0.526, clip=0, loss_scale=64, train_wall=70, gb_free=16.1, wall=16245
2023-07-27 19:06:37 | INFO | train_inner | epoch 013:    220 / 1474 loss=2.105, trans_loss=5.132, nll_loss=2.361, w2v_ctc_loss=0.712, task_loss=1.296, contrastive_loss=0.321, total=4202.62, n_correct=2575.79, ppl=5.14, accuracy=61.29, wps=11946.2, ups=1.42, wpb=8405.2, bsz=328.3, num_updates=17900, lr=0.000105703, gnorm=0.527, clip=0, loss_scale=64, train_wall=70, gb_free=17.2, wall=16315
2023-07-27 19:07:48 | INFO | train_inner | epoch 013:    320 / 1474 loss=2.086, trans_loss=5.121, nll_loss=2.344, w2v_ctc_loss=0.71, task_loss=1.442, contrastive_loss=0.082, total=4112.8, n_correct=2533.86, ppl=5.08, accuracy=61.609, wps=11652.9, ups=1.42, wpb=8225.6, bsz=296, num_updates=18000, lr=0.000105409, gnorm=0.533, clip=0, loss_scale=64, train_wall=70, gb_free=17.8, wall=16386
2023-07-27 19:07:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 19:08:12 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.263 | trans_loss 5.629 | nll_loss 2.922 | w2v_ctc_loss 1.399 | task_loss 4.62 | contrastive_loss 0.28 | total 4003.4 | n_correct 2438.2 | ppl 7.58 | accuracy 60.903 | uer 18.249 | wer 20.1 | raw_wer 20.1 | bleu 19.33 | wps 2087.8 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 19.49
2023-07-27 19:08:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-07-27 19:08:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_13_18000.pt
2023-07-27 19:08:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_13_18000.pt
2023-07-27 19:08:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 19.33) (writing took 15.721585124731064 seconds)
2023-07-27 19:09:39 | INFO | train_inner | epoch 013:    420 / 1474 loss=2.093, trans_loss=5.127, nll_loss=2.354, w2v_ctc_loss=0.721, task_loss=1.318, contrastive_loss=0.135, total=4176.06, n_correct=2569.46, ppl=5.11, accuracy=61.528, wps=7489.3, ups=0.9, wpb=8352.1, bsz=317.5, num_updates=18100, lr=0.000105118, gnorm=0.523, clip=0, loss_scale=64, train_wall=70, gb_free=16.4, wall=16497
2023-07-27 19:10:50 | INFO | train_inner | epoch 013:    520 / 1474 loss=2.1, trans_loss=5.138, nll_loss=2.368, w2v_ctc_loss=0.72, task_loss=1.364, contrastive_loss=0.17, total=4197.57, n_correct=2567.53, ppl=5.16, accuracy=61.167, wps=11962.3, ups=1.42, wpb=8395.1, bsz=318.1, num_updates=18200, lr=0.000104828, gnorm=0.53, clip=0, loss_scale=64, train_wall=69, gb_free=15.2, wall=16568
2023-07-27 19:12:00 | INFO | train_inner | epoch 013:    620 / 1474 loss=2.089, trans_loss=5.133, nll_loss=2.361, w2v_ctc_loss=0.717, task_loss=1.358, contrastive_loss=0.08, total=4160.12, n_correct=2552.47, ppl=5.14, accuracy=61.356, wps=11784.8, ups=1.42, wpb=8320.2, bsz=308.7, num_updates=18300, lr=0.000104542, gnorm=0.529, clip=0, loss_scale=64, train_wall=70, gb_free=16.3, wall=16638
2023-07-27 19:13:11 | INFO | train_inner | epoch 013:    720 / 1474 loss=2.107, trans_loss=5.144, nll_loss=2.375, w2v_ctc_loss=0.739, task_loss=1.552, contrastive_loss=0.078, total=4101.54, n_correct=2504.23, ppl=5.19, accuracy=61.056, wps=11564.4, ups=1.41, wpb=8203.1, bsz=285.7, num_updates=18400, lr=0.000104257, gnorm=0.536, clip=0, loss_scale=64, train_wall=70, gb_free=15.7, wall=16709
2023-07-27 19:14:22 | INFO | train_inner | epoch 013:    820 / 1474 loss=2.101, trans_loss=5.141, nll_loss=2.373, w2v_ctc_loss=0.723, task_loss=1.414, contrastive_loss=0.133, total=4126.37, n_correct=2521.76, ppl=5.18, accuracy=61.113, wps=11579.4, ups=1.4, wpb=8252.7, bsz=307, num_updates=18500, lr=0.000103975, gnorm=0.538, clip=0, loss_scale=64, train_wall=71, gb_free=17.7, wall=16780
2023-07-27 19:15:33 | INFO | train_inner | epoch 013:    920 / 1474 loss=2.099, trans_loss=5.146, nll_loss=2.378, w2v_ctc_loss=0.723, task_loss=1.438, contrastive_loss=0.091, total=4102.78, n_correct=2511.71, ppl=5.2, accuracy=61.22, wps=11682.5, ups=1.42, wpb=8205.6, bsz=295.9, num_updates=18600, lr=0.000103695, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=16851
2023-07-27 19:16:50 | INFO | train_inner | epoch 013:   1020 / 1474 loss=2.109, trans_loss=5.149, nll_loss=2.384, w2v_ctc_loss=0.73, task_loss=1.497, contrastive_loss=0.141, total=4071.32, n_correct=2483.84, ppl=5.22, accuracy=61.008, wps=10496.2, ups=1.29, wpb=8142.6, bsz=291.3, num_updates=18700, lr=0.000103418, gnorm=0.533, clip=0, loss_scale=64, train_wall=77, gb_free=11.3, wall=16928
2023-07-27 19:18:09 | INFO | train_inner | epoch 013:   1120 / 1474 loss=2.096, trans_loss=5.139, nll_loss=2.371, w2v_ctc_loss=0.718, task_loss=1.374, contrastive_loss=0.124, total=4115.28, n_correct=2522.44, ppl=5.17, accuracy=61.294, wps=10481.1, ups=1.27, wpb=8230.6, bsz=307.5, num_updates=18800, lr=0.000103142, gnorm=0.533, clip=0, loss_scale=64, train_wall=78, gb_free=12.2, wall=17007
2023-07-27 19:19:29 | INFO | train_inner | epoch 013:   1220 / 1474 loss=2.105, trans_loss=5.154, nll_loss=2.39, w2v_ctc_loss=0.73, task_loss=1.487, contrastive_loss=0.082, total=4105.36, n_correct=2506.01, ppl=5.24, accuracy=61.042, wps=10222, ups=1.24, wpb=8210.7, bsz=295.3, num_updates=18900, lr=0.000102869, gnorm=0.537, clip=0, loss_scale=64, train_wall=79, gb_free=16.3, wall=17087
2023-07-27 19:20:48 | INFO | train_inner | epoch 013:   1320 / 1474 loss=2.099, trans_loss=5.138, nll_loss=2.371, w2v_ctc_loss=0.72, task_loss=1.385, contrastive_loss=0.181, total=4114, n_correct=2524, ppl=5.17, accuracy=61.351, wps=10390, ups=1.26, wpb=8228, bsz=307.7, num_updates=19000, lr=0.000102598, gnorm=0.532, clip=0, loss_scale=64, train_wall=78, gb_free=17.8, wall=17166
2023-07-27 19:22:05 | INFO | train_inner | epoch 013:   1420 / 1474 loss=2.105, trans_loss=5.152, nll_loss=2.389, w2v_ctc_loss=0.718, task_loss=1.365, contrastive_loss=0.193, total=4182.16, n_correct=2554.65, ppl=5.24, accuracy=61.084, wps=10849.1, ups=1.3, wpb=8364.3, bsz=312.7, num_updates=19100, lr=0.000102329, gnorm=0.532, clip=0, loss_scale=64, train_wall=76, gb_free=12.9, wall=17243
2023-07-27 19:22:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 19:23:14 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.233 | trans_loss 5.619 | nll_loss 2.909 | w2v_ctc_loss 1.327 | task_loss 4.609 | contrastive_loss 0.27 | total 4003.4 | n_correct 2443.6 | ppl 7.51 | accuracy 61.038 | uer 17.904 | wer 19.641 | raw_wer 19.641 | bleu 19.53 | wps 1551.7 | wpb 4003.4 | bsz 141.8 | num_updates 19154 | best_bleu 19.53
2023-07-27 19:23:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19154 updates
2023-07-27 19:23:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 19:23:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 19:23:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 13 @ 19154 updates, score 19.53) (writing took 20.79093192704022 seconds)
2023-07-27 19:23:35 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-07-27 19:23:35 | INFO | train | epoch 013 | loss 2.098 | trans_loss 5.138 | nll_loss 2.368 | w2v_ctc_loss 0.721 | task_loss 1.4 | contrastive_loss 0.137 | total 4138.65 | n_correct 2535.97 | ppl 5.16 | accuracy 61.275 | wps 10321.1 | ups 1.25 | wpb 8277.3 | bsz 305.7 | num_updates 19154 | lr 0.000102185 | gnorm 0.532 | clip 0 | loss_scale 64 | train_wall 1067 | gb_free 17.6 | wall 17333
2023-07-27 19:23:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 19:23:36 | INFO | fairseq.trainer | begin training epoch 14
2023-07-27 19:23:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 19:24:23 | INFO | train_inner | epoch 014:     46 / 1474 loss=2.074, trans_loss=5.106, nll_loss=2.33, w2v_ctc_loss=0.707, task_loss=1.28, contrastive_loss=0.098, total=4173.07, n_correct=2590.65, ppl=5.03, accuracy=62.08, wps=6051.5, ups=0.73, wpb=8346.1, bsz=321.9, num_updates=19200, lr=0.000102062, gnorm=0.526, clip=0, loss_scale=64, train_wall=75, gb_free=16.1, wall=17381
2023-07-27 19:25:42 | INFO | train_inner | epoch 014:    146 / 1474 loss=2.075, trans_loss=5.096, nll_loss=2.314, w2v_ctc_loss=0.711, task_loss=1.416, contrastive_loss=0.077, total=4083.56, n_correct=2537.67, ppl=4.97, accuracy=62.144, wps=10431.1, ups=1.28, wpb=8167.1, bsz=298.5, num_updates=19300, lr=0.000101797, gnorm=0.528, clip=0, loss_scale=64, train_wall=77, gb_free=16.7, wall=17460
2023-07-27 19:27:02 | INFO | train_inner | epoch 014:    246 / 1474 loss=2.088, trans_loss=5.113, nll_loss=2.335, w2v_ctc_loss=0.709, task_loss=1.458, contrastive_loss=0.18, total=4109.88, n_correct=2538.01, ppl=5.05, accuracy=61.754, wps=10289, ups=1.25, wpb=8219.8, bsz=295.2, num_updates=19400, lr=0.000101535, gnorm=0.534, clip=0, loss_scale=64, train_wall=79, gb_free=16.3, wall=17539
2023-07-27 19:28:21 | INFO | train_inner | epoch 014:    346 / 1474 loss=2.076, trans_loss=5.105, nll_loss=2.326, w2v_ctc_loss=0.706, task_loss=1.289, contrastive_loss=0.117, total=4183.43, n_correct=2594.44, ppl=5.02, accuracy=62.017, wps=10580.9, ups=1.26, wpb=8366.9, bsz=322.5, num_updates=19500, lr=0.000101274, gnorm=0.535, clip=0, loss_scale=64, train_wall=78, gb_free=15.7, wall=17619
2023-07-27 19:29:39 | INFO | train_inner | epoch 014:    446 / 1474 loss=2.083, trans_loss=5.123, nll_loss=2.348, w2v_ctc_loss=0.707, task_loss=1.467, contrastive_loss=0.07, total=4102.25, n_correct=2529.4, ppl=5.09, accuracy=61.659, wps=10411.2, ups=1.27, wpb=8204.5, bsz=292.4, num_updates=19600, lr=0.000101015, gnorm=0.531, clip=0, loss_scale=64, train_wall=78, gb_free=16.8, wall=17697
2023-07-27 19:30:59 | INFO | train_inner | epoch 014:    546 / 1474 loss=2.093, trans_loss=5.121, nll_loss=2.346, w2v_ctc_loss=0.726, task_loss=1.469, contrastive_loss=0.112, total=4097.55, n_correct=2522.23, ppl=5.08, accuracy=61.555, wps=10286.1, ups=1.26, wpb=8195.1, bsz=297.6, num_updates=19700, lr=0.000100759, gnorm=0.541, clip=0, loss_scale=64, train_wall=79, gb_free=13, wall=17777
2023-07-27 19:32:17 | INFO | train_inner | epoch 014:    646 / 1474 loss=2.089, trans_loss=5.121, nll_loss=2.347, w2v_ctc_loss=0.712, task_loss=1.395, contrastive_loss=0.154, total=4162.25, n_correct=2564.72, ppl=5.09, accuracy=61.619, wps=10621.2, ups=1.28, wpb=8324.5, bsz=307.9, num_updates=19800, lr=0.000100504, gnorm=0.531, clip=0, loss_scale=128, train_wall=78, gb_free=15.3, wall=17855
2023-07-27 19:33:36 | INFO | train_inner | epoch 014:    746 / 1474 loss=2.076, trans_loss=5.111, nll_loss=2.335, w2v_ctc_loss=0.706, task_loss=1.364, contrastive_loss=0.085, total=4150.31, n_correct=2570.61, ppl=5.04, accuracy=61.938, wps=10599.3, ups=1.28, wpb=8300.6, bsz=310, num_updates=19900, lr=0.000100251, gnorm=0.527, clip=0, loss_scale=128, train_wall=78, gb_free=15.8, wall=17934
2023-07-27 19:34:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 19:34:55 | INFO | train_inner | epoch 014:    847 / 1474 loss=2.079, trans_loss=5.113, nll_loss=2.338, w2v_ctc_loss=0.71, task_loss=1.371, contrastive_loss=0.083, total=4138.93, n_correct=2558.79, ppl=5.06, accuracy=61.823, wps=10387.3, ups=1.25, wpb=8277.9, bsz=309.7, num_updates=20000, lr=0.0001, gnorm=0.53, clip=0, loss_scale=64, train_wall=79, gb_free=17.1, wall=18013
2023-07-27 19:34:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 19:35:27 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.241 | trans_loss 5.615 | nll_loss 2.901 | w2v_ctc_loss 1.362 | task_loss 4.625 | contrastive_loss 0.273 | total 4003.4 | n_correct 2452.9 | ppl 7.47 | accuracy 61.27 | uer 17.936 | wer 19.604 | raw_wer 19.604 | bleu 19.45 | wps 1647.1 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 19.53
2023-07-27 19:35:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-07-27 19:35:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_14_20000.pt
2023-07-27 19:35:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_14_20000.pt
2023-07-27 19:35:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 19.45) (writing took 26.369482653215528 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([56, 27]), X shape:torch.Size([56, 27, 512])
CTC Tokens:tensor([ 0, 67,  0, 19, 19], device='cuda:0'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:0'), New Tokens:tensor([ 0, 67,  0, 19,  0], device='cuda:0')
Org X:tensor([[ 0.2236, -0.7090,  0.8516,  ..., -0.8481,  1.5312, -2.0664],
        [-0.4497,  0.1406, -0.2030,  ...,  0.0649,  1.1611, -2.1621],
        [-0.3550, -0.1074,  1.5264,  ..., -0.0674,  0.4568, -2.3594],
        [-0.2766,  0.6001, -0.1885,  ..., -0.0754,  0.9043, -1.2607],
        [ 1.6436,  1.8164,  2.6074,  ..., -1.4775,  1.2666,  0.2341]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:0'), Mixup Sent Mask:tensor([ True, False, False, False,  True, False, False, False, False, False],
       device='cuda:0'), New X:tensor([[ 0.2236, -0.7090,  0.8516,  ..., -0.8481,  1.5312, -2.0664],
        [-0.4497,  0.1406, -0.2030,  ...,  0.0649,  1.1611, -2.1621],
        [-0.3550, -0.1074,  1.5264,  ..., -0.0674,  0.4568, -2.3594],
        [-0.2766,  0.6001, -0.1885,  ..., -0.0754,  0.9043, -1.2607],
        [ 1.6436,  1.8164,  2.6074,  ..., -1.4775,  1.2666,  0.2341]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:0')
2023-07-27 19:37:13 | INFO | train_inner | epoch 014:    947 / 1474 loss=2.085, trans_loss=5.123, nll_loss=2.351, w2v_ctc_loss=0.713, task_loss=1.425, contrastive_loss=0.087, total=4159.46, n_correct=2559.98, ppl=5.1, accuracy=61.546, wps=6048.9, ups=0.73, wpb=8318.9, bsz=306.7, num_updates=20100, lr=9.97509e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=78, gb_free=15.3, wall=18151
2023-07-27 19:38:31 | INFO | train_inner | epoch 014:   1047 / 1474 loss=2.089, trans_loss=5.128, nll_loss=2.357, w2v_ctc_loss=0.705, task_loss=1.394, contrastive_loss=0.153, total=4155.93, n_correct=2560.01, ppl=5.12, accuracy=61.599, wps=10700.9, ups=1.29, wpb=8311.9, bsz=305.9, num_updates=20200, lr=9.95037e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=77, gb_free=16.3, wall=18229
2023-07-27 19:39:49 | INFO | train_inner | epoch 014:   1147 / 1474 loss=2.107, trans_loss=5.129, nll_loss=2.359, w2v_ctc_loss=0.717, task_loss=1.317, contrastive_loss=0.382, total=4228.09, n_correct=2595.21, ppl=5.13, accuracy=61.38, wps=10765, ups=1.27, wpb=8456.2, bsz=326.3, num_updates=20300, lr=9.92583e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=78, gb_free=17.6, wall=18307
2023-07-27 19:41:08 | INFO | train_inner | epoch 014:   1247 / 1474 loss=2.102, trans_loss=5.145, nll_loss=2.378, w2v_ctc_loss=0.732, task_loss=1.633, contrastive_loss=0.066, total=4027.71, n_correct=2466.12, ppl=5.2, accuracy=61.229, wps=10278, ups=1.28, wpb=8055.4, bsz=273.6, num_updates=20400, lr=9.90148e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=77, gb_free=16.5, wall=18386
2023-07-27 19:42:29 | INFO | train_inner | epoch 014:   1347 / 1474 loss=2.083, trans_loss=5.131, nll_loss=2.362, w2v_ctc_loss=0.706, task_loss=1.344, contrastive_loss=0.085, total=4198.71, n_correct=2585.44, ppl=5.14, accuracy=61.577, wps=10332.9, ups=1.23, wpb=8397.4, bsz=315.4, num_updates=20500, lr=9.8773e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=80, gb_free=16.4, wall=18467
2023-07-27 19:43:41 | INFO | train_inner | epoch 014:   1447 / 1474 loss=2.091, trans_loss=5.139, nll_loss=2.372, w2v_ctc_loss=0.711, task_loss=1.384, contrastive_loss=0.125, total=4140.5, n_correct=2545.24, ppl=5.17, accuracy=61.472, wps=11477.5, ups=1.39, wpb=8281, bsz=307.1, num_updates=20600, lr=9.85329e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=71, gb_free=17.6, wall=18539
2023-07-27 19:44:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 79]), X shape:torch.Size([8, 79, 512])
CTC Tokens:tensor([29, 29, 29,  0, 91], device='cuda:6'), Shrink Mask:tensor([ True, False, False,  True,  True], device='cuda:6'), New Tokens:tensor([29,  0, 91, 12, 17], device='cuda:6')
Org X:tensor([[-0.8711,  0.2305,  0.5044,  ...,  0.3499, -0.3738,  0.6240],
        [-1.3682,  0.5962,  0.5337,  ...,  0.0327, -0.5010,  0.0358],
        [-1.0498,  0.5259,  0.4512,  ...,  0.1550, -0.5093,  0.4536],
        [ 0.3086,  0.3843,  0.9009,  ...,  0.2207, -0.0630,  0.8643],
        [-0.5376,  0.7441, -2.1191,  ...,  0.2192, -2.2188, -0.1720]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([ True, False, False, False,  True, False, False, False],
       device='cuda:6'), New X:tensor([[-0.8711,  0.2305,  0.5044,  ...,  0.3499, -0.3738,  0.6240],
        [-1.3682,  0.5962,  0.5337,  ...,  0.0327, -0.5010,  0.0358],
        [-1.0498,  0.5259,  0.4512,  ...,  0.1550, -0.5093,  0.4536],
        [ 0.3086,  0.3843,  0.9009,  ...,  0.2207, -0.0630,  0.8643],
        [-0.5376,  0.7441, -2.1191,  ...,  0.2192, -2.2188, -0.1720]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 98]), X shape:torch.Size([8, 98, 512])
CTC Tokens:tensor([ 0, 21, 34,  0,  0], device='cuda:4'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:4'), New Tokens:tensor([ 0, 21, 34,  0, 29], device='cuda:4')
Org X:tensor([[ 0.1678, -0.1567, -0.8047,  ..., -1.3037,  0.6353, -0.7778],
        [-0.2622,  0.3096, -0.2822,  ..., -2.4219,  0.7407, -0.1803],
        [-0.2620,  0.1934,  0.1379,  ..., -2.2422, -0.3457, -0.3540],
        [-0.9556, -0.0786,  1.0547,  ..., -1.3936, -0.5757, -0.0690],
        [-1.7920,  0.6401, -0.3149,  ...,  0.1099, -1.3213, -0.0220]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:4'), Mixup Sent Mask:tensor([ True, False, False, False,  True, False, False, False],
       device='cuda:4'), New X:tensor([[ 0.1678, -0.1567, -0.8047,  ..., -1.3037,  0.6353, -0.7778],
        [-0.2622,  0.3096, -0.2822,  ..., -2.4219,  0.7407, -0.1803],
        [-0.2620,  0.1934,  0.1379,  ..., -2.2422, -0.3457, -0.3540],
        [-0.9556, -0.0786,  1.0547,  ..., -1.3936, -0.5757, -0.0690],
        [-1.7920,  0.6401, -0.3149,  ...,  0.1099, -1.3213, -0.0220]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([56, 28]), X shape:torch.Size([56, 28, 512])
CTC Tokens:tensor([8, 8, 8, 0, 0], device='cuda:3'), Shrink Mask:tensor([ True, False, False,  True, False], device='cuda:3'), New Tokens:tensor([ 8,  0, 24,  0, 66], device='cuda:3')
Org X:tensor([[ 0.4321, -0.4744,  0.3735,  ...,  0.3042, -0.6519, -0.8711],
        [ 0.5508,  0.0227,  0.4375,  ..., -0.3906,  0.6782, -0.4578],
        [-0.7559,  0.2312,  0.2520,  ...,  0.0192,  1.0547, -0.4001],
        [ 0.5215,  0.4939,  0.8730,  ...,  0.3960,  1.0010, -0.5215],
        [-0.2869,  0.8774, -0.0769,  ...,  0.2029, -0.0400, -0.1545]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:3'), Mixup Sent Mask:tensor([ True, False, False, False,  True, False, False, False, False, False],
       device='cuda:3'), New X:tensor([[ 0.4321, -0.4744,  0.3735,  ...,  0.3042, -0.6519, -0.8711],
        [ 0.5508,  0.0227,  0.4375,  ..., -0.3906,  0.6782, -0.4578],
        [-0.7559,  0.2312,  0.2520,  ...,  0.0192,  1.0547, -0.4001],
        [ 0.5215,  0.4939,  0.8730,  ...,  0.3960,  1.0010, -0.5215],
        [-0.2869,  0.8774, -0.0769,  ...,  0.2029, -0.0400, -0.1545]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 85]), X shape:torch.Size([8, 85, 512])
CTC Tokens:tensor([ 0,  9, 33,  0,  0], device='cuda:5'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:5'), New Tokens:tensor([   0,    9,   33,    0, 1165], device='cuda:5')
Org X:tensor([[ 0.9253, -1.8359,  0.2959,  ..., -0.4302, -0.7686, -0.1440],
        [ 0.7817, -0.1164, -0.4150,  ...,  0.1938, -0.6030,  0.3647],
        [-0.8257,  0.3542, -1.3057,  ..., -0.0141, -0.4597,  0.1893],
        [-0.8384, -0.1926,  0.5347,  ..., -1.4551,  0.3079,  0.3254],
        [-0.4321,  0.1333,  0.6450,  ..., -0.9912,  1.8057,  0.0334]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([ True, False, False, False,  True, False, False, False],
       device='cuda:5'), New X:tensor([[ 0.9253, -1.8359,  0.2959,  ..., -0.4302, -0.7686, -0.1440],
        [ 0.7817, -0.1164, -0.4150,  ...,  0.1938, -0.6030,  0.3647],
        [-0.8257,  0.3542, -1.3057,  ..., -0.0141, -0.4597,  0.1893],
        [-0.8384, -0.1926,  0.5347,  ..., -1.4551,  0.3079,  0.3254],
        [-0.4321,  0.1333,  0.6450,  ..., -0.9912,  1.8057,  0.0334]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([5, 222]), X shape:torch.Size([5, 222, 512])
CTC Tokens:tensor([0, 8, 0, 0, 0], device='cuda:2'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:2'), New Tokens:tensor([  0,   8,   0, 115,   0], device='cuda:2')
Org X:tensor([[ 0.3987, -0.0357,  0.2510,  ...,  0.0419, -0.6968, -1.4688],
        [-0.5278,  0.2249, -0.0640,  ...,  0.1181, -1.3066, -0.6694],
        [ 0.4600,  0.3342,  1.4824,  ..., -0.6890,  0.0818, -0.7500],
        [ 0.0579,  0.8945, -0.3916,  ..., -0.3328, -1.0781,  0.6406],
        [-0.0631,  0.6963,  0.5752,  ..., -1.0908, -0.5361,  0.1860]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:2'), Mixup Sent Mask:tensor([ True, False, False, False,  True], device='cuda:2'), New X:tensor([[ 0.3987, -0.0357,  0.2510,  ...,  0.0419, -0.6968, -1.4688],
        [-0.5278,  0.2249, -0.0640,  ...,  0.1181, -1.3066, -0.6694],
        [ 0.4600,  0.3342,  1.4824,  ..., -0.6890,  0.0818, -0.7500],
        [ 0.0579,  0.8945, -0.3916,  ..., -0.3328, -1.0781,  0.6406],
        [-0.0631,  0.6963,  0.5752,  ..., -1.0908, -0.5361,  0.1860]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([80, 21]), X shape:torch.Size([80, 21, 512])
CTC Tokens:tensor([ 25,  25,  73, 150, 150], device='cuda:1'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:1'), New Tokens:tensor([ 25,  73, 150,   7,   0], device='cuda:1')
Org X:tensor([[ 1.4746e-01, -1.3994e+00, -5.2979e-01,  ...,  2.6440e-01,
          1.3945e+00, -2.6196e-01],
        [ 4.4531e-01,  1.7773e-01,  3.4326e-01,  ...,  3.7476e-01,
         -1.3506e+00, -4.2017e-01],
        [ 5.0977e-01,  4.8218e-01, -3.9673e-04,  ...,  4.3701e-01,
         -8.2764e-01, -9.8193e-01],
        [ 4.7705e-01,  3.5767e-01, -3.5859e+00,  ...,  2.5635e-01,
         -6.1768e-01, -1.3164e+00],
        [ 1.0449e+00, -6.3477e-01,  2.2012e+00,  ..., -5.3516e-01,
          1.0273e+00,  1.2314e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:1'), Mixup Sent Mask:tensor([ True, False, False, False,  True, False, False, False, False, False],
       device='cuda:1'), New X:tensor([[ 1.4746e-01, -1.3994e+00, -5.2979e-01,  ...,  2.6440e-01,
          1.3945e+00, -2.6196e-01],
        [ 4.4531e-01,  1.7773e-01,  3.4326e-01,  ...,  3.7476e-01,
         -1.3506e+00, -4.2017e-01],
        [ 5.0977e-01,  4.8218e-01, -3.9673e-04,  ...,  4.3701e-01,
         -8.2764e-01, -9.8193e-01],
        [ 4.7705e-01,  3.5767e-01, -3.5859e+00,  ...,  2.5635e-01,
         -6.1768e-01, -1.3164e+00],
        [ 1.0449e+00, -6.3477e-01,  2.2012e+00,  ..., -5.3516e-01,
          1.0273e+00,  1.2314e+00]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 73]), X shape:torch.Size([16, 73, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:7'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:7'), New Tokens:tensor([   0, 8433,    0, 1586,    0], device='cuda:7')
Org X:tensor([[ 0.1782, -0.7427,  0.5894,  ...,  0.5024, -0.0843,  0.5161],
        [ 0.9692,  1.0488,  2.2461,  ...,  0.2720,  1.1221, -0.9790],
        [ 1.2383,  0.4287,  2.6270,  ...,  0.2710,  1.5537, -1.3623],
        [ 0.7139,  0.5869,  0.2008,  ...,  0.1816,  1.1191, -0.4727],
        [ 0.5317,  0.8091,  1.4775,  ..., -0.2937,  0.9585, -0.3499]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:7'), Mixup Sent Mask:tensor([ True, False, False, False,  True, False, False, False, False, False],
       device='cuda:7'), New X:tensor([[ 0.1782, -0.7427,  0.5894,  ...,  0.5024, -0.0843,  0.5161],
        [ 0.9692,  1.0488,  2.2461,  ...,  0.2720,  1.1221, -0.9790],
        [ 1.2383,  0.4287,  2.6270,  ...,  0.2710,  1.5537, -1.3623],
        [ 0.7139,  0.5869,  0.2008,  ...,  0.1816,  1.1191, -0.4727],
        [ 0.5317,  0.8091,  1.4775,  ..., -0.2937,  0.9585, -0.3499]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:7')
2023-07-27 19:44:29 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.232 | trans_loss 5.608 | nll_loss 2.9 | w2v_ctc_loss 1.347 | task_loss 4.606 | contrastive_loss 0.274 | total 4003.4 | n_correct 2452.4 | ppl 7.46 | accuracy 61.258 | uer 17.854 | wer 19.671 | raw_wer 19.671 | bleu 19.4 | wps 1647.1 | wpb 4003.4 | bsz 141.8 | num_updates 20627 | best_bleu 19.53
2023-07-27 19:44:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20627 updates
2023-07-27 19:44:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.4009.pt
2023-07-27 19:44:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.4009.pt
2023-07-27 19:44:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.4009.pt (epoch 14 @ 20627 updates, score 19.4) (writing took 15.234391745179892 seconds)
2023-07-27 19:44:45 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-07-27 19:44:45 | INFO | train | epoch 014 | loss 2.086 | trans_loss 5.121 | nll_loss 2.347 | w2v_ctc_loss 0.712 | task_loss 1.403 | contrastive_loss 0.127 | total 4137.12 | n_correct 2551.56 | ppl 5.09 | accuracy 61.675 | wps 9602 | ups 1.16 | wpb 8274.2 | bsz 305.2 | num_updates 20627 | lr 9.84684e-05 | gnorm 0.532 | clip 0 | loss_scale 64 | train_wall 1143 | gb_free 16.4 | wall 18603
2023-07-27 19:44:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 19:44:45 | INFO | fairseq.trainer | begin training epoch 15
2023-07-27 19:44:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 19:45:50 | INFO | train_inner | epoch 015:     73 / 1474 loss=2.081, trans_loss=5.108, nll_loss=2.33, w2v_ctc_loss=0.703, task_loss=1.402, contrastive_loss=0.172, total=4083.93, n_correct=2528.5, ppl=5.03, accuracy=61.913, wps=6338.8, ups=0.78, wpb=8167.9, bsz=300.1, num_updates=20700, lr=9.82946e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=73, gb_free=15.6, wall=18668
2023-07-27 19:47:09 | INFO | train_inner | epoch 015:    173 / 1474 loss=2.074, trans_loss=5.096, nll_loss=2.314, w2v_ctc_loss=0.711, task_loss=1.452, contrastive_loss=0.083, total=4122.67, n_correct=2564.57, ppl=4.97, accuracy=62.207, wps=10494.1, ups=1.27, wpb=8245.3, bsz=299.1, num_updates=20800, lr=9.80581e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=78, gb_free=17.2, wall=18746
2023-07-27 19:48:26 | INFO | train_inner | epoch 015:    273 / 1474 loss=2.064, trans_loss=5.096, nll_loss=2.315, w2v_ctc_loss=0.694, task_loss=1.346, contrastive_loss=0.071, total=4190.11, n_correct=2609, ppl=4.98, accuracy=62.266, wps=10751.7, ups=1.28, wpb=8380.2, bsz=312.6, num_updates=20900, lr=9.78232e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=77, gb_free=17.1, wall=18824
2023-07-27 19:49:44 | INFO | train_inner | epoch 015:    373 / 1474 loss=2.071, trans_loss=5.093, nll_loss=2.31, w2v_ctc_loss=0.701, task_loss=1.439, contrastive_loss=0.097, total=4150.33, n_correct=2576.12, ppl=4.96, accuracy=62.07, wps=10776.1, ups=1.3, wpb=8300.7, bsz=301, num_updates=21000, lr=9.759e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=76, gb_free=15.5, wall=18901
2023-07-27 19:50:59 | INFO | train_inner | epoch 015:    473 / 1474 loss=2.074, trans_loss=5.099, nll_loss=2.318, w2v_ctc_loss=0.689, task_loss=1.436, contrastive_loss=0.191, total=4082.7, n_correct=2532.84, ppl=4.99, accuracy=62.038, wps=10850.4, ups=1.33, wpb=8165.4, bsz=298.5, num_updates=21100, lr=9.73585e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=74, gb_free=17.1, wall=18977
2023-07-27 19:52:14 | INFO | train_inner | epoch 015:    573 / 1474 loss=2.073, trans_loss=5.1, nll_loss=2.319, w2v_ctc_loss=0.705, task_loss=1.483, contrastive_loss=0.077, total=4130.96, n_correct=2562.98, ppl=4.99, accuracy=62.043, wps=10937.3, ups=1.32, wpb=8261.9, bsz=293.4, num_updates=21200, lr=9.71286e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=75, gb_free=17.2, wall=19052
2023-07-27 19:53:33 | INFO | train_inner | epoch 015:    673 / 1474 loss=2.078, trans_loss=5.101, nll_loss=2.322, w2v_ctc_loss=0.705, task_loss=1.392, contrastive_loss=0.155, total=4138.41, n_correct=2569.03, ppl=5, accuracy=62.078, wps=10565.3, ups=1.28, wpb=8276.8, bsz=309.4, num_updates=21300, lr=9.69003e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=77, gb_free=16.9, wall=19131
2023-07-27 19:54:51 | INFO | train_inner | epoch 015:    773 / 1474 loss=2.077, trans_loss=5.11, nll_loss=2.332, w2v_ctc_loss=0.708, task_loss=1.412, contrastive_loss=0.101, total=4186.48, n_correct=2594.16, ppl=5.04, accuracy=61.965, wps=10663.2, ups=1.27, wpb=8373, bsz=307.9, num_updates=21400, lr=9.66736e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=78, gb_free=16.6, wall=19209
2023-07-27 19:56:11 | INFO | train_inner | epoch 015:    873 / 1474 loss=2.084, trans_loss=5.118, nll_loss=2.344, w2v_ctc_loss=0.715, task_loss=1.524, contrastive_loss=0.077, total=4054.09, n_correct=2501.84, ppl=5.08, accuracy=61.712, wps=10213.4, ups=1.26, wpb=8108.2, bsz=286.2, num_updates=21500, lr=9.64486e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=78, gb_free=15.9, wall=19289
2023-07-27 19:57:32 | INFO | train_inner | epoch 015:    973 / 1474 loss=2.081, trans_loss=5.112, nll_loss=2.337, w2v_ctc_loss=0.701, task_loss=1.401, contrastive_loss=0.163, total=4126.63, n_correct=2554.88, ppl=5.05, accuracy=61.912, wps=10106, ups=1.22, wpb=8253.3, bsz=303, num_updates=21600, lr=9.6225e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=81, gb_free=16.8, wall=19370
2023-07-27 19:58:53 | INFO | train_inner | epoch 015:   1073 / 1474 loss=2.091, trans_loss=5.114, nll_loss=2.34, w2v_ctc_loss=0.704, task_loss=1.307, contrastive_loss=0.324, total=4199.33, n_correct=2594.37, ppl=5.06, accuracy=61.781, wps=10420.6, ups=1.24, wpb=8398.7, bsz=327.3, num_updates=21700, lr=9.60031e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=80, gb_free=11.4, wall=19451
2023-07-27 20:00:11 | INFO | train_inner | epoch 015:   1173 / 1474 loss=2.063, trans_loss=5.102, nll_loss=2.326, w2v_ctc_loss=0.686, task_loss=1.274, contrastive_loss=0.129, total=4172.81, n_correct=2597.99, ppl=5.02, accuracy=62.26, wps=10738.4, ups=1.29, wpb=8345.6, bsz=326.4, num_updates=21800, lr=9.57826e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=77, gb_free=12.7, wall=19529
2023-07-27 20:01:23 | INFO | train_inner | epoch 015:   1273 / 1474 loss=2.081, trans_loss=5.114, nll_loss=2.339, w2v_ctc_loss=0.714, task_loss=1.418, contrastive_loss=0.084, total=4152.76, n_correct=2568.7, ppl=5.06, accuracy=61.855, wps=11518.1, ups=1.39, wpb=8305.5, bsz=304.7, num_updates=21900, lr=9.55637e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=72, gb_free=16.6, wall=19601
2023-07-27 20:02:32 | INFO | train_inner | epoch 015:   1373 / 1474 loss=2.075, trans_loss=5.116, nll_loss=2.341, w2v_ctc_loss=0.703, task_loss=1.451, contrastive_loss=0.065, total=4107.77, n_correct=2542.15, ppl=5.07, accuracy=61.886, wps=11805.7, ups=1.44, wpb=8215.5, bsz=294.1, num_updates=22000, lr=9.53463e-05, gnorm=0.539, clip=0, loss_scale=128, train_wall=69, gb_free=15.2, wall=19670
2023-07-27 20:02:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 20:02:56 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.196 | trans_loss 5.601 | nll_loss 2.885 | w2v_ctc_loss 1.247 | task_loss 4.621 | contrastive_loss 0.269 | total 4003.4 | n_correct 2455.3 | ppl 7.39 | accuracy 61.33 | uer 17.333 | wer 19.071 | raw_wer 19.071 | bleu 19.38 | wps 2073.8 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 19.53
2023-07-27 20:02:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-07-27 20:02:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_15_22000.pt
2023-07-27 20:02:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_15_22000.pt
2023-07-27 20:03:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 19.38) (writing took 17.177477134391665 seconds)
2023-07-27 20:04:30 | INFO | train_inner | epoch 015:   1473 / 1474 loss=2.084, trans_loss=5.121, nll_loss=2.35, w2v_ctc_loss=0.705, task_loss=1.364, contrastive_loss=0.163, total=4157.38, n_correct=2569.59, ppl=5.1, accuracy=61.808, wps=7080.2, ups=0.85, wpb=8314.8, bsz=314.8, num_updates=22100, lr=9.51303e-05, gnorm=0.531, clip=0, loss_scale=128, train_wall=75, gb_free=17.2, wall=19788
2023-07-27 20:04:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 20:05:00 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.229 | trans_loss 5.599 | nll_loss 2.885 | w2v_ctc_loss 1.362 | task_loss 4.621 | contrastive_loss 0.268 | total 4003.4 | n_correct 2454.2 | ppl 7.39 | accuracy 61.303 | uer 17.455 | wer 19.242 | raw_wer 19.242 | bleu 19.68 | wps 1720.2 | wpb 4003.4 | bsz 141.8 | num_updates 22101 | best_bleu 19.68
2023-07-27 20:05:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22101 updates
2023-07-27 20:05:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 20:05:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 20:05:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 15 @ 22101 updates, score 19.68) (writing took 23.01167400367558 seconds)
2023-07-27 20:05:24 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-07-27 20:05:24 | INFO | train | epoch 015 | loss 2.076 | trans_loss 5.106 | nll_loss 2.328 | w2v_ctc_loss 0.702 | task_loss 1.402 | contrastive_loss 0.132 | total 4138.65 | n_correct 2566.47 | ppl 5.02 | accuracy 62.012 | wps 9844.8 | ups 1.19 | wpb 8277.3 | bsz 305.7 | num_updates 22101 | lr 9.51281e-05 | gnorm 0.534 | clip 0 | loss_scale 128 | train_wall 1121 | gb_free 16.9 | wall 19842
2023-07-27 20:05:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 20:05:24 | INFO | fairseq.trainer | begin training epoch 16
2023-07-27 20:05:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 20:06:52 | INFO | train_inner | epoch 016:     99 / 1474 loss=2.054, trans_loss=5.073, nll_loss=2.285, w2v_ctc_loss=0.69, task_loss=1.322, contrastive_loss=0.103, total=4113.74, n_correct=2576.89, ppl=4.88, accuracy=62.641, wps=5803.2, ups=0.71, wpb=8227.5, bsz=316, num_updates=22200, lr=9.49158e-05, gnorm=0.539, clip=0, loss_scale=128, train_wall=77, gb_free=17.5, wall=19929
2023-07-27 20:08:11 | INFO | train_inner | epoch 016:    199 / 1474 loss=2.053, trans_loss=5.072, nll_loss=2.283, w2v_ctc_loss=0.685, task_loss=1.455, contrastive_loss=0.072, total=4091.27, n_correct=2565.56, ppl=4.87, accuracy=62.708, wps=10270.6, ups=1.26, wpb=8182.5, bsz=294.3, num_updates=22300, lr=9.47027e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=79, gb_free=14.7, wall=20009
2023-07-27 20:09:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 20:09:34 | INFO | train_inner | epoch 016:    300 / 1474 loss=2.061, trans_loss=5.083, nll_loss=2.298, w2v_ctc_loss=0.696, task_loss=1.406, contrastive_loss=0.081, total=4158.96, n_correct=2599.63, ppl=4.92, accuracy=62.507, wps=10083.4, ups=1.21, wpb=8317.9, bsz=305.6, num_updates=22400, lr=9.44911e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=81, gb_free=17.2, wall=20092
2023-07-27 20:10:49 | INFO | train_inner | epoch 016:    400 / 1474 loss=2.073, trans_loss=5.086, nll_loss=2.301, w2v_ctc_loss=0.702, task_loss=1.494, contrastive_loss=0.16, total=4073.3, n_correct=2536.28, ppl=4.93, accuracy=62.266, wps=10790.5, ups=1.32, wpb=8146.6, bsz=288.1, num_updates=22500, lr=9.42809e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=75, gb_free=17, wall=20167
2023-07-27 20:11:59 | INFO | train_inner | epoch 016:    500 / 1474 loss=2.059, trans_loss=5.082, nll_loss=2.299, w2v_ctc_loss=0.691, task_loss=1.345, contrastive_loss=0.11, total=4174.67, n_correct=2614.17, ppl=4.92, accuracy=62.62, wps=11973.4, ups=1.43, wpb=8349.3, bsz=319.1, num_updates=22600, lr=9.40721e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=15.9, wall=20237
2023-07-27 20:13:08 | INFO | train_inner | epoch 016:    600 / 1474 loss=2.06, trans_loss=5.087, nll_loss=2.303, w2v_ctc_loss=0.691, task_loss=1.415, contrastive_loss=0.068, total=4124.65, n_correct=2573.66, ppl=4.94, accuracy=62.397, wps=11875.5, ups=1.44, wpb=8249.3, bsz=297.6, num_updates=22700, lr=9.38647e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=20306
2023-07-27 20:14:17 | INFO | train_inner | epoch 016:    700 / 1474 loss=2.063, trans_loss=5.091, nll_loss=2.308, w2v_ctc_loss=0.696, task_loss=1.435, contrastive_loss=0.071, total=4095.49, n_correct=2555.81, ppl=4.95, accuracy=62.405, wps=11888.8, ups=1.45, wpb=8191, bsz=296.3, num_updates=22800, lr=9.36586e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=68, gb_free=16.1, wall=20375
2023-07-27 20:15:27 | INFO | train_inner | epoch 016:    800 / 1474 loss=2.062, trans_loss=5.09, nll_loss=2.308, w2v_ctc_loss=0.688, task_loss=1.347, contrastive_loss=0.133, total=4174.94, n_correct=2605.68, ppl=4.95, accuracy=62.412, wps=12045, ups=1.44, wpb=8349.9, bsz=310.9, num_updates=22900, lr=9.34539e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=20445
2023-07-27 20:16:36 | INFO | train_inner | epoch 016:    900 / 1474 loss=2.062, trans_loss=5.088, nll_loss=2.307, w2v_ctc_loss=0.691, task_loss=1.358, contrastive_loss=0.128, total=4163.19, n_correct=2601.9, ppl=4.95, accuracy=62.498, wps=12000, ups=1.44, wpb=8326.4, bsz=310.6, num_updates=23000, lr=9.32505e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=16.7, wall=20514
2023-07-27 20:17:46 | INFO | train_inner | epoch 016:   1000 / 1474 loss=2.078, trans_loss=5.105, nll_loss=2.328, w2v_ctc_loss=0.708, task_loss=1.467, contrastive_loss=0.126, total=4103.45, n_correct=2543.37, ppl=5.02, accuracy=61.981, wps=11691.3, ups=1.42, wpb=8206.9, bsz=298.1, num_updates=23100, lr=9.30484e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=70, gb_free=14.8, wall=20584
2023-07-27 20:18:56 | INFO | train_inner | epoch 016:   1100 / 1474 loss=2.081, trans_loss=5.112, nll_loss=2.337, w2v_ctc_loss=0.713, task_loss=1.494, contrastive_loss=0.103, total=4119.27, n_correct=2550.84, ppl=5.05, accuracy=61.925, wps=11778, ups=1.43, wpb=8238.5, bsz=295.3, num_updates=23200, lr=9.28477e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=69, gb_free=17.8, wall=20654
2023-07-27 20:20:07 | INFO | train_inner | epoch 016:   1200 / 1474 loss=2.07, trans_loss=5.101, nll_loss=2.324, w2v_ctc_loss=0.684, task_loss=1.416, contrastive_loss=0.195, total=4165.11, n_correct=2589.19, ppl=5.01, accuracy=62.164, wps=11758.1, ups=1.41, wpb=8330.2, bsz=308.7, num_updates=23300, lr=9.26482e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=70, gb_free=16.7, wall=20725
2023-07-27 20:21:17 | INFO | train_inner | epoch 016:   1300 / 1474 loss=2.075, trans_loss=5.103, nll_loss=2.325, w2v_ctc_loss=0.702, task_loss=1.377, contrastive_loss=0.172, total=4134.61, n_correct=2570.86, ppl=5.01, accuracy=62.179, wps=11805.3, ups=1.43, wpb=8269.2, bsz=310.8, num_updates=23400, lr=9.245e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=16.7, wall=20795
2023-07-27 20:22:27 | INFO | train_inner | epoch 016:   1400 / 1474 loss=2.068, trans_loss=5.102, nll_loss=2.325, w2v_ctc_loss=0.7, task_loss=1.335, contrastive_loss=0.108, total=4206.33, n_correct=2615.25, ppl=5.01, accuracy=62.174, wps=11989.8, ups=1.43, wpb=8412.7, bsz=322.2, num_updates=23500, lr=9.22531e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=69, gb_free=15.5, wall=20865
2023-07-27 20:23:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 20:23:44 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 4.214 | trans_loss 5.593 | nll_loss 2.876 | w2v_ctc_loss 1.326 | task_loss 4.619 | contrastive_loss 0.266 | total 4003.4 | n_correct 2465.5 | ppl 7.34 | accuracy 61.585 | uer 17.434 | wer 19 | raw_wer 19 | bleu 20.08 | wps 1921.5 | wpb 4003.4 | bsz 141.8 | num_updates 23574 | best_bleu 20.08
2023-07-27 20:23:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23574 updates
2023-07-27 20:23:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 20:23:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 20:24:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 16 @ 23574 updates, score 20.08) (writing took 20.319163909181952 seconds)
2023-07-27 20:24:05 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-07-27 20:24:05 | INFO | train | epoch 016 | loss 2.066 | trans_loss 5.092 | nll_loss 2.31 | w2v_ctc_loss 0.695 | task_loss 1.403 | contrastive_loss 0.126 | total 4137.91 | n_correct 2579.49 | ppl 4.96 | accuracy 62.338 | wps 10871.2 | ups 1.31 | wpb 8275.8 | bsz 305.3 | num_updates 23574 | lr 9.21082e-05 | gnorm 0.537 | clip 0 | loss_scale 64 | train_wall 1053 | gb_free 15.4 | wall 20963
2023-07-27 20:24:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 20:24:06 | INFO | fairseq.trainer | begin training epoch 17
2023-07-27 20:24:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 20:24:33 | INFO | train_inner | epoch 017:     26 / 1474 loss=2.068, trans_loss=5.086, nll_loss=2.304, w2v_ctc_loss=0.688, task_loss=1.411, contrastive_loss=0.237, total=4152.31, n_correct=2591.76, ppl=4.94, accuracy=62.417, wps=6591.7, ups=0.79, wpb=8304.6, bsz=304.6, num_updates=23600, lr=9.20575e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=70, gb_free=13.7, wall=20991
2023-07-27 20:25:43 | INFO | train_inner | epoch 017:    126 / 1474 loss=2.052, trans_loss=5.062, nll_loss=2.27, w2v_ctc_loss=0.693, task_loss=1.447, contrastive_loss=0.074, total=4118.91, n_correct=2591.8, ppl=4.82, accuracy=62.924, wps=11777, ups=1.43, wpb=8237.8, bsz=295.8, num_updates=23700, lr=9.1863e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=21061
2023-07-27 20:26:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-27 20:26:54 | INFO | train_inner | epoch 017:    227 / 1474 loss=2.049, trans_loss=5.061, nll_loss=2.271, w2v_ctc_loss=0.677, task_loss=1.367, contrastive_loss=0.134, total=4123.81, n_correct=2597.83, ppl=4.83, accuracy=62.996, wps=11664.2, ups=1.41, wpb=8247.6, bsz=308.2, num_updates=23800, lr=9.16698e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=70, gb_free=16.9, wall=21132
2023-07-27 20:28:03 | INFO | train_inner | epoch 017:    327 / 1474 loss=2.059, trans_loss=5.07, nll_loss=2.282, w2v_ctc_loss=0.68, task_loss=1.395, contrastive_loss=0.244, total=4156.91, n_correct=2609.81, ppl=4.86, accuracy=62.782, wps=11995.9, ups=1.44, wpb=8313.8, bsz=305.7, num_updates=23900, lr=9.14779e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=21201
2023-07-27 20:29:14 | INFO | train_inner | epoch 017:    427 / 1474 loss=2.046, trans_loss=5.068, nll_loss=2.28, w2v_ctc_loss=0.682, task_loss=1.39, contrastive_loss=0.072, total=4146.43, n_correct=2610.35, ppl=4.86, accuracy=62.954, wps=11806.4, ups=1.42, wpb=8292.9, bsz=308, num_updates=24000, lr=9.12871e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=70, gb_free=16.2, wall=21271
2023-07-27 20:29:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 20:29:39 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.226 | trans_loss 5.603 | nll_loss 2.885 | w2v_ctc_loss 1.346 | task_loss 4.609 | contrastive_loss 0.265 | total 4003.4 | n_correct 2459.1 | ppl 7.39 | accuracy 61.425 | uer 17.448 | wer 19.138 | raw_wer 19.138 | bleu 19.59 | wps 1973.5 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 20.08
2023-07-27 20:29:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-07-27 20:29:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_17_24000.pt
2023-07-27 20:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_17_24000.pt
2023-07-27 20:29:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 19.59) (writing took 16.768056843429804 seconds)
2023-07-27 20:31:07 | INFO | train_inner | epoch 017:    527 / 1474 loss=2.057, trans_loss=5.074, nll_loss=2.288, w2v_ctc_loss=0.689, task_loss=1.455, contrastive_loss=0.124, total=4182.1, n_correct=2620.59, ppl=4.88, accuracy=62.662, wps=7377.7, ups=0.88, wpb=8364.2, bsz=307.9, num_updates=24100, lr=9.10975e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=70, gb_free=16.7, wall=21385
2023-07-27 20:32:16 | INFO | train_inner | epoch 017:    627 / 1474 loss=2.052, trans_loss=5.077, nll_loss=2.291, w2v_ctc_loss=0.685, task_loss=1.41, contrastive_loss=0.067, total=4167.27, n_correct=2616.54, ppl=4.9, accuracy=62.788, wps=12039, ups=1.44, wpb=8334.5, bsz=302.2, num_updates=24200, lr=9.09091e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=68, gb_free=10.6, wall=21454
2023-07-27 20:33:26 | INFO | train_inner | epoch 017:    727 / 1474 loss=2.065, trans_loss=5.085, nll_loss=2.302, w2v_ctc_loss=0.697, task_loss=1.385, contrastive_loss=0.123, total=4166.12, n_correct=2603.41, ppl=4.93, accuracy=62.49, wps=12004.6, ups=1.44, wpb=8332.2, bsz=308.1, num_updates=24300, lr=9.07218e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=21523
2023-07-27 20:34:35 | INFO | train_inner | epoch 017:    827 / 1474 loss=2.056, trans_loss=5.082, nll_loss=2.297, w2v_ctc_loss=0.687, task_loss=1.419, contrastive_loss=0.081, total=4091.64, n_correct=2562.41, ppl=4.91, accuracy=62.625, wps=11798.3, ups=1.44, wpb=8183.3, bsz=295.3, num_updates=24400, lr=9.05357e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=69, gb_free=17.3, wall=21593
2023-07-27 20:35:43 | INFO | train_inner | epoch 017:    927 / 1474 loss=2.053, trans_loss=5.082, nll_loss=2.298, w2v_ctc_loss=0.683, task_loss=1.381, contrastive_loss=0.079, total=4106.83, n_correct=2569.31, ppl=4.92, accuracy=62.562, wps=12064.7, ups=1.47, wpb=8213.7, bsz=304.6, num_updates=24500, lr=9.03508e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=67, gb_free=15.8, wall=21661
2023-07-27 20:36:52 | INFO | train_inner | epoch 017:   1027 / 1474 loss=2.054, trans_loss=5.081, nll_loss=2.298, w2v_ctc_loss=0.688, task_loss=1.385, contrastive_loss=0.085, total=4115.49, n_correct=2578.09, ppl=4.92, accuracy=62.644, wps=11866.5, ups=1.44, wpb=8231, bsz=305.8, num_updates=24600, lr=9.0167e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=21730
2023-07-27 20:38:02 | INFO | train_inner | epoch 017:   1127 / 1474 loss=2.054, trans_loss=5.084, nll_loss=2.301, w2v_ctc_loss=0.683, task_loss=1.457, contrastive_loss=0.068, total=4078.39, n_correct=2553.55, ppl=4.93, accuracy=62.612, wps=11771.8, ups=1.44, wpb=8156.8, bsz=293.7, num_updates=24700, lr=8.99843e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=21800
2023-07-27 20:39:12 | INFO | train_inner | epoch 017:   1227 / 1474 loss=2.073, trans_loss=5.091, nll_loss=2.312, w2v_ctc_loss=0.683, task_loss=1.359, contrastive_loss=0.319, total=4173.49, n_correct=2601.08, ppl=4.97, accuracy=62.324, wps=11945.2, ups=1.43, wpb=8347, bsz=323.7, num_updates=24800, lr=8.98027e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=69, gb_free=16, wall=21869
2023-07-27 20:40:21 | INFO | train_inner | epoch 017:   1327 / 1474 loss=2.058, trans_loss=5.088, nll_loss=2.307, w2v_ctc_loss=0.675, task_loss=1.385, contrastive_loss=0.155, total=4156.28, n_correct=2597.87, ppl=4.95, accuracy=62.505, wps=11961.2, ups=1.44, wpb=8312.6, bsz=308, num_updates=24900, lr=8.96221e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=69, gb_free=17.8, wall=21939
2023-07-27 20:41:31 | INFO | train_inner | epoch 017:   1427 / 1474 loss=2.055, trans_loss=5.089, nll_loss=2.307, w2v_ctc_loss=0.684, task_loss=1.411, contrastive_loss=0.073, total=4112.95, n_correct=2566.47, ppl=4.95, accuracy=62.4, wps=11785.7, ups=1.43, wpb=8225.9, bsz=303.2, num_updates=25000, lr=8.94427e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=22009
Mixup rate:0.5, token after shrink shape:torch.Size([8, 87]), X shape:torch.Size([8, 87, 512])
CTC Tokens:tensor([   0,    7,    0, 4226,    0], device='cuda:0'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:0'), New Tokens:tensor([   0,    7,    0, 4226,    0], device='cuda:0')
Org X:tensor([[-0.0233,  0.1844, -0.0858,  ..., -0.1688, -0.2404, -1.8320],
        [ 0.0504,  0.4331, -1.0596,  ...,  0.0403,  0.6694, -1.0361],
        [-0.1317,  0.4995,  2.5605,  ..., -0.5488,  1.2148,  0.1190],
        [-0.8652,  0.2825,  1.3906,  ..., -0.8462,  0.5410,  0.1051],
        [-1.1338,  0.3577,  2.0586,  ..., -0.9165,  0.6504,  0.5698]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:0'), Mixup Sent Mask:tensor([ True, False, False,  True, False, False, False, False],
       device='cuda:0'), New X:tensor([[-0.0233,  0.1844, -0.0858,  ..., -0.1688, -0.2404, -1.8320],
        [ 0.0504,  0.4331, -1.0596,  ...,  0.0403,  0.6694, -1.0361],
        [-0.1317,  0.4995,  2.5605,  ..., -0.5488,  1.2148,  0.1190],
        [-0.8652,  0.2825,  1.3906,  ..., -0.8462,  0.5410,  0.1051],
        [-1.1338,  0.3577,  2.0586,  ..., -0.9165,  0.6504,  0.5698]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:0')
2023-07-27 20:42:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([24, 51]), X shape:torch.Size([24, 51, 512])
CTC Tokens:tensor([ 21,  11,   6,   6, 378], device='cuda:6'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:6'), New Tokens:tensor([ 21,  11,   6, 378,   0], device='cuda:6')
Org X:tensor([[ 0.1488,  0.1018, -2.1562,  ..., -0.3474, -0.6206, -0.5249],
        [ 0.3228,  0.2450, -1.9160,  ..., -1.3926,  0.6001, -0.4641],
        [ 1.4082,  0.6738, -1.4727,  ..., -3.3770, -0.3538,  0.2573],
        [-0.1412,  0.8013, -0.6089,  ..., -0.5410,  0.8423, -0.2925],
        [-0.1981,  0.9790,  0.0468,  ..., -2.2285,  0.7334, -0.3440]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([ True, False, False,  True, False, False, False, False,  True, False],
       device='cuda:6'), New X:tensor([[ 0.1488,  0.1018, -2.1562,  ..., -0.3474, -0.6206, -0.5249],
        [ 0.3228,  0.2450, -1.9160,  ..., -1.3926,  0.6001, -0.4641],
        [ 1.4082,  0.6738, -1.4727,  ..., -3.3770, -0.3538,  0.2573],
        [-0.1412,  0.8013, -0.6089,  ..., -0.5410,  0.8423, -0.2925],
        [-0.1981,  0.9790,  0.0468,  ..., -2.2285,  0.7334, -0.3440]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 117]), X shape:torch.Size([8, 117, 512])
CTC Tokens:tensor([  0, 205,   0,   0,   0], device='cuda:4'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:4'), New Tokens:tensor([   0,  205,    0, 2433,    0], device='cuda:4')
Org X:tensor([[ 0.0627,  0.3303,  1.3340,  ..., -0.0831, -0.2039, -1.3506],
        [-0.4319,  1.4512,  1.4336,  ...,  0.2034, -2.0195, -1.7256],
        [-0.9541,  2.2305,  1.1270,  ..., -0.0365, -1.8027, -1.9658],
        [ 0.7979,  0.4473,  1.0898,  ...,  0.1578,  0.1708, -2.4297],
        [ 1.5020,  0.3286,  2.7422,  ...,  0.0653, -0.5156, -1.7598]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:4'), Mixup Sent Mask:tensor([ True, False, False,  True, False, False, False, False],
       device='cuda:4'), New X:tensor([[ 0.0627,  0.3303,  1.3340,  ..., -0.0831, -0.2039, -1.3506],
        [-0.4319,  1.4512,  1.4336,  ...,  0.2034, -2.0195, -1.7256],
        [-0.9541,  2.2305,  1.1270,  ..., -0.0365, -1.8027, -1.9658],
        [ 0.7979,  0.4473,  1.0898,  ...,  0.1578,  0.1708, -2.4297],
        [ 1.5020,  0.3286,  2.7422,  ...,  0.0653, -0.5156, -1.7598]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([5, 209]), X shape:torch.Size([5, 209, 512])
CTC Tokens:tensor([ 0, 84, 26,  0, 13], device='cuda:3'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:3'), New Tokens:tensor([ 0, 84, 26,  0, 13], device='cuda:3')
Org X:tensor([[-0.7148,  0.6958, -0.3755,  ..., -1.1650, -0.3625, -1.0664],
        [-1.1709,  0.3906, -0.5283,  ..., -1.2510,  0.2786, -1.2578],
        [ 0.3350,  0.2551,  0.2595,  ..., -1.2900,  0.4895, -1.0156],
        [ 0.3821,  0.1908,  2.5371,  ..., -3.0449, -0.3560,  0.2605],
        [-0.3699,  0.5239, -0.2690,  ..., -1.7539, -0.1300,  0.3306]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:3'), Mixup Sent Mask:tensor([ True, False, False,  True, False], device='cuda:3'), New X:tensor([[-0.7148,  0.6958, -0.3755,  ..., -1.1650, -0.3625, -1.0664],
        [-1.1709,  0.3906, -0.5283,  ..., -1.2510,  0.2786, -1.2578],
        [ 0.3350,  0.2551,  0.2595,  ..., -1.2900,  0.4895, -1.0156],
        [ 0.3821,  0.1908,  2.5371,  ..., -3.0449, -0.3560,  0.2605],
        [-0.3699,  0.5239, -0.2690,  ..., -1.7539, -0.1300,  0.3306]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([48, 31]), X shape:torch.Size([48, 31, 512])
CTC Tokens:tensor([  33,    0,    0, 1382,    0], device='cuda:1'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:1'), New Tokens:tensor([  33,    0, 1382,    0,    8], device='cuda:1')
Org X:tensor([[-0.2786,  0.2939,  0.1957,  ..., -1.9619, -0.6079,  0.9150],
        [ 0.3687,  0.2520,  0.5176,  ..., -2.5820, -0.4321,  0.8545],
        [ 1.1025,  0.1451,  2.1250,  ..., -2.0898, -0.4609,  0.5405],
        [ 1.1963,  0.1204,  2.2637,  ..., -1.7939, -0.3525,  0.6113],
        [-0.9146,  0.5825, -0.0552,  ..., -1.2637, -1.1445,  0.8149]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:1'), Mixup Sent Mask:tensor([ True, False, False,  True, False, False, False, False,  True, False],
       device='cuda:1'), New X:tensor([[-0.2786,  0.2939,  0.1957,  ..., -1.9619, -0.6079,  0.9150],
        [ 0.3687,  0.2520,  0.5176,  ..., -2.5820, -0.4321,  0.8545],
        [ 1.1025,  0.1451,  2.1250,  ..., -2.0898, -0.4609,  0.5405],
        [ 1.1963,  0.1204,  2.2637,  ..., -1.7939, -0.3525,  0.6113],
        [-0.9146,  0.5825, -0.0552,  ..., -1.2637, -1.1445,  0.8149]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 88]), X shape:torch.Size([8, 88, 512])
CTC Tokens:tensor([  0, 120,   0,   0,   0], device='cuda:2'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:2'), New Tokens:tensor([   0,  120,    0, 1387,   54], device='cuda:2')
Org X:tensor([[-0.0742, -1.3291,  1.5576,  ..., -0.7681, -0.4192,  0.7422],
        [-0.1873, -0.2263,  0.2152,  ...,  0.1671, -1.3213, -0.0294],
        [-0.2710, -0.8950,  2.3301,  ...,  0.0374, -1.3896,  0.1034],
        [-0.5264, -0.0308,  2.5039,  ...,  0.0676,  0.2598, -0.2261],
        [-0.3230, -0.5854,  3.0547,  ...,  0.0936, -0.1578, -0.1281]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:2'), Mixup Sent Mask:tensor([ True, False, False,  True, False, False, False, False],
       device='cuda:2'), New X:tensor([[-0.0742, -1.3291,  1.5576,  ..., -0.7681, -0.4192,  0.7422],
        [-0.1873, -0.2263,  0.2152,  ...,  0.1671, -1.3213, -0.0294],
        [-0.2710, -0.8950,  2.3301,  ...,  0.0374, -1.3896,  0.1034],
        [-0.5264, -0.0308,  2.5039,  ...,  0.0676,  0.2598, -0.2261],
        [-0.3230, -0.5854,  3.0547,  ...,  0.0936, -0.1578, -0.1281]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 146]), X shape:torch.Size([8, 146, 512])
CTC Tokens:tensor([67,  0, 70, 12, 33], device='cuda:7'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:7'), New Tokens:tensor([67,  0, 70, 12, 33], device='cuda:7')
Org X:tensor([[-1.3369e+00, -4.7333e-02,  4.8633e-01,  ...,  1.0681e-03,
          3.7500e-01, -6.7041e-01],
        [-2.1758e+00,  2.2900e-01,  2.3340e+00,  ..., -9.0576e-02,
          1.0410e+00, -1.9617e-01],
        [-2.9219e+00,  1.3721e-01,  1.4023e+00,  ...,  2.6489e-01,
          4.9365e-01,  1.9714e-01],
        [-1.9395e+00,  4.9487e-01,  7.3828e-01,  ...,  2.9321e-01,
         -1.7249e-01,  3.9990e-01],
        [-1.3008e+00,  7.7490e-01, -1.0957e+00,  ..., -1.4561e+00,
         -5.6885e-01,  2.0593e-01]], device='cuda:7', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:7'), Mixup Sent Mask:tensor([ True, False, False,  True, False, False, False, False],
       device='cuda:7'), New X:tensor([[-1.3369e+00, -4.7333e-02,  4.8633e-01,  ...,  1.0681e-03,
          3.7500e-01, -6.7041e-01],
        [-2.1758e+00,  2.2900e-01,  2.3340e+00,  ..., -9.0576e-02,
          1.0410e+00, -1.9617e-01],
        [-2.9219e+00,  1.3721e-01,  1.4023e+00,  ...,  2.6489e-01,
          4.9365e-01,  1.9714e-01],
        [-1.9395e+00,  4.9487e-01,  7.3828e-01,  ...,  2.9321e-01,
         -1.7249e-01,  3.9990e-01],
        [-1.3008e+00,  7.7490e-01, -1.0957e+00,  ..., -1.4561e+00,
         -5.6885e-01,  2.0593e-01]], device='cuda:7', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 85]), X shape:torch.Size([8, 85, 512])
CTC Tokens:tensor([ 0,  7,  0, 18, 15], device='cuda:5'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:5'), New Tokens:tensor([ 0,  7,  0, 18, 15], device='cuda:5')
Org X:tensor([[-0.3711, -0.3347,  0.0680,  ..., -0.2537, -0.1261, -0.7905],
        [-0.3721,  0.5459,  0.2479,  ..., -0.4224, -0.3521, -0.0667],
        [ 0.1176, -0.2037,  0.5493,  ..., -0.6284, -0.8926, -0.0399],
        [ 0.5767, -0.1545,  1.2188,  ...,  0.0802, -1.1396,  0.0690],
        [ 1.1426, -0.2683,  3.2559,  ...,  0.3699, -0.7930, -0.1732]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([ True, False, False,  True, False, False, False, False],
       device='cuda:5'), New X:tensor([[-0.3711, -0.3347,  0.0680,  ..., -0.2537, -0.1261, -0.7905],
        [-0.3721,  0.5459,  0.2479,  ..., -0.4224, -0.3521, -0.0667],
        [ 0.1176, -0.2037,  0.5493,  ..., -0.6284, -0.8926, -0.0399],
        [ 0.5767, -0.1545,  1.2188,  ...,  0.0802, -1.1396,  0.0690],
        [ 1.1426, -0.2683,  3.2559,  ...,  0.3699, -0.7930, -0.1732]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:5')
2023-07-27 20:42:28 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.209 | trans_loss 5.587 | nll_loss 2.87 | w2v_ctc_loss 1.324 | task_loss 4.628 | contrastive_loss 0.268 | total 4003.4 | n_correct 2468 | ppl 7.31 | accuracy 61.648 | uer 17.371 | wer 18.952 | raw_wer 18.952 | bleu 20 | wps 2033.3 | wpb 4003.4 | bsz 141.8 | num_updates 25047 | best_bleu 20.08
2023-07-27 20:42:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25047 updates
2023-07-27 20:42:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.0005.pt
2023-07-27 20:42:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.0005.pt
2023-07-27 20:42:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.0005.pt (epoch 17 @ 25047 updates, score 20.0) (writing took 11.998251684010029 seconds)
2023-07-27 20:42:40 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-07-27 20:42:40 | INFO | train | epoch 017 | loss 2.056 | trans_loss 5.078 | nll_loss 2.293 | w2v_ctc_loss 0.685 | task_loss 1.403 | contrastive_loss 0.12 | total 4136.62 | n_correct 2592.14 | ppl 4.9 | accuracy 62.663 | wps 10931.4 | ups 1.32 | wpb 8273.2 | bsz 305.1 | num_updates 25047 | lr 8.93588e-05 | gnorm 0.538 | clip 0 | loss_scale 32 | train_wall 1016 | gb_free 16.3 | wall 22078
2023-07-27 20:42:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 20:42:40 | INFO | fairseq.trainer | begin training epoch 18
2023-07-27 20:42:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 20:43:26 | INFO | train_inner | epoch 018:     53 / 1474 loss=2.055, trans_loss=5.074, nll_loss=2.288, w2v_ctc_loss=0.692, task_loss=1.427, contrastive_loss=0.084, total=4139.04, n_correct=2595.91, ppl=4.88, accuracy=62.718, wps=7181.3, ups=0.87, wpb=8278.1, bsz=303.3, num_updates=25100, lr=8.92644e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=69, gb_free=17, wall=22124
2023-07-27 20:44:35 | INFO | train_inner | epoch 018:    153 / 1474 loss=2.041, trans_loss=5.045, nll_loss=2.25, w2v_ctc_loss=0.66, task_loss=1.34, contrastive_loss=0.211, total=4154.85, n_correct=2628.96, ppl=4.76, accuracy=63.274, wps=12074.4, ups=1.45, wpb=8309.7, bsz=312.7, num_updates=25200, lr=8.90871e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=68, gb_free=16.7, wall=22193
2023-07-27 20:45:45 | INFO | train_inner | epoch 018:    253 / 1474 loss=2.035, trans_loss=5.049, nll_loss=2.255, w2v_ctc_loss=0.673, task_loss=1.359, contrastive_loss=0.075, total=4162.72, n_correct=2638.87, ppl=4.77, accuracy=63.393, wps=11803.9, ups=1.42, wpb=8325.4, bsz=312.9, num_updates=25300, lr=8.89108e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=70, gb_free=16.1, wall=22263
2023-07-27 20:46:55 | INFO | train_inner | epoch 018:    353 / 1474 loss=2.044, trans_loss=5.057, nll_loss=2.265, w2v_ctc_loss=0.678, task_loss=1.422, contrastive_loss=0.089, total=4161.22, n_correct=2620.31, ppl=4.81, accuracy=62.97, wps=11911.4, ups=1.43, wpb=8322.4, bsz=301.5, num_updates=25400, lr=8.87357e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=14.3, wall=22333
2023-07-27 20:48:05 | INFO | train_inner | epoch 018:    453 / 1474 loss=2.053, trans_loss=5.065, nll_loss=2.275, w2v_ctc_loss=0.675, task_loss=1.494, contrastive_loss=0.184, total=4092.36, n_correct=2572.78, ppl=4.84, accuracy=62.868, wps=11720.5, ups=1.43, wpb=8184.7, bsz=295.3, num_updates=25500, lr=8.85615e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=22403
2023-07-27 20:49:15 | INFO | train_inner | epoch 018:    553 / 1474 loss=2.032, trans_loss=5.049, nll_loss=2.257, w2v_ctc_loss=0.667, task_loss=1.256, contrastive_loss=0.091, total=4206.45, n_correct=2662.32, ppl=4.78, accuracy=63.291, wps=12137.8, ups=1.44, wpb=8412.9, bsz=328.9, num_updates=25600, lr=8.83883e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=17.8, wall=22472
2023-07-27 20:50:24 | INFO | train_inner | epoch 018:    653 / 1474 loss=2.057, trans_loss=5.077, nll_loss=2.292, w2v_ctc_loss=0.683, task_loss=1.442, contrastive_loss=0.159, total=4097.96, n_correct=2568.24, ppl=4.9, accuracy=62.671, wps=11857.1, ups=1.45, wpb=8195.9, bsz=298.6, num_updates=25700, lr=8.82162e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=68, gb_free=12, wall=22542
2023-07-27 20:51:33 | INFO | train_inner | epoch 018:    753 / 1474 loss=2.06, trans_loss=5.073, nll_loss=2.288, w2v_ctc_loss=0.688, task_loss=1.34, contrastive_loss=0.251, total=4208.5, n_correct=2641.56, ppl=4.88, accuracy=62.767, wps=12069.2, ups=1.43, wpb=8417, bsz=322.6, num_updates=25800, lr=8.80451e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=22611
2023-07-27 20:52:43 | INFO | train_inner | epoch 018:    853 / 1474 loss=2.046, trans_loss=5.069, nll_loss=2.281, w2v_ctc_loss=0.678, task_loss=1.417, contrastive_loss=0.063, total=4166.07, n_correct=2619.73, ppl=4.86, accuracy=62.883, wps=12037.4, ups=1.44, wpb=8332.1, bsz=302.4, num_updates=25900, lr=8.7875e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=22681
2023-07-27 20:53:52 | INFO | train_inner | epoch 018:    953 / 1474 loss=2.036, trans_loss=5.06, nll_loss=2.272, w2v_ctc_loss=0.666, task_loss=1.304, contrastive_loss=0.089, total=4141.27, n_correct=2613.22, ppl=4.83, accuracy=63.102, wps=11876.2, ups=1.43, wpb=8282.5, bsz=316, num_updates=26000, lr=8.77058e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=22750
2023-07-27 20:53:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 20:54:16 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.214 | trans_loss 5.595 | nll_loss 2.877 | w2v_ctc_loss 1.321 | task_loss 4.64 | contrastive_loss 0.273 | total 4003.4 | n_correct 2469 | ppl 7.35 | accuracy 61.673 | uer 17.477 | wer 19.22 | raw_wer 19.22 | bleu 20.21 | wps 2089.5 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 20.21
2023-07-27 20:54:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-07-27 20:54:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_18_26000.pt
2023-07-27 20:54:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_18_26000.pt
2023-07-27 20:54:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 20.21) (writing took 20.608674639835954 seconds)
2023-07-27 20:55:47 | INFO | train_inner | epoch 018:   1053 / 1474 loss=2.045, trans_loss=5.073, nll_loss=2.287, w2v_ctc_loss=0.67, task_loss=1.455, contrastive_loss=0.077, total=4134.55, n_correct=2599.91, ppl=4.88, accuracy=62.883, wps=7185.8, ups=0.87, wpb=8269.1, bsz=300.8, num_updates=26100, lr=8.75376e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=69, gb_free=15.9, wall=22865
2023-07-27 20:56:57 | INFO | train_inner | epoch 018:   1153 / 1474 loss=2.048, trans_loss=5.06, nll_loss=2.271, w2v_ctc_loss=0.675, task_loss=1.334, contrastive_loss=0.184, total=4157.63, n_correct=2619.44, ppl=4.83, accuracy=63.003, wps=12033, ups=1.45, wpb=8315.3, bsz=314, num_updates=26200, lr=8.73704e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=68, gb_free=17.1, wall=22934
2023-07-27 20:58:06 | INFO | train_inner | epoch 018:   1253 / 1474 loss=2.052, trans_loss=5.082, nll_loss=2.299, w2v_ctc_loss=0.679, task_loss=1.509, contrastive_loss=0.069, total=4085.66, n_correct=2559.47, ppl=4.92, accuracy=62.645, wps=11726.9, ups=1.44, wpb=8171.3, bsz=286.6, num_updates=26300, lr=8.72041e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=69, gb_free=17.4, wall=23004
2023-07-27 20:59:15 | INFO | train_inner | epoch 018:   1353 / 1474 loss=2.063, trans_loss=5.087, nll_loss=2.306, w2v_ctc_loss=0.695, task_loss=1.495, contrastive_loss=0.096, total=4065.6, n_correct=2541.33, ppl=4.95, accuracy=62.508, wps=11772.6, ups=1.45, wpb=8131.2, bsz=291.1, num_updates=26400, lr=8.70388e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=68, gb_free=13.1, wall=23073
2023-07-27 21:00:24 | INFO | train_inner | epoch 018:   1453 / 1474 loss=2.052, trans_loss=5.08, nll_loss=2.297, w2v_ctc_loss=0.682, task_loss=1.464, contrastive_loss=0.082, total=4122.48, n_correct=2583.07, ppl=4.91, accuracy=62.658, wps=11926.6, ups=1.45, wpb=8245, bsz=299.5, num_updates=26500, lr=8.68744e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=68, gb_free=17.1, wall=23142
2023-07-27 21:00:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 21:01:04 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.228 | trans_loss 5.592 | nll_loss 2.877 | w2v_ctc_loss 1.374 | task_loss 4.64 | contrastive_loss 0.264 | total 4003.4 | n_correct 2468.2 | ppl 7.34 | accuracy 61.653 | uer 17.219 | wer 19.086 | raw_wer 19.086 | bleu 19.91 | wps 2000.4 | wpb 4003.4 | bsz 141.8 | num_updates 26521 | best_bleu 20.21
2023-07-27 21:01:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26521 updates
2023-07-27 21:01:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9105.pt
2023-07-27 21:01:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9105.pt
2023-07-27 21:01:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9105.pt (epoch 18 @ 26521 updates, score 19.91) (writing took 11.597671573981643 seconds)
2023-07-27 21:01:16 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-07-27 21:01:16 | INFO | train | epoch 018 | loss 2.048 | trans_loss 5.066 | nll_loss 2.278 | w2v_ctc_loss 0.676 | task_loss 1.4 | contrastive_loss 0.126 | total 4138.65 | n_correct 2604.11 | ppl 4.85 | accuracy 62.922 | wps 10936.3 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 26521 | lr 8.684e-05 | gnorm 0.539 | clip 0 | loss_scale 64 | train_wall 1014 | gb_free 15.9 | wall 23194
2023-07-27 21:01:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 21:01:16 | INFO | fairseq.trainer | begin training epoch 19
2023-07-27 21:01:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 21:02:20 | INFO | train_inner | epoch 019:     79 / 1474 loss=2.035, trans_loss=5.039, nll_loss=2.242, w2v_ctc_loss=0.668, task_loss=1.404, contrastive_loss=0.13, total=4101.48, n_correct=2598.98, ppl=4.73, accuracy=63.367, wps=7109, ups=0.87, wpb=8203, bsz=296.9, num_updates=26600, lr=8.6711e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=23258
2023-07-27 21:03:30 | INFO | train_inner | epoch 019:    179 / 1474 loss=2.037, trans_loss=5.037, nll_loss=2.24, w2v_ctc_loss=0.679, task_loss=1.307, contrastive_loss=0.13, total=4227.39, n_correct=2682.4, ppl=4.72, accuracy=63.453, wps=12058, ups=1.43, wpb=8454.8, bsz=324.7, num_updates=26700, lr=8.65485e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=15.5, wall=23328
2023-07-27 21:04:39 | INFO | train_inner | epoch 019:    279 / 1474 loss=2.027, trans_loss=5.034, nll_loss=2.235, w2v_ctc_loss=0.668, task_loss=1.384, contrastive_loss=0.065, total=4186.65, n_correct=2665.03, ppl=4.71, accuracy=63.655, wps=12086.9, ups=1.44, wpb=8373.3, bsz=306.4, num_updates=26800, lr=8.63868e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=69, gb_free=13.7, wall=23397
2023-07-27 21:05:49 | INFO | train_inner | epoch 019:    379 / 1474 loss=2.038, trans_loss=5.043, nll_loss=2.248, w2v_ctc_loss=0.663, task_loss=1.393, contrastive_loss=0.177, total=4165.84, n_correct=2640.1, ppl=4.75, accuracy=63.375, wps=11945.3, ups=1.43, wpb=8331.7, bsz=310, num_updates=26900, lr=8.62261e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=69, gb_free=15.9, wall=23467
2023-07-27 21:06:58 | INFO | train_inner | epoch 019:    479 / 1474 loss=2.037, trans_loss=5.049, nll_loss=2.255, w2v_ctc_loss=0.674, task_loss=1.432, contrastive_loss=0.085, total=4122.98, n_correct=2609.99, ppl=4.77, accuracy=63.303, wps=11920.9, ups=1.45, wpb=8246, bsz=302.7, num_updates=27000, lr=8.60663e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=68, gb_free=17.1, wall=23536
2023-07-27 21:08:07 | INFO | train_inner | epoch 019:    579 / 1474 loss=2.038, trans_loss=5.049, nll_loss=2.256, w2v_ctc_loss=0.668, task_loss=1.387, contrastive_loss=0.145, total=4121.66, n_correct=2609.52, ppl=4.78, accuracy=63.312, wps=11954.2, ups=1.45, wpb=8243.3, bsz=304.4, num_updates=27100, lr=8.59074e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=68, gb_free=16.8, wall=23605
2023-07-27 21:09:17 | INFO | train_inner | epoch 019:    679 / 1474 loss=2.022, trans_loss=5.047, nll_loss=2.254, w2v_ctc_loss=0.651, task_loss=1.267, contrastive_loss=0.074, total=4205.65, n_correct=2668.86, ppl=4.77, accuracy=63.459, wps=12064.7, ups=1.43, wpb=8411.3, bsz=322.9, num_updates=27200, lr=8.57493e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=23675
2023-07-27 21:10:27 | INFO | train_inner | epoch 019:    779 / 1474 loss=2.038, trans_loss=5.051, nll_loss=2.258, w2v_ctc_loss=0.674, task_loss=1.448, contrastive_loss=0.079, total=4120.36, n_correct=2605.51, ppl=4.78, accuracy=63.235, wps=11772.2, ups=1.43, wpb=8240.7, bsz=298.5, num_updates=27300, lr=8.55921e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=13.8, wall=23745
2023-07-27 21:11:36 | INFO | train_inner | epoch 019:    879 / 1474 loss=2.042, trans_loss=5.063, nll_loss=2.274, w2v_ctc_loss=0.677, task_loss=1.398, contrastive_loss=0.077, total=4176.52, n_correct=2633.41, ppl=4.84, accuracy=63.053, wps=12035.7, ups=1.44, wpb=8353, bsz=309.8, num_updates=27400, lr=8.54358e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=69, gb_free=16.9, wall=23814
2023-07-27 21:12:46 | INFO | train_inner | epoch 019:    979 / 1474 loss=2.059, trans_loss=5.073, nll_loss=2.289, w2v_ctc_loss=0.673, task_loss=1.429, contrastive_loss=0.309, total=4079.93, n_correct=2558.38, ppl=4.89, accuracy=62.706, wps=11664, ups=1.43, wpb=8159.9, bsz=305, num_updates=27500, lr=8.52803e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=69, gb_free=16.4, wall=23884
2023-07-27 21:13:56 | INFO | train_inner | epoch 019:   1079 / 1474 loss=2.046, trans_loss=5.07, nll_loss=2.283, w2v_ctc_loss=0.672, task_loss=1.485, contrastive_loss=0.116, total=4041.08, n_correct=2545.48, ppl=4.87, accuracy=62.99, wps=11588.3, ups=1.43, wpb=8082.2, bsz=292.3, num_updates=27600, lr=8.51257e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=23954
2023-07-27 21:15:06 | INFO | train_inner | epoch 019:   1179 / 1474 loss=2.056, trans_loss=5.072, nll_loss=2.287, w2v_ctc_loss=0.677, task_loss=1.414, contrastive_loss=0.201, total=4146.59, n_correct=2601.44, ppl=4.88, accuracy=62.737, wps=11808.5, ups=1.42, wpb=8293.2, bsz=310.1, num_updates=27700, lr=8.49719e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=70, gb_free=12.1, wall=24024
2023-07-27 21:16:15 | INFO | train_inner | epoch 019:   1279 / 1474 loss=2.043, trans_loss=5.071, nll_loss=2.285, w2v_ctc_loss=0.665, task_loss=1.418, contrastive_loss=0.094, total=4142.96, n_correct=2606.03, ppl=4.87, accuracy=62.903, wps=12125, ups=1.46, wpb=8285.9, bsz=300.3, num_updates=27800, lr=8.48189e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=68, gb_free=16.1, wall=24093
2023-07-27 21:16:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 21:17:25 | INFO | train_inner | epoch 019:   1380 / 1474 loss=2.043, trans_loss=5.067, nll_loss=2.28, w2v_ctc_loss=0.673, task_loss=1.438, contrastive_loss=0.082, total=4131.76, n_correct=2604.08, ppl=4.86, accuracy=63.026, wps=11743.5, ups=1.42, wpb=8263.5, bsz=300.5, num_updates=27900, lr=8.46668e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=70, gb_free=16.5, wall=24163
2023-07-27 21:18:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 21:18:56 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 4.198 | trans_loss 5.582 | nll_loss 2.865 | w2v_ctc_loss 1.298 | task_loss 4.648 | contrastive_loss 0.265 | total 4003.4 | n_correct 2477 | ppl 7.28 | accuracy 61.872 | uer 17.057 | wer 18.724 | raw_wer 18.724 | bleu 19.96 | wps 1950.4 | wpb 4003.4 | bsz 141.8 | num_updates 27994 | best_bleu 20.21
2023-07-27 21:18:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27994 updates
2023-07-27 21:18:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9608.pt
2023-07-27 21:18:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9608.pt
2023-07-27 21:19:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9608.pt (epoch 19 @ 27994 updates, score 19.96) (writing took 14.763930771499872 seconds)
2023-07-27 21:19:11 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-07-27 21:19:11 | INFO | train | epoch 019 | loss 2.04 | trans_loss 5.055 | nll_loss 2.264 | w2v_ctc_loss 0.67 | task_loss 1.402 | contrastive_loss 0.125 | total 4138.66 | n_correct 2615.08 | ppl 4.8 | accuracy 63.187 | wps 11336.2 | ups 1.37 | wpb 8277.3 | bsz 305.7 | num_updates 27994 | lr 8.45245e-05 | gnorm 0.542 | clip 0 | loss_scale 64 | train_wall 1016 | gb_free 17.3 | wall 24269
2023-07-27 21:19:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 21:19:12 | INFO | fairseq.trainer | begin training epoch 20
2023-07-27 21:19:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 21:19:25 | INFO | train_inner | epoch 020:      6 / 1474 loss=2.043, trans_loss=5.062, nll_loss=2.274, w2v_ctc_loss=0.665, task_loss=1.419, contrastive_loss=0.166, total=4117.61, n_correct=2599.49, ppl=4.84, accuracy=63.131, wps=6844.7, ups=0.83, wpb=8235.2, bsz=303, num_updates=28000, lr=8.45154e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=70, gb_free=16.4, wall=24283
2023-07-27 21:19:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 21:19:50 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.198 | trans_loss 5.583 | nll_loss 2.862 | w2v_ctc_loss 1.298 | task_loss 4.64 | contrastive_loss 0.264 | total 4003.4 | n_correct 2474.8 | ppl 7.27 | accuracy 61.817 | uer 16.948 | wer 18.523 | raw_wer 18.523 | bleu 20.12 | wps 2036.4 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 20.21
2023-07-27 21:19:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-07-27 21:19:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_20_28000.pt
2023-07-27 21:19:53 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_20_28000.pt
2023-07-27 21:20:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 20.12) (writing took 27.505360148847103 seconds)
2023-07-27 21:21:27 | INFO | train_inner | epoch 020:    106 / 1474 loss=2.019, trans_loss=5.022, nll_loss=2.221, w2v_ctc_loss=0.655, task_loss=1.358, contrastive_loss=0.087, total=4192.82, n_correct=2675.21, ppl=4.66, accuracy=63.805, wps=6876.5, ups=0.82, wpb=8385.6, bsz=312.8, num_updates=28100, lr=8.43649e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=24405
2023-07-27 21:22:38 | INFO | train_inner | epoch 020:    206 / 1474 loss=2.028, trans_loss=5.031, nll_loss=2.232, w2v_ctc_loss=0.658, task_loss=1.449, contrastive_loss=0.137, total=4155.9, n_correct=2643.16, ppl=4.7, accuracy=63.6, wps=11830.4, ups=1.42, wpb=8311.8, bsz=302.3, num_updates=28200, lr=8.42152e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=70, gb_free=11.6, wall=24475
2023-07-27 21:23:47 | INFO | train_inner | epoch 020:    306 / 1474 loss=2.017, trans_loss=5.023, nll_loss=2.223, w2v_ctc_loss=0.66, task_loss=1.256, contrastive_loss=0.078, total=4192.69, n_correct=2679.16, ppl=4.67, accuracy=63.901, wps=12086.3, ups=1.44, wpb=8385.4, bsz=327.6, num_updates=28300, lr=8.40663e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=24545
2023-07-27 21:24:56 | INFO | train_inner | epoch 020:    406 / 1474 loss=2.021, trans_loss=5.03, nll_loss=2.23, w2v_ctc_loss=0.654, task_loss=1.423, contrastive_loss=0.074, total=4116.96, n_correct=2621.7, ppl=4.69, accuracy=63.68, wps=11891.5, ups=1.44, wpb=8233.9, bsz=296.8, num_updates=28400, lr=8.39181e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=12.5, wall=24614
2023-07-27 21:26:06 | INFO | train_inner | epoch 020:    506 / 1474 loss=2.035, trans_loss=5.046, nll_loss=2.252, w2v_ctc_loss=0.66, task_loss=1.443, contrastive_loss=0.163, total=4100.73, n_correct=2599.83, ppl=4.76, accuracy=63.399, wps=11822.4, ups=1.44, wpb=8201.5, bsz=298.4, num_updates=28500, lr=8.37708e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=24683
2023-07-27 21:27:15 | INFO | train_inner | epoch 020:    606 / 1474 loss=2.039, trans_loss=5.045, nll_loss=2.251, w2v_ctc_loss=0.665, task_loss=1.458, contrastive_loss=0.167, total=4101.99, n_correct=2595.53, ppl=4.76, accuracy=63.275, wps=11773.6, ups=1.44, wpb=8204, bsz=298.3, num_updates=28600, lr=8.36242e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=69, gb_free=13.4, wall=24753
2023-07-27 21:28:24 | INFO | train_inner | epoch 020:    706 / 1474 loss=2.032, trans_loss=5.047, nll_loss=2.253, w2v_ctc_loss=0.667, task_loss=1.427, contrastive_loss=0.065, total=4124.25, n_correct=2614.77, ppl=4.77, accuracy=63.4, wps=11906.2, ups=1.44, wpb=8248.5, bsz=297.2, num_updates=28700, lr=8.34784e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=16.6, wall=24822
2023-07-27 21:29:34 | INFO | train_inner | epoch 020:    806 / 1474 loss=2.029, trans_loss=5.044, nll_loss=2.25, w2v_ctc_loss=0.669, task_loss=1.374, contrastive_loss=0.073, total=4153.23, n_correct=2637.02, ppl=4.76, accuracy=63.493, wps=12006, ups=1.45, wpb=8306.5, bsz=308.5, num_updates=28800, lr=8.33333e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=68, gb_free=17.6, wall=24892
2023-07-27 21:30:44 | INFO | train_inner | epoch 020:    906 / 1474 loss=2.055, trans_loss=5.056, nll_loss=2.267, w2v_ctc_loss=0.665, task_loss=1.35, contrastive_loss=0.372, total=4153.72, n_correct=2620.49, ppl=4.81, accuracy=63.088, wps=11829.9, ups=1.42, wpb=8307.4, bsz=320.7, num_updates=28900, lr=8.3189e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=69, gb_free=16, wall=24962
2023-07-27 21:31:54 | INFO | train_inner | epoch 020:   1006 / 1474 loss=2.03, trans_loss=5.05, nll_loss=2.257, w2v_ctc_loss=0.658, task_loss=1.402, contrastive_loss=0.076, total=4156.05, n_correct=2633.96, ppl=4.78, accuracy=63.377, wps=11914.4, ups=1.43, wpb=8312.1, bsz=305.3, num_updates=29000, lr=8.30455e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=69, gb_free=15, wall=25032
2023-07-27 21:33:03 | INFO | train_inner | epoch 020:   1106 / 1474 loss=2.04, trans_loss=5.051, nll_loss=2.26, w2v_ctc_loss=0.662, task_loss=1.321, contrastive_loss=0.216, total=4181.53, n_correct=2648.95, ppl=4.79, accuracy=63.349, wps=12154.7, ups=1.45, wpb=8363.1, bsz=320.2, num_updates=29100, lr=8.29027e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=68, gb_free=17.1, wall=25100
2023-07-27 21:34:11 | INFO | train_inner | epoch 020:   1206 / 1474 loss=2.038, trans_loss=5.05, nll_loss=2.258, w2v_ctc_loss=0.675, task_loss=1.552, contrastive_loss=0.06, total=4029.26, n_correct=2546.87, ppl=4.78, accuracy=63.209, wps=11694.7, ups=1.45, wpb=8058.5, bsz=282.4, num_updates=29200, lr=8.27606e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=68, gb_free=16.6, wall=25169
2023-07-27 21:35:22 | INFO | train_inner | epoch 020:   1306 / 1474 loss=2.032, trans_loss=5.055, nll_loss=2.265, w2v_ctc_loss=0.661, task_loss=1.461, contrastive_loss=0.075, total=4127.21, n_correct=2611.29, ppl=4.81, accuracy=63.27, wps=11723.9, ups=1.42, wpb=8254.4, bsz=299.9, num_updates=29300, lr=8.26192e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=70, gb_free=14.7, wall=25240
2023-07-27 21:36:32 | INFO | train_inner | epoch 020:   1406 / 1474 loss=2.036, trans_loss=5.057, nll_loss=2.267, w2v_ctc_loss=0.664, task_loss=1.499, contrastive_loss=0.068, total=4110.89, n_correct=2596.78, ppl=4.81, accuracy=63.168, wps=11797.3, ups=1.43, wpb=8221.8, bsz=291.6, num_updates=29400, lr=8.24786e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=69, gb_free=13.2, wall=25309
2023-07-27 21:37:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 21:37:44 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.19 | trans_loss 5.576 | nll_loss 2.85 | w2v_ctc_loss 1.287 | task_loss 4.604 | contrastive_loss 0.265 | total 4003.4 | n_correct 2474.9 | ppl 7.21 | accuracy 61.82 | uer 16.959 | wer 18.668 | raw_wer 18.668 | bleu 19.95 | wps 1939.6 | wpb 4003.4 | bsz 141.8 | num_updates 29468 | best_bleu 20.21
2023-07-27 21:37:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29468 updates
2023-07-27 21:37:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9500.pt
2023-07-27 21:37:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9500.pt
2023-07-27 21:37:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9500.pt (epoch 20 @ 29468 updates, score 19.95) (writing took 11.916654165834188 seconds)
2023-07-27 21:37:56 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-07-27 21:37:56 | INFO | train | epoch 020 | loss 2.032 | trans_loss 5.044 | nll_loss 2.25 | w2v_ctc_loss 0.662 | task_loss 1.401 | contrastive_loss 0.123 | total 4138.65 | n_correct 2624.91 | ppl 4.76 | accuracy 63.424 | wps 10847 | ups 1.31 | wpb 8277.3 | bsz 305.7 | num_updates 29468 | lr 8.23834e-05 | gnorm 0.541 | clip 0 | loss_scale 64 | train_wall 1015 | gb_free 16.1 | wall 25394
2023-07-27 21:37:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 21:37:56 | INFO | fairseq.trainer | begin training epoch 21
2023-07-27 21:37:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 21:38:28 | INFO | train_inner | epoch 021:     32 / 1474 loss=2.036, trans_loss=5.049, nll_loss=2.258, w2v_ctc_loss=0.658, task_loss=1.317, contrastive_loss=0.193, total=4166.35, n_correct=2637.78, ppl=4.78, accuracy=63.312, wps=7146.8, ups=0.86, wpb=8332.7, bsz=319.4, num_updates=29500, lr=8.23387e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=69, gb_free=11, wall=25426
2023-07-27 21:39:38 | INFO | train_inner | epoch 021:    132 / 1474 loss=2.018, trans_loss=5.013, nll_loss=2.209, w2v_ctc_loss=0.648, task_loss=1.32, contrastive_loss=0.184, total=4181.45, n_correct=2678.05, ppl=4.62, accuracy=64.046, wps=12052.7, ups=1.44, wpb=8362.9, bsz=317.6, num_updates=29600, lr=8.21995e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=69, gb_free=15.1, wall=25495
2023-07-27 21:40:47 | INFO | train_inner | epoch 021:    232 / 1474 loss=2.012, trans_loss=5.018, nll_loss=2.215, w2v_ctc_loss=0.642, task_loss=1.319, contrastive_loss=0.133, total=4167.12, n_correct=2668.89, ppl=4.64, accuracy=64.046, wps=12021.4, ups=1.44, wpb=8334.2, bsz=315.1, num_updates=29700, lr=8.2061e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=69, gb_free=11.6, wall=25565
2023-07-27 21:41:57 | INFO | train_inner | epoch 021:    332 / 1474 loss=2.024, trans_loss=5.025, nll_loss=2.224, w2v_ctc_loss=0.659, task_loss=1.43, contrastive_loss=0.136, total=4130.24, n_correct=2630.62, ppl=4.67, accuracy=63.692, wps=11745.4, ups=1.42, wpb=8260.5, bsz=303.6, num_updates=29800, lr=8.19232e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=70, gb_free=17.2, wall=25635
2023-07-27 21:43:07 | INFO | train_inner | epoch 021:    432 / 1474 loss=2.013, trans_loss=5.023, nll_loss=2.223, w2v_ctc_loss=0.648, task_loss=1.341, contrastive_loss=0.067, total=4186.28, n_correct=2674.29, ppl=4.67, accuracy=63.882, wps=12043.3, ups=1.44, wpb=8372.6, bsz=310.3, num_updates=29900, lr=8.17861e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=25705
2023-07-27 21:44:16 | INFO | train_inner | epoch 021:    532 / 1474 loss=2.013, trans_loss=5.02, nll_loss=2.218, w2v_ctc_loss=0.65, task_loss=1.435, contrastive_loss=0.062, total=4096.33, n_correct=2621.38, ppl=4.65, accuracy=63.993, wps=11824.2, ups=1.44, wpb=8192.7, bsz=298, num_updates=30000, lr=8.16497e-05, gnorm=0.54, clip=0, loss_scale=128, train_wall=69, gb_free=17.2, wall=25774
2023-07-27 21:44:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 21:44:40 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.207 | trans_loss 5.59 | nll_loss 2.869 | w2v_ctc_loss 1.314 | task_loss 4.649 | contrastive_loss 0.255 | total 4003.4 | n_correct 2474.7 | ppl 7.31 | accuracy 61.815 | uer 17.209 | wer 18.933 | raw_wer 18.933 | bleu 19.68 | wps 2121.5 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 20.21
2023-07-27 21:44:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-07-27 21:44:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_21_30000.pt
2023-07-27 21:44:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_21_30000.pt
2023-07-27 21:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 19.68) (writing took 18.796915432438254 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([8, 114]), X shape:torch.Size([8, 114, 512])
CTC Tokens:tensor([ 0,  0, 29,  0, 70], device='cuda:0'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:0'), New Tokens:tensor([ 0, 29,  0, 70,  0], device='cuda:0')
Org X:tensor([[ 0.1843,  0.2035,  0.2729,  ...,  0.6987, -0.0369, -1.2383],
        [-1.7441,  0.4807, -0.1420,  ...,  0.2312, -0.6147, -0.6968],
        [-0.4099,  0.5137,  1.0088,  ..., -0.5127, -0.4468, -1.1426],
        [-1.4463,  0.4070, -0.4451,  ..., -0.0464, -0.0636, -1.0645],
        [-0.3333,  0.4324,  1.8574,  ..., -2.2617,  0.5303,  0.2057]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:0'), Mixup Sent Mask:tensor([ True,  True, False, False, False,  True, False, False],
       device='cuda:0'), New X:tensor([[-0.6411, -1.0908,  0.2615,  ..., -2.7227, -1.6865, -1.5762],
        [-1.7441,  0.4807, -0.1420,  ...,  0.2312, -0.6147, -0.6968],
        [-0.4099,  0.5137,  1.0088,  ..., -0.5127, -0.4468, -1.1426],
        [-0.3010, -0.3882,  0.0255,  ...,  0.1537, -1.7168, -2.9199],
        [-0.6411, -1.0908,  0.2615,  ..., -2.7227, -1.6865, -1.5762]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:0')
2023-07-27 21:46:09 | INFO | train_inner | epoch 021:    632 / 1474 loss=2.029, trans_loss=5.033, nll_loss=2.236, w2v_ctc_loss=0.648, task_loss=1.391, contrastive_loss=0.237, total=4215.02, n_correct=2682.87, ppl=4.71, accuracy=63.65, wps=7465.2, ups=0.89, wpb=8430, bsz=314.6, num_updates=30100, lr=8.15139e-05, gnorm=0.539, clip=0, loss_scale=128, train_wall=69, gb_free=16.4, wall=25887
2023-07-27 21:46:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-27 21:47:20 | INFO | train_inner | epoch 021:    733 / 1474 loss=2.022, trans_loss=5.037, nll_loss=2.24, w2v_ctc_loss=0.652, task_loss=1.414, contrastive_loss=0.078, total=4138.42, n_correct=2637.25, ppl=4.73, accuracy=63.726, wps=11701.3, ups=1.41, wpb=8276.8, bsz=305.7, num_updates=30200, lr=8.13788e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=70, gb_free=11.8, wall=25958
2023-07-27 21:48:30 | INFO | train_inner | epoch 021:    833 / 1474 loss=2.029, trans_loss=5.041, nll_loss=2.245, w2v_ctc_loss=0.656, task_loss=1.49, contrastive_loss=0.109, total=4062.56, n_correct=2579.05, ppl=4.74, accuracy=63.483, wps=11576.7, ups=1.42, wpb=8125.1, bsz=293, num_updates=30300, lr=8.12444e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=26028
2023-07-27 21:49:38 | INFO | train_inner | epoch 021:    933 / 1474 loss=2.021, trans_loss=5.033, nll_loss=2.236, w2v_ctc_loss=0.655, task_loss=1.396, contrastive_loss=0.082, total=4103.66, n_correct=2609.2, ppl=4.71, accuracy=63.582, wps=11961.5, ups=1.46, wpb=8207.3, bsz=301.5, num_updates=30400, lr=8.11107e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=68, gb_free=17.1, wall=26096
2023-07-27 21:50:48 | INFO | train_inner | epoch 021:   1033 / 1474 loss=2.028, trans_loss=5.049, nll_loss=2.257, w2v_ctc_loss=0.658, task_loss=1.431, contrastive_loss=0.079, total=4100.54, n_correct=2599.39, ppl=4.78, accuracy=63.391, wps=11853.4, ups=1.45, wpb=8201.1, bsz=298.2, num_updates=30500, lr=8.09776e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=69, gb_free=17.8, wall=26166
2023-07-27 21:51:57 | INFO | train_inner | epoch 021:   1133 / 1474 loss=2.025, trans_loss=5.038, nll_loss=2.242, w2v_ctc_loss=0.656, task_loss=1.505, contrastive_loss=0.081, total=4119.98, n_correct=2618.85, ppl=4.73, accuracy=63.565, wps=11850.8, ups=1.44, wpb=8240, bsz=294, num_updates=30600, lr=8.08452e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=69, gb_free=17.8, wall=26235
2023-07-27 21:53:06 | INFO | train_inner | epoch 021:   1233 / 1474 loss=2.027, trans_loss=5.041, nll_loss=2.247, w2v_ctc_loss=0.654, task_loss=1.321, contrastive_loss=0.136, total=4161.49, n_correct=2645.48, ppl=4.75, accuracy=63.57, wps=12083.5, ups=1.45, wpb=8323, bsz=313, num_updates=30700, lr=8.07134e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=68, gb_free=16.8, wall=26304
2023-07-27 21:54:15 | INFO | train_inner | epoch 021:   1333 / 1474 loss=2.026, trans_loss=5.041, nll_loss=2.248, w2v_ctc_loss=0.659, task_loss=1.357, contrastive_loss=0.097, total=4141.76, n_correct=2637.68, ppl=4.75, accuracy=63.685, wps=12002.9, ups=1.45, wpb=8283.5, bsz=311.7, num_updates=30800, lr=8.05823e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=68, gb_free=17.3, wall=26373
2023-07-27 21:55:26 | INFO | train_inner | epoch 021:   1433 / 1474 loss=2.042, trans_loss=5.052, nll_loss=2.261, w2v_ctc_loss=0.673, task_loss=1.478, contrastive_loss=0.147, total=4127.02, n_correct=2609.61, ppl=4.79, accuracy=63.232, wps=11711.8, ups=1.42, wpb=8254, bsz=302.1, num_updates=30900, lr=8.04518e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=70, gb_free=16.4, wall=26444
2023-07-27 21:55:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 88]), X shape:torch.Size([8, 88, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:5'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:5'), New Tokens:tensor([   0, 1094,    0,  211,    0], device='cuda:5')
Org X:tensor([[-0.2319, -0.1862,  1.4219,  ..., -0.8374,  0.8208,  0.3154],
        [ 0.3088,  0.0170,  0.1965,  ..., -1.0137,  0.5669, -0.1428],
        [ 0.3833,  0.0341,  0.3584,  ..., -2.0566,  0.0303,  0.1307],
        [ 0.0640,  0.4075, -0.9609,  ..., -0.1273, -0.2722,  1.0742],
        [-0.2810,  0.7168,  0.1971,  ..., -1.1719,  0.1837,  0.4580]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([ True,  True, False, False, False,  True, False, False],
       device='cuda:5'), New X:tensor([[-0.2319, -0.1862,  1.4219,  ..., -0.8374,  0.8208,  0.3154],
        [ 0.3088,  0.0170,  0.1965,  ..., -1.0137,  0.5669, -0.1428],
        [-0.6411, -1.0908,  0.2615,  ..., -2.7227, -1.6865, -1.5762],
        [-0.2214,  0.1409,  0.3928,  ...,  0.3250, -1.2031, -2.3594],
        [-0.2810,  0.7168,  0.1971,  ..., -1.1719,  0.1837,  0.4580]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([56, 27]), X shape:torch.Size([56, 27, 512])
CTC Tokens:tensor([413,  44,   0,   0,   7], device='cuda:7'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:7'), New Tokens:tensor([413,  44,   0,   7,   0], device='cuda:7')
Org X:tensor([[ 0.5332,  0.7070,  0.2144,  ...,  0.1000,  0.8257,  0.0472],
        [ 0.8984,  0.9014, -0.8721,  ...,  0.3340,  0.5693, -0.3142],
        [ 0.9004,  0.6660, -0.4426,  ...,  0.3433,  0.5166, -0.6729],
        [ 0.3843,  0.3445, -0.9258,  ...,  0.3708,  0.9351, -1.3125],
        [ 0.2998, -0.0269, -0.0137,  ...,  0.2083,  1.2090,  0.5537]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:7'), Mixup Sent Mask:tensor([ True,  True, False, False, False,  True, False, False,  True, False],
       device='cuda:7'), New X:tensor([[ 2.6250,  1.2246, -0.9717,  ...,  2.6562,  1.9697, -0.6875],
        [ 0.8984,  0.9014, -0.8721,  ...,  0.3340,  0.5693, -0.3142],
        [-0.6411, -1.0908,  0.2615,  ..., -2.7227, -1.6865, -1.5762],
        [ 0.3843,  0.3445, -0.9258,  ...,  0.3708,  0.9351, -1.3125],
        [-0.6411, -1.0908,  0.2615,  ..., -2.7227, -1.6865, -1.5762]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 68]), X shape:torch.Size([16, 68, 512])
CTC Tokens:tensor([21, 11, 11,  6,  7], device='cuda:1'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:1'), New Tokens:tensor([21, 11,  6,  7,  0], device='cuda:1')
Org X:tensor([[ 1.3672e-01, -1.0107e-01, -2.0996e+00,  ...,  1.8530e-01,
         -3.6890e-01, -1.2421e-01],
        [ 2.7759e-01,  9.4849e-02, -1.8096e+00,  ..., -7.7295e-01,
          2.4744e-01, -5.5542e-02],
        [ 2.0352e+00,  7.9785e-01,  5.0879e-01,  ..., -1.7988e+00,
         -6.6211e-01, -1.1121e-01],
        [ 6.0516e-02,  6.8408e-01, -2.4707e+00,  ..., -6.1865e-01,
         -1.4980e+00, -5.6641e-01],
        [ 1.6846e-01,  5.7434e-02,  1.9707e+00,  ..., -4.7363e-01,
          1.3506e+00,  1.5869e-03]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:1'), Mixup Sent Mask:tensor([ True,  True, False, False, False,  True, False, False,  True, False],
       device='cuda:1'), New X:tensor([[ 1.3672e-01, -1.0107e-01, -2.0996e+00,  ...,  1.8530e-01,
         -3.6890e-01, -1.2421e-01],
        [-4.1455e-01, -1.9617e-01, -5.5225e-01,  ..., -2.3809e+00,
         -2.3059e-01, -4.1836e+00],
        [ 2.0352e+00,  7.9785e-01,  5.0879e-01,  ..., -1.7988e+00,
         -6.6211e-01, -1.1121e-01],
        [-2.9248e-01, -2.0435e-01, -2.0996e-01,  ..., -3.3667e-01,
         -1.6777e+00, -1.4766e+00],
        [ 1.6846e-01,  5.7434e-02,  1.9707e+00,  ..., -4.7363e-01,
          1.3506e+00,  1.5869e-03]], device='cuda:1', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 87]), X shape:torch.Size([8, 87, 512])
CTC Tokens:tensor([67, 67,  0, 84,  0], device='cuda:6'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:6'), New Tokens:tensor([67,  0, 84,  0, 26], device='cuda:6')
Org X:tensor([[-0.1216,  0.2203,  0.4346,  ...,  0.0443, -0.8423, -2.0332],
        [-0.7495,  0.5801, -1.0742,  ..., -2.2754,  0.1212, -1.0117],
        [-2.5547,  0.3777, -1.7168,  ..., -0.6094,  0.6392, -0.8198],
        [-0.1917,  0.2854, -0.0566,  ..., -2.7676,  1.1455, -1.5098],
        [ 0.2227, -0.1157,  0.6494,  ..., -1.3350,  0.4390, -0.1161]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([ True,  True, False, False, False,  True, False, False],
       device='cuda:6'), New X:tensor([[-0.1216,  0.2203,  0.4346,  ...,  0.0443, -0.8423, -2.0332],
        [-0.7495,  0.5801, -1.0742,  ..., -2.2754,  0.1212, -1.0117],
        [-2.5547,  0.3777, -1.7168,  ..., -0.6094,  0.6392, -0.8198],
        [-0.6411, -1.0908,  0.2615,  ..., -2.7227, -1.6865, -1.5762],
        [-0.1744, -0.0256,  0.1951,  ..., -1.7549, -1.0625, -4.3438]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 94]), X shape:torch.Size([8, 94, 512])
CTC Tokens:tensor([2514,   24, 1006,    0,    0], device='cuda:4'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:4'), New Tokens:tensor([2514,   24, 1006,    0, 5382], device='cuda:4')
Org X:tensor([[-1.0615, -0.7651, -1.0791,  ..., -0.0217,  1.4492,  0.3591],
        [-0.7529,  0.2561, -0.1906,  ..., -0.0482,  0.4104,  1.3652],
        [-0.5674,  1.4424, -0.5068,  ...,  0.2261,  0.1951,  1.0723],
        [ 0.6133,  1.3369,  0.1427,  ...,  0.1309, -0.4458,  1.1582],
        [-2.3066, -0.2034, -0.3687,  ...,  0.0552,  0.4075, -0.0098]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:4'), Mixup Sent Mask:tensor([ True,  True, False, False, False,  True, False, False],
       device='cuda:4'), New X:tensor([[ 2.6855,  1.8281,  1.7080,  ..., -0.7168, -0.8604, -0.7246],
        [-0.7529,  0.2561, -0.1906,  ..., -0.0482,  0.4104,  1.3652],
        [-1.2012,  0.2183, -0.6821,  ..., -2.3828,  0.1635, -1.0645],
        [ 0.6133,  1.3369,  0.1427,  ...,  0.1309, -0.4458,  1.1582],
        [-0.7900,  1.1426, -0.9268,  ..., -0.4250,  2.9590, -0.9917]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 73]), X shape:torch.Size([16, 73, 512])
CTC Tokens:tensor([   0,    0,    9,    0, 1491], device='cuda:2'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:2'), New Tokens:tensor([   0,    9,    0, 1491,    0], device='cuda:2')
Org X:tensor([[ 0.2007, -0.7310,  0.3257,  ...,  0.5093, -1.3047, -0.5845],
        [ 0.0963, -0.2408, -1.9727,  ...,  0.3142,  0.7407,  0.4888],
        [ 0.1030,  0.1995,  1.3975,  ...,  0.1580,  0.9141,  0.5503],
        [-1.1885,  0.3335, -0.3250,  ...,  0.2083,  1.3789,  0.8525],
        [-1.8887,  0.5029, -0.2258,  ...,  0.2461,  1.3906,  0.6060]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:2'), Mixup Sent Mask:tensor([ True,  True, False, False, False,  True, False, False,  True, False],
       device='cuda:2'), New X:tensor([[ 0.2007, -0.7310,  0.3257,  ...,  0.5093, -1.3047, -0.5845],
        [-0.1812, -0.4526, -0.3157,  ...,  0.3010, -0.9780,  1.7432],
        [-0.6411, -1.0908,  0.2615,  ..., -2.7227, -1.6865, -1.5762],
        [ 0.4001,  1.2031, -0.0695,  ...,  2.2793,  1.5127, -1.1367],
        [-1.8887,  0.5029, -0.2258,  ...,  0.2461,  1.3906,  0.6060]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 60]), X shape:torch.Size([16, 60, 512])
CTC Tokens:tensor([ 0,  0, 89,  0,  0], device='cuda:3'), Shrink Mask:tensor([ True, False,  True,  True, False], device='cuda:3'), New Tokens:tensor([   0,   89,    0, 1970,    0], device='cuda:3')
Org X:tensor([[-0.3787,  0.4180,  0.7881,  ..., -0.4165,  1.3867,  0.9819],
        [ 0.0415,  0.9365, -0.8477,  ..., -0.1938,  0.6338, -0.5791],
        [-0.6558,  2.0781, -0.3760,  ..., -1.3584,  1.1572, -0.3005],
        [ 0.2786,  1.2607,  0.4658,  ..., -0.0447, -0.6484, -0.8320],
        [-0.2458,  0.7114,  1.3291,  ..., -1.3457, -0.1360, -0.4099]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:3'), Mixup Sent Mask:tensor([ True,  True, False, False, False,  True, False, False,  True, False],
       device='cuda:3'), New X:tensor([[-0.3787,  0.4180,  0.7881,  ..., -0.4165,  1.3867,  0.9819],
        [ 0.4070, -0.2014, -0.1334,  ...,  0.6855, -0.9590, -1.5098],
        [-0.6558,  2.0781, -0.3760,  ..., -1.3584,  1.1572, -0.3005],
        [ 0.3362,  1.0781, -1.6475,  ...,  1.7529, -0.3809, -0.9731],
        [-0.2458,  0.7114,  1.3291,  ..., -1.3457, -0.1360, -0.4099]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:3')
2023-07-27 21:56:20 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.21 | trans_loss 5.589 | nll_loss 2.87 | w2v_ctc_loss 1.319 | task_loss 4.626 | contrastive_loss 0.274 | total 4003.4 | n_correct 2472.7 | ppl 7.31 | accuracy 61.765 | uer 17.182 | wer 18.94 | raw_wer 18.94 | bleu 19.9 | wps 1871.9 | wpb 4003.4 | bsz 141.8 | num_updates 30941 | best_bleu 20.21
2023-07-27 21:56:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30941 updates
2023-07-27 21:56:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9004.pt
2023-07-27 21:56:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9004.pt
2023-07-27 21:56:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.9004.pt (epoch 21 @ 30941 updates, score 19.9) (writing took 12.973353469744325 seconds)
2023-07-27 21:56:33 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-07-27 21:56:33 | INFO | train | epoch 021 | loss 2.024 | trans_loss 5.033 | nll_loss 2.236 | w2v_ctc_loss 0.654 | task_loss 1.401 | contrastive_loss 0.121 | total 4137.86 | n_correct 2634.72 | ppl 4.71 | accuracy 63.673 | wps 10912.4 | ups 1.32 | wpb 8275.7 | bsz 305.4 | num_updates 30941 | lr 8.03985e-05 | gnorm 0.541 | clip 0 | loss_scale 64 | train_wall 1015 | gb_free 15.4 | wall 26511
2023-07-27 21:56:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 21:56:34 | INFO | fairseq.trainer | begin training epoch 22
2023-07-27 21:56:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 21:57:24 | INFO | train_inner | epoch 022:     59 / 1474 loss=2.016, trans_loss=5.021, nll_loss=2.22, w2v_ctc_loss=0.655, task_loss=1.415, contrastive_loss=0.062, total=4140.16, n_correct=2647.89, ppl=4.66, accuracy=63.956, wps=7015.8, ups=0.85, wpb=8280.3, bsz=300.1, num_updates=31000, lr=8.03219e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=26562
2023-07-27 21:58:33 | INFO | train_inner | epoch 022:    159 / 1474 loss=2.014, trans_loss=5.009, nll_loss=2.204, w2v_ctc_loss=0.647, task_loss=1.413, contrastive_loss=0.149, total=4115.86, n_correct=2638.63, ppl=4.61, accuracy=64.109, wps=11894.6, ups=1.44, wpb=8231.7, bsz=309.4, num_updates=31100, lr=8.01927e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=68, gb_free=17.3, wall=26631
2023-07-27 21:59:43 | INFO | train_inner | epoch 022:    259 / 1474 loss=2.001, trans_loss=5.005, nll_loss=2.2, w2v_ctc_loss=0.636, task_loss=1.271, contrastive_loss=0.076, total=4247.73, n_correct=2735.41, ppl=4.59, accuracy=64.397, wps=12144.5, ups=1.43, wpb=8495.5, bsz=323.2, num_updates=31200, lr=8.00641e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=69, gb_free=13.8, wall=26701
2023-07-27 22:00:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-27 22:00:54 | INFO | train_inner | epoch 022:    360 / 1474 loss=2.022, trans_loss=5.018, nll_loss=2.216, w2v_ctc_loss=0.65, task_loss=1.416, contrastive_loss=0.195, total=4186, n_correct=2677.16, ppl=4.65, accuracy=63.955, wps=11764.2, ups=1.41, wpb=8372, bsz=310, num_updates=31300, lr=7.99361e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=70, gb_free=17.5, wall=26772
2023-07-27 22:02:04 | INFO | train_inner | epoch 022:    460 / 1474 loss=2.025, trans_loss=5.027, nll_loss=2.226, w2v_ctc_loss=0.654, task_loss=1.475, contrastive_loss=0.13, total=4132.62, n_correct=2636.57, ppl=4.68, accuracy=63.799, wps=11880.6, ups=1.44, wpb=8265.2, bsz=297.2, num_updates=31400, lr=7.98087e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=26841
2023-07-27 22:03:14 | INFO | train_inner | epoch 022:    560 / 1474 loss=2.014, trans_loss=5.02, nll_loss=2.219, w2v_ctc_loss=0.652, task_loss=1.406, contrastive_loss=0.075, total=4155.5, n_correct=2657.86, ppl=4.65, accuracy=63.96, wps=11857.3, ups=1.43, wpb=8311, bsz=307.3, num_updates=31500, lr=7.96819e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=26912
2023-07-27 22:04:23 | INFO | train_inner | epoch 022:    660 / 1474 loss=2.007, trans_loss=5.01, nll_loss=2.207, w2v_ctc_loss=0.633, task_loss=1.327, contrastive_loss=0.158, total=4147.84, n_correct=2661.3, ppl=4.62, accuracy=64.161, wps=11955.4, ups=1.44, wpb=8295.7, bsz=313.1, num_updates=31600, lr=7.95557e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=69, gb_free=12.9, wall=26981
2023-07-27 22:05:33 | INFO | train_inner | epoch 022:    760 / 1474 loss=2.014, trans_loss=5.019, nll_loss=2.217, w2v_ctc_loss=0.651, task_loss=1.433, contrastive_loss=0.077, total=4166.89, n_correct=2665.43, ppl=4.65, accuracy=63.967, wps=11973.7, ups=1.44, wpb=8333.8, bsz=304.3, num_updates=31700, lr=7.94301e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=69, gb_free=15.9, wall=27051
2023-07-27 22:06:43 | INFO | train_inner | epoch 022:    860 / 1474 loss=2.022, trans_loss=5.034, nll_loss=2.237, w2v_ctc_loss=0.657, task_loss=1.525, contrastive_loss=0.061, total=4074.75, n_correct=2590.21, ppl=4.71, accuracy=63.567, wps=11633.4, ups=1.43, wpb=8149.5, bsz=288.4, num_updates=31800, lr=7.93052e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=27121
2023-07-27 22:07:52 | INFO | train_inner | epoch 022:    960 / 1474 loss=2.01, trans_loss=5.024, nll_loss=2.224, w2v_ctc_loss=0.642, task_loss=1.406, contrastive_loss=0.062, total=4136.34, n_correct=2647.74, ppl=4.67, accuracy=64.012, wps=11889.5, ups=1.44, wpb=8272.7, bsz=303.7, num_updates=31900, lr=7.91808e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=27190
2023-07-27 22:09:01 | INFO | train_inner | epoch 022:   1060 / 1474 loss=2.02, trans_loss=5.024, nll_loss=2.225, w2v_ctc_loss=0.641, task_loss=1.335, contrastive_loss=0.234, total=4157.21, n_correct=2656.98, ppl=4.68, accuracy=63.913, wps=12041.1, ups=1.45, wpb=8314.4, bsz=315.4, num_updates=32000, lr=7.90569e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=68, gb_free=11.9, wall=27259
2023-07-27 22:09:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 22:09:26 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.201 | trans_loss 5.581 | nll_loss 2.861 | w2v_ctc_loss 1.314 | task_loss 4.63 | contrastive_loss 0.261 | total 4003.4 | n_correct 2477.3 | ppl 7.27 | accuracy 61.88 | uer 17.052 | wer 19.022 | raw_wer 19.022 | bleu 19.8 | wps 1999.6 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 20.21
2023-07-27 22:09:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-07-27 22:09:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_22_32000.pt
2023-07-27 22:09:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_22_32000.pt
2023-07-27 22:09:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 19.8) (writing took 15.836254755035043 seconds)
2023-07-27 22:10:51 | INFO | train_inner | epoch 022:   1160 / 1474 loss=2.033, trans_loss=5.047, nll_loss=2.254, w2v_ctc_loss=0.661, task_loss=1.46, contrastive_loss=0.116, total=4092.91, n_correct=2594.52, ppl=4.77, accuracy=63.391, wps=7437.3, ups=0.91, wpb=8185.8, bsz=294.3, num_updates=32100, lr=7.89337e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=68, gb_free=15.9, wall=27369
2023-07-27 22:12:01 | INFO | train_inner | epoch 022:   1260 / 1474 loss=2.021, trans_loss=5.037, nll_loss=2.243, w2v_ctc_loss=0.651, task_loss=1.299, contrastive_loss=0.115, total=4182.65, n_correct=2658.95, ppl=4.73, accuracy=63.571, wps=11995.1, ups=1.43, wpb=8365.3, bsz=323.6, num_updates=32200, lr=7.8811e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=27439
2023-07-27 22:13:10 | INFO | train_inner | epoch 022:   1360 / 1474 loss=2.015, trans_loss=5.029, nll_loss=2.232, w2v_ctc_loss=0.64, task_loss=1.392, contrastive_loss=0.13, total=4071.58, n_correct=2603.14, ppl=4.7, accuracy=63.934, wps=11863.7, ups=1.46, wpb=8143.2, bsz=300.6, num_updates=32300, lr=7.86889e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=68, gb_free=16.7, wall=27508
2023-07-27 22:14:19 | INFO | train_inner | epoch 022:   1460 / 1474 loss=2.03, trans_loss=5.045, nll_loss=2.251, w2v_ctc_loss=0.664, task_loss=1.499, contrastive_loss=0.078, total=4077.83, n_correct=2586.65, ppl=4.76, accuracy=63.432, wps=11849, ups=1.45, wpb=8155.7, bsz=288, num_updates=32400, lr=7.85674e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=68, gb_free=16.1, wall=27577
2023-07-27 22:14:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 22:14:52 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.201 | trans_loss 5.58 | nll_loss 2.857 | w2v_ctc_loss 1.315 | task_loss 4.614 | contrastive_loss 0.263 | total 4003.4 | n_correct 2481.1 | ppl 7.24 | accuracy 61.975 | uer 16.954 | wer 18.679 | raw_wer 18.679 | bleu 19.57 | wps 2230.8 | wpb 4003.4 | bsz 141.8 | num_updates 32414 | best_bleu 20.21
2023-07-27 22:14:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32414 updates
2023-07-27 22:14:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-27 22:15:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-27 22:15:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt (epoch 22 @ 32414 updates, score 19.57) (writing took 11.461468672379851 seconds)
2023-07-27 22:15:03 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-07-27 22:15:03 | INFO | train | epoch 022 | loss 2.017 | trans_loss 5.024 | nll_loss 2.224 | w2v_ctc_loss 0.649 | task_loss 1.404 | contrastive_loss 0.116 | total 4137.49 | n_correct 2643.18 | ppl 4.67 | accuracy 63.884 | wps 10983.4 | ups 1.33 | wpb 8275 | bsz 305.2 | num_updates 32414 | lr 7.85505e-05 | gnorm 0.544 | clip 0 | loss_scale 32 | train_wall 1014 | gb_free 11.6 | wall 27621
2023-07-27 22:15:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 22:15:03 | INFO | fairseq.trainer | begin training epoch 23
2023-07-27 22:15:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 22:16:12 | INFO | train_inner | epoch 023:     86 / 1474 loss=2.007, trans_loss=5.002, nll_loss=2.196, w2v_ctc_loss=0.65, task_loss=1.449, contrastive_loss=0.07, total=4089.8, n_correct=2629.91, ppl=4.58, accuracy=64.304, wps=7212.7, ups=0.88, wpb=8179.6, bsz=299.5, num_updates=32500, lr=7.84465e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=70, gb_free=16.6, wall=27690
2023-07-27 22:17:22 | INFO | train_inner | epoch 023:    186 / 1474 loss=2.001, trans_loss=4.996, nll_loss=2.187, w2v_ctc_loss=0.637, task_loss=1.474, contrastive_loss=0.067, total=4117.76, n_correct=2653.98, ppl=4.55, accuracy=64.452, wps=11726.2, ups=1.42, wpb=8235.5, bsz=296, num_updates=32600, lr=7.8326e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=69, gb_free=15.5, wall=27760
2023-07-27 22:18:31 | INFO | train_inner | epoch 023:    286 / 1474 loss=2.007, trans_loss=5.006, nll_loss=2.2, w2v_ctc_loss=0.631, task_loss=1.423, contrastive_loss=0.149, total=4144.73, n_correct=2663.32, ppl=4.59, accuracy=64.258, wps=11986.4, ups=1.45, wpb=8289.5, bsz=304, num_updates=32700, lr=7.82062e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=68, gb_free=17.5, wall=27829
2023-07-27 22:19:41 | INFO | train_inner | epoch 023:    386 / 1474 loss=2.001, trans_loss=5.003, nll_loss=2.196, w2v_ctc_loss=0.637, task_loss=1.438, contrastive_loss=0.057, total=4126.79, n_correct=2656.6, ppl=4.58, accuracy=64.374, wps=11884.1, ups=1.44, wpb=8253.6, bsz=296.4, num_updates=32800, lr=7.80869e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=27899
2023-07-27 22:20:51 | INFO | train_inner | epoch 023:    486 / 1474 loss=2.012, trans_loss=5.013, nll_loss=2.21, w2v_ctc_loss=0.644, task_loss=1.37, contrastive_loss=0.122, total=4150.15, n_correct=2659.29, ppl=4.63, accuracy=64.077, wps=11915.8, ups=1.44, wpb=8300.3, bsz=312.1, num_updates=32900, lr=7.79681e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=69, gb_free=16.2, wall=27969
2023-07-27 22:21:59 | INFO | train_inner | epoch 023:    586 / 1474 loss=1.994, trans_loss=4.997, nll_loss=2.19, w2v_ctc_loss=0.63, task_loss=1.328, contrastive_loss=0.064, total=4174.6, n_correct=2696.43, ppl=4.56, accuracy=64.591, wps=12147.4, ups=1.45, wpb=8349.2, bsz=316.3, num_updates=33000, lr=7.78499e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=68, gb_free=16.4, wall=28037
2023-07-27 22:23:08 | INFO | train_inner | epoch 023:    686 / 1474 loss=2.005, trans_loss=5.006, nll_loss=2.201, w2v_ctc_loss=0.635, task_loss=1.409, contrastive_loss=0.105, total=4136.6, n_correct=2660.43, ppl=4.6, accuracy=64.314, wps=12058.7, ups=1.46, wpb=8273.2, bsz=301.2, num_updates=33100, lr=7.77322e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=68, gb_free=17.7, wall=28106
2023-07-27 22:24:17 | INFO | train_inner | epoch 023:    786 / 1474 loss=2.01, trans_loss=5.016, nll_loss=2.214, w2v_ctc_loss=0.643, task_loss=1.415, contrastive_loss=0.086, total=4147.22, n_correct=2659.64, ppl=4.64, accuracy=64.131, wps=11924, ups=1.44, wpb=8294.4, bsz=305.1, num_updates=33200, lr=7.76151e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=69, gb_free=17.3, wall=28175
2023-07-27 22:25:27 | INFO | train_inner | epoch 023:    886 / 1474 loss=2.009, trans_loss=5.012, nll_loss=2.211, w2v_ctc_loss=0.637, task_loss=1.27, contrastive_loss=0.165, total=4193.16, n_correct=2690.78, ppl=4.63, accuracy=64.171, wps=12127.9, ups=1.45, wpb=8386.3, bsz=327.3, num_updates=33300, lr=7.74984e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=68, gb_free=15.9, wall=28245
2023-07-27 22:26:36 | INFO | train_inner | epoch 023:    986 / 1474 loss=2.02, trans_loss=5.017, nll_loss=2.216, w2v_ctc_loss=0.631, task_loss=1.396, contrastive_loss=0.321, total=4164.33, n_correct=2665.31, ppl=4.65, accuracy=64.003, wps=11994.2, ups=1.44, wpb=8328.7, bsz=310.1, num_updates=33400, lr=7.73823e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=28314
2023-07-27 22:27:45 | INFO | train_inner | epoch 023:   1086 / 1474 loss=2.017, trans_loss=5.024, nll_loss=2.224, w2v_ctc_loss=0.654, task_loss=1.504, contrastive_loss=0.071, total=4088.37, n_correct=2613.43, ppl=4.67, accuracy=63.924, wps=11830.5, ups=1.45, wpb=8176.7, bsz=289.6, num_updates=33500, lr=7.72667e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=68, gb_free=15.5, wall=28383
2023-07-27 22:28:55 | INFO | train_inner | epoch 023:   1186 / 1474 loss=2.008, trans_loss=5.022, nll_loss=2.222, w2v_ctc_loss=0.644, task_loss=1.391, contrastive_loss=0.064, total=4162.3, n_correct=2665.17, ppl=4.67, accuracy=64.031, wps=11919.6, ups=1.43, wpb=8324.6, bsz=309, num_updates=33600, lr=7.71517e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=69, gb_free=15.6, wall=28453
2023-07-27 22:30:04 | INFO | train_inner | epoch 023:   1286 / 1474 loss=2.005, trans_loss=5.021, nll_loss=2.222, w2v_ctc_loss=0.635, task_loss=1.364, contrastive_loss=0.077, total=4131.74, n_correct=2650.83, ppl=4.66, accuracy=64.158, wps=11979.1, ups=1.45, wpb=8263.5, bsz=308.7, num_updates=33700, lr=7.70371e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=68, gb_free=17, wall=28522
2023-07-27 22:31:14 | INFO | train_inner | epoch 023:   1386 / 1474 loss=2.023, trans_loss=5.04, nll_loss=2.245, w2v_ctc_loss=0.647, task_loss=1.418, contrastive_loss=0.135, total=4141.25, n_correct=2638.26, ppl=4.74, accuracy=63.707, wps=11897.1, ups=1.44, wpb=8282.5, bsz=304.7, num_updates=33800, lr=7.69231e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=69, gb_free=16.7, wall=28592
2023-07-27 22:32:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 22:32:39 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 4.213 | trans_loss 5.579 | nll_loss 2.858 | w2v_ctc_loss 1.356 | task_loss 4.638 | contrastive_loss 0.263 | total 4003.4 | n_correct 2471.6 | ppl 7.25 | accuracy 61.738 | uer 17.065 | wer 18.698 | raw_wer 18.698 | bleu 19.76 | wps 2057.2 | wpb 4003.4 | bsz 141.8 | num_updates 33888 | best_bleu 20.21
2023-07-27 22:32:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33888 updates
2023-07-27 22:32:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.7605.pt
2023-07-27 22:32:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.7605.pt
2023-07-27 22:32:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.7605.pt (epoch 23 @ 33888 updates, score 19.76) (writing took 12.420893488451838 seconds)
2023-07-27 22:32:52 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-07-27 22:32:52 | INFO | train | epoch 023 | loss 2.01 | trans_loss 5.014 | nll_loss 2.211 | w2v_ctc_loss 0.64 | task_loss 1.402 | contrastive_loss 0.119 | total 4138.65 | n_correct 2655.15 | ppl 4.63 | accuracy 64.155 | wps 11412.8 | ups 1.38 | wpb 8277.3 | bsz 305.7 | num_updates 33888 | lr 7.68231e-05 | gnorm 0.545 | clip 0 | loss_scale 64 | train_wall 1012 | gb_free 13.6 | wall 28690
2023-07-27 22:32:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 22:32:52 | INFO | fairseq.trainer | begin training epoch 24
2023-07-27 22:32:52 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 22:33:09 | INFO | train_inner | epoch 024:     12 / 1474 loss=2.024, trans_loss=5.032, nll_loss=2.236, w2v_ctc_loss=0.639, task_loss=1.396, contrastive_loss=0.213, total=4095.53, n_correct=2613.65, ppl=4.71, accuracy=63.817, wps=7085.8, ups=0.87, wpb=8191.1, bsz=306.3, num_updates=33900, lr=7.68095e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=69, gb_free=17.3, wall=28707
2023-07-27 22:33:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-27 22:34:19 | INFO | train_inner | epoch 024:    113 / 1474 loss=1.998, trans_loss=4.986, nll_loss=2.175, w2v_ctc_loss=0.63, task_loss=1.336, contrastive_loss=0.167, total=4140.2, n_correct=2676.8, ppl=4.52, accuracy=64.654, wps=11859.3, ups=1.43, wpb=8280.4, bsz=315.3, num_updates=34000, lr=7.66965e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=69, gb_free=16.4, wall=28777
2023-07-27 22:34:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 22:34:43 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.206 | trans_loss 5.579 | nll_loss 2.856 | w2v_ctc_loss 1.337 | task_loss 4.638 | contrastive_loss 0.253 | total 4003.4 | n_correct 2470.8 | ppl 7.24 | accuracy 61.718 | uer 17.055 | wer 18.769 | raw_wer 18.769 | bleu 19.93 | wps 2005 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 20.21
2023-07-27 22:34:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-07-27 22:34:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_24_34000.pt
2023-07-27 22:34:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_24_34000.pt
2023-07-27 22:34:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 19.93) (writing took 14.690186593681574 seconds)
2023-07-27 22:36:08 | INFO | train_inner | epoch 024:    213 / 1474 loss=2, trans_loss=4.987, nll_loss=2.177, w2v_ctc_loss=0.619, task_loss=1.226, contrastive_loss=0.291, total=4251.29, n_correct=2749.81, ppl=4.52, accuracy=64.682, wps=7794, ups=0.92, wpb=8502.6, bsz=340.8, num_updates=34100, lr=7.6584e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=69, gb_free=16.7, wall=28886
2023-07-27 22:37:17 | INFO | train_inner | epoch 024:    313 / 1474 loss=1.993, trans_loss=4.991, nll_loss=2.182, w2v_ctc_loss=0.631, task_loss=1.37, contrastive_loss=0.06, total=4128.18, n_correct=2668.51, ppl=4.54, accuracy=64.641, wps=11915.5, ups=1.44, wpb=8256.4, bsz=305.5, num_updates=34200, lr=7.64719e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=68, gb_free=16.1, wall=28955
2023-07-27 22:38:27 | INFO | train_inner | epoch 024:    413 / 1474 loss=2.017, trans_loss=5.001, nll_loss=2.195, w2v_ctc_loss=0.642, task_loss=1.466, contrastive_loss=0.21, total=4158.92, n_correct=2671.04, ppl=4.58, accuracy=64.224, wps=12003.5, ups=1.44, wpb=8317.8, bsz=299.7, num_updates=34300, lr=7.63604e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=29025
2023-07-27 22:39:37 | INFO | train_inner | epoch 024:    513 / 1474 loss=2.004, trans_loss=5, nll_loss=2.193, w2v_ctc_loss=0.636, task_loss=1.422, contrastive_loss=0.128, total=4144.91, n_correct=2667.86, ppl=4.57, accuracy=64.365, wps=11892.5, ups=1.43, wpb=8289.8, bsz=303.3, num_updates=34400, lr=7.62493e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=29094
2023-07-27 22:40:46 | INFO | train_inner | epoch 024:    613 / 1474 loss=1.999, trans_loss=5.001, nll_loss=2.195, w2v_ctc_loss=0.627, task_loss=1.406, contrastive_loss=0.096, total=4165.3, n_correct=2682.96, ppl=4.58, accuracy=64.412, wps=11950.9, ups=1.43, wpb=8330.6, bsz=307.7, num_updates=34500, lr=7.61387e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=69, gb_free=15.6, wall=29164
2023-07-27 22:41:55 | INFO | train_inner | epoch 024:    713 / 1474 loss=2.006, trans_loss=5.01, nll_loss=2.206, w2v_ctc_loss=0.635, task_loss=1.442, contrastive_loss=0.104, total=4102.21, n_correct=2634.13, ppl=4.61, accuracy=64.212, wps=11863.5, ups=1.45, wpb=8204.4, bsz=295.1, num_updates=34600, lr=7.60286e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=29233
2023-07-27 22:43:05 | INFO | train_inner | epoch 024:    813 / 1474 loss=2.003, trans_loss=5.012, nll_loss=2.21, w2v_ctc_loss=0.632, task_loss=1.412, contrastive_loss=0.085, total=4110.6, n_correct=2638.48, ppl=4.63, accuracy=64.187, wps=11815.3, ups=1.44, wpb=8221.2, bsz=305.3, num_updates=34700, lr=7.5919e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=29303
2023-07-27 22:44:14 | INFO | train_inner | epoch 024:    913 / 1474 loss=2.012, trans_loss=5.019, nll_loss=2.216, w2v_ctc_loss=0.644, task_loss=1.555, contrastive_loss=0.055, total=4043.03, n_correct=2585.61, ppl=4.65, accuracy=63.952, wps=11790.4, ups=1.46, wpb=8086.1, bsz=281, num_updates=34800, lr=7.58098e-05, gnorm=0.558, clip=0, loss_scale=32, train_wall=68, gb_free=11.1, wall=29371
2023-07-27 22:45:23 | INFO | train_inner | epoch 024:   1013 / 1474 loss=2.003, trans_loss=5.016, nll_loss=2.214, w2v_ctc_loss=0.634, task_loss=1.446, contrastive_loss=0.06, total=4136.81, n_correct=2656.77, ppl=4.64, accuracy=64.223, wps=11863.5, ups=1.43, wpb=8273.6, bsz=298.4, num_updates=34900, lr=7.57011e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=29441
2023-07-27 22:46:32 | INFO | train_inner | epoch 024:   1113 / 1474 loss=2.003, trans_loss=5.003, nll_loss=2.198, w2v_ctc_loss=0.64, task_loss=1.351, contrastive_loss=0.107, total=4135.73, n_correct=2660.89, ppl=4.59, accuracy=64.339, wps=12007.8, ups=1.45, wpb=8271.5, bsz=308.9, num_updates=35000, lr=7.55929e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=68, gb_free=17, wall=29510
Mixup rate:0.5, token after shrink shape:torch.Size([16, 64]), X shape:torch.Size([16, 64, 512])
CTC Tokens:tensor([   0,    0,    0,    0, 3396], device='cuda:0'), Shrink Mask:tensor([ True, False, False, False,  True], device='cuda:0'), New Tokens:tensor([   0, 3396,    0,   94,    0], device='cuda:0')
Org X:tensor([[ 0.8047, -2.1934,  1.1982,  ..., -0.4087,  0.3274,  0.9346],
        [-2.2988, -0.3474, -0.9082,  ..., -0.0765,  0.3867,  0.3582],
        [-1.1025,  0.0489,  1.8643,  ..., -0.2246,  0.9893,  1.0781],
        [ 0.5127,  0.2571, -1.0596,  ..., -1.7139,  1.4658, -0.1438],
        [ 1.4443,  0.2922,  0.7017,  ..., -0.6177,  0.8081,  0.3298]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:0'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:0'), New X:tensor([[ 0.8047, -2.1934,  1.1982,  ..., -0.4087,  0.3274,  0.9346],
        [-2.2988, -0.3474, -0.9082,  ..., -0.0765,  0.3867,  0.3582],
        [-1.1025,  0.0489,  1.8643,  ..., -0.2246,  0.9893,  1.0781],
        [ 0.5127,  0.2571, -1.0596,  ..., -1.7139,  1.4658, -0.1438],
        [ 1.4443,  0.2922,  0.7017,  ..., -0.6177,  0.8081,  0.3298]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:0')
2023-07-27 22:47:42 | INFO | train_inner | epoch 024:   1213 / 1474 loss=2.003, trans_loss=5.011, nll_loss=2.209, w2v_ctc_loss=0.633, task_loss=1.388, contrastive_loss=0.096, total=4148.3, n_correct=2668.38, ppl=4.62, accuracy=64.325, wps=11917, ups=1.44, wpb=8296.6, bsz=310.8, num_updates=35100, lr=7.54851e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=69, gb_free=16.6, wall=29580
2023-07-27 22:48:51 | INFO | train_inner | epoch 024:   1313 / 1474 loss=2.012, trans_loss=5.021, nll_loss=2.22, w2v_ctc_loss=0.649, task_loss=1.49, contrastive_loss=0.065, total=4110.05, n_correct=2632.49, ppl=4.66, accuracy=64.05, wps=11795.8, ups=1.43, wpb=8220.1, bsz=294.3, num_updates=35200, lr=7.53778e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=29649
2023-07-27 22:50:00 | INFO | train_inner | epoch 024:   1413 / 1474 loss=2.014, trans_loss=5.025, nll_loss=2.226, w2v_ctc_loss=0.651, task_loss=1.465, contrastive_loss=0.064, total=4090.91, n_correct=2613.88, ppl=4.68, accuracy=63.895, wps=11931.9, ups=1.46, wpb=8181.8, bsz=292.7, num_updates=35300, lr=7.5271e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=68, gb_free=16, wall=29718
2023-07-27 22:50:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([8, 84]), X shape:torch.Size([8, 84, 512])
CTC Tokens:tensor([29, 29, 29,  0,  0], device='cuda:6'), Shrink Mask:tensor([ True, False, False,  True, False], device='cuda:6'), New Tokens:tensor([ 29,   0,  24, 192,  11], device='cuda:6')
Org X:tensor([[-0.3479,  0.1245,  0.6079,  ...,  0.3445, -0.8379, -0.1445],
        [-0.1724,  0.7505,  2.0059,  ..., -0.1138, -0.2690, -0.0235],
        [-1.9922,  0.3052, -0.4014,  ..., -0.8125,  0.7002, -0.8574],
        [ 0.4924,  0.5435, -0.6987,  ...,  0.1533,  0.3267, -1.0332],
        [ 1.5312,  0.4351, -2.0410,  ..., -0.5337,  0.6055, -0.6738]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True],
       device='cuda:6'), New X:tensor([[-0.3479,  0.1245,  0.6079,  ...,  0.3445, -0.8379, -0.1445],
        [-0.1724,  0.7505,  2.0059,  ..., -0.1138, -0.2690, -0.0235],
        [-1.9922,  0.3052, -0.4014,  ..., -0.8125,  0.7002, -0.8574],
        [ 0.4924,  0.5435, -0.6987,  ...,  0.1533,  0.3267, -1.0332],
        [ 1.5312,  0.4351, -2.0410,  ..., -0.5337,  0.6055, -0.6738]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 98]), X shape:torch.Size([8, 98, 512])
CTC Tokens:tensor([2266,   19,   11,  121,  226], device='cuda:4'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:4'), New Tokens:tensor([2266,   19,   11,  121,  226], device='cuda:4')
Org X:tensor([[ 0.3960, -0.2798,  0.4033,  ...,  0.7441, -0.6963, -0.1615],
        [ 0.1521,  0.1219, -1.8525,  ...,  0.0680, -0.0246, -0.5273],
        [ 1.6094,  0.3853, -1.7803,  ...,  0.3711,  0.5254, -0.4175],
        [ 1.9307,  0.6387, -1.0234,  ...,  0.2361, -0.8945,  0.1406],
        [ 0.5039,  0.9658, -0.1709,  ..., -0.4270, -0.4836,  0.1567]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:4'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True],
       device='cuda:4'), New X:tensor([[ 0.3960, -0.2798,  0.4033,  ...,  0.7441, -0.6963, -0.1615],
        [ 0.1521,  0.1219, -1.8525,  ...,  0.0680, -0.0246, -0.5273],
        [ 1.6094,  0.3853, -1.7803,  ...,  0.3711,  0.5254, -0.4175],
        [ 1.9307,  0.6387, -1.0234,  ...,  0.2361, -0.8945,  0.1406],
        [ 0.5039,  0.9658, -0.1709,  ..., -0.4270, -0.4836,  0.1567]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 91]), X shape:torch.Size([8, 91, 512])
CTC Tokens:tensor([33, 33, 34, 91, 77], device='cuda:7'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:7'), New Tokens:tensor([33, 34, 91, 77, 12], device='cuda:7')
Org X:tensor([[-0.0035,  0.1519,  0.3491,  ..., -1.3301, -0.0064,  0.5586],
        [ 0.9644,  0.0460,  2.0039,  ..., -1.5430, -1.9561, -0.2377],
        [-1.2227,  0.0578,  0.9253,  ...,  0.0518, -1.6055,  0.9014],
        [ 0.5137,  0.1223,  1.1504,  ...,  0.2551, -1.2432,  0.8701],
        [ 0.0432,  0.4980,  0.1729,  ..., -0.8296, -0.3113,  0.6221]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]]], device='cuda:7'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True],
       device='cuda:7'), New X:tensor([[-0.0035,  0.1519,  0.3491,  ..., -1.3301, -0.0064,  0.5586],
        [ 0.9644,  0.0460,  2.0039,  ..., -1.5430, -1.9561, -0.2377],
        [-1.2227,  0.0578,  0.9253,  ...,  0.0518, -1.6055,  0.9014],
        [ 0.5137,  0.1223,  1.1504,  ...,  0.2551, -1.2432,  0.8701],
        [ 0.0432,  0.4980,  0.1729,  ..., -0.8296, -0.3113,  0.6221]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 76]), X shape:torch.Size([8, 76, 512])
CTC Tokens:tensor([  0, 103,   0,   0,  24], device='cuda:1'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:1'), New Tokens:tensor([  0, 103,   0,  24,   0], device='cuda:1')
Org X:tensor([[ 0.5127, -0.7998,  0.5361,  ..., -0.6709,  1.3936, -0.3264],
        [ 0.2400,  0.2959, -0.1453,  ...,  0.2397,  1.0840,  0.1478],
        [ 0.1741,  0.3682,  0.3291,  ..., -0.2759,  0.3438,  0.3069],
        [-1.8867,  0.3669,  0.0487,  ..., -1.0283,  0.5498, -0.7490],
        [ 0.5054,  0.3608,  1.7832,  ..., -0.7168,  0.9990,  0.0064]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:1'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True],
       device='cuda:1'), New X:tensor([[ 0.5127, -0.7998,  0.5361,  ..., -0.6709,  1.3936, -0.3264],
        [ 0.2400,  0.2959, -0.1453,  ...,  0.2397,  1.0840,  0.1478],
        [ 0.1741,  0.3682,  0.3291,  ..., -0.2759,  0.3438,  0.3069],
        [-1.8867,  0.3669,  0.0487,  ..., -1.0283,  0.5498, -0.7490],
        [ 0.5054,  0.3608,  1.7832,  ..., -0.7168,  0.9990,  0.0064]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 82]), X shape:torch.Size([8, 82, 512])
CTC Tokens:tensor([  0,  24,   0, 135,   0], device='cuda:3'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:3'), New Tokens:tensor([  0,  24,   0, 135,   0], device='cuda:3')
Org X:tensor([[-0.5508, -0.6489,  0.0362,  ..., -0.3293,  1.2803, -0.8262],
        [-1.1572,  0.8452,  0.3718,  ..., -0.0225,  1.3516, -0.7783],
        [ 1.5283,  0.3772,  3.5000,  ..., -0.4170, -0.1256,  0.1824],
        [ 0.2576,  0.8760,  0.9800,  ..., -0.1274, -1.0732,  0.1257],
        [ 0.7983,  0.9082,  1.1387,  ..., -2.3984, -0.9360, -0.7148]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:3'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True],
       device='cuda:3'), New X:tensor([[-0.5508, -0.6489,  0.0362,  ..., -0.3293,  1.2803, -0.8262],
        [-1.1572,  0.8452,  0.3718,  ..., -0.0225,  1.3516, -0.7783],
        [ 1.5283,  0.3772,  3.5000,  ..., -0.4170, -0.1256,  0.1824],
        [ 0.2576,  0.8760,  0.9800,  ..., -0.1274, -1.0732,  0.1257],
        [ 0.7983,  0.9082,  1.1387,  ..., -2.3984, -0.9360, -0.7148]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 58]), X shape:torch.Size([16, 58, 512])
CTC Tokens:tensor([  0,  67,   0, 100,   0], device='cuda:5'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:5'), New Tokens:tensor([  0,  67,   0, 100,   0], device='cuda:5')
Org X:tensor([[-0.2778, -0.3359,  0.5210,  ...,  0.5625, -0.7974, -0.4807],
        [-0.4321, -0.4675,  1.2422,  ...,  0.2385, -0.7642, -0.1660],
        [ 0.6001,  0.1301,  2.2090,  ...,  0.4365, -0.2097,  0.3938],
        [-0.3921,  0.2356,  0.3135,  ...,  0.4937, -1.4375,  0.0148],
        [ 0.4980,  0.7803,  1.7754,  ..., -0.2939, -2.2812, -0.1577]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:5'), New X:tensor([[-0.2778, -0.3359,  0.5210,  ...,  0.5625, -0.7974, -0.4807],
        [-0.4321, -0.4675,  1.2422,  ...,  0.2385, -0.7642, -0.1660],
        [ 0.6001,  0.1301,  2.2090,  ...,  0.4365, -0.2097,  0.3938],
        [-0.3921,  0.2356,  0.3135,  ...,  0.4937, -1.4375,  0.0148],
        [ 0.4980,  0.7803,  1.7754,  ..., -0.2939, -2.2812, -0.1577]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([40, 38]), X shape:torch.Size([40, 38, 512])
CTC Tokens:tensor([ 0, 21,  0,  0,  0], device='cuda:2'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:2'), New Tokens:tensor([   0,   21,    0, 3119,    0], device='cuda:2')
Org X:tensor([[ 0.1803, -0.2095, -0.4094,  ..., -0.9424, -0.0279, -1.3799],
        [-0.3396,  0.2090, -2.2324,  ..., -1.9883,  0.6323, -1.0674],
        [ 0.6074, -0.3691,  1.7041,  ..., -2.1543,  1.4473,  0.2942],
        [-0.2795,  0.0632, -0.1489,  ..., -0.2319, -0.3105,  0.2405],
        [ 0.3430,  0.2180,  0.4282,  ...,  0.1184,  0.0580,  0.7568]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:2'), Mixup Sent Mask:tensor([False, False,  True, False, False, False, False,  True, False, False],
       device='cuda:2'), New X:tensor([[ 0.1803, -0.2095, -0.4094,  ..., -0.9424, -0.0279, -1.3799],
        [-0.3396,  0.2090, -2.2324,  ..., -1.9883,  0.6323, -1.0674],
        [ 0.6074, -0.3691,  1.7041,  ..., -2.1543,  1.4473,  0.2942],
        [-0.2795,  0.0632, -0.1489,  ..., -0.2319, -0.3105,  0.2405],
        [ 0.3430,  0.2180,  0.4282,  ...,  0.1184,  0.0580,  0.7568]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:2')
2023-07-27 22:51:07 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.201 | trans_loss 5.581 | nll_loss 2.86 | w2v_ctc_loss 1.315 | task_loss 4.641 | contrastive_loss 0.259 | total 4003.4 | n_correct 2476.6 | ppl 7.26 | accuracy 61.862 | uer 17.084 | wer 18.911 | raw_wer 18.911 | bleu 19.87 | wps 2035 | wpb 4003.4 | bsz 141.8 | num_updates 35361 | best_bleu 20.21
2023-07-27 22:51:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35361 updates
2023-07-27 22:51:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.8703.pt
2023-07-27 22:51:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.8703.pt
2023-07-27 22:51:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_19.8703.pt (epoch 24 @ 35361 updates, score 19.87) (writing took 11.176662428304553 seconds)
2023-07-27 22:51:18 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-07-27 22:51:18 | INFO | train | epoch 024 | loss 2.004 | trans_loss 5.006 | nll_loss 2.201 | w2v_ctc_loss 0.635 | task_loss 1.403 | contrastive_loss 0.114 | total 4136.82 | n_correct 2660.32 | ppl 4.6 | accuracy 64.308 | wps 11014.8 | ups 1.33 | wpb 8273.6 | bsz 305.1 | num_updates 35361 | lr 7.5206e-05 | gnorm 0.548 | clip 0 | loss_scale 32 | train_wall 1011 | gb_free 16.1 | wall 29796
2023-07-27 22:51:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 22:51:19 | INFO | fairseq.trainer | begin training epoch 25
2023-07-27 22:51:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 22:51:54 | INFO | train_inner | epoch 025:     39 / 1474 loss=1.991, trans_loss=4.996, nll_loss=2.189, w2v_ctc_loss=0.626, task_loss=1.342, contrastive_loss=0.071, total=4166.95, n_correct=2693.36, ppl=4.56, accuracy=64.636, wps=7309.1, ups=0.88, wpb=8333.9, bsz=312, num_updates=35400, lr=7.51646e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=68, gb_free=16.4, wall=29832
2023-07-27 22:53:04 | INFO | train_inner | epoch 025:    139 / 1474 loss=1.984, trans_loss=4.977, nll_loss=2.163, w2v_ctc_loss=0.622, task_loss=1.369, contrastive_loss=0.07, total=4133.64, n_correct=2683.82, ppl=4.48, accuracy=64.926, wps=11903.9, ups=1.44, wpb=8267.3, bsz=307.7, num_updates=35500, lr=7.50587e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=29902
2023-07-27 22:54:13 | INFO | train_inner | epoch 025:    239 / 1474 loss=1.989, trans_loss=4.983, nll_loss=2.171, w2v_ctc_loss=0.626, task_loss=1.44, contrastive_loss=0.074, total=4114.53, n_correct=2664.97, ppl=4.5, accuracy=64.77, wps=11766.8, ups=1.43, wpb=8229.1, bsz=302.7, num_updates=35600, lr=7.49532e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=69, gb_free=17, wall=29971
2023-07-27 22:55:23 | INFO | train_inner | epoch 025:    339 / 1474 loss=1.997, trans_loss=4.988, nll_loss=2.177, w2v_ctc_loss=0.63, task_loss=1.491, contrastive_loss=0.103, total=4148.7, n_correct=2678.09, ppl=4.52, accuracy=64.553, wps=11855.3, ups=1.43, wpb=8297.4, bsz=295.1, num_updates=35700, lr=7.48481e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=30041
2023-07-27 22:56:33 | INFO | train_inner | epoch 025:    439 / 1474 loss=2.011, trans_loss=4.994, nll_loss=2.185, w2v_ctc_loss=0.643, task_loss=1.457, contrastive_loss=0.183, total=4167.03, n_correct=2683.98, ppl=4.55, accuracy=64.41, wps=11929.1, ups=1.43, wpb=8334.1, bsz=298.3, num_updates=35800, lr=7.47435e-05, gnorm=0.555, clip=0, loss_scale=32, train_wall=69, gb_free=12.2, wall=30111
2023-07-27 22:57:43 | INFO | train_inner | epoch 025:    539 / 1474 loss=1.995, trans_loss=4.999, nll_loss=2.192, w2v_ctc_loss=0.63, task_loss=1.371, contrastive_loss=0.074, total=4156.93, n_correct=2682.73, ppl=4.57, accuracy=64.536, wps=11977.7, ups=1.44, wpb=8313.9, bsz=312.8, num_updates=35900, lr=7.46393e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=30181
2023-07-27 22:58:52 | INFO | train_inner | epoch 025:    639 / 1474 loss=2.001, trans_loss=4.994, nll_loss=2.186, w2v_ctc_loss=0.636, task_loss=1.392, contrastive_loss=0.141, total=4153.23, n_correct=2680.63, ppl=4.55, accuracy=64.543, wps=12016.7, ups=1.45, wpb=8306.5, bsz=309.6, num_updates=36000, lr=7.45356e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=68, gb_free=14.8, wall=30250
2023-07-27 22:58:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 22:59:16 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.204 | trans_loss 5.574 | nll_loss 2.852 | w2v_ctc_loss 1.34 | task_loss 4.64 | contrastive_loss 0.252 | total 4003.4 | n_correct 2479.3 | ppl 7.22 | accuracy 61.93 | uer 16.933 | wer 18.754 | raw_wer 18.754 | bleu 19.64 | wps 2041.7 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 20.21
2023-07-27 22:59:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-07-27 22:59:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_25_36000.pt
2023-07-27 22:59:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_25_36000.pt
2023-07-27 22:59:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 19.64) (writing took 11.514973709359765 seconds)
2023-07-27 23:00:37 | INFO | train_inner | epoch 025:    739 / 1474 loss=2.003, trans_loss=4.997, nll_loss=2.191, w2v_ctc_loss=0.632, task_loss=1.419, contrastive_loss=0.137, total=4123.21, n_correct=2658.47, ppl=4.56, accuracy=64.476, wps=7881, ups=0.96, wpb=8246.4, bsz=300.7, num_updates=36100, lr=7.44323e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=68, gb_free=14.6, wall=30354
2023-07-27 23:01:47 | INFO | train_inner | epoch 025:    839 / 1474 loss=1.99, trans_loss=4.997, nll_loss=2.191, w2v_ctc_loss=0.624, task_loss=1.287, contrastive_loss=0.085, total=4197.27, n_correct=2711.84, ppl=4.57, accuracy=64.61, wps=11958.5, ups=1.42, wpb=8394.5, bsz=328.2, num_updates=36200, lr=7.43294e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=30425
2023-07-27 23:02:56 | INFO | train_inner | epoch 025:    939 / 1474 loss=2.002, trans_loss=5.002, nll_loss=2.198, w2v_ctc_loss=0.634, task_loss=1.348, contrastive_loss=0.142, total=4137.23, n_correct=2664.04, ppl=4.59, accuracy=64.392, wps=11968.8, ups=1.45, wpb=8274.5, bsz=313.3, num_updates=36300, lr=7.4227e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=30494
2023-07-27 23:04:05 | INFO | train_inner | epoch 025:   1039 / 1474 loss=2.009, trans_loss=5.01, nll_loss=2.207, w2v_ctc_loss=0.623, task_loss=1.386, contrastive_loss=0.255, total=4183.45, n_correct=2688.74, ppl=4.62, accuracy=64.271, wps=12087.3, ups=1.44, wpb=8366.9, bsz=311, num_updates=36400, lr=7.41249e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=69, gb_free=16.6, wall=30563
2023-07-27 23:05:14 | INFO | train_inner | epoch 025:   1139 / 1474 loss=1.993, trans_loss=5.001, nll_loss=2.194, w2v_ctc_loss=0.621, task_loss=1.508, contrastive_loss=0.054, total=4045.24, n_correct=2609.85, ppl=4.58, accuracy=64.517, wps=11677.8, ups=1.44, wpb=8090.5, bsz=287, num_updates=36500, lr=7.40233e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=69, gb_free=16.3, wall=30632
2023-07-27 23:06:22 | INFO | train_inner | epoch 025:   1239 / 1474 loss=1.998, trans_loss=5.008, nll_loss=2.204, w2v_ctc_loss=0.63, task_loss=1.437, contrastive_loss=0.058, total=4079.17, n_correct=2625.69, ppl=4.61, accuracy=64.368, wps=12017.6, ups=1.47, wpb=8158.3, bsz=292.3, num_updates=36600, lr=7.39221e-05, gnorm=0.555, clip=0, loss_scale=64, train_wall=67, gb_free=16.9, wall=30700
2023-07-27 23:07:31 | INFO | train_inner | epoch 025:   1339 / 1474 loss=2.006, trans_loss=5.008, nll_loss=2.205, w2v_ctc_loss=0.634, task_loss=1.353, contrastive_loss=0.165, total=4173.55, n_correct=2683.03, ppl=4.61, accuracy=64.287, wps=12145.9, ups=1.46, wpb=8347.1, bsz=312.7, num_updates=36700, lr=7.38213e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=68, gb_free=16.7, wall=30769
2023-07-27 23:08:40 | INFO | train_inner | epoch 025:   1439 / 1474 loss=2.011, trans_loss=5.022, nll_loss=2.223, w2v_ctc_loss=0.636, task_loss=1.454, contrastive_loss=0.111, total=4102.27, n_correct=2626.2, ppl=4.67, accuracy=64.018, wps=11814, ups=1.44, wpb=8204.5, bsz=299.9, num_updates=36800, lr=7.3721e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=69, gb_free=15.8, wall=30838
2023-07-27 23:09:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 23:09:29 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.209 | trans_loss 5.572 | nll_loss 2.851 | w2v_ctc_loss 1.363 | task_loss 4.62 | contrastive_loss 0.256 | total 4003.4 | n_correct 2482.4 | ppl 7.21 | accuracy 62.007 | uer 17.113 | wer 19.004 | raw_wer 19.004 | bleu 19.82 | wps 2085.4 | wpb 4003.4 | bsz 141.8 | num_updates 36835 | best_bleu 20.21
2023-07-27 23:09:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36835 updates
2023-07-27 23:09:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-27 23:09:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-27 23:09:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt (epoch 25 @ 36835 updates, score 19.82) (writing took 10.543325519189239 seconds)
2023-07-27 23:09:40 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-07-27 23:09:40 | INFO | train | epoch 025 | loss 1.999 | trans_loss 4.998 | nll_loss 2.192 | w2v_ctc_loss 0.63 | task_loss 1.401 | contrastive_loss 0.117 | total 4138.65 | n_correct 2668.67 | ppl 4.57 | accuracy 64.482 | wps 11080.4 | ups 1.34 | wpb 8277.3 | bsz 305.7 | num_updates 36835 | lr 7.36859e-05 | gnorm 0.55 | clip 0 | loss_scale 64 | train_wall 1011 | gb_free 14.2 | wall 30898
2023-07-27 23:09:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 23:09:40 | INFO | fairseq.trainer | begin training epoch 26
2023-07-27 23:09:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 23:10:34 | INFO | train_inner | epoch 026:     65 / 1474 loss=1.984, trans_loss=4.977, nll_loss=2.164, w2v_ctc_loss=0.62, task_loss=1.325, contrastive_loss=0.098, total=4178.19, n_correct=2711.42, ppl=4.48, accuracy=64.895, wps=7375.8, ups=0.88, wpb=8356.4, bsz=317.5, num_updates=36900, lr=7.3621e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=69, gb_free=14.2, wall=30952
2023-07-27 23:11:43 | INFO | train_inner | epoch 026:    165 / 1474 loss=1.988, trans_loss=4.973, nll_loss=2.16, w2v_ctc_loss=0.608, task_loss=1.226, contrastive_loss=0.283, total=4269.55, n_correct=2777.01, ppl=4.47, accuracy=65.042, wps=12266.1, ups=1.44, wpb=8539.1, bsz=341.4, num_updates=37000, lr=7.35215e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=69, gb_free=15.3, wall=31021
2023-07-27 23:12:52 | INFO | train_inner | epoch 026:    265 / 1474 loss=1.995, trans_loss=4.98, nll_loss=2.167, w2v_ctc_loss=0.629, task_loss=1.387, contrastive_loss=0.155, total=4128.39, n_correct=2675.88, ppl=4.49, accuracy=64.817, wps=11956.9, ups=1.45, wpb=8256.8, bsz=306.8, num_updates=37100, lr=7.34223e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=68, gb_free=9.8, wall=31090
2023-07-27 23:14:02 | INFO | train_inner | epoch 026:    365 / 1474 loss=1.99, trans_loss=4.981, nll_loss=2.17, w2v_ctc_loss=0.625, task_loss=1.342, contrastive_loss=0.116, total=4166.22, n_correct=2699.37, ppl=4.5, accuracy=64.792, wps=12027.2, ups=1.44, wpb=8332.4, bsz=315, num_updates=37200, lr=7.33236e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=68, gb_free=16.6, wall=31160
2023-07-27 23:15:10 | INFO | train_inner | epoch 026:    465 / 1474 loss=1.988, trans_loss=4.974, nll_loss=2.16, w2v_ctc_loss=0.622, task_loss=1.334, contrastive_loss=0.159, total=4171.18, n_correct=2713.19, ppl=4.47, accuracy=65.046, wps=12179.4, ups=1.46, wpb=8342.4, bsz=315.5, num_updates=37300, lr=7.32252e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=31228
2023-07-27 23:16:20 | INFO | train_inner | epoch 026:    565 / 1474 loss=1.997, trans_loss=4.991, nll_loss=2.18, w2v_ctc_loss=0.638, task_loss=1.434, contrastive_loss=0.077, total=4139.82, n_correct=2677.67, ppl=4.53, accuracy=64.681, wps=11949.1, ups=1.44, wpb=8279.6, bsz=300, num_updates=37400, lr=7.31272e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=69, gb_free=15, wall=31298
2023-07-27 23:17:29 | INFO | train_inner | epoch 026:    665 / 1474 loss=1.987, trans_loss=4.987, nll_loss=2.177, w2v_ctc_loss=0.619, task_loss=1.41, contrastive_loss=0.064, total=4146.72, n_correct=2683.51, ppl=4.52, accuracy=64.714, wps=11950.3, ups=1.44, wpb=8293.4, bsz=302.7, num_updates=37500, lr=7.30297e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=31367
2023-07-27 23:17:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-27 23:18:38 | INFO | train_inner | epoch 026:    766 / 1474 loss=1.992, trans_loss=4.991, nll_loss=2.182, w2v_ctc_loss=0.627, task_loss=1.462, contrastive_loss=0.059, total=4066.26, n_correct=2629.17, ppl=4.54, accuracy=64.658, wps=11797.5, ups=1.45, wpb=8132.5, bsz=290.7, num_updates=37600, lr=7.29325e-05, gnorm=0.559, clip=0, loss_scale=32, train_wall=68, gb_free=15, wall=31436
2023-07-27 23:19:47 | INFO | train_inner | epoch 026:    866 / 1474 loss=1.994, trans_loss=4.991, nll_loss=2.181, w2v_ctc_loss=0.63, task_loss=1.385, contrastive_loss=0.078, total=4183.26, n_correct=2703.72, ppl=4.54, accuracy=64.632, wps=12110.8, ups=1.45, wpb=8366.5, bsz=308.1, num_updates=37700, lr=7.28357e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=68, gb_free=17.3, wall=31505
2023-07-27 23:20:57 | INFO | train_inner | epoch 026:    966 / 1474 loss=1.997, trans_loss=5, nll_loss=2.194, w2v_ctc_loss=0.617, task_loss=1.45, contrastive_loss=0.131, total=4137.96, n_correct=2667.4, ppl=4.57, accuracy=64.462, wps=11860.3, ups=1.43, wpb=8275.9, bsz=299, num_updates=37800, lr=7.27393e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=31575
2023-07-27 23:22:05 | INFO | train_inner | epoch 026:   1066 / 1474 loss=1.993, trans_loss=4.996, nll_loss=2.189, w2v_ctc_loss=0.626, task_loss=1.469, contrastive_loss=0.061, total=4120.53, n_correct=2662, ppl=4.56, accuracy=64.603, wps=12053.9, ups=1.46, wpb=8241.1, bsz=294.3, num_updates=37900, lr=7.26433e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=68, gb_free=16.6, wall=31643
2023-07-27 23:23:14 | INFO | train_inner | epoch 026:   1166 / 1474 loss=1.998, trans_loss=5.002, nll_loss=2.196, w2v_ctc_loss=0.626, task_loss=1.466, contrastive_loss=0.101, total=4113.86, n_correct=2649.05, ppl=4.58, accuracy=64.393, wps=11874.2, ups=1.44, wpb=8227.7, bsz=298.5, num_updates=38000, lr=7.25476e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=31712
2023-07-27 23:23:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 23:23:39 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.213 | trans_loss 5.579 | nll_loss 2.857 | w2v_ctc_loss 1.357 | task_loss 4.63 | contrastive_loss 0.254 | total 4003.4 | n_correct 2473.2 | ppl 7.25 | accuracy 61.777 | uer 17.187 | wer 18.978 | raw_wer 18.978 | bleu 19.86 | wps 2064.2 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 20.21
2023-07-27 23:23:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-07-27 23:23:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_26_38000.pt
2023-07-27 23:23:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_26_38000.pt
2023-07-27 23:24:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 19.86) (writing took 29.52161436714232 seconds)
2023-07-27 23:25:18 | INFO | train_inner | epoch 026:   1266 / 1474 loss=2.007, trans_loss=5.013, nll_loss=2.21, w2v_ctc_loss=0.644, task_loss=1.564, contrastive_loss=0.063, total=3996.19, n_correct=2565.69, ppl=4.63, accuracy=64.203, wps=6475.6, ups=0.81, wpb=7992.4, bsz=279.3, num_updates=38100, lr=7.24524e-05, gnorm=0.567, clip=0, loss_scale=32, train_wall=68, gb_free=17.7, wall=31836
2023-07-27 23:26:28 | INFO | train_inner | epoch 026:   1366 / 1474 loss=1.993, trans_loss=5.005, nll_loss=2.201, w2v_ctc_loss=0.62, task_loss=1.395, contrastive_loss=0.079, total=4159.74, n_correct=2685.73, ppl=4.6, accuracy=64.565, wps=11831.2, ups=1.42, wpb=8319.5, bsz=311.4, num_updates=38200, lr=7.23575e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=70, gb_free=17.2, wall=31906
2023-07-27 23:27:37 | INFO | train_inner | epoch 026:   1466 / 1474 loss=1.986, trans_loss=4.998, nll_loss=2.192, w2v_ctc_loss=0.615, task_loss=1.327, contrastive_loss=0.073, total=4165.66, n_correct=2697.54, ppl=4.57, accuracy=64.757, wps=12102.8, ups=1.45, wpb=8331.3, bsz=317.5, num_updates=38300, lr=7.22629e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=68, gb_free=16.2, wall=31975
2023-07-27 23:27:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 23:28:07 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.191 | trans_loss 5.577 | nll_loss 2.855 | w2v_ctc_loss 1.29 | task_loss 4.594 | contrastive_loss 0.259 | total 4003.4 | n_correct 2480.8 | ppl 7.24 | accuracy 61.967 | uer 16.869 | wer 18.646 | raw_wer 18.646 | bleu 19.69 | wps 2070 | wpb 4003.4 | bsz 141.8 | num_updates 38308 | best_bleu 20.21
2023-07-27 23:28:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38308 updates
2023-07-27 23:28:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-27 23:28:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-27 23:28:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt (epoch 26 @ 38308 updates, score 19.69) (writing took 10.353750012814999 seconds)
2023-07-27 23:28:17 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-07-27 23:28:17 | INFO | train | epoch 026 | loss 1.992 | trans_loss 4.99 | nll_loss 2.181 | w2v_ctc_loss 0.624 | task_loss 1.403 | contrastive_loss 0.108 | total 4137.32 | n_correct 2676.63 | ppl 4.53 | accuracy 64.695 | wps 10908 | ups 1.32 | wpb 8274.6 | bsz 305.2 | num_updates 38308 | lr 7.22554e-05 | gnorm 0.551 | clip 0 | loss_scale 32 | train_wall 1009 | gb_free 15.9 | wall 32015
2023-07-27 23:28:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 23:28:17 | INFO | fairseq.trainer | begin training epoch 27
2023-07-27 23:28:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 23:29:29 | INFO | train_inner | epoch 027:     92 / 1474 loss=1.974, trans_loss=4.957, nll_loss=2.136, w2v_ctc_loss=0.61, task_loss=1.51, contrastive_loss=0.05, total=4054.57, n_correct=2647.14, ppl=4.4, accuracy=65.288, wps=7237.5, ups=0.89, wpb=8109.1, bsz=282.4, num_updates=38400, lr=7.21688e-05, gnorm=0.56, clip=0, loss_scale=32, train_wall=68, gb_free=16.2, wall=32087
2023-07-27 23:30:39 | INFO | train_inner | epoch 027:    192 / 1474 loss=1.976, trans_loss=4.96, nll_loss=2.142, w2v_ctc_loss=0.619, task_loss=1.332, contrastive_loss=0.085, total=4195.2, n_correct=2737.03, ppl=4.41, accuracy=65.242, wps=12058.4, ups=1.44, wpb=8390.4, bsz=323.2, num_updates=38500, lr=7.2075e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=32157
2023-07-27 23:31:49 | INFO | train_inner | epoch 027:    292 / 1474 loss=1.979, trans_loss=4.969, nll_loss=2.153, w2v_ctc_loss=0.617, task_loss=1.403, contrastive_loss=0.063, total=4162.23, n_correct=2712.39, ppl=4.45, accuracy=65.167, wps=11928.6, ups=1.43, wpb=8324.5, bsz=305.5, num_updates=38600, lr=7.19816e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=69, gb_free=17.1, wall=32226
2023-07-27 23:32:58 | INFO | train_inner | epoch 027:    392 / 1474 loss=1.996, trans_loss=4.979, nll_loss=2.166, w2v_ctc_loss=0.614, task_loss=1.468, contrastive_loss=0.247, total=4079.05, n_correct=2644.64, ppl=4.49, accuracy=64.835, wps=11684.6, ups=1.43, wpb=8158.1, bsz=297.1, num_updates=38700, lr=7.18885e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=69, gb_free=16.9, wall=32296
2023-07-27 23:34:09 | INFO | train_inner | epoch 027:    492 / 1474 loss=1.991, trans_loss=4.984, nll_loss=2.174, w2v_ctc_loss=0.617, task_loss=1.286, contrastive_loss=0.191, total=4243.25, n_correct=2749.28, ppl=4.51, accuracy=64.792, wps=12085.6, ups=1.42, wpb=8486.5, bsz=331, num_updates=38800, lr=7.17958e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=69, gb_free=15.6, wall=32367
2023-07-27 23:35:18 | INFO | train_inner | epoch 027:    592 / 1474 loss=1.987, trans_loss=4.979, nll_loss=2.166, w2v_ctc_loss=0.621, task_loss=1.36, contrastive_loss=0.125, total=4137.92, n_correct=2687.8, ppl=4.49, accuracy=64.955, wps=11941.1, ups=1.44, wpb=8275.8, bsz=313.5, num_updates=38900, lr=7.17035e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=69, gb_free=15.8, wall=32436
2023-07-27 23:36:27 | INFO | train_inner | epoch 027:    692 / 1474 loss=1.992, trans_loss=4.987, nll_loss=2.178, w2v_ctc_loss=0.626, task_loss=1.403, contrastive_loss=0.102, total=4158.48, n_correct=2691.47, ppl=4.52, accuracy=64.722, wps=12007.3, ups=1.44, wpb=8317, bsz=304.1, num_updates=39000, lr=7.16115e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=69, gb_free=15.6, wall=32505
2023-07-27 23:37:36 | INFO | train_inner | epoch 027:    792 / 1474 loss=1.989, trans_loss=4.986, nll_loss=2.176, w2v_ctc_loss=0.625, task_loss=1.485, contrastive_loss=0.064, total=4100.88, n_correct=2653.82, ppl=4.52, accuracy=64.713, wps=11896.2, ups=1.45, wpb=8201.8, bsz=292.2, num_updates=39100, lr=7.15199e-05, gnorm=0.556, clip=0, loss_scale=32, train_wall=68, gb_free=15.6, wall=32574
2023-07-27 23:38:45 | INFO | train_inner | epoch 027:    892 / 1474 loss=1.982, trans_loss=4.988, nll_loss=2.178, w2v_ctc_loss=0.612, task_loss=1.445, contrastive_loss=0.055, total=4111.94, n_correct=2668.17, ppl=4.53, accuracy=64.888, wps=11926.1, ups=1.45, wpb=8223.9, bsz=294.9, num_updates=39200, lr=7.14286e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=68, gb_free=16, wall=32643
2023-07-27 23:39:56 | INFO | train_inner | epoch 027:    992 / 1474 loss=1.995, trans_loss=4.986, nll_loss=2.177, w2v_ctc_loss=0.616, task_loss=1.36, contrastive_loss=0.247, total=4189.27, n_correct=2714.62, ppl=4.52, accuracy=64.799, wps=11877.1, ups=1.42, wpb=8378.5, bsz=314.9, num_updates=39300, lr=7.13376e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=70, gb_free=14.2, wall=32714
2023-07-27 23:41:05 | INFO | train_inner | epoch 027:   1092 / 1474 loss=1.98, trans_loss=4.982, nll_loss=2.171, w2v_ctc_loss=0.61, task_loss=1.396, contrastive_loss=0.075, total=4160.42, n_correct=2700.27, ppl=4.5, accuracy=64.904, wps=11955.4, ups=1.44, wpb=8320.8, bsz=307.3, num_updates=39400, lr=7.1247e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=69, gb_free=17, wall=32783
2023-07-27 23:42:15 | INFO | train_inner | epoch 027:   1192 / 1474 loss=1.995, trans_loss=4.992, nll_loss=2.184, w2v_ctc_loss=0.632, task_loss=1.467, contrastive_loss=0.082, total=4103.72, n_correct=2653.63, ppl=4.54, accuracy=64.664, wps=11848.1, ups=1.44, wpb=8207.4, bsz=297.1, num_updates=39500, lr=7.11568e-05, gnorm=0.555, clip=0, loss_scale=32, train_wall=69, gb_free=17.6, wall=32852
2023-07-27 23:43:23 | INFO | train_inner | epoch 027:   1292 / 1474 loss=2, trans_loss=5, nll_loss=2.195, w2v_ctc_loss=0.625, task_loss=1.496, contrastive_loss=0.129, total=4065.94, n_correct=2618.27, ppl=4.58, accuracy=64.395, wps=11791.8, ups=1.45, wpb=8131.9, bsz=292.4, num_updates=39600, lr=7.10669e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=68, gb_free=16.1, wall=32921
2023-07-27 23:44:32 | INFO | train_inner | epoch 027:   1392 / 1474 loss=1.985, trans_loss=4.988, nll_loss=2.181, w2v_ctc_loss=0.611, task_loss=1.32, contrastive_loss=0.113, total=4149.21, n_correct=2691.25, ppl=4.53, accuracy=64.862, wps=12116.7, ups=1.46, wpb=8298.4, bsz=312.6, num_updates=39700, lr=7.09773e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=68, gb_free=16.8, wall=32990
2023-07-27 23:45:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 23:45:52 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 4.202 | trans_loss 5.571 | nll_loss 2.847 | w2v_ctc_loss 1.338 | task_loss 4.615 | contrastive_loss 0.259 | total 4003.4 | n_correct 2479.7 | ppl 7.2 | accuracy 61.94 | uer 16.933 | wer 18.806 | raw_wer 18.806 | bleu 20.24 | wps 2151.1 | wpb 4003.4 | bsz 141.8 | num_updates 39782 | best_bleu 20.24
2023-07-27 23:45:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39782 updates
2023-07-27 23:45:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 23:46:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-27 23:46:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 27 @ 39782 updates, score 20.24) (writing took 20.74716630205512 seconds)
2023-07-27 23:46:14 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-07-27 23:46:14 | INFO | train | epoch 027 | loss 1.987 | trans_loss 4.981 | nll_loss 2.17 | w2v_ctc_loss 0.618 | task_loss 1.4 | contrastive_loss 0.115 | total 4138.65 | n_correct 2684.99 | ppl 4.5 | accuracy 64.876 | wps 11331.7 | ups 1.37 | wpb 8277.3 | bsz 305.7 | num_updates 39782 | lr 7.09042e-05 | gnorm 0.551 | clip 0 | loss_scale 64 | train_wall 1012 | gb_free 17.8 | wall 33092
2023-07-27 23:46:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-27 23:46:14 | INFO | fairseq.trainer | begin training epoch 28
2023-07-27 23:46:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-27 23:46:35 | INFO | train_inner | epoch 028:     18 / 1474 loss=1.979, trans_loss=4.981, nll_loss=2.17, w2v_ctc_loss=0.613, task_loss=1.357, contrastive_loss=0.064, total=4106.72, n_correct=2664.92, ppl=4.5, accuracy=64.892, wps=6669.4, ups=0.81, wpb=8213.4, bsz=305, num_updates=39800, lr=7.08881e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=68, gb_free=16.1, wall=33113
2023-07-27 23:47:45 | INFO | train_inner | epoch 028:    118 / 1474 loss=1.971, trans_loss=4.952, nll_loss=2.131, w2v_ctc_loss=0.61, task_loss=1.472, contrastive_loss=0.059, total=4103.42, n_correct=2689.02, ppl=4.38, accuracy=65.531, wps=11808.3, ups=1.44, wpb=8206.8, bsz=292, num_updates=39900, lr=7.07992e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=69, gb_free=16.1, wall=33183
2023-07-27 23:48:53 | INFO | train_inner | epoch 028:    218 / 1474 loss=1.966, trans_loss=4.957, nll_loss=2.138, w2v_ctc_loss=0.604, task_loss=1.312, contrastive_loss=0.069, total=4200.12, n_correct=2750.72, ppl=4.4, accuracy=65.491, wps=12199.5, ups=1.45, wpb=8400.2, bsz=317.8, num_updates=40000, lr=7.07107e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=68, gb_free=10.8, wall=33251
2023-07-27 23:48:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-27 23:49:19 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.203 | trans_loss 5.581 | nll_loss 2.858 | w2v_ctc_loss 1.323 | task_loss 4.599 | contrastive_loss 0.256 | total 4003.4 | n_correct 2475.5 | ppl 7.25 | accuracy 61.835 | uer 16.935 | wer 18.679 | raw_wer 18.679 | bleu 20.02 | wps 1918.1 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 20.24
2023-07-27 23:49:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-07-27 23:49:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_28_40000.pt
2023-07-27 23:49:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_28_40000.pt
2023-07-27 23:49:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 20.02) (writing took 15.075759470462799 seconds)
Mixup rate:0.5, token after shrink shape:torch.Size([16, 85]), X shape:torch.Size([16, 85, 512])
CTC Tokens:tensor([29,  0, 24, 11,  0], device='cuda:0'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:0'), New Tokens:tensor([29,  0, 24, 11,  0], device='cuda:0')
Org X:tensor([[-0.9077,  0.2313,  0.2837,  ...,  0.4263, -1.5332,  0.0335],
        [-0.5933,  0.4163,  1.0244,  ..., -0.7461, -0.6875,  0.1073],
        [-1.5752, -0.0981, -0.4819,  ..., -0.0963,  0.6738, -0.3740],
        [-1.8984,  0.5088,  0.6465,  ...,  0.3213,  1.2646, -0.0621],
        [ 1.2510,  0.3823,  1.3701,  ..., -0.0428,  0.1033, -0.4734]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:0'), Mixup Sent Mask:tensor([False,  True, False,  True, False, False, False, False, False,  True],
       device='cuda:0'), New X:tensor([[-1.0254,  0.0208, -0.2908,  ...,  0.0370, -2.1602,  3.5195],
        [-0.5933,  0.4163,  1.0244,  ..., -0.7461, -0.6875,  0.1073],
        [-1.5752, -0.0981, -0.4819,  ..., -0.0963,  0.6738, -0.3740],
        [-0.1989, -0.1073, -0.5220,  ..., -2.3457, -0.3206, -4.1055],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898]],
       device='cuda:0', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:0')
2023-07-27 23:50:46 | INFO | train_inner | epoch 028:    318 / 1474 loss=2, trans_loss=4.973, nll_loss=2.16, w2v_ctc_loss=0.604, task_loss=1.396, contrastive_loss=0.403, total=4147.36, n_correct=2688.23, ppl=4.47, accuracy=64.818, wps=7393.9, ups=0.89, wpb=8294.7, bsz=315.4, num_updates=40100, lr=7.06225e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=70, gb_free=16.7, wall=33364
2023-07-27 23:51:55 | INFO | train_inner | epoch 028:    418 / 1474 loss=1.979, trans_loss=4.97, nll_loss=2.155, w2v_ctc_loss=0.617, task_loss=1.447, contrastive_loss=0.054, total=4087.34, n_correct=2662.71, ppl=4.45, accuracy=65.145, wps=11817.9, ups=1.45, wpb=8174.7, bsz=295.1, num_updates=40200, lr=7.05346e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=68, gb_free=16.8, wall=33433
2023-07-27 23:53:04 | INFO | train_inner | epoch 028:    518 / 1474 loss=1.977, trans_loss=4.969, nll_loss=2.153, w2v_ctc_loss=0.61, task_loss=1.452, contrastive_loss=0.067, total=4099.71, n_correct=2672.23, ppl=4.45, accuracy=65.181, wps=11907.8, ups=1.45, wpb=8199.4, bsz=296.2, num_updates=40300, lr=7.0447e-05, gnorm=0.555, clip=0, loss_scale=64, train_wall=68, gb_free=15.2, wall=33502
2023-07-27 23:54:13 | INFO | train_inner | epoch 028:    618 / 1474 loss=1.982, trans_loss=4.98, nll_loss=2.167, w2v_ctc_loss=0.618, task_loss=1.42, contrastive_loss=0.068, total=4177.06, n_correct=2713.1, ppl=4.49, accuracy=64.952, wps=12031, ups=1.44, wpb=8354.1, bsz=304.1, num_updates=40400, lr=7.03598e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=33571
2023-07-27 23:55:22 | INFO | train_inner | epoch 028:    718 / 1474 loss=1.983, trans_loss=4.977, nll_loss=2.166, w2v_ctc_loss=0.61, task_loss=1.258, contrastive_loss=0.181, total=4190.74, n_correct=2725.47, ppl=4.49, accuracy=65.036, wps=12099.2, ups=1.44, wpb=8381.5, bsz=328.5, num_updates=40500, lr=7.02728e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=69, gb_free=15.4, wall=33640
2023-07-27 23:56:32 | INFO | train_inner | epoch 028:    818 / 1474 loss=1.974, trans_loss=4.975, nll_loss=2.162, w2v_ctc_loss=0.608, task_loss=1.381, contrastive_loss=0.06, total=4091.75, n_correct=2667.9, ppl=4.48, accuracy=65.202, wps=11822.7, ups=1.44, wpb=8183.5, bsz=306, num_updates=40600, lr=7.01862e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=68, gb_free=16.4, wall=33710
2023-07-27 23:57:42 | INFO | train_inner | epoch 028:    918 / 1474 loss=1.989, trans_loss=4.984, nll_loss=2.173, w2v_ctc_loss=0.618, task_loss=1.439, contrastive_loss=0.123, total=4123.89, n_correct=2673.28, ppl=4.51, accuracy=64.824, wps=11809.2, ups=1.43, wpb=8247.8, bsz=301, num_updates=40700, lr=7.01e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=33779
2023-07-27 23:58:51 | INFO | train_inner | epoch 028:   1018 / 1474 loss=1.995, trans_loss=4.986, nll_loss=2.176, w2v_ctc_loss=0.622, task_loss=1.363, contrastive_loss=0.179, total=4176.06, n_correct=2705.58, ppl=4.52, accuracy=64.788, wps=11991.1, ups=1.44, wpb=8352.1, bsz=311.4, num_updates=40800, lr=7.0014e-05, gnorm=0.56, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=33849
2023-07-28 00:00:01 | INFO | train_inner | epoch 028:   1118 / 1474 loss=1.977, trans_loss=4.973, nll_loss=2.161, w2v_ctc_loss=0.61, task_loss=1.364, contrastive_loss=0.082, total=4206.08, n_correct=2738.71, ppl=4.47, accuracy=65.113, wps=12019.1, ups=1.43, wpb=8412.2, bsz=317.3, num_updates=40900, lr=6.99284e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=33919
2023-07-28 00:01:10 | INFO | train_inner | epoch 028:   1218 / 1474 loss=1.978, trans_loss=4.981, nll_loss=2.17, w2v_ctc_loss=0.609, task_loss=1.378, contrastive_loss=0.069, total=4109.72, n_correct=2670.1, ppl=4.5, accuracy=64.97, wps=11964.4, ups=1.46, wpb=8219.4, bsz=306.9, num_updates=41000, lr=6.9843e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=68, gb_free=16.1, wall=33988
2023-07-28 00:02:19 | INFO | train_inner | epoch 028:   1318 / 1474 loss=1.995, trans_loss=4.991, nll_loss=2.182, w2v_ctc_loss=0.629, task_loss=1.524, contrastive_loss=0.082, total=4085.44, n_correct=2640.92, ppl=4.54, accuracy=64.642, wps=11741.2, ups=1.44, wpb=8170.9, bsz=285.6, num_updates=41100, lr=6.9758e-05, gnorm=0.559, clip=0, loss_scale=64, train_wall=69, gb_free=16.2, wall=34057
2023-07-28 00:03:30 | INFO | train_inner | epoch 028:   1418 / 1474 loss=1.989, trans_loss=4.988, nll_loss=2.178, w2v_ctc_loss=0.615, task_loss=1.483, contrastive_loss=0.102, total=4137.47, n_correct=2677.86, ppl=4.53, accuracy=64.722, wps=11785.4, ups=1.42, wpb=8274.9, bsz=294.8, num_updates=41200, lr=6.96733e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=70, gb_free=16.8, wall=34128
2023-07-28 00:04:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([16, 78]), X shape:torch.Size([16, 78, 512])
CTC Tokens:tensor([29,  0,  0, 71,  0], device='cuda:5'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:5'), New Tokens:tensor([29,  0, 71,  0,  7], device='cuda:5')
Org X:tensor([[-1.1377e+00,  2.6685e-01,  3.1494e-01,  ...,  3.5791e-01,
         -9.2090e-01,  5.6934e-01],
        [-4.2822e-01,  6.6699e-01,  8.7402e-01,  ...,  6.4087e-04,
         -7.6367e-01,  1.0205e-01],
        [-3.9819e-01,  1.6760e-01, -6.2451e-01,  ...,  1.9385e-01,
          1.2881e+00,  1.0293e+00],
        [ 8.7280e-03,  2.0081e-01, -3.0664e-01,  ...,  3.4515e-02,
         -2.6318e-01,  1.1340e-01],
        [ 1.4893e-02,  1.2878e-01, -1.8184e+00,  ...,  2.2937e-01,
         -8.2910e-01, -5.5542e-02]], device='cuda:5', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([False,  True, False,  True, False, False, False, False, False,  True],
       device='cuda:5'), New X:tensor([[-1.0254,  0.0208, -0.2908,  ...,  0.0370, -2.1602,  3.5195],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [-0.3982,  0.1676, -0.6245,  ...,  0.1938,  1.2881,  1.0293],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [ 0.0149,  0.1288, -1.8184,  ...,  0.2294, -0.8291, -0.0555]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 116]), X shape:torch.Size([8, 116, 512])
CTC Tokens:tensor([   8,    0,    0,    0, 7685], device='cuda:7'), Shrink Mask:tensor([ True,  True, False, False,  True], device='cuda:7'), New Tokens:tensor([   8,    0, 7685,    0,   26], device='cuda:7')
Org X:tensor([[-0.7134, -0.3757,  0.0550,  ...,  0.2930, -0.1724, -0.2888],
        [-0.8862, -0.2314,  0.7544,  ..., -0.1544,  0.0422,  0.2654],
        [ 0.1981,  0.9009,  0.6123,  ..., -1.0713, -0.6294,  0.6553],
        [ 0.3486,  0.5854,  1.8340,  ..., -0.2181, -1.0078,  0.5205],
        [-0.7085, -1.2002,  0.2228,  ..., -0.0235, -0.1203,  0.4343]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:7'), Mixup Sent Mask:tensor([False,  True, False,  True, False, False, False, False],
       device='cuda:7'), New X:tensor([[-0.4651, -0.0114, -0.3420,  ...,  0.9185, -1.7100, -1.5342],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [ 0.1981,  0.9009,  0.6123,  ..., -1.0713, -0.6294,  0.6553],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [-0.7085, -1.2002,  0.2228,  ..., -0.0235, -0.1203,  0.4343]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:7')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 122]), X shape:torch.Size([8, 122, 512])
CTC Tokens:tensor([  0,   0, 999,   0, 111], device='cuda:3'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:3'), New Tokens:tensor([  0, 999,   0, 111,  66], device='cuda:3')
Org X:tensor([[ 4.0771e-01, -1.0977e+00,  1.3262e+00,  ..., -3.1128e-01,
          6.9287e-01, -1.8311e-04],
        [-2.2559e+00, -1.3904e-01, -4.5410e-01,  ..., -2.7710e-01,
          3.2666e-01, -1.2676e+00],
        [ 8.6914e-01, -1.8286e-01, -5.3320e-01,  ..., -3.5791e-01,
          7.2510e-01, -1.8721e+00],
        [ 7.6562e-01, -1.9080e-01, -4.9951e-01,  ...,  2.9639e-01,
          1.3506e+00, -9.7070e-01],
        [ 7.6709e-01,  3.5254e-01, -6.1670e-01,  ..., -8.0176e-01,
          1.4465e-01, -3.3228e-01]], device='cuda:3', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:3'), Mixup Sent Mask:tensor([False,  True, False,  True, False, False, False, False],
       device='cuda:3'), New X:tensor([[ 4.0771e-01, -1.0977e+00,  1.3262e+00,  ..., -3.1128e-01,
          6.9287e-01, -1.8311e-04],
        [-2.2559e+00, -1.3904e-01, -4.5410e-01,  ..., -2.7710e-01,
          3.2666e-01, -1.2676e+00],
        [-5.4346e-01, -1.0391e+00,  3.5767e-01,  ..., -2.6953e+00,
         -1.5811e+00, -1.5898e+00],
        [ 7.6562e-01, -1.9080e-01, -4.9951e-01,  ...,  2.9639e-01,
          1.3506e+00, -9.7070e-01],
        [ 7.6709e-01,  3.5254e-01, -6.1670e-01,  ..., -8.0176e-01,
          1.4465e-01, -3.3228e-01]], device='cuda:3', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 61]), X shape:torch.Size([16, 61, 512])
CTC Tokens:tensor([ 0, 67,  0,  0,  0], device='cuda:4'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:4'), New Tokens:tensor([ 0, 67,  0, 19,  0], device='cuda:4')
Org X:tensor([[ 0.0706,  0.3113,  0.6362,  ..., -1.4072,  0.1544, -1.3896],
        [ 0.0369,  0.4382,  0.3198,  ..., -0.0289, -0.5708, -1.9619],
        [-0.0994,  0.4602,  1.6553,  ..., -0.8511, -1.0605, -1.2285],
        [-0.2053,  0.4602, -0.6909,  ...,  0.0938,  0.1445,  0.1277],
        [ 2.5410,  0.8296, -0.2100,  ..., -0.0930,  0.1698,  0.0813]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:4'), Mixup Sent Mask:tensor([False,  True, False,  True, False, False, False, False, False,  True],
       device='cuda:4'), New X:tensor([[-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [ 0.0734, -0.1293, -0.4978,  ...,  0.9868, -1.9932, -3.5391],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [ 0.0465, -0.2908, -0.1182,  ..., -0.9814,  0.2957, -1.4463],
        [ 2.5410,  0.8296, -0.2100,  ..., -0.0930,  0.1698,  0.0813]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([5, 187]), X shape:torch.Size([5, 187, 512])
CTC Tokens:tensor([ 29,   0, 220, 103, 103], device='cuda:2'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:2'), New Tokens:tensor([ 29,   0, 220, 103,  19], device='cuda:2')
Org X:tensor([[-1.1338,  0.3245,  0.2803,  ...,  0.6885, -1.7539,  0.6055],
        [-0.7930,  0.3721,  0.5176,  ...,  0.3499, -1.6846, -0.0921],
        [-1.0869,  0.4307,  0.7808,  ...,  0.3652, -1.5098, -0.4236],
        [-1.6426,  0.3191, -0.5488,  ...,  0.2140,  0.4827, -0.2365],
        [-0.1572,  0.4194, -0.9385,  ...,  0.2971,  1.2266,  0.0176]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:2'), Mixup Sent Mask:tensor([False,  True, False,  True, False], device='cuda:2'), New X:tensor([[-1.1338,  0.3245,  0.2803,  ...,  0.6885, -1.7539,  0.6055],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [-1.0869,  0.4307,  0.7808,  ...,  0.3652, -1.5098, -0.4236],
        [-1.6426,  0.3191, -0.5488,  ...,  0.2140,  0.4827, -0.2365],
        [ 0.0465, -0.2908, -0.1182,  ..., -0.9814,  0.2957, -1.4463]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 71]), X shape:torch.Size([16, 71, 512])
CTC Tokens:tensor([  0, 144,   0,  19,   0], device='cuda:6'), Shrink Mask:tensor([True, True, True, True, True], device='cuda:6'), New Tokens:tensor([  0, 144,   0,  19,   0], device='cuda:6')
Org X:tensor([[ 0.6138, -0.5796,  1.6445,  ..., -2.0859,  1.5977,  0.4177],
        [ 0.3975,  0.1744,  0.2896,  ..., -0.3330,  0.0541, -0.6006],
        [ 0.9326, -0.6904,  0.7119,  ..., -0.8120,  0.0779, -0.5347],
        [-0.0758,  0.2998,  0.3118,  ..., -0.4041,  0.4263,  0.3870],
        [ 1.6416, -0.2268,  2.3984,  ..., -1.4307,  0.4397,  0.6592]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([False,  True, False,  True, False, False, False, False, False,  True],
       device='cuda:6'), New X:tensor([[ 0.6138, -0.5796,  1.6445,  ..., -2.0859,  1.5977,  0.4177],
        [ 0.3975,  0.1744,  0.2896,  ..., -0.3330,  0.0541, -0.6006],
        [ 0.9326, -0.6904,  0.7119,  ..., -0.8120,  0.0779, -0.5347],
        [-0.0758,  0.2998,  0.3118,  ..., -0.4041,  0.4263,  0.3870],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 65]), X shape:torch.Size([16, 65, 512])
CTC Tokens:tensor([  7,   7,   0, 558,   0], device='cuda:1'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:1'), New Tokens:tensor([  7,   0, 558,   0, 279], device='cuda:1')
Org X:tensor([[-0.0673,  0.0109,  0.0654,  ..., -0.0526,  0.0529, -1.5986],
        [ 0.4646, -0.1847,  0.6455,  ..., -0.0551,  1.2324, -0.1047],
        [-0.1398,  0.1385,  0.1914,  ...,  0.1863,  1.2275,  0.4182],
        [ 0.1797,  0.7646,  0.8848,  ...,  0.2739,  0.1307,  0.6602],
        [ 0.0703,  1.0244, -0.7700,  ..., -0.5557,  1.2412,  0.1013]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:1'), Mixup Sent Mask:tensor([False,  True, False,  True, False, False, False, False, False,  True],
       device='cuda:1'), New X:tensor([[-0.0673,  0.0109,  0.0654,  ..., -0.0526,  0.0529, -1.5986],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [-0.2214,  0.0735, -0.2852,  ...,  0.6621,  3.4922, -1.8730],
        [-0.5435, -1.0391,  0.3577,  ..., -2.6953, -1.5811, -1.5898],
        [ 0.0703,  1.0244, -0.7700,  ..., -0.5557,  1.2412,  0.1013]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:1')
2023-07-28 00:04:33 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.197 | trans_loss 5.577 | nll_loss 2.853 | w2v_ctc_loss 1.314 | task_loss 4.626 | contrastive_loss 0.251 | total 4003.4 | n_correct 2480.5 | ppl 7.23 | accuracy 61.96 | uer 16.871 | wer 18.653 | raw_wer 18.653 | bleu 20.27 | wps 2033.9 | wpb 4003.4 | bsz 141.8 | num_updates 41256 | best_bleu 20.27
2023-07-28 00:04:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41256 updates
2023-07-28 00:04:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-28 00:04:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt
2023-07-28 00:04:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_best.pt (epoch 28 @ 41256 updates, score 20.27) (writing took 20.664108177646995 seconds)
2023-07-28 00:04:54 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-07-28 00:04:54 | INFO | train | epoch 028 | loss 1.982 | trans_loss 4.975 | nll_loss 2.162 | w2v_ctc_loss 0.614 | task_loss 1.399 | contrastive_loss 0.114 | total 4138.65 | n_correct 2691.59 | ppl 4.48 | accuracy 65.035 | wps 10889 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 41256 | lr 6.9626e-05 | gnorm 0.553 | clip 0 | loss_scale 64 | train_wall 1013 | gb_free 16.4 | wall 34212
2023-07-28 00:04:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-28 00:04:55 | INFO | fairseq.trainer | begin training epoch 29
2023-07-28 00:04:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-28 00:05:35 | INFO | train_inner | epoch 029:     44 / 1474 loss=1.974, trans_loss=4.963, nll_loss=2.147, w2v_ctc_loss=0.616, task_loss=1.346, contrastive_loss=0.075, total=4168.25, n_correct=2723.27, ppl=4.43, accuracy=65.334, wps=6672, ups=0.8, wpb=8336.5, bsz=315.7, num_updates=41300, lr=6.95889e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=34253
2023-07-28 00:06:44 | INFO | train_inner | epoch 029:    144 / 1474 loss=1.973, trans_loss=4.957, nll_loss=2.137, w2v_ctc_loss=0.608, task_loss=1.374, contrastive_loss=0.108, total=4117.66, n_correct=2693.07, ppl=4.4, accuracy=65.403, wps=11794.3, ups=1.43, wpb=8235.3, bsz=308.4, num_updates=41400, lr=6.95048e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=34322
2023-07-28 00:07:54 | INFO | train_inner | epoch 029:    244 / 1474 loss=1.967, trans_loss=4.948, nll_loss=2.128, w2v_ctc_loss=0.594, task_loss=1.275, contrastive_loss=0.186, total=4198.99, n_correct=2750.83, ppl=4.37, accuracy=65.512, wps=12046.4, ups=1.43, wpb=8398, bsz=330.7, num_updates=41500, lr=6.9421e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=69, gb_free=15.2, wall=34392
2023-07-28 00:09:04 | INFO | train_inner | epoch 029:    344 / 1474 loss=1.981, trans_loss=4.97, nll_loss=2.154, w2v_ctc_loss=0.621, task_loss=1.505, contrastive_loss=0.063, total=4091.28, n_correct=2663.97, ppl=4.45, accuracy=65.113, wps=11784, ups=1.44, wpb=8182.6, bsz=290.4, num_updates=41600, lr=6.93375e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=69, gb_free=17.3, wall=34462
2023-07-28 00:10:13 | INFO | train_inner | epoch 029:    444 / 1474 loss=1.963, trans_loss=4.947, nll_loss=2.124, w2v_ctc_loss=0.605, task_loss=1.36, contrastive_loss=0.054, total=4158.09, n_correct=2728.66, ppl=4.36, accuracy=65.623, wps=11992.7, ups=1.44, wpb=8316.2, bsz=307.2, num_updates=41700, lr=6.92543e-05, gnorm=0.555, clip=0, loss_scale=128, train_wall=69, gb_free=15.9, wall=34531
2023-07-28 00:10:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-28 00:11:24 | INFO | train_inner | epoch 029:    545 / 1474 loss=1.985, trans_loss=4.971, nll_loss=2.156, w2v_ctc_loss=0.608, task_loss=1.482, contrastive_loss=0.154, total=4163.81, n_correct=2708.19, ppl=4.46, accuracy=65.041, wps=11780.7, ups=1.41, wpb=8327.6, bsz=296.6, num_updates=41800, lr=6.91714e-05, gnorm=0.557, clip=0, loss_scale=64, train_wall=70, gb_free=16.2, wall=34602
2023-07-28 00:12:33 | INFO | train_inner | epoch 029:    645 / 1474 loss=1.976, trans_loss=4.959, nll_loss=2.143, w2v_ctc_loss=0.603, task_loss=1.325, contrastive_loss=0.222, total=4143.76, n_correct=2708.27, ppl=4.42, accuracy=65.358, wps=11942.7, ups=1.44, wpb=8287.5, bsz=318.8, num_updates=41900, lr=6.90889e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=69, gb_free=17.3, wall=34671
2023-07-28 00:13:43 | INFO | train_inner | epoch 029:    745 / 1474 loss=1.97, trans_loss=4.958, nll_loss=2.14, w2v_ctc_loss=0.6, task_loss=1.298, contrastive_loss=0.144, total=4234.8, n_correct=2769.55, ppl=4.41, accuracy=65.4, wps=12089.2, ups=1.43, wpb=8469.6, bsz=328.1, num_updates=42000, lr=6.90066e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=34741
2023-07-28 00:13:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 00:14:06 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.201 | trans_loss 5.573 | nll_loss 2.848 | w2v_ctc_loss 1.332 | task_loss 4.598 | contrastive_loss 0.257 | total 4003.4 | n_correct 2484.8 | ppl 7.2 | accuracy 62.067 | uer 16.84 | wer 18.594 | raw_wer 18.594 | bleu 19.89 | wps 2138.5 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 20.27
2023-07-28 00:14:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-07-28 00:14:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_29_42000.pt
2023-07-28 00:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_29_42000.pt
2023-07-28 00:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 19.89) (writing took 11.563298618420959 seconds)
2023-07-28 00:15:28 | INFO | train_inner | epoch 029:    845 / 1474 loss=1.983, trans_loss=4.983, nll_loss=2.171, w2v_ctc_loss=0.614, task_loss=1.547, contrastive_loss=0.055, total=4033.21, n_correct=2617, ppl=4.5, accuracy=64.886, wps=7700.8, ups=0.95, wpb=8066.4, bsz=281.6, num_updates=42100, lr=6.89246e-05, gnorm=0.564, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=34846
2023-07-28 00:16:37 | INFO | train_inner | epoch 029:    945 / 1474 loss=1.981, trans_loss=4.979, nll_loss=2.167, w2v_ctc_loss=0.615, task_loss=1.426, contrastive_loss=0.068, total=4085.97, n_correct=2656.63, ppl=4.49, accuracy=65.018, wps=11795.8, ups=1.44, wpb=8171.9, bsz=296.8, num_updates=42200, lr=6.88428e-05, gnorm=0.564, clip=0, loss_scale=64, train_wall=69, gb_free=17.2, wall=34915
2023-07-28 00:17:46 | INFO | train_inner | epoch 029:   1045 / 1474 loss=1.977, trans_loss=4.969, nll_loss=2.154, w2v_ctc_loss=0.605, task_loss=1.398, contrastive_loss=0.14, total=4140.84, n_correct=2696.16, ppl=4.45, accuracy=65.111, wps=12036.2, ups=1.45, wpb=8281.7, bsz=306.7, num_updates=42300, lr=6.87614e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=68, gb_free=16.7, wall=34984
2023-07-28 00:18:55 | INFO | train_inner | epoch 029:   1145 / 1474 loss=1.981, trans_loss=4.979, nll_loss=2.166, w2v_ctc_loss=0.619, task_loss=1.521, contrastive_loss=0.05, total=4068.4, n_correct=2646.58, ppl=4.49, accuracy=65.052, wps=11754.9, ups=1.44, wpb=8136.8, bsz=284.2, num_updates=42400, lr=6.86803e-05, gnorm=0.561, clip=0, loss_scale=64, train_wall=69, gb_free=17.5, wall=35053
2023-07-28 00:20:05 | INFO | train_inner | epoch 029:   1245 / 1474 loss=1.981, trans_loss=4.984, nll_loss=2.175, w2v_ctc_loss=0.614, task_loss=1.432, contrastive_loss=0.058, total=4154.79, n_correct=2699.29, ppl=4.51, accuracy=64.968, wps=11935.1, ups=1.44, wpb=8309.6, bsz=299.6, num_updates=42500, lr=6.85994e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=69, gb_free=16.8, wall=35123
2023-07-28 00:21:14 | INFO | train_inner | epoch 029:   1345 / 1474 loss=1.979, trans_loss=4.973, nll_loss=2.16, w2v_ctc_loss=0.604, task_loss=1.371, contrastive_loss=0.127, total=4166.4, n_correct=2711.62, ppl=4.47, accuracy=65.083, wps=12027.1, ups=1.44, wpb=8332.8, bsz=311.5, num_updates=42600, lr=6.85189e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=35192
2023-07-28 00:22:23 | INFO | train_inner | epoch 029:   1445 / 1474 loss=1.983, trans_loss=4.975, nll_loss=2.163, w2v_ctc_loss=0.614, task_loss=1.372, contrastive_loss=0.153, total=4169.4, n_correct=2711.43, ppl=4.48, accuracy=65.032, wps=12069.8, ups=1.45, wpb=8338.8, bsz=312, num_updates=42700, lr=6.84386e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=68, gb_free=15.9, wall=35261
2023-07-28 00:22:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 00:23:08 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.194 | trans_loss 5.571 | nll_loss 2.846 | w2v_ctc_loss 1.318 | task_loss 4.635 | contrastive_loss 0.248 | total 4003.4 | n_correct 2482.3 | ppl 7.19 | accuracy 62.005 | uer 16.492 | wer 18.377 | raw_wer 18.377 | bleu 20.1 | wps 1977.6 | wpb 4003.4 | bsz 141.8 | num_updates 42729 | best_bleu 20.27
2023-07-28 00:23:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42729 updates
2023-07-28 00:23:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1003.pt
2023-07-28 00:23:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1003.pt
2023-07-28 00:23:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1003.pt (epoch 29 @ 42729 updates, score 20.1) (writing took 11.77685440890491 seconds)
2023-07-28 00:23:20 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-07-28 00:23:20 | INFO | train | epoch 029 | loss 1.977 | trans_loss 4.967 | nll_loss 2.152 | w2v_ctc_loss 0.608 | task_loss 1.4 | contrastive_loss 0.112 | total 4138.71 | n_correct 2698.56 | ppl 4.44 | accuracy 65.203 | wps 11026.3 | ups 1.33 | wpb 8277.4 | bsz 305.7 | num_updates 42729 | lr 6.84154e-05 | gnorm 0.555 | clip 0 | loss_scale 64 | train_wall 1013 | gb_free 16 | wall 35318
2023-07-28 00:23:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-28 00:23:20 | INFO | fairseq.trainer | begin training epoch 30
2023-07-28 00:23:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-28 00:24:20 | INFO | train_inner | epoch 030:     71 / 1474 loss=1.968, trans_loss=4.952, nll_loss=2.133, w2v_ctc_loss=0.591, task_loss=1.332, contrastive_loss=0.171, total=4176.73, n_correct=2736.81, ppl=4.39, accuracy=65.525, wps=7179.3, ups=0.86, wpb=8353.5, bsz=319, num_updates=42800, lr=6.83586e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=35378
2023-07-28 00:25:29 | INFO | train_inner | epoch 030:    171 / 1474 loss=1.961, trans_loss=4.935, nll_loss=2.11, w2v_ctc_loss=0.602, task_loss=1.305, contrastive_loss=0.104, total=4202.84, n_correct=2769.9, ppl=4.32, accuracy=65.905, wps=12115.2, ups=1.44, wpb=8405.7, bsz=318.6, num_updates=42900, lr=6.82789e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=69, gb_free=12.6, wall=35447
2023-07-28 00:26:38 | INFO | train_inner | epoch 030:    271 / 1474 loss=1.969, trans_loss=4.952, nll_loss=2.13, w2v_ctc_loss=0.612, task_loss=1.441, contrastive_loss=0.053, total=4120.08, n_correct=2697.64, ppl=4.38, accuracy=65.475, wps=11936.4, ups=1.45, wpb=8240.2, bsz=296.4, num_updates=43000, lr=6.81994e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=68, gb_free=16.8, wall=35516
2023-07-28 00:27:48 | INFO | train_inner | epoch 030:    371 / 1474 loss=1.96, trans_loss=4.946, nll_loss=2.123, w2v_ctc_loss=0.597, task_loss=1.403, contrastive_loss=0.057, total=4175.82, n_correct=2741.5, ppl=4.36, accuracy=65.652, wps=11964.9, ups=1.43, wpb=8351.6, bsz=306.1, num_updates=43100, lr=6.81203e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=69, gb_free=15.6, wall=35586
2023-07-28 00:28:56 | INFO | train_inner | epoch 030:    471 / 1474 loss=1.965, trans_loss=4.947, nll_loss=2.126, w2v_ctc_loss=0.597, task_loss=1.348, contrastive_loss=0.125, total=4128.9, n_correct=2706.23, ppl=4.36, accuracy=65.544, wps=12042.6, ups=1.46, wpb=8257.8, bsz=312.5, num_updates=43200, lr=6.80414e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=68, gb_free=16.5, wall=35654
2023-07-28 00:30:06 | INFO | train_inner | epoch 030:    571 / 1474 loss=1.969, trans_loss=4.962, nll_loss=2.145, w2v_ctc_loss=0.602, task_loss=1.369, contrastive_loss=0.085, total=4162.83, n_correct=2720.59, ppl=4.42, accuracy=65.354, wps=12006.2, ups=1.44, wpb=8325.7, bsz=311.5, num_updates=43300, lr=6.79628e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=69, gb_free=15, wall=35724
2023-07-28 00:31:15 | INFO | train_inner | epoch 030:    671 / 1474 loss=1.973, trans_loss=4.96, nll_loss=2.143, w2v_ctc_loss=0.61, task_loss=1.362, contrastive_loss=0.103, total=4197.56, n_correct=2741.4, ppl=4.42, accuracy=65.309, wps=12100.2, ups=1.44, wpb=8395.1, bsz=317.6, num_updates=43400, lr=6.78844e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=35793
2023-07-28 00:32:25 | INFO | train_inner | epoch 030:    771 / 1474 loss=1.988, trans_loss=4.974, nll_loss=2.16, w2v_ctc_loss=0.619, task_loss=1.443, contrastive_loss=0.18, total=4097.27, n_correct=2662.31, ppl=4.47, accuracy=64.978, wps=11755.8, ups=1.43, wpb=8194.5, bsz=301.1, num_updates=43500, lr=6.78064e-05, gnorm=0.57, clip=0, loss_scale=64, train_wall=69, gb_free=14.8, wall=35863
2023-07-28 00:33:34 | INFO | train_inner | epoch 030:    871 / 1474 loss=1.973, trans_loss=4.967, nll_loss=2.15, w2v_ctc_loss=0.605, task_loss=1.464, contrastive_loss=0.06, total=4097.18, n_correct=2674.96, ppl=4.44, accuracy=65.288, wps=11903.2, ups=1.45, wpb=8194.4, bsz=292.2, num_updates=43600, lr=6.77285e-05, gnorm=0.557, clip=0, loss_scale=64, train_wall=68, gb_free=16.8, wall=35932
2023-07-28 00:34:43 | INFO | train_inner | epoch 030:    971 / 1474 loss=1.976, trans_loss=4.971, nll_loss=2.156, w2v_ctc_loss=0.608, task_loss=1.408, contrastive_loss=0.081, total=4140.12, n_correct=2696.69, ppl=4.46, accuracy=65.136, wps=11909.8, ups=1.44, wpb=8280.2, bsz=304.6, num_updates=43700, lr=6.7651e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=69, gb_free=15.3, wall=36001
2023-07-28 00:35:53 | INFO | train_inner | epoch 030:   1071 / 1474 loss=1.992, trans_loss=4.984, nll_loss=2.173, w2v_ctc_loss=0.617, task_loss=1.578, contrastive_loss=0.147, total=4099.61, n_correct=2659.4, ppl=4.51, accuracy=64.87, wps=11669.7, ups=1.42, wpb=8199.2, bsz=281.4, num_updates=43800, lr=6.75737e-05, gnorm=0.561, clip=0, loss_scale=64, train_wall=70, gb_free=17.4, wall=36071
2023-07-28 00:37:03 | INFO | train_inner | epoch 030:   1171 / 1474 loss=1.97, trans_loss=4.967, nll_loss=2.153, w2v_ctc_loss=0.594, task_loss=1.341, contrastive_loss=0.131, total=4164.38, n_correct=2719.95, ppl=4.45, accuracy=65.315, wps=11966.7, ups=1.44, wpb=8328.8, bsz=314.8, num_updates=43900, lr=6.74967e-05, gnorm=0.556, clip=0, loss_scale=128, train_wall=69, gb_free=17.4, wall=36141
2023-07-28 00:38:13 | INFO | train_inner | epoch 030:   1271 / 1474 loss=1.981, trans_loss=4.976, nll_loss=2.162, w2v_ctc_loss=0.614, task_loss=1.572, contrastive_loss=0.063, total=4030.53, n_correct=2619.63, ppl=4.48, accuracy=64.995, wps=11584.2, ups=1.44, wpb=8061.1, bsz=281.4, num_updates=44000, lr=6.742e-05, gnorm=0.567, clip=0, loss_scale=128, train_wall=69, gb_free=17.2, wall=36211
2023-07-28 00:38:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 00:38:37 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.202 | trans_loss 5.572 | nll_loss 2.844 | w2v_ctc_loss 1.342 | task_loss 4.61 | contrastive_loss 0.251 | total 4003.4 | n_correct 2484.8 | ppl 7.18 | accuracy 62.067 | uer 16.802 | wer 18.698 | raw_wer 18.698 | bleu 20.17 | wps 2029 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 20.27
2023-07-28 00:38:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-07-28 00:38:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_30_44000.pt
2023-07-28 00:38:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_30_44000.pt
2023-07-28 00:38:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 20.17) (writing took 17.48431358858943 seconds)
2023-07-28 00:40:05 | INFO | train_inner | epoch 030:   1371 / 1474 loss=1.969, trans_loss=4.969, nll_loss=2.156, w2v_ctc_loss=0.602, task_loss=1.322, contrastive_loss=0.074, total=4163.24, n_correct=2716.59, ppl=4.46, accuracy=65.252, wps=7382, ups=0.89, wpb=8326.5, bsz=320.7, num_updates=44100, lr=6.73435e-05, gnorm=0.552, clip=0, loss_scale=128, train_wall=69, gb_free=17.2, wall=36323
2023-07-28 00:41:14 | INFO | train_inner | epoch 030:   1471 / 1474 loss=1.975, trans_loss=4.967, nll_loss=2.154, w2v_ctc_loss=0.596, task_loss=1.332, contrastive_loss=0.22, total=4130.55, n_correct=2698.11, ppl=4.45, accuracy=65.321, wps=11995.1, ups=1.45, wpb=8261.1, bsz=311.7, num_updates=44200, lr=6.72673e-05, gnorm=0.552, clip=0, loss_scale=128, train_wall=68, gb_free=16.5, wall=36392
2023-07-28 00:41:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 00:41:42 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.203 | trans_loss 5.573 | nll_loss 2.848 | w2v_ctc_loss 1.342 | task_loss 4.617 | contrastive_loss 0.255 | total 4003.4 | n_correct 2489.2 | ppl 7.2 | accuracy 62.177 | uer 16.882 | wer 18.679 | raw_wer 18.679 | bleu 20.15 | wps 1947 | wpb 4003.4 | bsz 141.8 | num_updates 44203 | best_bleu 20.27
2023-07-28 00:41:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44203 updates
2023-07-28 00:41:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1507.pt
2023-07-28 00:41:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1507.pt
2023-07-28 00:41:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1507.pt (epoch 30 @ 44203 updates, score 20.15) (writing took 11.816180124878883 seconds)
2023-07-28 00:41:54 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-07-28 00:41:54 | INFO | train | epoch 030 | loss 1.973 | trans_loss 4.962 | nll_loss 2.145 | w2v_ctc_loss 0.604 | task_loss 1.4 | contrastive_loss 0.111 | total 4138.65 | n_correct 2703.82 | ppl 4.42 | accuracy 65.331 | wps 10952.5 | ups 1.32 | wpb 8277.3 | bsz 305.7 | num_updates 44203 | lr 6.7265e-05 | gnorm 0.555 | clip 0 | loss_scale 128 | train_wall 1014 | gb_free 17.1 | wall 36432
2023-07-28 00:41:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-28 00:41:54 | INFO | fairseq.trainer | begin training epoch 31
2023-07-28 00:41:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-28 00:43:09 | INFO | train_inner | epoch 031:     97 / 1474 loss=1.961, trans_loss=4.943, nll_loss=2.119, w2v_ctc_loss=0.601, task_loss=1.442, contrastive_loss=0.06, total=4081.82, n_correct=2684.3, ppl=4.34, accuracy=65.762, wps=7098.9, ups=0.87, wpb=8163.6, bsz=295.5, num_updates=44300, lr=6.71913e-05, gnorm=0.562, clip=0, loss_scale=128, train_wall=68, gb_free=13.4, wall=36507
2023-07-28 00:44:19 | INFO | train_inner | epoch 031:    197 / 1474 loss=1.966, trans_loss=4.945, nll_loss=2.121, w2v_ctc_loss=0.604, task_loss=1.439, contrastive_loss=0.087, total=4143.18, n_correct=2723.28, ppl=4.35, accuracy=65.729, wps=11944.4, ups=1.44, wpb=8286.4, bsz=301.1, num_updates=44400, lr=6.71156e-05, gnorm=0.564, clip=0, loss_scale=128, train_wall=69, gb_free=17.2, wall=36577
2023-07-28 00:45:29 | INFO | train_inner | epoch 031:    297 / 1474 loss=1.965, trans_loss=4.943, nll_loss=2.119, w2v_ctc_loss=0.599, task_loss=1.434, contrastive_loss=0.124, total=4154.03, n_correct=2734.46, ppl=4.34, accuracy=65.827, wps=11773.7, ups=1.42, wpb=8308.1, bsz=301.5, num_updates=44500, lr=6.70402e-05, gnorm=0.55, clip=0, loss_scale=128, train_wall=70, gb_free=16.3, wall=36647
2023-07-28 00:46:38 | INFO | train_inner | epoch 031:    397 / 1474 loss=1.968, trans_loss=4.955, nll_loss=2.134, w2v_ctc_loss=0.602, task_loss=1.533, contrastive_loss=0.06, total=4079.78, n_correct=2669.39, ppl=4.39, accuracy=65.43, wps=11875.7, ups=1.46, wpb=8159.6, bsz=284.9, num_updates=44600, lr=6.6965e-05, gnorm=0.561, clip=0, loss_scale=128, train_wall=68, gb_free=15.7, wall=36716
2023-07-28 00:47:47 | INFO | train_inner | epoch 031:    497 / 1474 loss=1.967, trans_loss=4.949, nll_loss=2.127, w2v_ctc_loss=0.61, task_loss=1.451, contrastive_loss=0.071, total=4122.36, n_correct=2702.97, ppl=4.37, accuracy=65.569, wps=11870.9, ups=1.44, wpb=8244.7, bsz=302, num_updates=44700, lr=6.689e-05, gnorm=0.561, clip=0, loss_scale=128, train_wall=69, gb_free=16.6, wall=36785
2023-07-28 00:48:57 | INFO | train_inner | epoch 031:    597 / 1474 loss=1.96, trans_loss=4.948, nll_loss=2.127, w2v_ctc_loss=0.593, task_loss=1.461, contrastive_loss=0.058, total=4082.24, n_correct=2682.07, ppl=4.37, accuracy=65.701, wps=11700.5, ups=1.43, wpb=8164.5, bsz=294.3, num_updates=44800, lr=6.68153e-05, gnorm=0.554, clip=0, loss_scale=128, train_wall=69, gb_free=16.6, wall=36855
2023-07-28 00:50:07 | INFO | train_inner | epoch 031:    697 / 1474 loss=1.957, trans_loss=4.946, nll_loss=2.124, w2v_ctc_loss=0.592, task_loss=1.336, contrastive_loss=0.06, total=4207.46, n_correct=2762.55, ppl=4.36, accuracy=65.658, wps=12118.5, ups=1.44, wpb=8414.9, bsz=313.6, num_updates=44900, lr=6.67409e-05, gnorm=0.55, clip=0, loss_scale=128, train_wall=69, gb_free=15.7, wall=36925
2023-07-28 00:51:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-28 00:51:17 | INFO | train_inner | epoch 031:    798 / 1474 loss=1.971, trans_loss=4.962, nll_loss=2.145, w2v_ctc_loss=0.604, task_loss=1.49, contrastive_loss=0.072, total=4089.04, n_correct=2670.72, ppl=4.42, accuracy=65.314, wps=11642.4, ups=1.42, wpb=8178.1, bsz=291.7, num_updates=45000, lr=6.66667e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=69, gb_free=16.9, wall=36995
Mixup rate:0.5, token after shrink shape:torch.Size([8, 131]), X shape:torch.Size([8, 131, 512])
CTC Tokens:tensor([  8,   0,   0, 860,   0], device='cuda:0'), Shrink Mask:tensor([ True,  True, False,  True,  True], device='cuda:0'), New Tokens:tensor([   8,    0,  860,    0, 8822], device='cuda:0')
Org X:tensor([[ 3.7628e-02, -3.1586e-02,  1.1157e-01,  ...,  8.8501e-04,
         -6.2842e-01, -1.3008e+00],
        [-5.4248e-01, -2.7588e-01,  8.2666e-01,  ..., -1.1104e+00,
         -3.4717e-01,  5.4657e-02],
        [-1.3096e+00,  1.6250e+00,  3.8452e-01,  ..., -1.0371e+00,
         -1.3701e+00,  5.3955e-01],
        [-1.6846e+00,  2.0879e+00,  2.6978e-01,  ..., -1.0762e+00,
         -1.7617e+00,  4.3701e-01],
        [-7.5586e-01,  2.9175e-01, -1.4648e-01,  ..., -1.7832e+00,
         -7.3975e-01, -1.1875e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:0'), Mixup Sent Mask:tensor([False, False,  True,  True, False, False, False, False],
       device='cuda:0'), New X:tensor([[ 3.7628e-02, -3.1586e-02,  1.1157e-01,  ...,  8.8501e-04,
         -6.2842e-01, -1.3008e+00],
        [-5.4248e-01, -2.7588e-01,  8.2666e-01,  ..., -1.1104e+00,
         -3.4717e-01,  5.4657e-02],
        [-1.3096e+00,  1.6250e+00,  3.8452e-01,  ..., -1.0371e+00,
         -1.3701e+00,  5.3955e-01],
        [-1.6846e+00,  2.0879e+00,  2.6978e-01,  ..., -1.0762e+00,
         -1.7617e+00,  4.3701e-01],
        [-7.5586e-01,  2.9175e-01, -1.4648e-01,  ..., -1.7832e+00,
         -7.3975e-01, -1.1875e+00]], device='cuda:0', dtype=torch.float16,
       grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:0')
2023-07-28 00:52:26 | INFO | train_inner | epoch 031:    898 / 1474 loss=1.967, trans_loss=4.953, nll_loss=2.134, w2v_ctc_loss=0.601, task_loss=1.451, contrastive_loss=0.076, total=4101.05, n_correct=2682.97, ppl=4.39, accuracy=65.422, wps=11802.4, ups=1.44, wpb=8202.1, bsz=296.9, num_updates=45100, lr=6.65927e-05, gnorm=0.564, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=37064
2023-07-28 00:53:36 | INFO | train_inner | epoch 031:    998 / 1474 loss=1.972, trans_loss=4.963, nll_loss=2.148, w2v_ctc_loss=0.597, task_loss=1.327, contrastive_loss=0.162, total=4186.3, n_correct=2737.89, ppl=4.43, accuracy=65.401, wps=12103.9, ups=1.45, wpb=8372.6, bsz=318.6, num_updates=45200, lr=6.6519e-05, gnorm=0.555, clip=0, loss_scale=64, train_wall=68, gb_free=16.3, wall=37134
2023-07-28 00:54:44 | INFO | train_inner | epoch 031:   1098 / 1474 loss=1.968, trans_loss=4.959, nll_loss=2.143, w2v_ctc_loss=0.598, task_loss=1.37, contrastive_loss=0.112, total=4147.34, n_correct=2714.97, ppl=4.42, accuracy=65.463, wps=12053.2, ups=1.45, wpb=8294.7, bsz=314.6, num_updates=45300, lr=6.64455e-05, gnorm=0.563, clip=0, loss_scale=64, train_wall=68, gb_free=17.1, wall=37202
2023-07-28 00:55:54 | INFO | train_inner | epoch 031:   1198 / 1474 loss=1.973, trans_loss=4.963, nll_loss=2.149, w2v_ctc_loss=0.595, task_loss=1.303, contrastive_loss=0.225, total=4185.34, n_correct=2734.84, ppl=4.43, accuracy=65.343, wps=12039.5, ups=1.44, wpb=8370.7, bsz=321.6, num_updates=45400, lr=6.63723e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=69, gb_free=17, wall=37272
2023-07-28 00:57:03 | INFO | train_inner | epoch 031:   1298 / 1474 loss=1.964, trans_loss=4.963, nll_loss=2.147, w2v_ctc_loss=0.6, task_loss=1.266, contrastive_loss=0.068, total=4223.54, n_correct=2766.65, ppl=4.43, accuracy=65.505, wps=12198.1, ups=1.44, wpb=8447.1, bsz=325, num_updates=45500, lr=6.62994e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=68, gb_free=13.4, wall=37341
2023-07-28 00:58:13 | INFO | train_inner | epoch 031:   1398 / 1474 loss=1.984, trans_loss=4.966, nll_loss=2.153, w2v_ctc_loss=0.6, task_loss=1.277, contrastive_loss=0.272, total=4195.76, n_correct=2735.3, ppl=4.45, accuracy=65.192, wps=12093.6, ups=1.44, wpb=8391.5, bsz=328.2, num_updates=45600, lr=6.62266e-05, gnorm=0.559, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=37411
2023-07-28 00:59:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
Mixup rate:0.5, token after shrink shape:torch.Size([40, 36]), X shape:torch.Size([40, 36, 512])
CTC Tokens:tensor([  0,  19, 135, 135,  25], device='cuda:6'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:6'), New Tokens:tensor([  0,  19, 135,  25,  11], device='cuda:6')
Org X:tensor([[ 0.4680,  0.2852,  0.3525,  ...,  0.6616, -0.1799, -1.6416],
        [ 0.5205,  0.5269, -0.8003,  ...,  0.2341,  0.2529, -0.7227],
        [ 0.8945,  0.4617,  0.2086,  ...,  0.3784, -0.3999, -0.1204],
        [ 0.2908,  0.4045, -3.6934,  ..., -0.0493,  0.2175, -1.9980],
        [ 0.7617,  0.3604, -2.6055,  ..., -1.6035,  0.3345, -0.4014]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:6'), Mixup Sent Mask:tensor([False, False,  True,  True, False, False, False, False,  True, False],
       device='cuda:6'), New X:tensor([[ 0.4680,  0.2852,  0.3525,  ...,  0.6616, -0.1799, -1.6416],
        [ 0.5205,  0.5269, -0.8003,  ...,  0.2341,  0.2529, -0.7227],
        [ 0.8945,  0.4617,  0.2086,  ...,  0.3784, -0.3999, -0.1204],
        [ 0.2908,  0.4045, -3.6934,  ..., -0.0493,  0.2175, -1.9980],
        [ 0.7617,  0.3604, -2.6055,  ..., -1.6035,  0.3345, -0.4014]],
       device='cuda:6', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:6')
Mixup rate:0.5, token after shrink shape:torch.Size([7, 147]), X shape:torch.Size([7, 147, 512])
CTC Tokens:tensor([ 8,  0, 70,  0,  0], device='cuda:1'), Shrink Mask:tensor([ True,  True,  True,  True, False], device='cuda:1'), New Tokens:tensor([ 8,  0, 70,  0, 17], device='cuda:1')
Org X:tensor([[ 0.0773,  0.2891,  0.1306,  ...,  0.9644, -1.4912, -1.3486],
        [-0.6089,  0.2284,  1.1084,  ..., -0.8716,  0.2456, -0.2502],
        [-1.2949,  0.3315, -0.6416,  ..., -0.3076,  0.1359, -0.8950],
        [-1.6006,  0.6382,  0.4426,  ..., -0.7573, -1.0488, -0.6401],
        [-1.4639,  0.3999, -2.5098,  ..., -1.1787,  1.1855, -0.2876]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:1'), Mixup Sent Mask:tensor([False, False,  True,  True, False, False, False], device='cuda:1'), New X:tensor([[ 0.0773,  0.2891,  0.1306,  ...,  0.9644, -1.4912, -1.3486],
        [-0.6089,  0.2284,  1.1084,  ..., -0.8716,  0.2456, -0.2502],
        [-1.2949,  0.3315, -0.6416,  ..., -0.3076,  0.1359, -0.8950],
        [-1.6006,  0.6382,  0.4426,  ..., -0.7573, -1.0488, -0.6401],
        [-1.4639,  0.3999, -2.5098,  ..., -1.1787,  1.1855, -0.2876]],
       device='cuda:1', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:1')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 59]), X shape:torch.Size([16, 59, 512])
CTC Tokens:tensor([  8,   0,  12,  12, 538], device='cuda:4'), Shrink Mask:tensor([ True,  True,  True, False,  True], device='cuda:4'), New Tokens:tensor([  8,   0,  12, 538,   0], device='cuda:4')
Org X:tensor([[-0.0437,  0.1740,  0.0701,  ...,  0.9712, -0.9482, -1.5000],
        [-0.2930, -0.1720,  1.6143,  ...,  0.0923,  0.3711,  0.0367],
        [-0.1718,  0.2228,  0.7690,  ...,  0.2732,  0.7866,  0.3926],
        [-1.2002,  0.2092,  1.4355,  ...,  0.0795,  0.0422,  0.1545],
        [-1.0137,  0.5166,  2.5098,  ..., -2.2109, -0.0206, -0.4966]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False]]], device='cuda:4'), Mixup Sent Mask:tensor([False, False,  True,  True, False, False, False, False,  True, False],
       device='cuda:4'), New X:tensor([[-0.0437,  0.1740,  0.0701,  ...,  0.9712, -0.9482, -1.5000],
        [-0.2930, -0.1720,  1.6143,  ...,  0.0923,  0.3711,  0.0367],
        [-0.1718,  0.2228,  0.7690,  ...,  0.2732,  0.7866,  0.3926],
        [-1.2002,  0.2092,  1.4355,  ...,  0.0795,  0.0422,  0.1545],
        [-1.0137,  0.5166,  2.5098,  ..., -2.2109, -0.0206, -0.4966]],
       device='cuda:4', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:4')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 83]), X shape:torch.Size([8, 83, 512])
CTC Tokens:tensor([ 19,  19,   0, 591,   0], device='cuda:2'), Shrink Mask:tensor([ True, False,  True,  True,  True], device='cuda:2'), New Tokens:tensor([  19,    0,  591,    0, 1122], device='cuda:2')
Org X:tensor([[-0.0057, -0.1000,  0.1879,  ..., -0.1403,  1.0234, -1.4688],
        [ 1.2197,  1.2666,  3.4961,  ..., -1.9365,  0.2766, -0.8813],
        [ 0.3999,  2.0957,  1.6045,  ..., -0.9639, -1.4844, -0.4302],
        [ 1.9209,  1.3789,  2.5078,  ..., -2.6777, -2.8320, -0.2096],
        [ 0.1031,  0.3638, -1.5166,  ..., -1.0918, -0.5356,  0.7148]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:2'), Mixup Sent Mask:tensor([False, False,  True,  True, False, False, False, False],
       device='cuda:2'), New X:tensor([[-0.0057, -0.1000,  0.1879,  ..., -0.1403,  1.0234, -1.4688],
        [ 1.2197,  1.2666,  3.4961,  ..., -1.9365,  0.2766, -0.8813],
        [ 0.3999,  2.0957,  1.6045,  ..., -0.9639, -1.4844, -0.4302],
        [ 1.9209,  1.3789,  2.5078,  ..., -2.6777, -2.8320, -0.2096],
        [ 0.1031,  0.3638, -1.5166,  ..., -1.0918, -0.5356,  0.7148]],
       device='cuda:2', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:2')
Mixup rate:0.5, token after shrink shape:torch.Size([16, 76]), X shape:torch.Size([16, 76, 512])
CTC Tokens:tensor([ 0, 29, 29,  0,  0], device='cuda:3'), Shrink Mask:tensor([ True,  True, False,  True, False], device='cuda:3'), New Tokens:tensor([  0,  29,   0, 103,   0], device='cuda:3')
Org X:tensor([[-0.0586, -0.2449,  1.4697,  ..., -0.8442, -0.6147,  0.7886],
        [-0.1523,  0.5513,  1.3115,  ...,  0.0248, -1.1719,  0.4106],
        [-0.4626,  1.0059,  2.4473,  ..., -0.9429, -0.8745,  0.0255],
        [-0.1724,  0.3511, -0.5708,  ...,  0.2166,  0.6470,  0.4556],
        [-0.0931,  0.3389, -0.4580,  ...,  0.1963,  1.0137,  0.5864]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]]], device='cuda:3'), Mixup Sent Mask:tensor([False, False,  True,  True, False, False, False, False,  True, False],
       device='cuda:3'), New X:tensor([[-0.0586, -0.2449,  1.4697,  ..., -0.8442, -0.6147,  0.7886],
        [-0.1523,  0.5513,  1.3115,  ...,  0.0248, -1.1719,  0.4106],
        [-0.4626,  1.0059,  2.4473,  ..., -0.9429, -0.8745,  0.0255],
        [-0.1724,  0.3511, -0.5708,  ...,  0.2166,  0.6470,  0.4556],
        [-0.0931,  0.3389, -0.4580,  ...,  0.1963,  1.0137,  0.5864]],
       device='cuda:3', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:3')
Mixup rate:0.5, token after shrink shape:torch.Size([8, 132]), X shape:torch.Size([8, 132, 512])
CTC Tokens:tensor([ 0, 77,  0,  0,  0], device='cuda:5'), Shrink Mask:tensor([ True,  True,  True, False, False], device='cuda:5'), New Tokens:tensor([   0,   77,    0, 9025,    6], device='cuda:5')
Org X:tensor([[-0.8560, -0.0146,  1.5010,  ..., -2.0488,  0.7749, -1.0898],
        [-2.7227,  1.2988,  0.2277,  ..., -0.3101,  0.1558, -0.3906],
        [-0.7832,  1.0508,  1.4199,  ..., -0.6763, -0.2434,  0.4158],
        [ 0.0944,  0.2825, -1.0117,  ...,  0.1133,  0.2397,  0.6045],
        [-0.1240,  0.2795,  0.1556,  ..., -0.2588,  0.1417,  1.1562]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:5'), Mixup Sent Mask:tensor([False, False,  True,  True, False, False, False, False],
       device='cuda:5'), New X:tensor([[-0.8560, -0.0146,  1.5010,  ..., -2.0488,  0.7749, -1.0898],
        [-2.7227,  1.2988,  0.2277,  ..., -0.3101,  0.1558, -0.3906],
        [-0.7832,  1.0508,  1.4199,  ..., -0.6763, -0.2434,  0.4158],
        [ 0.0944,  0.2825, -1.0117,  ...,  0.1133,  0.2397,  0.6045],
        [-0.1240,  0.2795,  0.1556,  ..., -0.2588,  0.1417,  1.1562]],
       device='cuda:5', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:5')
Mixup rate:0.5, token after shrink shape:torch.Size([24, 51]), X shape:torch.Size([24, 51, 512])
CTC Tokens:tensor([0, 0, 0, 0, 0], device='cuda:7'), Shrink Mask:tensor([ True, False, False, False, False], device='cuda:7'), New Tokens:tensor([ 0, 42, 29,  0, 19], device='cuda:7')
Org X:tensor([[ 0.3379, -0.5767,  1.4121,  ...,  0.3872,  1.0645,  0.7056],
        [ 0.1248,  1.0820,  0.4985,  ...,  0.0220, -0.0250,  0.3643],
        [-0.8701,  0.3850,  1.1660,  ...,  0.1758,  0.3855,  0.9976],
        [-0.7720,  0.2939,  1.1787,  ...,  0.2363,  0.2473,  0.4490],
        [-0.4172,  0.2491,  0.8071,  ...,  0.1428,  0.3511,  0.5386]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
Mixup Mask:tensor([[[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [ True,  True,  True,  ...,  True,  True,  True],
         [False, False, False,  ..., False, False, False],
         ...,
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True],
         [ True,  True,  True,  ...,  True,  True,  True]],

        ...,

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]],

        [[False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         ...,
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False],
         [False, False, False,  ..., False, False, False]]], device='cuda:7'), Mixup Sent Mask:tensor([False, False,  True,  True, False, False, False, False,  True, False],
       device='cuda:7'), New X:tensor([[ 0.3379, -0.5767,  1.4121,  ...,  0.3872,  1.0645,  0.7056],
        [ 0.1248,  1.0820,  0.4985,  ...,  0.0220, -0.0250,  0.3643],
        [-0.8701,  0.3850,  1.1660,  ...,  0.1758,  0.3855,  0.9976],
        [-0.7720,  0.2939,  1.1787,  ...,  0.2363,  0.2473,  0.4490],
        [-0.4172,  0.2491,  0.8071,  ...,  0.1428,  0.3511,  0.5386]],
       device='cuda:7', dtype=torch.float16, grad_fn=<SliceBackward0>)
mt_weight tensor(1.)
asr_weight tensor(0.2252, device='cuda:7')
2023-07-28 00:59:29 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 4.208 | trans_loss 5.573 | nll_loss 2.848 | w2v_ctc_loss 1.356 | task_loss 4.633 | contrastive_loss 0.258 | total 4003.4 | n_correct 2486.8 | ppl 7.2 | accuracy 62.117 | uer 16.76 | wer 18.732 | raw_wer 18.732 | bleu 20.24 | wps 1940 | wpb 4003.4 | bsz 141.8 | num_updates 45676 | best_bleu 20.27
2023-07-28 00:59:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45676 updates
2023-07-28 00:59:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.2404.pt
2023-07-28 00:59:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.2404.pt
2023-07-28 00:59:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.2404.pt (epoch 31 @ 45676 updates, score 20.24) (writing took 13.31607248261571 seconds)
2023-07-28 00:59:43 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-07-28 00:59:43 | INFO | train | epoch 031 | loss 1.968 | trans_loss 4.955 | nll_loss 2.136 | w2v_ctc_loss 0.6 | task_loss 1.4 | contrastive_loss 0.107 | total 4137.82 | n_correct 2710.8 | ppl 4.39 | accuracy 65.513 | wps 11402.3 | ups 1.38 | wpb 8275.6 | bsz 305.3 | num_updates 45676 | lr 6.61715e-05 | gnorm 0.558 | clip 0 | loss_scale 64 | train_wall 1012 | gb_free 12 | wall 37501
2023-07-28 00:59:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-28 00:59:43 | INFO | fairseq.trainer | begin training epoch 32
2023-07-28 00:59:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-28 01:00:09 | INFO | train_inner | epoch 032:     24 / 1474 loss=1.966, trans_loss=4.955, nll_loss=2.137, w2v_ctc_loss=0.602, task_loss=1.485, contrastive_loss=0.053, total=4039.04, n_correct=2644.53, ppl=4.4, accuracy=65.474, wps=6930.7, ups=0.86, wpb=8078.1, bsz=287.3, num_updates=45700, lr=6.61541e-05, gnorm=0.572, clip=0, loss_scale=64, train_wall=69, gb_free=17.7, wall=37527
2023-07-28 01:01:19 | INFO | train_inner | epoch 032:    124 / 1474 loss=1.942, trans_loss=4.92, nll_loss=2.091, w2v_ctc_loss=0.58, task_loss=1.295, contrastive_loss=0.069, total=4224.84, n_correct=2798.02, ppl=4.26, accuracy=66.228, wps=12084.6, ups=1.43, wpb=8449.7, bsz=322.5, num_updates=45800, lr=6.60819e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=37597
2023-07-28 01:02:29 | INFO | train_inner | epoch 032:    224 / 1474 loss=1.952, trans_loss=4.937, nll_loss=2.112, w2v_ctc_loss=0.59, task_loss=1.325, contrastive_loss=0.079, total=4163.01, n_correct=2743.03, ppl=4.32, accuracy=65.891, wps=11964.4, ups=1.44, wpb=8326, bsz=322.2, num_updates=45900, lr=6.60098e-05, gnorm=0.557, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=37667
2023-07-28 01:03:38 | INFO | train_inner | epoch 032:    324 / 1474 loss=1.946, trans_loss=4.927, nll_loss=2.1, w2v_ctc_loss=0.581, task_loss=1.317, contrastive_loss=0.07, total=4185.21, n_correct=2771.26, ppl=4.29, accuracy=66.216, wps=12153, ups=1.45, wpb=8370.4, bsz=315.1, num_updates=46000, lr=6.5938e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=68, gb_free=16.4, wall=37736
2023-07-28 01:03:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 01:04:01 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.204 | trans_loss 5.576 | nll_loss 2.852 | w2v_ctc_loss 1.339 | task_loss 4.627 | contrastive_loss 0.249 | total 4003.4 | n_correct 2488.6 | ppl 7.22 | accuracy 62.162 | uer 16.768 | wer 18.657 | raw_wer 18.657 | bleu 19.9 | wps 2071.8 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 20.27
2023-07-28 01:04:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-07-28 01:04:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_32_46000.pt
2023-07-28 01:04:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_32_46000.pt
2023-07-28 01:04:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 19.9) (writing took 11.812260553240776 seconds)
2023-07-28 01:05:23 | INFO | train_inner | epoch 032:    424 / 1474 loss=1.953, trans_loss=4.935, nll_loss=2.11, w2v_ctc_loss=0.592, task_loss=1.395, contrastive_loss=0.065, total=4156.71, n_correct=2742.76, ppl=4.32, accuracy=65.984, wps=7901.8, ups=0.95, wpb=8313.4, bsz=305.7, num_updates=46100, lr=6.58665e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=69, gb_free=10.6, wall=37841
2023-07-28 01:06:33 | INFO | train_inner | epoch 032:    524 / 1474 loss=1.966, trans_loss=4.947, nll_loss=2.126, w2v_ctc_loss=0.598, task_loss=1.348, contrastive_loss=0.155, total=4195.32, n_correct=2756.44, ppl=4.37, accuracy=65.703, wps=12035.5, ups=1.43, wpb=8390.6, bsz=318.2, num_updates=46200, lr=6.57952e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=37910
2023-07-28 01:07:42 | INFO | train_inner | epoch 032:    624 / 1474 loss=1.963, trans_loss=4.95, nll_loss=2.129, w2v_ctc_loss=0.596, task_loss=1.447, contrastive_loss=0.075, total=4141.99, n_correct=2716.36, ppl=4.37, accuracy=65.581, wps=11901.8, ups=1.44, wpb=8284, bsz=300.9, num_updates=46300, lr=6.57241e-05, gnorm=0.56, clip=0, loss_scale=64, train_wall=69, gb_free=16.5, wall=37980
2023-07-28 01:08:52 | INFO | train_inner | epoch 032:    724 / 1474 loss=1.965, trans_loss=4.953, nll_loss=2.133, w2v_ctc_loss=0.604, task_loss=1.43, contrastive_loss=0.057, total=4153.97, n_correct=2727.14, ppl=4.39, accuracy=65.651, wps=11816.9, ups=1.42, wpb=8307.9, bsz=301.5, num_updates=46400, lr=6.56532e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=70, gb_free=17.1, wall=38050
2023-07-28 01:10:01 | INFO | train_inner | epoch 032:    824 / 1474 loss=1.956, trans_loss=4.945, nll_loss=2.122, w2v_ctc_loss=0.59, task_loss=1.448, contrastive_loss=0.053, total=4119.08, n_correct=2710.16, ppl=4.35, accuracy=65.795, wps=11972.4, ups=1.45, wpb=8238.2, bsz=295.1, num_updates=46500, lr=6.55826e-05, gnorm=0.555, clip=0, loss_scale=64, train_wall=68, gb_free=16.8, wall=38119
2023-07-28 01:11:10 | INFO | train_inner | epoch 032:    924 / 1474 loss=1.96, trans_loss=4.951, nll_loss=2.131, w2v_ctc_loss=0.592, task_loss=1.447, contrastive_loss=0.053, total=4142.37, n_correct=2715.1, ppl=4.38, accuracy=65.545, wps=11997.2, ups=1.45, wpb=8284.7, bsz=299.1, num_updates=46600, lr=6.55122e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=68, gb_free=15.6, wall=38188
2023-07-28 01:12:20 | INFO | train_inner | epoch 032:   1024 / 1474 loss=1.971, trans_loss=4.959, nll_loss=2.141, w2v_ctc_loss=0.599, task_loss=1.403, contrastive_loss=0.15, total=4112.12, n_correct=2689.55, ppl=4.41, accuracy=65.405, wps=11781.9, ups=1.43, wpb=8224.2, bsz=302.5, num_updates=46700, lr=6.5442e-05, gnorm=0.56, clip=0, loss_scale=64, train_wall=69, gb_free=16.7, wall=38258
2023-07-28 01:13:30 | INFO | train_inner | epoch 032:   1124 / 1474 loss=1.975, trans_loss=4.962, nll_loss=2.144, w2v_ctc_loss=0.603, task_loss=1.632, contrastive_loss=0.091, total=4022.64, n_correct=2628.66, ppl=4.42, accuracy=65.347, wps=11490.6, ups=1.43, wpb=8045.3, bsz=273.1, num_updates=46800, lr=6.5372e-05, gnorm=0.567, clip=0, loss_scale=64, train_wall=69, gb_free=11.1, wall=38328
2023-07-28 01:14:40 | INFO | train_inner | epoch 032:   1224 / 1474 loss=1.982, trans_loss=4.972, nll_loss=2.16, w2v_ctc_loss=0.599, task_loss=1.4, contrastive_loss=0.204, total=4145.44, n_correct=2699.19, ppl=4.47, accuracy=65.112, wps=11926.2, ups=1.44, wpb=8290.9, bsz=308.7, num_updates=46900, lr=6.53023e-05, gnorm=0.565, clip=0, loss_scale=64, train_wall=69, gb_free=17.1, wall=38398
2023-07-28 01:15:48 | INFO | train_inner | epoch 032:   1324 / 1474 loss=1.965, trans_loss=4.958, nll_loss=2.14, w2v_ctc_loss=0.597, task_loss=1.434, contrastive_loss=0.053, total=4083.72, n_correct=2674.44, ppl=4.41, accuracy=65.49, wps=11908.3, ups=1.46, wpb=8167.4, bsz=297.1, num_updates=47000, lr=6.52328e-05, gnorm=0.561, clip=0, loss_scale=64, train_wall=68, gb_free=15.8, wall=38466
2023-07-28 01:16:58 | INFO | train_inner | epoch 032:   1424 / 1474 loss=1.986, trans_loss=4.966, nll_loss=2.151, w2v_ctc_loss=0.606, task_loss=1.412, contrastive_loss=0.29, total=4105.33, n_correct=2683.02, ppl=4.44, accuracy=65.355, wps=11816.4, ups=1.44, wpb=8210.7, bsz=305.7, num_updates=47100, lr=6.51635e-05, gnorm=0.562, clip=0, loss_scale=128, train_wall=69, gb_free=16.4, wall=38536
2023-07-28 01:17:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 01:17:57 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.209 | trans_loss 5.571 | nll_loss 2.846 | w2v_ctc_loss 1.367 | task_loss 4.65 | contrastive_loss 0.25 | total 4003.4 | n_correct 2487.4 | ppl 7.19 | accuracy 62.132 | uer 16.651 | wer 18.612 | raw_wer 18.612 | bleu 20.13 | wps 2059.5 | wpb 4003.4 | bsz 141.8 | num_updates 47150 | best_bleu 20.27
2023-07-28 01:17:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47150 updates
2023-07-28 01:17:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1305.pt
2023-07-28 01:18:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1305.pt
2023-07-28 01:18:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint.best_bleu_20.1305.pt (epoch 32 @ 47150 updates, score 20.13) (writing took 14.647660546004772 seconds)
2023-07-28 01:18:12 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-07-28 01:18:12 | INFO | train | epoch 032 | loss 1.963 | trans_loss 4.949 | nll_loss 2.128 | w2v_ctc_loss 0.594 | task_loss 1.4 | contrastive_loss 0.11 | total 4138.65 | n_correct 2717.6 | ppl 4.37 | accuracy 65.664 | wps 11005.6 | ups 1.33 | wpb 8277.3 | bsz 305.7 | num_updates 47150 | lr 6.5129e-05 | gnorm 0.557 | clip 0 | loss_scale 128 | train_wall 1014 | gb_free 16.4 | wall 38610
2023-07-28 01:18:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-28 01:18:12 | INFO | fairseq.trainer | begin training epoch 33
2023-07-28 01:18:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-28 01:18:57 | INFO | train_inner | epoch 033:     50 / 1474 loss=1.964, trans_loss=4.946, nll_loss=2.125, w2v_ctc_loss=0.591, task_loss=1.312, contrastive_loss=0.162, total=4161.67, n_correct=2733.49, ppl=4.36, accuracy=65.683, wps=7005.9, ups=0.84, wpb=8323.3, bsz=322.6, num_updates=47200, lr=6.50945e-05, gnorm=0.559, clip=0, loss_scale=128, train_wall=69, gb_free=17.8, wall=38655
2023-07-28 01:20:06 | INFO | train_inner | epoch 033:    150 / 1474 loss=1.946, trans_loss=4.924, nll_loss=2.094, w2v_ctc_loss=0.578, task_loss=1.506, contrastive_loss=0.042, total=4067.33, n_correct=2690.04, ppl=4.27, accuracy=66.138, wps=11803.3, ups=1.45, wpb=8134.7, bsz=284.1, num_updates=47300, lr=6.50256e-05, gnorm=0.554, clip=0, loss_scale=128, train_wall=68, gb_free=17.3, wall=38723
2023-07-28 01:20:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-07-28 01:21:16 | INFO | train_inner | epoch 033:    251 / 1474 loss=1.951, trans_loss=4.922, nll_loss=2.094, w2v_ctc_loss=0.586, task_loss=1.23, contrastive_loss=0.166, total=4257.02, n_correct=2818.34, ppl=4.27, accuracy=66.205, wps=12004.9, ups=1.41, wpb=8514, bsz=337.3, num_updates=47400, lr=6.4957e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=70, gb_free=16.4, wall=38794
2023-07-28 01:22:25 | INFO | train_inner | epoch 033:    351 / 1474 loss=1.956, trans_loss=4.937, nll_loss=2.113, w2v_ctc_loss=0.59, task_loss=1.439, contrastive_loss=0.076, total=4111.69, n_correct=2708.29, ppl=4.32, accuracy=65.868, wps=11927.6, ups=1.45, wpb=8223.4, bsz=298.4, num_updates=47500, lr=6.48886e-05, gnorm=0.565, clip=0, loss_scale=64, train_wall=68, gb_free=16.9, wall=38863
2023-07-28 01:23:34 | INFO | train_inner | epoch 033:    451 / 1474 loss=1.943, trans_loss=4.925, nll_loss=2.097, w2v_ctc_loss=0.584, task_loss=1.313, contrastive_loss=0.054, total=4147.28, n_correct=2745.03, ppl=4.28, accuracy=66.189, wps=12110.5, ups=1.46, wpb=8294.6, bsz=313.5, num_updates=47600, lr=6.48204e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=68, gb_free=16.6, wall=38932
2023-07-28 01:24:44 | INFO | train_inner | epoch 033:    551 / 1474 loss=1.963, trans_loss=4.945, nll_loss=2.122, w2v_ctc_loss=0.597, task_loss=1.464, contrastive_loss=0.076, total=4127.68, n_correct=2711.8, ppl=4.35, accuracy=65.698, wps=11769.5, ups=1.43, wpb=8255.4, bsz=292.6, num_updates=47700, lr=6.47524e-05, gnorm=0.564, clip=0, loss_scale=64, train_wall=69, gb_free=15.6, wall=39002
2023-07-28 01:25:54 | INFO | train_inner | epoch 033:    651 / 1474 loss=1.963, trans_loss=4.951, nll_loss=2.131, w2v_ctc_loss=0.591, task_loss=1.43, contrastive_loss=0.109, total=4164.1, n_correct=2731.14, ppl=4.38, accuracy=65.588, wps=11896.6, ups=1.43, wpb=8328.2, bsz=302.4, num_updates=47800, lr=6.46846e-05, gnorm=0.559, clip=0, loss_scale=64, train_wall=69, gb_free=15.3, wall=39072
2023-07-28 01:27:03 | INFO | train_inner | epoch 033:    751 / 1474 loss=1.967, trans_loss=4.952, nll_loss=2.131, w2v_ctc_loss=0.607, task_loss=1.531, contrastive_loss=0.053, total=4064.29, n_correct=2663.45, ppl=4.38, accuracy=65.533, wps=11759.1, ups=1.45, wpb=8128.6, bsz=285.8, num_updates=47900, lr=6.46171e-05, gnorm=0.573, clip=0, loss_scale=64, train_wall=68, gb_free=16.6, wall=39141
2023-07-28 01:28:13 | INFO | train_inner | epoch 033:    851 / 1474 loss=1.947, trans_loss=4.935, nll_loss=2.111, w2v_ctc_loss=0.575, task_loss=1.32, contrastive_loss=0.123, total=4141.12, n_correct=2735.57, ppl=4.32, accuracy=66.059, wps=11914.2, ups=1.44, wpb=8282.2, bsz=318.5, num_updates=48000, lr=6.45497e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=69, gb_free=16.6, wall=39211
2023-07-28 01:28:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 01:28:36 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.209 | trans_loss 5.579 | nll_loss 2.852 | w2v_ctc_loss 1.348 | task_loss 4.637 | contrastive_loss 0.251 | total 4003.4 | n_correct 2483.8 | ppl 7.22 | accuracy 62.042 | uer 16.699 | wer 18.374 | raw_wer 18.374 | bleu 20.17 | wps 2190.9 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 20.27
2023-07-28 01:28:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-07-28 01:28:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_33_48000.pt
2023-07-28 01:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_33_48000.pt
2023-07-28 01:28:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 20.17) (writing took 15.050397884100676 seconds)
2023-07-28 01:30:03 | INFO | train_inner | epoch 033:    951 / 1474 loss=1.961, trans_loss=4.947, nll_loss=2.126, w2v_ctc_loss=0.6, task_loss=1.399, contrastive_loss=0.068, total=4147.76, n_correct=2727.13, ppl=4.37, accuracy=65.749, wps=7537, ups=0.91, wpb=8295.5, bsz=308.2, num_updates=48100, lr=6.44826e-05, gnorm=0.563, clip=0, loss_scale=64, train_wall=69, gb_free=17.6, wall=39321
2023-07-28 01:30:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-07-28 01:31:13 | INFO | train_inner | epoch 033:   1052 / 1474 loss=1.958, trans_loss=4.946, nll_loss=2.124, w2v_ctc_loss=0.594, task_loss=1.441, contrastive_loss=0.059, total=4116.77, n_correct=2704.35, ppl=4.36, accuracy=65.691, wps=11725.5, ups=1.42, wpb=8233.5, bsz=299.3, num_updates=48200, lr=6.44157e-05, gnorm=0.562, clip=0, loss_scale=32, train_wall=69, gb_free=11.6, wall=39391
2023-07-28 01:32:23 | INFO | train_inner | epoch 033:   1152 / 1474 loss=1.965, trans_loss=4.954, nll_loss=2.135, w2v_ctc_loss=0.587, task_loss=1.404, contrastive_loss=0.159, total=4182.67, n_correct=2740.1, ppl=4.39, accuracy=65.511, wps=11967.5, ups=1.43, wpb=8365.3, bsz=309.5, num_updates=48300, lr=6.43489e-05, gnorm=0.561, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=39461
2023-07-28 01:33:33 | INFO | train_inner | epoch 033:   1252 / 1474 loss=1.962, trans_loss=4.95, nll_loss=2.129, w2v_ctc_loss=0.598, task_loss=1.475, contrastive_loss=0.059, total=4110.02, n_correct=2698.6, ppl=4.37, accuracy=65.659, wps=11767.4, ups=1.43, wpb=8220, bsz=294.4, num_updates=48400, lr=6.42824e-05, gnorm=0.567, clip=0, loss_scale=32, train_wall=69, gb_free=16.8, wall=39531
2023-07-28 01:34:42 | INFO | train_inner | epoch 033:   1352 / 1474 loss=1.958, trans_loss=4.95, nll_loss=2.131, w2v_ctc_loss=0.593, task_loss=1.374, contrastive_loss=0.081, total=4128.82, n_correct=2714.2, ppl=4.38, accuracy=65.738, wps=11851.3, ups=1.44, wpb=8257.6, bsz=312.4, num_updates=48500, lr=6.42161e-05, gnorm=0.564, clip=0, loss_scale=32, train_wall=69, gb_free=16.1, wall=39600
2023-07-28 01:35:52 | INFO | train_inner | epoch 033:   1452 / 1474 loss=1.967, trans_loss=4.949, nll_loss=2.13, w2v_ctc_loss=0.589, task_loss=1.395, contrastive_loss=0.224, total=4123.47, n_correct=2706.66, ppl=4.38, accuracy=65.64, wps=11939.8, ups=1.45, wpb=8246.9, bsz=308.9, num_updates=48600, lr=6.415e-05, gnorm=0.563, clip=0, loss_scale=32, train_wall=68, gb_free=16.5, wall=39669
2023-07-28 01:36:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 01:36:30 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.2 | trans_loss 5.576 | nll_loss 2.847 | w2v_ctc_loss 1.329 | task_loss 4.632 | contrastive_loss 0.248 | total 4003.4 | n_correct 2484.2 | ppl 7.19 | accuracy 62.052 | uer 16.664 | wer 18.433 | raw_wer 18.433 | bleu 19.93 | wps 2219.2 | wpb 4003.4 | bsz 141.8 | num_updates 48622 | best_bleu 20.27
2023-07-28 01:36:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48622 updates
2023-07-28 01:36:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-28 01:36:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt
2023-07-28 01:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_last.pt (epoch 33 @ 48622 updates, score 19.93) (writing took 11.166496319696307 seconds)
2023-07-28 01:36:41 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-07-28 01:36:41 | INFO | train | epoch 033 | loss 1.958 | trans_loss 4.942 | nll_loss 2.119 | w2v_ctc_loss 0.591 | task_loss 1.405 | contrastive_loss 0.096 | total 4135.45 | n_correct 2721.38 | ppl 4.34 | accuracy 65.806 | wps 10974.3 | ups 1.33 | wpb 8270.9 | bsz 304.5 | num_updates 48622 | lr 6.41355e-05 | gnorm 0.561 | clip 0 | loss_scale 32 | train_wall 1014 | gb_free 17.8 | wall 39719
2023-07-28 01:36:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-07-28 01:36:41 | INFO | fairseq.trainer | begin training epoch 34
2023-07-28 01:36:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-07-28 01:37:45 | INFO | train_inner | epoch 034:     78 / 1474 loss=1.947, trans_loss=4.924, nll_loss=2.096, w2v_ctc_loss=0.587, task_loss=1.38, contrastive_loss=0.061, total=4128.94, n_correct=2732.26, ppl=4.27, accuracy=66.173, wps=7284.4, ups=0.88, wpb=8257.9, bsz=302.1, num_updates=48700, lr=6.40841e-05, gnorm=0.561, clip=0, loss_scale=32, train_wall=69, gb_free=15.1, wall=39783
2023-07-28 01:38:54 | INFO | train_inner | epoch 034:    178 / 1474 loss=1.945, trans_loss=4.918, nll_loss=2.087, w2v_ctc_loss=0.583, task_loss=1.458, contrastive_loss=0.061, total=4071.22, n_correct=2699.56, ppl=4.25, accuracy=66.308, wps=11772, ups=1.45, wpb=8142.4, bsz=295.1, num_updates=48800, lr=6.40184e-05, gnorm=0.559, clip=0, loss_scale=32, train_wall=68, gb_free=15.5, wall=39852
2023-07-28 01:40:04 | INFO | train_inner | epoch 034:    278 / 1474 loss=1.966, trans_loss=4.936, nll_loss=2.113, w2v_ctc_loss=0.584, task_loss=1.319, contrastive_loss=0.277, total=4237.89, n_correct=2787.36, ppl=4.33, accuracy=65.772, wps=12202.9, ups=1.44, wpb=8475.8, bsz=326.9, num_updates=48900, lr=6.39529e-05, gnorm=0.555, clip=0, loss_scale=32, train_wall=69, gb_free=10.3, wall=39921
2023-07-28 01:41:13 | INFO | train_inner | epoch 034:    378 / 1474 loss=1.947, trans_loss=4.92, nll_loss=2.091, w2v_ctc_loss=0.576, task_loss=1.32, contrastive_loss=0.16, total=4167, n_correct=2764.17, ppl=4.26, accuracy=66.335, wps=12010.9, ups=1.44, wpb=8334, bsz=319, num_updates=49000, lr=6.38877e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=69, gb_free=17.3, wall=39991
2023-07-28 01:42:22 | INFO | train_inner | epoch 034:    478 / 1474 loss=1.958, trans_loss=4.938, nll_loss=2.113, w2v_ctc_loss=0.597, task_loss=1.531, contrastive_loss=0.054, total=4071.65, n_correct=2680.09, ppl=4.33, accuracy=65.823, wps=11740.5, ups=1.44, wpb=8143.3, bsz=284.8, num_updates=49100, lr=6.38226e-05, gnorm=0.567, clip=0, loss_scale=32, train_wall=69, gb_free=11.4, wall=40060
2023-07-28 01:43:31 | INFO | train_inner | epoch 034:    578 / 1474 loss=1.947, trans_loss=4.925, nll_loss=2.097, w2v_ctc_loss=0.586, task_loss=1.421, contrastive_loss=0.057, total=4110.13, n_correct=2721.35, ppl=4.28, accuracy=66.211, wps=11920.8, ups=1.45, wpb=8220.3, bsz=299, num_updates=49200, lr=6.37577e-05, gnorm=0.561, clip=0, loss_scale=32, train_wall=68, gb_free=16.6, wall=40129
2023-07-28 01:44:41 | INFO | train_inner | epoch 034:    678 / 1474 loss=1.947, trans_loss=4.93, nll_loss=2.104, w2v_ctc_loss=0.585, task_loss=1.416, contrastive_loss=0.051, total=4128.65, n_correct=2731.36, ppl=4.3, accuracy=66.156, wps=11918.8, ups=1.44, wpb=8257.3, bsz=300.7, num_updates=49300, lr=6.3693e-05, gnorm=0.558, clip=0, loss_scale=32, train_wall=68, gb_free=17.8, wall=40198
2023-07-28 01:45:50 | INFO | train_inner | epoch 034:    778 / 1474 loss=1.96, trans_loss=4.952, nll_loss=2.132, w2v_ctc_loss=0.579, task_loss=1.469, contrastive_loss=0.123, total=4075.69, n_correct=2676.68, ppl=4.38, accuracy=65.674, wps=11652.8, ups=1.43, wpb=8151.4, bsz=294.5, num_updates=49400, lr=6.36285e-05, gnorm=0.564, clip=0, loss_scale=32, train_wall=69, gb_free=17.2, wall=40268
2023-07-28 01:47:00 | INFO | train_inner | epoch 034:    878 / 1474 loss=1.956, trans_loss=4.941, nll_loss=2.118, w2v_ctc_loss=0.587, task_loss=1.481, contrastive_loss=0.083, total=4104.97, n_correct=2704.49, ppl=4.34, accuracy=65.883, wps=11824.9, ups=1.44, wpb=8209.9, bsz=296.3, num_updates=49500, lr=6.35642e-05, gnorm=0.568, clip=0, loss_scale=32, train_wall=69, gb_free=17.8, wall=40338
2023-07-28 01:48:09 | INFO | train_inner | epoch 034:    978 / 1474 loss=1.958, trans_loss=4.946, nll_loss=2.125, w2v_ctc_loss=0.593, task_loss=1.373, contrastive_loss=0.078, total=4168.94, n_correct=2740.15, ppl=4.36, accuracy=65.728, wps=12042.3, ups=1.44, wpb=8337.9, bsz=312.8, num_updates=49600, lr=6.35001e-05, gnorm=0.563, clip=0, loss_scale=32, train_wall=69, gb_free=14.9, wall=40407
2023-07-28 01:49:18 | INFO | train_inner | epoch 034:   1078 / 1474 loss=1.957, trans_loss=4.945, nll_loss=2.124, w2v_ctc_loss=0.595, task_loss=1.352, contrastive_loss=0.058, total=4155.12, n_correct=2734.55, ppl=4.36, accuracy=65.812, wps=12009.6, ups=1.45, wpb=8310.2, bsz=309.1, num_updates=49700, lr=6.34361e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=68, gb_free=16.7, wall=40476
2023-07-28 01:50:27 | INFO | train_inner | epoch 034:   1178 / 1474 loss=1.956, trans_loss=4.943, nll_loss=2.121, w2v_ctc_loss=0.588, task_loss=1.445, contrastive_loss=0.07, total=4096.48, n_correct=2695.07, ppl=4.35, accuracy=65.79, wps=11860.2, ups=1.45, wpb=8193, bsz=297.2, num_updates=49800, lr=6.33724e-05, gnorm=0.561, clip=0, loss_scale=32, train_wall=68, gb_free=16.6, wall=40545
2023-07-28 01:51:36 | INFO | train_inner | epoch 034:   1278 / 1474 loss=1.954, trans_loss=4.943, nll_loss=2.121, w2v_ctc_loss=0.588, task_loss=1.42, contrastive_loss=0.052, total=4149.03, n_correct=2731.43, ppl=4.35, accuracy=65.833, wps=12046.2, ups=1.45, wpb=8298.1, bsz=299.7, num_updates=49900, lr=6.33089e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=68, gb_free=16.4, wall=40614
2023-07-28 01:52:46 | INFO | train_inner | epoch 034:   1378 / 1474 loss=1.966, trans_loss=4.951, nll_loss=2.133, w2v_ctc_loss=0.602, task_loss=1.34, contrastive_loss=0.124, total=4200.34, n_correct=2752.91, ppl=4.38, accuracy=65.54, wps=11983.2, ups=1.43, wpb=8400.7, bsz=321.9, num_updates=50000, lr=6.32456e-05, gnorm=0.566, clip=0, loss_scale=32, train_wall=69, gb_free=15, wall=40684
2023-07-28 01:52:46 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-07-28 01:52:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-07-28 01:53:10 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 4.219 | trans_loss 5.571 | nll_loss 2.848 | w2v_ctc_loss 1.395 | task_loss 4.665 | contrastive_loss 0.254 | total 4003.4 | n_correct 2491.4 | ppl 7.2 | accuracy 62.232 | uer 16.606 | wer 18.586 | raw_wer 18.586 | bleu 20.46 | wps 2135.2 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 20.46
2023-07-28 01:53:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-07-28 01:53:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_34_50000.pt
2023-07-28 01:53:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_34_50000.pt
2023-07-28 01:53:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_AT_sentence_scale1_mixup_sen0.3_tok0.5_mt_0727/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 20.46) (writing took 32.97054458037019 seconds)
2023-07-28 01:53:44 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-07-28 01:53:44 | INFO | train | epoch 034 | loss 1.955 | trans_loss 4.937 | nll_loss 2.112 | w2v_ctc_loss 0.588 | task_loss 1.408 | contrastive_loss 0.096 | total 4133.33 | n_correct 2725.29 | ppl 4.32 | accuracy 65.934 | wps 11137.1 | ups 1.35 | wpb 8266.7 | bsz 304.3 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.56 | clip 0 | loss_scale 32 | train_wall 946 | gb_free 15 | wall 40742
2023-07-28 01:53:44 | INFO | fairseq_cli.train | done training in 40661.6 seconds
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
