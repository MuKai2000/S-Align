2023-08-17 09:16:10 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:17846
2023-08-17 09:16:10 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:17846
2023-08-17 09:16:10 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:17846
2023-08-17 09:16:11 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:17846
2023-08-17 09:16:11 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:17846
2023-08-17 09:16:11 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:17846
2023-08-17 09:16:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-17 09:16:11 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:17846
2023-08-17 09:16:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-17 09:16:11 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:17846
2023-08-17 09:16:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-17 09:16:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-17 09:16:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-17 09:16:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:16:12 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-17 09:16:12 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:16:12 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:16:12 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-17 09:16:12 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:16:12 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:16:12 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:16:12 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-17 09:16:12 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-17 09:16:16 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17846', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-17 09:16:16 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-17 09:16:16 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-17 09:16:16 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-17 09:16:16 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-17 09:16:16 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 09:16:20 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-17 09:16:20 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-17 09:16:20 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-17 09:16:23 | INFO | root | load pretrained hubert
2023-08-17 09:16:25 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 09:16:26 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 09:16:29 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 09:16:29 | INFO | root | share the sematic adapter and textual encoder
2023-08-17 09:16:29 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-17 09:16:29 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-17 09:16:29 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-17 09:16:29 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-17 09:16:29 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-17 09:16:29 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-17 09:16:29 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 09:16:29 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:16:29 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:16:29 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:16:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-17 09:16:34 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-17 09:16:34 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-17 09:16:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 09:16:34 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:16:34 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:16:34 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:16:34 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:16:34 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:16:34 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:16:34 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:16:34 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:16:34 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 09:16:34 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-17 09:16:34 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-17 09:16:34 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 09:16:34 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 09:16:34 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-17 09:16:34 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 09:16:34 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:16:34 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:16:36 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:16:38 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:17:26 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-17 09:17:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 09:17:26 | INFO | fairseq.trainer | begin training epoch 1
2023-08-17 09:17:26 | INFO | fairseq_cli.train | Start iterating over samples
True tensor([[ 800,   91,    0,  103,  886,  424, 3848,   11,    6,  853,  200,  237,
           26,  593, 3401,  120, 3213,  187, 2044,    6, 1570,    6,  199,    7,
          964,    0,  568,   17,  641,   21,   11,    6,  735,  226,  593, 3401,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24, 8396, 1469,  106,    0,  108, 2914,   37,   93,    0,  131, 2532,
          336, 3494,    0,    0,   67, 1223,   24,   77,   66,   33, 1738,    0,
         2914,   37,   93,   17,   24,   63,  746,    0,   12,    0, 5472, 1117,
          108, 3183,    6,    8,    0, 1459,  994,   26,  126,   84,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2937,  982,  218,  943,    6,    9,  108, 6249,  451,   66,  774,   62,
         3928,    0,    8, 1800,  282,   13, 2333,   10,  175,  850, 1408, 1072,
            6,    0,  109, 3119,  294, 2518,   12,  215, 3928,  254,  956,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  125,   29,  294,   12,    7, 2787,    9, 1370, 2924,   11,   18,
          116,    9,  670,    0,   53,   11,   57,    9, 1037,    8, 1237,    0,
           70,   25,  113,  296,    0, 6957,    0,   26,  143,   69,    7,  230,
          874,  926,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  103,   25, 1816,   66,   13, 5590,   20,  521,   20,  874,
           93,    0,  115,   11,    6,   13,  324,  183,   10,  388,   21,    9,
          155,  874,    8,  598,   21,    0, 2603,   69,    7,  467,    0,   17,
           11,    6,  916,    0,   25,   11,  158,  446,  126,  347,   21,   11,
            6,  434,   13,  862,  365,  232, 1218,   20, 5590,   20,    0,    2,
            1,    1],
        [ 248,  215,  581,   85, 1366,    0,   19,  154,   46,   19,   11,  121,
          367,   10,   33, 7034,   46,   19,  154,   19,  659,   66,  226, 5187,
          106,   13, 3643,  384, 7710,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,    0,    0,    0,    0,    9,    0,   89, 6525,  230,
          115,    0,   38,  187,  144,   33, 1850, 2536,   48,    0, 1850,   62,
           69,    0,    7, 1850,    6,   20, 2286,  307,    0,    0, 4813,  248,
          215,  581,   69, 2154, 1374,   18,  881,   11,    6,  383,    0,   21,
           34,    7,  133, 2354,    0,    7,  473, 2829,    9,    7,  535, 8311,
            0,    2],
        [  29,  115,    0,  103,   24,  274,  270,   85,    7, 3253,    0,   85,
          108, 8196,    6,   17,  162, 5690,   54, 1004,    7, 3253,    0,  117,
         8196,    6,   63,  100,    7,  384,  174, 2003,   12,  108, 2626,  567,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:2') tensor([38, 48, 39, 40, 60, 31, 62, 38], device='cuda:2') tensor([1.0000, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 1.0000],
       device='cuda:2', dtype=torch.float16)
True tensor([[  67,  108, 8830,   48,  589,    0, 7557,    6,    0,  166,   63,    7,
         7557,    6,   17,  601,  170,   10, 6108,    8, 2544,    8,   51,    0,
            0,   13,  562,   18,    0,    8, 9063,   62,    8,   29, 4404,    8,
           29,   69,    0,   53,    0,   63,  261,  143, 7678,   62,  131, 1912,
          688,    0,    2],
        [  24,  116,  144, 6985, 2421,    6, 4502,   80,  854,   39, 3350,  581,
           77, 1004,    7, 6869, 9271,    0, 3381,   70,   11,    6,  956,    0,
            8,   24,   11,  121, 2138,   13,  325,   12,  183,    9,    7, 1726,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 185, 2193, 6248,   22,    6,    9, 1777,  150,  156,   79,   93,  675,
           45,    9, 5222, 1290, 1373,    6,    6,  693,    0,   67,  294,   73,
           51, 5853,  131,    7, 2932, 3288,    0,    8,  203, 1203, 1015,  922,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 106, 1244,   35, 1966,  673,   54, 2409,   10, 2465, 3602,    6,   10,
           33, 2178, 2445,  266, 1192,   69,   89,  906,    0, 3927,   63, 4015,
         5869,   12, 2434,  108,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  154,   21,   26, 2497, 1226,   55, 1221,   10, 1772,    9,    7,
          245,  561,  929,   13, 2226, 1757,   55,   70,  169, 1085,   10,    7,
          461,  266,  763,  480,    6, 1042,    7, 3677,  188, 3699,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7, 4100, 1649, 1085,  143, 1149,    0,   67,  914,   69,    7, 1296,
           12,   13, 4400,   15,   22,  996,    0,  333,  555, 7691,  109,  333,
          555, 3619,  215,    0,   67,   21,   11,    6,  499,  250,   10,   51,
         4837,   12,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7745,  659,   51,    0,   67,   21,  689,   11,   18, 4432,
           10,  452, 1539,  128,    6,    0,   17,   24,   66, 5020, 6083, 5475,
            6,   46,    7, 1320,   59,  816,  300, 1079, 1643, 1794,  230,   85,
            7, 1219,  859,   35, 2514, 4209,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   9,  409,    0,    7,   29,   70,   26,    0,    9,    7,  572,    0,
         2317,    9, 7418, 3782,    0,    7,  214,   17,  229,  155,  572,  283,
            0,    7,  214,   17,  229,  155, 3236,  283,    9,  155,  572,    0,
           63, 8220,  111,  415,   59, 8186,   71, 2317,    9, 2011, 3043,    0,
            2,    1,    1]], device='cuda:7') tensor([51, 38, 38, 31, 36, 40, 44, 49], device='cuda:7') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
True tensor([[  67,  490,   24,   87,   17,    0,   19,   11,   48,  100,   25,   10,
          154,   17,   85,  419, 1800,  183,  120,   25,   11,   57, 2202,   84,
            0,    9,  155, 1992, 5896, 1247,    0,    7, 2583, 8748, 9169, 3413,
           54,   10,  155, 9879,    0,    7, 9879,   26, 3413,   54,   10,  155,
         2583, 8748, 9169,    8,   33,   26,  138,  155, 4941,   26, 3803,    8,
         8965,   48,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [5609,   10,    7, 1793,  183,  368, 5673,    0, 1276,    6,  499,   87,
         4986,   79,  261, 1269, 2536,   20,   79, 1970,    6,    0,  166,   26,
          509,  254,   21,   34,    9,   95,  424, 3400, 2197,   11,    6,  383,
            0,   67,   19,  499,  154,   17,  250,  225, 2396,   26, 4803, 5040,
            0,   38,  187,   66,   86,  226, 2373,    9,    7, 9091, 1313, 9094,
            3,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  56,  525,   11,    6, 4197,  246,   10,  267, 1848,    0,   38,  160,
            0, 1491,    0,    0,   33, 4925, 2823,    6,   55,  815, 6091, 1086,
            3,  267,    0, 1848,    0,  148,   63, 1102, 4907,    0, 1260,   69,
            0, 4340,  982,    0,    0,   38,  511,   17,   11,    6,   86,   77,
           46,  415, 1040,    0,   26, 1028,  126,    0,   71,   13,  264,    0,
         1375,    0, 2199, 1132,  820,    3,  225,    0,    0,  246,    0,    0,
           38, 2207, 4925,  164,    0, 2823,   55, 1581,    3,    3,  225,    0,
         8640,   48,    8,  246,    0,    0,   38,  187,  192,   11,   18,  135,
            0,  103,   17,   11,    6,    0, 1581,    3, 1995,  241,  221,  109,
            0, 1581,    3, 1793,    0, 1086,    0,    0,   67,    0, 3296,    0,
           21,   11,    6, 1581,    3,    3,   68,  194,   65,    2],
        [   8,    9,    7,  370,  207,  294,   12,  170,   63,  115, 5954, 4273,
            6,   17, 3412,  108, 1259, 1882,    0,  108,  203,    6,  777, 1991,
            0,  108, 3857,    0,   69,    7, 1080,    6,   17,   33,  659,  601,
          170, 3560, 4390,    0,   24,   73,  884, 1179, 5625,   54,    8,   39,
          128,   93,  195,   54,    7, 1289,   24, 2056,    0,   24, 8099,    0,
           24, 3836,    0,   24, 2153,    0,   73,  450,  170, 4324,   12,  183,
         1179,  250,  659,  205, 1226,   71,  108, 3227,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   71,   33,  822, 5108,    9,   13, 3708, 4774,    0,   24,   66,
         6960,   48,    9,  824, 2723,   45,  726, 1090,   17,   24,   73, 3814,
            7, 1729,   35, 1727,  430,  690, 3275,  110,  249,  132,   10, 1421,
          439,   12,   70,   26, 5914, 1417,   46,  248, 1086,  690, 3275,  110,
          249,    0,   24,   87,   21,   85, 7738,    0,  820,  828,    0,    8,
           24,   73,  499,  875,   17, 3151,  346,    0,  630,   12, 1823,    0,
          103,   25,   73,  875,    9,  143, 3708,    6,    0,   24,   73,  204,
          229,    7,  126, 2446,  276, 3028,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 875,   54,   94,  970,   10,  970,   71,  108, 3648,   26,   13,  207,
           12,  875,   54,  134,  970,   10,  970,   71,   94, 1004,  183,    0,
         1004,  672,    0, 4355,  931,  659,   66,  226,  133,  341,   10,  108,
          447,    0,   67,  148,    0,  100,  170,    0,  144, 1080,    6,    8,
         4989,    0, 2084,  335,   18, 1991,    6,    8, 3128, 2261,    9,  159,
          931,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([ 64,  65, 130,  82,  92,  63], device='cuda:3') tensor([1.0000, 1.0000, 0.8271, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
True tensor([[3706,   55,    7,    0, 3210,   12, 2058,   35,  249,   59,  240, 1144,
            0,   53,   11,   57,  133,  946, 2787,    9, 2047,   12,    0,    0,
           73,    0,   24,    0,  694,  214,   10, 2231, 2736,  941,    0, 1645,
         2078, 2399,   70, 2892, 1075,    0,   73,   87,    8, 2293,  359, 4531,
          946, 6710,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  13,  143, 4089,  207,   10,  154,   80,   89, 6674,    6,  506,   51,
            9, 2047,   12, 1387,  687,    8, 4480,    0,  206,   85,  419, 1800,
          613,    9,  183,    0,   71,  419, 1127,  723,    0,   19,  217, 1102,
         1387,    8, 8219,  106,   17, 2216,    0,   77,   79,   13, 2443,   12,
           70,   19,  296,   10,   87,  230,  115,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   7, 7374,   17, 5290, 1247,    0,   79, 4546, 1247,    0,  220,  508,
         3667,   10,   13,  733, 6608,   12,  341, 1603, 4345,   46, 1259, 1247,
            0,  618,   59, 1247,    0, 3109,  140,   57,  411,  266,   26, 2856,
         1247,   46, 4574,   48,    7, 2303,   12,    7, 1827,    8,    7, 4715,
           12,    7, 1136,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [4157,    0, 7957, 6266, 1119, 1017,   21, 1962,    0,   85,  109, 3019,
          964, 4157,    0,    8,  180, 4587,   13, 1206,  567,   10, 1005,   21,
           84,    0, 4958,    7,   59,  627, 1202,  290, 1408,    0, 6712, 5933,
         6962,   26, 2770,   10, 5989,   85, 1362,   20,  586,  922, 4157,   71,
         3955,   45,  502, 8366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   39,    9, 1324,    9, 1954, 6935,    0,   19, 1346,
           80,   13, 1361,   12, 8188, 4039,  148,  144, 1828,  244, 1339, 1027,
          587,  195, 3485,    0,    8,   13, 8188, 1793,  148,  692,   10,  508,
         4460,   69,   17, 3485,    0,    8,  211,  218, 3280,  692,   10,  205,
            8,  601,  267,  508, 4460,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 103,    0,   84,   11,    6,    0,    0,    0,    0,   91,  759, 4039,
           46,    8,   19,  172,  135,   33,   46,  148,   63,    0, 3163,   10,
          229,    0, 1583,  583,    6,    0,  148,   63,    0,    0, 3163,   10,
           51,   69, 3836,    0,   19,  217, 2497, 1123,   17,   24,   73,    0,
          204,  468,    7,  538,   12, 1261,    0, 2318,    0,   55,    7, 4959,
           12, 1146,    0,    2,    1,    1,    1],
        [   8,   29,   19,  246,    0,   38,    6, 1049,    3,    0,   29,   19,
            0,  175,  518,    0,    8,  225, 1559,   69,    0,    8,   71,   13,
          277,  523,   12,    7,  170, 1522,    0,    0,    0,   13,  176,    0,
            0,  180,  225, 1518,  336,    0,    8,  225, 1143,    0,   80,  423,
         2360,    0,    8,    0,    0,  225, 1518,  270,  336,    0,    8,  225,
           11,    6,   77, 6087,    6,    0,    2],
        [  85,    7,  183,   19,   34, 1931,   54,    9,  117,  341, 1427,    0,
           19,  474,   19,   34, 3258,   54, 4029, 1352,    0,   67,   69, 1858,
         2007, 1498, 1261, 9096,  237, 1181,    0,    8,   19, 4600,   19,   11,
           48,  204,  226, 3258,   54,   13, 1127,  546,   55,  143,  254,  423,
          215,    0,    8,    7, 2838,   69,  264, 1736,   34,   21,    6, 2402,
          119, 8197,  271,    0,    2,    1,    1]], device='cuda:5') tensor([52, 57, 53, 54, 55, 64, 67, 65], device='cuda:5') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8584, 1.0000],
       device='cuda:5', dtype=torch.float16)
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 863, in train_step
    raise e
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 620, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 488, in _per_task_train_loss
    loss_at, loss_st, loss_mt, sample_size, logging_output = criterion(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge.py", line 357, in forward
    prob_mt = model.task_net(encoder_output_mt, encoder_padding_mask_mt if self.at_nopad else None)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq/models/speech_to_text/s2t_joint.py", line 1067, in forward
    mask_3d = padding_mask.unsqueeze(-1).expand(x.shape)        # [bs, len, dim=512]
RuntimeError: The expanded size of the tensor (62) must match the existing size (60) at non-singleton dimension 1.  Target sizes: [8, 62, 512].  Tensor sizes: [8, 60, 1]

/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 72 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-08-17 09:23:46 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10745
2023-08-17 09:23:46 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10745
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10745
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10745
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10745
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10745
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10745
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10745
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:23:47 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-17 09:23:47 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-17 09:23:52 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10745', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-17 09:23:52 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-17 09:23:52 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-17 09:23:52 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-17 09:23:52 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-17 09:23:52 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 09:23:56 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-17 09:23:56 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-17 09:23:56 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-17 09:23:58 | INFO | root | load pretrained hubert
2023-08-17 09:24:01 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 09:24:02 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 09:24:02 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 09:24:03 | INFO | root | share the sematic adapter and textual encoder
2023-08-17 09:24:03 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-17 09:24:03 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-17 09:24:03 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-17 09:24:03 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-17 09:24:03 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-17 09:24:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-17 09:24:03 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 09:24:03 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:24:03 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:24:03 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:24:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-17 09:24:06 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-17 09:24:06 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-17 09:24:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 09:24:06 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:24:06 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:24:06 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:24:06 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:24:06 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:24:06 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:24:06 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:24:06 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:24:06 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 09:24:06 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-17 09:24:06 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-17 09:24:06 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 09:24:06 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 09:24:06 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-17 09:24:06 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 09:24:06 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:24:06 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:24:08 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:24:09 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:24:57 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-17 09:24:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 09:24:57 | INFO | fairseq.trainer | begin training epoch 1
2023-08-17 09:24:57 | INFO | fairseq_cli.train | Start iterating over samples
False tensor([[   7,  422,  874,    0,    0,    9, 6788,  369,   12, 3648,    0,   26,
            7,  874,    9, 2761,   12,   13,  572,    0,    7,  572,   26,    0,
            0,    9, 2761,    0,   12,   13,  874,    0,    8,  722,   26,    7,
         7465,  131,  166,  251,    0,  248,   63, 3249,   62,    9,    7,  772,
          207,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [1179,   25,  135,   21,  109,   86,    0, 1335, 2348,   12, 1613,   71,
          747, 2693,   11,    6,    8, 1335, 2348,   12,   13,    0,  265,    0,
         6466,    6,  148,  205,   10,    7,  281,  909,  119,  236, 1563, 8562,
            9,  108,  753, 1772,    9,  909,   35, 3080,  109,  909,   35, 9376,
           54,    8, 4429,    0,    8,   53,  467,  132, 2762,  159, 2767,    6,
            0,    2],
        [  67,   21,  465,   11,   18,  468,    7, 2932, 1880,  567,    0,    8,
          113,    7, 2932, 3612, 1381,  832,   18,  233, 1181,   33, 3612, 1181,
         3860,   59, 1710,   10, 5111,   15,   21,    6,  768,   10, 5574,    7,
         1749, 1381,    8,    7,  341,  409, 1955,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   56, 1285,    0,  347,  192,   11,   18,   25,  450,  117, 6010,
           70,   21,   11,    6,  100,  378, 7000,   71,   33, 4716,  434,  427,
          821,  407,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0, 2753,  427, 1105,  810, 3565,   17,   21,  659,   51,
         1953,    6,   12, 1655,   12,  215,  801,    0,  166,  818,   17,   33,
         1167,  659,   66,  245,   13, 2003,   15,  106,    7, 1247,   12,   13,
          258, 1591,   17, 2963, 1585, 1763,    7, 1644,  511,  762, 1054,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,   13,  563,   93, 2484, 9473,   13, 4866,   10, 2266,
          389,  552, 1252,   11,    6,   38, 5001,  411,  293,    3,   24,   73,
          205,   10,   91,   12,    7,  822,  608,   56, 9694, 1075,    0, 3109,
         3902,    0,    8,  150,  113,   13, 1523, 3609,   12, 2718, 7719,    0,
          106,  747,    9, 1665,   10, 1962,    9, 1912,    0,    2,    1,    1,
            1,    1],
        [  33, 2780,   34, 6291,  111, 2770,   55,    7, 2932, 1232,    0,   67,
          115,   21,   11,    6, 2823,   54,  100, 3139,  140, 2704,    6,    9,
            7,  832,    0,    6,    0,    8, 2627,    0,  206, 2406,    6,   63,
         4213,   48,   10, 3067, 3316, 1006,   85, 3646, 1729,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 108, 2476,   26,   10, 8711,   17,   53, 1516,   71,   13,  509,  841,
           12,  138,   10,  229,  214,  254,  120,   53, 6565,    0,    8,    7,
         2027, 6936,  475, 2041,   17,   25,   73, 1531,  214,  126,  131, 1974,
          416,   54,  336,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([51, 62, 45, 28, 50, 58, 47, 41], device='cuda:0') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(90.0032, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:25:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
False tensor([[  55,  663,    0,    9,  264,  109,  232,  221,    6,   24,   66,  682,
         3196,   22, 2932,  896,    0,  206,   53, 3860,   75,  241,  221,   77,
          236,  866,    8, 6387,    8,   29,  234,  452, 6029, 7401,    0,    2,
            1,    1,    1],
        [ 115,   24,   11,  121,  492, 1110,   13, 7123,  100,   33,   12,   13,
         1470, 3189,    0,   67, 1470, 3189,    6,   73,   51, 1346,    0,  276,
          103,   53,   11,   57,   86, 1110,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [ 168,   26,  138,  488,   12,   13,  775,  411,   37,   34,  859,    0,
           21,    0,   34, 1523,    0,    0,   10,    0,    0,  508,   25,   13,
          841,   12,    7,    0, 1823,    0,    0,    0,   84,   25,  205,    0,
            2,    1,    1],
        [ 115,    7, 1885, 5055,    0,   12,  538,    0,   26,  206,   13,  325,
           12,    7, 2394, 4793,    8, 2791,  513,  199,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,   21,  818,   17,   51,   20, 7931,  373,   63,  133,  324,   85,
          203, 4030,  140,   54, 2429,   35, 2611,    6,    0,    8,   29,   53,
           11,  121,  226,  529,   10, 3258,  251, 4513,   96,    0,    2,    1,
            1,    1,    1],
        [  24,   66,    7, 2216,  768,   10,  205,    0,   38, 5276,   26,   70,
           19,  213,   10,   87,    3,    8,    9,   13,  613,  982, 1479,    0,
           17,    0,   10,  110,    0,   26,   13, 1968,  279,    0,    2,    1,
            1,    1,    1],
        [  25,   73, 6265,  281,   12,   17,   13, 1674,    6,    0,  125,  103,
           25,   11,   57, 4298,  336, 3594,    0,    8,    7,  595, 3336, 1233,
         2233, 4457,  111,    0,   25,   11,   57,  492,  142,   10, 1713,   17,
          595,    0,    2],
        [  29,    0,   85,   33,  613,    0,   19,   87,  213,   10,  450,   25,
           17,  185,   12,    7, 2767, 4131,   24,   66,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 101, 1119,    0,   38,   18, 2704,  518,  155,  227,  715,   96,    3,
          125,   33,   26, 1320,  111, 2059,    0,   77, 2808,   10,    7, 9510,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  131,   13,  316,  287,    0, 7492,  136,  760,  322,   93,
            8,  697, 1262, 2178,  713,    0,    8,   53,   11,   57, 2091,    9,
         2567, 1507,    0,  116, 5000,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   0,    9,  264, 1884,  166,    0,   63,  142,   10,   51, 1405,    0,
           33,  169,   86,   51, 3118, 1623,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  67,   21,  465,   11,   18,  229,    0,   13, 1878,   10,  565,    0,
          103,    0,  284,  708,  162,  142,   10,   51,    0,   13, 2988,  109,
            0,   13, 2088,    0,   53,  162,    0,  142,   10,  205,   10,  670,
            0,    2,    1],
        [  67,   24,  499,  213,   10,   51, 1123,   17,   33,   26,    7,  281,
         4089,    8, 2426, 3180,   17,   24,   73,   51, 2985,  199, 2406,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  768,   62,  131, 1962,   35,  699,  119, 3050,    6,  411,
          639,    0, 7176,   20,  111,  752,   10, 1914, 4907,   10,  875,  134,
          199,    7, 1232,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,    7, 5714,   17, 7076,    9,    7, 3227,   12,  423,  759,
         2270, 1098,   18,  500,    6,  148,  618,   69, 1102,  926,    6,   12,
            7, 8783,    8, 6907, 4895,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  213,   25,   10,  150,  138,   33, 7223,   26, 5802,   62,    0,
            8,   21,   11,    6,  142,   10, 2546,  436,   29,   25,  150,   17,
           21,   11,    6, 1117,  126,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([36, 32, 37, 22, 35, 35, 39, 22, 26, 31, 20, 38, 26, 29, 31, 31],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8418, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8721, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(50.3735, device='cuda:4', grad_fn=<MulBackward0>)
False False tensor([[   9,   13, 1880, 4745,    0,   25,  498,   71,  211, 2837,    8,  211,
         4774,   10, 1393,   17,  199, 2332,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,  131, 3696,    7, 4221,    0,   24,   73,  446,  126,  261,
          143,   80,  155, 1037,  254,   25,  914,  135,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   11,    6,    7, 6099,   12,  958,    9,    7, 6869, 8521, 2353,
           12, 5205,    0,    9,  421,  234,   48,  365,  221,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,  162,   86, 2841,    9, 1979,    0,  290,    0,   69,    7, 5225,
           38, 2862,  781,  918,   93,  197, 1017,    0, 2503,   18,  650,  568,
           86, 2115,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [3398, 7936,    8,  284, 1543, 1948, 1985,   87,   21, 2373,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   84,   11,    6,  288,   91,  227,   15,    6, 2158,    0,    0,
          207,   10,    0,  205,  106,   84,    0,    0,   25,  175,  717,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,   10,  101,  128,    0,    0,   10, 1008,    0,   10,  289,    0,
           38,  533, 2056,   13, 1738, 1234,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 206,   11,    6,    7, 2808,    0,   38,  533, 1412, 2215,  159, 2360,
           10,    7, 2182,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 473,  111,    0,   33,   26,   13, 2322,   35, 5444, 2595,  856,  293,
            0,  125,   19,   66,   29,  294, 1516,    6,    9,   89, 1995,  918,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [7494,  185, 1118,  307,   34,   39, 1830,   35, 1933,   35, 1178, 2988,
          120,  101,   34, 1920,   22, 1371,   62,  106, 3022, 1146,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   33,   26,  116,  133,  946,   46,  183,   26,   69,    7, 3089,
           35, 7988,  340,    8,    7, 2509,   26,   69,    7, 1995,   35, 7988,
          340,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,    0,  169,  154, 1827, 1794,    0,  162, 4491,    0,  166,  818,
           53,  451,  601,   25,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,    8,   10,   87,   17,    0,   25, 1275,    0,   66,    0,   10,
         4988,   13, 1472,   12, 2066,  416, 1374,  572,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 1220,   25,  659,   51, 1211,   12, 1835,    0,   38,  187,
           11,   45,   86,    9,   18, 2687,  241,   54,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,   11,    6,    7,  613,   12,  117, 3742, 3228,    6,   17,   25,
           11,  121,  591,    0,   38,  511,  101,  246,    3,  238,    0,   19,
           11,  121,  211,  479,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [  33,  188, 8166,   19,  362, 1838, 1428,   55,   70,    7, 1479,   26,
          142,   10,   87,    9,    7,  858,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,   11,    6, 5653,   10, 1719,   59, 5292, 3147,  111, 1830,  439,
           55, 2677,    8,  248,   10,  391,  439,   55, 5031,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,  180, 2568,  347,   19,  305,    0,    0,   29,  261, 7003,  106,
         4990,  589,  596,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7, 2403,   26,   17,   84,   11,    6,   13,  785,   35, 9058, 3900,
            0,   86,    7, 1953,  322,   35, 3149,   35,  290,   35, 8856, 3900,
          100,   10, 4482, 2147,    0,   67,   13,  785,   35, 9058, 3900,    0,
            2],
        [   8,    7, 1714,  608,  423,  439,    0,   53,  305,   80,  248,  439,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 244, 2869, 4021,    0, 2008, 6248, 1405,   91,   12,    7,  281, 3861,
          338, 3232,  128, 4621,   24,  135,   80,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,   11,   45,  142,   10,  575,   25,  138, 7218,  204, 2895,    6,
           71,  108, 5967,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 110,  153,  240, 1547,    6, 3067,   62, 6873,    0,    8, 1645, 9745,
            6,    0,    0,    0,  593,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,  103,   25,  192,   11,   18,   66, 6759,   35,    6, 2174,  297,
           62, 3867,    0,  180,    7,  630,   12, 1703, 6230,   26,   13,  341,
           91, 5020,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:6') tensor([20, 22, 23, 28, 12, 25, 20, 17, 26, 24, 27, 18, 22, 22, 30, 20, 23, 17,
        37, 14, 21, 18, 19, 28], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8721, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8916, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000, 0.8887,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000], device='cuda:6',
       dtype=torch.float16)
 > at.  tensor(38.1071, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[1250,    0,   53,  ...,    1,    1,    1],
        [ 832, 1337,   22,  ...,    1,    1,    1],
        [  67,   25,  113,  ...,    1,    1,    1],
        ...,
        [  24,  278,    0,  ...,    1,    1,    1],
        [  24,  321,   12,  ...,    1,    1,    1],
        [  19,   11,   45,  ...,    1,    1,    1]], device='cuda:6') tensor([11, 11,  9, 13, 12, 12, 12, 12, 16,  9, 10, 11, 10, 11, 11, 12, 13, 11,
        11, 10,  9, 13, 17, 15,  7,  9, 11, 12, 17,  6,  7,  9, 10,  8, 10,  8,
        10, 11, 14, 13, 11,  9,  8, 19, 12,  9,  9,  6,  8, 16,  8, 12, 11, 13,
        10, 12, 10, 13, 10, 15,  7, 12, 11, 12, 13,  9, 12,  8, 13, 20,  8,  8,
         8,  7,  9, 11, 12, 11, 11, 12], device='cuda:6') tensor([0.8091, 1.0000, 1.0000, 0.8789, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000,
        0.8555, 1.0000, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8218, 0.8652, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 0.8984, 1.0000,
        0.8286, 1.0000, 1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8838, 0.8677, 1.0000, 1.0000, 1.0000, 0.8369,
        0.8652, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  False tensor([[  67,    7,  143,   24, 1260,   85,   21,    0,    7,  143,   24, 1873,
           17, 1057,  453, 3540,   34,    7,  133, 1890,  279,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,    0,   24,  552,  132,   71,   13, 4445,    0,    8,    0,  204,
           70,  956,   34,   91,    0, 2767, 2923,   37,    9,  264, 1736,   46,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 211,    0,    0,   19,  641,    0,    0,   19,  154,   46,    0,  682,
            0,   67,   26,   17,  155, 4234,    0,  846,  174,    0,    0,  211,
            0,    0,  682,    0,   70,   34,   17,   91,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,   89, 9264,    0,   86,  860,   13,  488,   91,    0,   19,
          213,   10,  289,  116,   13, 1455,   80, 2263,  556, 5610,   54,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  142,   10, 4431,   35,  140, 4076,   69,  832, 1486,
          335,    0,    8,   24,   73,  150,  832, 1486,  335, 2546,  829,   69,
           21,    6,  926, 1585,   71,   21,    6, 4241,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [   9, 1296,   10, 2572,   25,   33, 1678,    0,   19,   11,   45,  142,
           10,  296,  185, 5825,    6,   10,  367,  132,   69,    6, 5072,  172,
         2117,    0,    8,   24,   11,   57,  142,   10,   87,   13,  277, 1860,
           35,  399,  102,  287,    0,    2],
        [  24,  465,   11,   18,   87,   33,  131,  204, 2905, 3141,    0,  100,
           25,   87,    9,   13, 1192,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17, 1073,   13,    0, 1878,  629, 1891,    7,  595,    0,    9, 1416,
           12,   25,    8,    0, 4958,   54,    7, 4292,    0,  109,  893,  199,
           39, 4292,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 115,   26,   33,  250,   17,   24,  451,   51,  101,   62, 1256,   12,
            0,   12,  538,    0,   21,   11,    6,  528,   17,   24,  135,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,   12,   33,   10,  289,   17,    7, 4719,  629,   70,   24,   73,
         7039,    8,   70,   24,   73, 2032,   26,  288,  142,   10, 4038,   22,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,  106,   38,  469,  291,   18,  187,   62, 1224,   12, 1491,
            3,    9,  564,  868, 1047,    0,   84,  162, 1107,  820, 5149,  690,
         7653,   20,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  60,  116,  188,   13,  264, 3742, 6211,   17,  368,    6, 1019,  908,
          768,  254,  108, 6211,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 5346,   12, 1009,   17, 1241,   13, 1127,  567,   26,    0,  554,
            0, 2816,  419, 1661,   57,  969,    6,  369,  109,  419, 1620,   54,
         5974,  440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 214,   63,  341,  125,  115,   19,  135,   17, 4751,   26,    7, 1037,
         2829,   17, 1459,  188,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 103,   25, 1260,   85, 2264,  241,  140, 1782, 5152, 1160,  158,   25,
          162, 2639,    0,   25, 2220, 1631,  830,  150,  347,   53, 5953,   48,
         6387,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,    0,  692,   10,   66,   13,  740,  672,   71,    0,    7,  230,
            0,  688,    0,   71,    7,  230, 4157,    0,   71,    7,  230, 1404,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([23, 25, 34, 25, 35, 42, 19, 28, 26, 26, 28, 19, 28, 18, 27, 26],
       device='cuda:0') tensor([1.0000, 0.8755, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8677],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(49.9238, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:25:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
False tensor([[  67,  108, 8830,   48,  589,    0, 7557,    6,    0,  166,   63,    7,
         7557,    6,   17,  601,  170,   10, 6108,    8, 2544,    8,   51,    0,
            0,   13,  562,   18,    0,    8, 9063,   62,    8,   29, 4404,    8,
           29,   69,    0,   53,    0,   63,  261,  143, 7678,   62,  131, 1912,
          688,    0,    2],
        [  24,  116,  144, 6985, 2421,    6, 4502,   80,  854,   39, 3350,  581,
           77, 1004,    7, 6869, 9271,    0, 3381,   70,   11,    6,  956,    0,
            8,   24,   11,  121, 2138,   13,  325,   12,  183,    9,    7, 1726,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 185, 2193, 6248,   22,    6,    9, 1777,  150,  156,   79,   93,  675,
           45,    9, 5222, 1290, 1373,    6,    6,  693,    0,   67,  294,   73,
           51, 5853,  131,    7, 2932, 3288,    0,    8,  203, 1203, 1015,  922,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 106, 1244,   35, 1966,  673,   54, 2409,   10, 2465, 3602,    6,   10,
           33, 2178, 2445,  266, 1192,   69,   89,  906,    0, 3927,   63, 4015,
         5869,   12, 2434,  108,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  154,   21,   26, 2497, 1226,   55, 1221,   10, 1772,    9,    7,
          245,  561,  929,   13, 2226, 1757,   55,   70,  169, 1085,   10,    7,
          461,  266,  763,  480,    6, 1042,    7, 3677,  188, 3699,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7, 4100, 1649, 1085,  143, 1149,    0,   67,  914,   69,    7, 1296,
           12,   13, 4400,   15,   22,  996,    0,  333,  555, 7691,  109,  333,
          555, 3619,  215,    0,   67,   21,   11,    6,  499,  250,   10,   51,
         4837,   12,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7745,  659,   51,    0,   67,   21,  689,   11,   18, 4432,
           10,  452, 1539,  128,    6,    0,   17,   24,   66, 5020, 6083, 5475,
            6,   46,    7, 1320,   59,  816,  300, 1079, 1643, 1794,  230,   85,
            7, 1219,  859,   35, 2514, 4209,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   9,  409,    0,    7,   29,   70,   26,    0,    9,    7,  572,    0,
         2317,    9, 7418, 3782,    0,    7,  214,   17,  229,  155,  572,  283,
            0,    7,  214,   17,  229,  155, 3236,  283,    9,  155,  572,    0,
           63, 8220,  111,  415,   59, 8186,   71, 2317,    9, 2011, 3043,    0,
            2,    1,    1]], device='cuda:7') tensor([51, 38, 38, 31, 36, 40, 44, 49], device='cuda:7') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(71.0548, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[ 131, 1183,  148,   26, 1948,  664,   62,  132,   71,    7, 4617,  292,
         2109, 1775, 4040,    6,   12,   33, 6536,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   19,   11,   48,  100,   10,  227, 2174,  232,  445,  159,  283,
           71,   13,  277, 7074, 4112,    0,    8,    7, 4112,   26,   33,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1425,   19,  144,   10, 1336,  235,    0,   19,  442,   17, 3486,
            8,   79, 2392,   79,   19,  442,   21,    0,   39, 2285, 1790,   12,
         2922,  552,  244,  110,    0,    2,    1,    1,    1,    1],
        [ 877,  290, 2689, 2345, 1212,  188, 2277,   48,   10, 2699,  143,    8,
          143, 5593,   10,   33,    0, 5413, 3301, 5690,    6,  199,  459,  510,
           96,   17,  719, 2070,    0,    2,    1,    1,    1,    1],
        [  29,   24, 6103,  154,   12, 4781,   79,  378, 3816,  214,   17,   24,
          991,   55,  133, 3926, 3965,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  12,  538,    0,  108, 4734,  192,   11,   18,  116,  468, 1004,  183,
            0,   53,  113,  468,  106,  561,   10,  561,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,   66, 4595, 1122,   59,  699,   35,    6,  819,  424, 1942,   96,
            8, 4515,    6,   17,   63,   86, 3412,   62,  131,  883,   48,  293,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103, 2263,   96,  498,    0,   10, 3424,    0,  180,    7,  723,
           73, 5415,    0,    0,    7,  524,    8, 3536,    7, 2517,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 427,  412,  430,  158,   13,  188,   13, 1905,  321,   12, 6873, 1117,
           17,    0,  120, 8491,   48,    0,    7,  427,  412,  430,  158, 3336,
           10, 1051, 1672,  153,  336,    0,  116, 7743,    0,    2],
        [  29,  417,  140, 2362,    6,   93,   26, 3423,   13, 1396, 2233,  199,
           39, 3993, 1874,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  188,  226,    7, 3841,    6,   20,   12,  333, 1670, 2127, 9746,
            6,    0, 3448, 7414,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  225, 4576,    6,   10,  110,    7, 1790,  225, 1831,   79,   13,
          785,   35, 1933,   35, 1178, 3642,    9,   17, 6460, 4057,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  116,  323,   13, 4454, 2761,   69, 5335,   80,    7, 5169,    8,
         1095,  248, 8099,    6,  558,   10,  545,  218,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 4639,   17,   24,  220,   51, 1155,  143,  254, 3857,    8, 1064,
            0,   67,  138,   10, 3918,    6,   20,  126,    7,   91,  106,    7,
          218,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2551,   12,  324,    8, 6790,  214,   26, 2720,   48,   10,
         7572,    8,  211,  234, 1410,  837, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  101,   11,    6,    0,   13, 4961, 1531,    0,    0,   79, 3708,
           18, 3304,    0,  169,   66,  246,    0,   79,  238,   79,   13, 1880,
         1531,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   10, 4637,  373,    0, 2192,   26,  116,  486, 6101,  567, 4111,
           10,   51, 4637,   62,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 8900,   62,  250,   19,  172,  144,   13, 2184,   22,  311,   69,
            0,   26,   17,    7, 1956,  204,  213,    6,   10,  283,   55,  159,
          110,  128,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,  593, 1706,   12,   13,  183, 2760,   55,   77,
            7,  341, 1565,  409, 1215,   17,   24,  135,   10,  468,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1777,  188, 1220, 6380,   48,   17, 2439,  558,  464,    0,   53,   11,
           57, 5332,   54,   13, 4369,  200, 1515, 2902,    8, 3585,  567,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   63,  423,   12,  117, 3357,    0,    8,   53,   63, 4673,    9,
          159, 4268,    8,  159, 4972,    0,    8,   53,   63, 2762,   94,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  976, 8372,   17,   24,   66,  535, 1313, 5260,    7,
          613,  206, 4387,    6, 3480,  108,   24, 1591, 2637,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  717,   35, 1315,  322,  801, 5725,    6,  221, 1339, 5061,  292,
            6,    6, 3222, 1026,   14,   48,   71,   33,    9,   21,    6, 9442,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  69,    7,    0,  939, 2836,  834,   26,    0,    7,   56,  111,    6,
         3620,  266,  207,   17,  368,    6, 2829, 7337, 7393,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([20, 25, 30, 30, 20, 22, 26, 24, 34, 17, 18, 24, 22, 27, 21, 27, 18, 28,
        24, 25, 25, 23, 26, 23], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8550, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8975], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(42.7609, device='cuda:7', grad_fn=<MulBackward0>)
False False tensor([[ 800,   91,    0,  103,  886,  424, 3848,   11,    6,  853,  200,  237,
           26,  593, 3401,  120, 3213,  187, 2044,    6, 1570,    6,  199,    7,
          964,    0,  568,   17,  641,   21,   11,    6,  735,  226,  593, 3401,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24, 8396, 1469,  106,    0,  108, 2914,   37,   93,    0,  131, 2532,
          336, 3494,    0,    0,   67, 1223,   24,   77,   66,   33, 1738,    0,
         2914,   37,   93,   17,   24,   63,  746,    0,   12,    0, 5472, 1117,
          108, 3183,    6,    8,    0, 1459,  994,   26,  126,   84,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2937,  982,  218,  943,    6,    9,  108, 6249,  451,   66,  774,   62,
         3928,    0,    8, 1800,  282,   13, 2333,   10,  175,  850, 1408, 1072,
            6,    0,  109, 3119,  294, 2518,   12,  215, 3928,  254,  956,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  125,   29,  294,   12,    7, 2787,    9, 1370, 2924,   11,   18,
          116,    9,  670,    0,   53,   11,   57,    9, 1037,    8, 1237,    0,
           70,   25,  113,  296,    0, 6957,    0,   26,  143,   69,    7,  230,
          874,  926,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  103,   25, 1816,   66,   13, 5590,   20,  521,   20,  874,
           93,    0,  115,   11,    6,   13,  324,  183,   10,  388,   21,    9,
          155,  874,    8,  598,   21,    0, 2603,   69,    7,  467,    0,   17,
           11,    6,  916,    0,   25,   11,  158,  446,  126,  347,   21,   11,
            6,  434,   13,  862,  365,  232, 1218,   20, 5590,   20,    0,    2,
            1,    1],
        [ 248,  215,  581,   85, 1366,    0,   19,  154,   46,   19,   11,  121,
          367,   10,   33, 7034,   46,   19,  154,   19,  659,   66,  226, 5187,
          106,   13, 3643,  384, 7710,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,    0,    0,    0,    0,    9,    0,   89, 6525,  230,
          115,    0,   38,  187,  144,   33, 1850, 2536,   48,    0, 1850,   62,
           69,    0,    7, 1850,    6,   20, 2286,  307,    0,    0, 4813,  248,
          215,  581,   69, 2154, 1374,   18,  881,   11,    6,  383,    0,   21,
           34,    7,  133, 2354,    0,    7,  473, 2829,    9,    7,  535, 8311,
            0,    2],
        [  29,  115,    0,  103,   24,  274,  270,   85,    7, 3253,    0,   85,
          108, 8196,    6,   17,  162, 5690,   54, 1004,    7, 3253,    0,  117,
         8196,    6,   63,  100,    7,  384,  174, 2003,   12,  108, 2626,  567,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:2') tensor([38, 48, 39, 40, 60, 31, 62, 38], device='cuda:2') tensor([1.0000, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(78.3692, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[ 225,    0,  509,  ...,    1,    1,    1],
        [ 101,  876,  140,  ...,    1,    1,    1],
        [1979,  265,    0,  ...,    1,    1,    1],
        ...,
        [ 109,   63,   24,  ...,    1,    1,    1],
        [ 117,   63, 4700,  ...,    1,    1,    1],
        [   8,   33,  321,  ...,    1,    1,    1]], device='cuda:2') tensor([14, 12, 21, 10, 15, 16, 11, 12, 18, 11, 13, 17, 11, 18,  9, 13, 12, 24,
        19, 14, 20, 21, 20, 19, 17, 18, 19, 24, 10, 15, 10, 13, 18, 14, 10, 15,
        17, 17, 19, 22, 21, 17, 19, 18, 20, 14, 12, 12], device='cuda:2') tensor([1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740, 0.8652, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8677, 1.0000, 1.0000, 1.0000, 0.8979, 1.0000, 0.8882, 1.0000, 0.8799,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
 > at.  tensor(24.2805, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[   7,  245,   91,   26,    0,   25,  305,   17,  524, 4474,  199,   21,
            6,  523,    6,    8, 3675,   29,   17,   25,   73, 5382, 7394,  251,
          523,    6,    8, 3675,    0,    8,  180,   12,  538,   25,   87,    7,
          744,  461,    0,   25,  388,   77,   12,  117,  523,    6,    8, 3675,
          270,  540,  554,   10,  367,   10,  155, 7034,    0,    2,    1,    1],
        [  24,  169,  492,   87,  860,   13, 3310,  279,    0,    0,   67,   46,
            9,  419,    0, 1165,    0,   24,    0,  591,  103,   24,  116, 1223,
         2396,    0,  134,    0,   77,  346,    8, 1296,   62,    0,  134,    0,
            0,   17,  281,    0,   94,  169,  204, 2990,   70,    7, 1296,   54,
          451,   51,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1141,   26, 1111, 6504,    6,    0,    8, 1008,    0,  423,
           12,  117, 3049, 4515,    6,  162, 1028, 1004,    7,   13, 1966,  407,
          816,  333, 1127, 1303,    0,   29,   70,  162,  117, 1816,  401,   71,
           77,    7,  839,   53,  162,  961,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [7616,    8,  415,  584,   26,   91, 4401,    0,    0,    0,   68,  386,
           65,   67,    9,  108, 5605,  336,    7,  179,    0,   24,   66,   77,
         1587,   12,  218, 4401,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,  278,   13, 1335, 1793, 7470,   97, 1411,   37, 2091,  890,
           10, 1557,   13, 1543,   12, 9829,    6,   10, 3022, 1146,   71,  170,
           10, 3522,    7, 4749, 5242,   11,    6, 1051, 2256,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   21,   11,    6,  321,   12,  100,    0,   25, 1057,   13, 1329,
            9, 7419,    0,    8,  180,   25,  175, 6899, 1314,    0,   10,    0,
          574,    0, 9784,    0,    0,    0,    8,  155,  886,  153,   26,   86,
         1767,   80,   33,  125,   25,   11,  121,  278,   10, 1703, 1404, 1610,
           57,    0,    0,  125,   25,   11,   57,    0, 2107, 1978,    0,    2],
        [  29, 8093, 8187,    6, 1452,  736,  439,   12,  159, 1422,    8,  736,
          439,   12,  159, 3857,    0, 6872, 1752,   35,  494,   15,   18, 1568,
         8187,    6, 1452,  736,  439,   12,  159, 1422,    0,   67,  116,  100,
          419, 3460,    8, 4197,    0, 1452,  288,  655,  439,   12,  159, 3857,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53, 1918,   10, 2554,  392,  759, 1086,    0,   29,    0,  100,   29,
          294, 1881,  440,    0,    7, 3516,  278,  540,    8, 6732,   62, 5725,
         2502,    6,    0,    8,  853,  174, 8227,   48,    0,   25,  150,    0,
          853,  174,  689,   11,   18,  703,    9,  906, 2937,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([58, 52, 44, 31, 35, 60, 50, 48], device='cuda:2') False tensor([[  67,   24,  150,   17,  545,   91,   12,  117,   26,  204, 4637,   54,
            0,    8,  188,   13,  341,  207,   12, 3101,  126,  138,   10,   87,
          282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   89, 1970,   34,   13, 3090,  756,    0,    8,   89,  291, 3769,
            0,    0,   34,   13, 1390,   37,  684,    8,    0,  113,   13, 1116,
         1040,   37,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  164,  450,   25,    0,   13,  546,   12,  248,  341,    0, 2352,
            0,    0,  248, 1284, 2352,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  391, 1551, 1065,    0,   17, 2182,  144, 3814,   48,   89,  637,
            8,  333,  473,  279,    9,   21, 3706,   55,  110,   10,   13, 1098,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 3195, 6482,   35, 6937,  227, 3134,  221,    0,   19,    0,   20,
            0,  108, 2805,    0,  132,   10,  108, 7009,    6,   11, 1492, 1485,
           12, 3402,   93,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  246,   53,  169,  780,   29,  835,   10, 1008,    7, 8365,    6,
           12,    7, 3311,    0, 7443,   17, 1573,  162, 3754,    8,  401,    7,
          370,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  289, 1228, 3374,    0,  281,   12,   25,  154,   80,  708,
            0,   21,   11,    6, 1301,    0, 2703,  901,  439,   12,  708,   87,
          722, 1228, 3374,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 291, 1105,   18, 3396, 1095,    0,    7, 4480,   79, 1019, 2071,    0,
         1095,    7,    0,    0,    0, 5640, 1375,   79, 1019, 2071,  755,    0,
          254,   94,  148,  162,    9,  509, 1862,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  116, 2213,   54,    8, 9171,    0,   38, 3127,   89,
         3397,   26,    3,   70,   11,    6, 1226,   71,  682,  616,  249,  407,
            6,    0,   38,  187,   11,  121,  144,  185,  453,  110,  128,    6,
           85,  682,  616,  249,  407,    6,    0,    2],
        [ 101, 5523,   17, 1381,  220,   51, 1405,  131,    7,   94,    0,    8,
           24,  583,  251,   94, 9475, 4637,  373,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  162, 1052, 1563,   94,    0,   67,   53,  162,  113,  172, 7176,
           20, 3979, 2120,    0,    8,   53,  162, 9231,   62,   12, 4391, 3834,
           12, 3716,   35, 7305, 2527,   15,  609,   96,    0, 5653,   10,    7,
          218,  415, 2633, 2120,    0,    2,    1,    1],
        [  85, 4615,    0, 9785,  369,   46,   17,  101,   11,   48,  226,  987,
         1181,  131,   39, 6651, 2266,  160,   22,    0,   13, 5289,  126,   10,
         2585,   20,  366,  565,    0,  276,   10, 9304,    7,  282,  126,   12,
          565,    0,    2,    1,    1,    1,    1,    1],
        [   0,    8,   53,  276,    0, 2982,   80, 1790, 8661,    0,    0,  276,
            0, 1445,   53,  162,    9, 3454,    0,  276, 1445,   53,  162,  830,
         1785,  810,    7, 5167,  608, 2020,   12,  159,  931,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  117, 4312,   22,    6,    0, 1025, 3043,   54,    0,   53,  415,
          158, 1515,   71,    7, 7224,    0,    8,    9,  117,  415,  158,  340,
         1955,   53, 3121,   13, 1905, 1762,   12,  941,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   85, 8195,   12, 2931, 2735, 1221,   46, 1016, 4618,
          866,  693,   46,  120,  248, 4618,  866,  693,  162,   86,  952,   10,
          545,  218,    0,   19,  169,  939, 8966,  834,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 131,    0, 5114,  235, 2399, 1006,    0,  889,   63,    0,    0,    9,
           13,    0,  841,    0,    0, 1449, 1778,  159, 5186,   10,    7, 1722,
         3609,   12,  422, 6222,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([27, 28, 19, 26, 29, 27, 29, 33, 44, 21, 42, 39, 35, 34, 34, 30],
       device='cuda:1') tensor([1.0000, 0.8887, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000, 0.8208],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(58.5012, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[  53, 2828,  987,  ...,    1,    1,    1],
        [6938,   25,   66,  ...,    1,    1,    1],
        [   8,   12,  538,  ...,    1,    1,    1],
        ...,
        [  29,   24,   66,  ...,    1,    1,    1],
        [  67,  985,  687,  ...,    1,    1,    1],
        [   0,    8,  432,  ...,    1,    1,    1]], device='cuda:1') tensor([14, 13, 27, 18, 21, 17, 19, 15, 18, 17, 24, 12, 15, 15, 33, 26, 20, 11,
        22, 14, 15, 22, 24, 13, 20, 16, 15, 17, 15,  9, 16, 21, 15, 13, 16, 26,
        19, 13, 24, 13], device='cuda:1') tensor([1.0000, 1.0000, 0.8550, 0.8975, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8848, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8677], device='cuda:1', dtype=torch.float16)
 > at.  tensor(28.6373, device='cuda:1', grad_fn=<MulBackward0>)
False False tensor([[  67,  490,   24,   87,   17,    0,   19,   11,   48,  100,   25,   10,
          154,   17,   85,  419, 1800,  183,  120,   25,   11,   57, 2202,   84,
            0,    9,  155, 1992, 5896, 1247,    0,    7, 2583, 8748, 9169, 3413,
           54,   10,  155, 9879,    0,    7, 9879,   26, 3413,   54,   10,  155,
         2583, 8748, 9169,    8,   33,   26,  138,  155, 4941,   26, 3803,    8,
         8965,   48,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [5609,   10,    7, 1793,  183,  368, 5673,    0, 1276,    6,  499,   87,
         4986,   79,  261, 1269, 2536,   20,   79, 1970,    6,    0,  166,   26,
          509,  254,   21,   34,    9,   95,  424, 3400, 2197,   11,    6,  383,
            0,   67,   19,  499,  154,   17,  250,  225, 2396,   26, 4803, 5040,
            0,   38,  187,   66,   86,  226, 2373,    9,    7, 9091, 1313, 9094,
            3,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  56,  525,   11,    6, 4197,  246,   10,  267, 1848,    0,   38,  160,
            0, 1491,    0,    0,   33, 4925, 2823,    6,   55,  815, 6091, 1086,
            3,  267,    0, 1848,    0,  148,   63, 1102, 4907,    0, 1260,   69,
            0, 4340,  982,    0,    0,   38,  511,   17,   11,    6,   86,   77,
           46,  415, 1040,    0,   26, 1028,  126,    0,   71,   13,  264,    0,
         1375,    0, 2199, 1132,  820,    3,  225,    0,    0,  246,    0,    0,
           38, 2207, 4925,  164,    0, 2823,   55, 1581,    3,    3,  225,    0,
         8640,   48,    8,  246,    0,    0,   38,  187,  192,   11,   18,  135,
            0,  103,   17,   11,    6,    0, 1581,    3, 1995,  241,  221,  109,
            0, 1581,    3, 1793,    0, 1086,    0,    0,   67,    0, 3296,    0,
           21,   11,    6, 1581,    3,    3,   68,  194,   65,    2],
        [   8,    9,    7,  370,  207,  294,   12,  170,   63,  115, 5954, 4273,
            6,   17, 3412,  108, 1259, 1882,    0,  108,  203,    6,  777, 1991,
            0,  108, 3857,    0,   69,    7, 1080,    6,   17,   33,  659,  601,
          170, 3560, 4390,    0,   24,   73,  884, 1179, 5625,   54,    8,   39,
          128,   93,  195,   54,    7, 1289,   24, 2056,    0,   24, 8099,    0,
           24, 3836,    0,   24, 2153,    0,   73,  450,  170, 4324,   12,  183,
         1179,  250,  659,  205, 1226,   71,  108, 3227,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   71,   33,  822, 5108,    9,   13, 3708, 4774,    0,   24,   66,
         6960,   48,    9,  824, 2723,   45,  726, 1090,   17,   24,   73, 3814,
            7, 1729,   35, 1727,  430,  690, 3275,  110,  249,  132,   10, 1421,
          439,   12,   70,   26, 5914, 1417,   46,  248, 1086,  690, 3275,  110,
          249,    0,   24,   87,   21,   85, 7738,    0,  820,  828,    0,    8,
           24,   73,  499,  875,   17, 3151,  346,    0,  630,   12, 1823,    0,
          103,   25,   73,  875,    9,  143, 3708,    6,    0,   24,   73,  204,
          229,    7,  126, 2446,  276, 3028,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 875,   54,   94,  970,   10,  970,   71,  108, 3648,   26,   13,  207,
           12,  875,   54,  134,  970,   10,  970,   71,   94, 1004,  183,    0,
         1004,  672,    0, 4355,  931,  659,   66,  226,  133,  341,   10,  108,
          447,    0,   67,  148,    0,  100,  170,    0,  144, 1080,    6,    8,
         4989,    0, 2084,  335,   18, 1991,    6,    8, 3128, 2261,    9,  159,
          931,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([ 64,  65, 130,  82,  92,  63], device='cuda:3') tensor([1.0000, 1.0000, 0.8271, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
 > at.  tensor(163.2629, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[  67,   25,  296,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        [  29,   70,  321,  ...,    1,    1,    1],
        ...,
        [  29,   24,   73,  ...,    1,    1,    1],
        [ 229, 1118,   63,  ...,    1,    1,    1],
        [ 103,   25,   11,  ...,    1,    1,    1]], device='cuda:3') tensor([ 9,  9, 13, 11, 14, 10, 14, 16, 15, 10, 13, 13,  9, 18, 11, 17, 20, 14,
        17, 14, 13, 11,  7, 14,  7, 14, 13, 13, 16, 10, 12, 14, 13,  9, 11, 13,
        12, 14, 11, 12, 15, 10, 11, 11, 14,  9, 10, 11, 15, 13,  7, 15, 15, 13,
         8, 17, 13, 20,  7, 16, 14, 13,  7, 13], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 0.8105, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8555, 1.0000,
        1.0000, 0.8579, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000,
        1.0000, 0.8984, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8369,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8882,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:3', dtype=torch.float16)
 > at.  tensor(18.6333, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[3929,  155,  874,  103,   25,  154,   25,  135,    0,   84,   63, 1230,
            0,   91,   26,  850, 6566,  131,    7, 2932,    8,    7,  218, 1498,
         5804,   10,  170,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    7,  467,    0,    7,  453, 6578,  148,   34,    7, 1951,   12,
           13, 1318,    8, 5555,   55,   24, 6868,    7, 1406,    0, 2082,    8,
          858, 4007,    6,   10,  367, 4404,    8,   51,   13,  461,   12,    7,
          546,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   21,   11,    6, 3849,    0,   21, 3560,    6,    0, 1259, 1599,
            0,    0,   21, 2293,    6, 1890,   13,  525,  287, 7195,    6,    8,
         1890, 3577,  901, 7195,    6,    0,  100,    0,   56, 2175, 1488,   35,
          868,    6,    0,  166,   26,  133,  341,  106,    0, 1120,  419,  218,
         2475,   12, 6293,    0,    2,    1,    1,    1,    1],
        [  21,  204, 1847,  214,   24,  169, 2807, 3301,   46,  214,  100,  150,
           48, 2184,    6,  156,    6,  109, 4377,   93, 2058,  424,    6,    6,
           46,    8,   73, 2575,  134,  199,   13, 4458,   18,  160, 1032, 1116,
          111,   45,   37,    0,  166,   25,   73,  774,  199, 1120,  419, 1862,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1384, 1224,  368,    6,   80,  423,  759, 9162,    6,   13,
          383,    0,  166,   26,   80, 1450,  439,   12,   77,    7, 1947,  619,
         5948,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1291,   22,  140,  416,   22,  110,  556, 1949,   11,    6,   13,  391,
           35,  511,   35,  290,   35, 6979,   35,  715,  234, 1927, 3156,   12,
         1954, 6935,    9,    7,   43,   59,  371, 1644,  827, 1151, 3144,    6,
            0,   85,   80,    3, 5494, 1360, 5997,    0,    8,   19,   11,  121,
          226, 1797,   54,   84,   55,  294,  215,    0,    2],
        [  94,  506,  204,  498, 1557,   54,  126, 8282,    6,    0,  934,    0,
            0,  109,    0,   53,  506,   86,   66,    0,   13, 4561,  583,   71,
         1230,   94,   10,    0,  456,   80,   13, 6711,  120,   53,  220,  116,
           87,   13,    0, 4454, 3836,    8,    0,  175,   21,  691,    0,   71,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   7,  853,   22,  335,   26,   17,   25,   73,  446, 2533, 5203,  336,
           21,    0,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,   11,  121,  116, 3498,   25,   63, 3302,   12,    7,  133,
          946,    8, 9743, 1081,   17, 5577,    6,    8,  896,  227, 2174,  232,
          445, 6442, 1052,   62,   48,  562,    6,    8, 1122, 1079,  292, 3751,
            6,   73, 9478, 2808,   55,  159,  447, 3965,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  274, 6269,    8, 6269,  199,    7, 1710,   12, 5554,    0,    8,
            9,   17,  207,   24,  175,  914, 4616,    8, 4616,   10,   33, 2570,
         2240,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  218, 2604,    6,   12,  813,    0,   79,   25,   11,   57,  211,
         6449,    0,    0,    0, 4837,   63, 2875,   80,  860, 4825,    6,    0,
          106, 3836, 4506,    6,    0,    0, 1817, 4506,    6,    0, 1817,  837,
         5002,    8,   29, 4404,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   77,   87,   33,   13,  277, 4838,    0,  166,    0,   26,
          347,   24,    0,   73,    0,   77,  274,  132,   85,    7,  370, 1303,
            0, 3253,    8,  150,    0,   33,    8,  113,   33,    8,  113,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 225, 1639,   85,    7, 1749,  747,  670,   79,   13, 2829, 1020,    9,
            7,  384,  221,   11,    6, 2100,    0,   29,  225,  278,   10,  150,
           77,    7,  807,   17,  278, 3172,   22,  126,   12, 2092,    0,   55,
         1976, 1043,    0,  148,  162, 4111,   10,   51, 7806,   48,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 116,  336,   17,  183,  206,   19,   34,  961,   17, 7374,    0,   19,
         1260, 1004,    7, 4895,   12, 5205,    8, 4776,   84,   34,  486,  546,
           24,  162, 5054,    0,    7,   82,    9, 7723,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   34, 3642,   84,    0,  116,   13,  110,   57, 1802,   35, 1933,
           35, 1178,    0,    8,   19,  934,   48,   13,  422,  378,    9,   13,
         2414, 2196,    0,  125,   33, 5353,  220,   86,  601, 1584,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45, 2464, 1778,    0,   68,  194,   65,  211,    0,   53,
          162,  172,  488,  184, 2148,    6,    0,    8,   53,  465,   11,   18,
          135,  261,   80,  998,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:4') tensor([29, 39, 53, 50, 30, 57, 50, 18, 47, 27, 42, 38, 48, 34, 36, 30],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8652, 1.0000, 1.0000, 1.0000, 0.8394, 1.0000, 1.0000,
        1.0000, 0.8887, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(71.0567, device='cuda:4', grad_fn=<MulBackward0>)
False False tensor([[3706,   55,    7,    0, 3210,   12, 2058,   35,  249,   59,  240, 1144,
            0,   53,   11,   57,  133,  946, 2787,    9, 2047,   12,    0,    0,
           73,    0,   24,    0,  694,  214,   10, 2231, 2736,  941,    0, 1645,
         2078, 2399,   70, 2892, 1075,    0,   73,   87,    8, 2293,  359, 4531,
          946, 6710,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  13,  143, 4089,  207,   10,  154,   80,   89, 6674,    6,  506,   51,
            9, 2047,   12, 1387,  687,    8, 4480,    0,  206,   85,  419, 1800,
          613,    9,  183,    0,   71,  419, 1127,  723,    0,   19,  217, 1102,
         1387,    8, 8219,  106,   17, 2216,    0,   77,   79,   13, 2443,   12,
           70,   19,  296,   10,   87,  230,  115,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   7, 7374,   17, 5290, 1247,    0,   79, 4546, 1247,    0,  220,  508,
         3667,   10,   13,  733, 6608,   12,  341, 1603, 4345,   46, 1259, 1247,
            0,  618,   59, 1247,    0, 3109,  140,   57,  411,  266,   26, 2856,
         1247,   46, 4574,   48,    7, 2303,   12,    7, 1827,    8,    7, 4715,
           12,    7, 1136,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [4157,    0, 7957, 6266, 1119, 1017,   21, 1962,    0,   85,  109, 3019,
          964, 4157,    0,    8,  180, 4587,   13, 1206,  567,   10, 1005,   21,
           84,    0, 4958,    7,   59,  627, 1202,  290, 1408,    0, 6712, 5933,
         6962,   26, 2770,   10, 5989,   85, 1362,   20,  586,  922, 4157,   71,
         3955,   45,  502, 8366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   39,    9, 1324,    9, 1954, 6935,    0,   19, 1346,
           80,   13, 1361,   12, 8188, 4039,  148,  144, 1828,  244, 1339, 1027,
          587,  195, 3485,    0,    8,   13, 8188, 1793,  148,  692,   10,  508,
         4460,   69,   17, 3485,    0,    8,  211,  218, 3280,  692,   10,  205,
            8,  601,  267,  508, 4460,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 103,    0,   84,   11,    6,    0,    0,    0,    0,   91,  759, 4039,
           46,    8,   19,  172,  135,   33,   46,  148,   63,    0, 3163,   10,
          229,    0, 1583,  583,    6,    0,  148,   63,    0,    0, 3163,   10,
           51,   69, 3836,    0,   19,  217, 2497, 1123,   17,   24,   73,    0,
          204,  468,    7,  538,   12, 1261,    0, 2318,    0,   55,    7, 4959,
           12, 1146,    0,    2,    1,    1,    1],
        [   8,   29,   19,  246,    0,   38,    6, 1049,    3,    0,   29,   19,
            0,  175,  518,    0,    8,  225, 1559,   69,    0,    8,   71,   13,
          277,  523,   12,    7,  170, 1522,    0,    0,    0,   13,  176,    0,
            0,  180,  225, 1518,  336,    0,    8,  225, 1143,    0,   80,  423,
         2360,    0,    8,    0,    0,  225, 1518,  270,  336,    0,    8,  225,
           11,    6,   77, 6087,    6,    0,    2],
        [  85,    7,  183,   19,   34, 1931,   54,    9,  117,  341, 1427,    0,
           19,  474,   19,   34, 3258,   54, 4029, 1352,    0,   67,   69, 1858,
         2007, 1498, 1261, 9096,  237, 1181,    0,    8,   19, 4600,   19,   11,
           48,  204,  226, 3258,   54,   13, 1127,  546,   55,  143,  254,  423,
          215,    0,    8,    7, 2838,   69,  264, 1736,   34,   21,    6, 2402,
          119, 8197,  271,    0,    2,    1,    1]], device='cuda:5') tensor([52, 57, 53, 54, 55, 64, 67, 65], device='cuda:5') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8584, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(113.0635, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[  21,   11,    6,    0,   86,   79,  103,   19,  144,    0,  419,    0,
          479,   70,   21,  169,  274,  100,    0,    0,   67,   84,   34,   13,
          841,   17,   24,  169,   66,   10,   87,  250,   71,  264, 9520,    0,
           12, 5149,    8,  250, 1057,   10,   87,   71,    0, 1588,  468,    8,
          250, 1057,   10,   87,   71, 1136, 1370,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  116,  226, 4174,    0,  166,  818,  251,  148,  162,
         7220,  144,   10,   66,   13,  207,   12, 8254,  839,   10, 1703,  251,
          248,    8,   13,  854,  759, 1086,    0,    8, 1065,    0,  143,   55,
            7, 6580, 8068,    0,    8,    7, 3742, 7856,  249,    0,    8,    7,
         4528,    0,    8, 3097,    7, 2248,  181, 3414,   20,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  67,    7,  630,   13, 2003,   96,    0, 1189,    0,   29,  138,  261,
           87,   56,  234, 1265,  241, 2142,    0, 2994,  122,  241, 2142,    0,
           13,   59,  380,   18, 3110,    8, 3098,    7, 1503,   12,  886,  412,
         3276,  287,    9, 4928, 1703,   55,   17, 7752,    9, 2446,   10,   17,
         2340,   10,    7, 1503,   12, 7121,  484,    0,  166, 2231,    6,   17,
         3833, 1548,    0,    2,    1,    1],
        [  21,   11,    6,  434, 3919, 8570,    0,    8,   21, 6430,    6,    0,
         2584,  218,  214,    0, 7753, 2736,  296, 1212,    8,  227,   93, 1069,
           96,    0, 2797,   54, 1868,  187, 3635, 1319, 3562, 7289,    8,  218,
         2808,   35, 5444, 2840,    6,   10, 3814, 3001, 3188, 1068,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,  258,  636,  132,    9, 5950,   20, 1006,    0,    8,    7, 3564,
          162,  172, 3633,   17,    7, 5317,  144,  226,   13, 2786,    0,  125,
           85,   17, 2110,    0,   19,  144,   13,  277,  523,   12, 2080,    9,
           91,   12,   89,  488,   10,   96,    0,    8,   19,  474,    0,   38,
          122,   57,  411,    0,  125,   19,   11,   45,  142,   10,    7, 8022,
            6,    3,   68,  194,   65,    2],
        [   8,  115,  339,  110,  873,  908, 2955,  187,  445,  128,    0,    8,
          289,   17, 4586,  931,   46,  166,  185,   12,    7, 4793,   24,  175,
          568,   69,   33, 4959,   46,  120,   25, 2554,    7,  282,   12, 2307,
            0,   13, 2980,   37,    0,   13, 3700,    0,   13, 1276,    0,   53,
           63,  521, 1015, 3127,   54, 7553,  111,  199,    7, 2340,    0,    2,
            1,    1,    1,    1,    1,    1],
        [   0,   19,  113,  213,   10, 2678,   33,   91,  132,   13,    0,    0,
          523,    0,  138,   73,    0,   53, 4477,    0,   19,   11,   45,    0,
          142,   10,  388,  134,    0,   69,   13,  824,  586,  457, 5445,    0,
            0,  166,    0,   26,    7,   38,  122,  241, 1515,   10, 4853, 1435,
           85, 8251, 1243,    3, 1189,    0,   29,  168,   21,   26,    0,    2,
            1,    1,    1,    1,    1,    1],
        [1220,    9, 3023,    0, 2058,  181,  541, 1775,  271, 4814,    6,   66,
          226,  619,   10, 1754, 7371,  955, 2028,    0,  100, 4896,    6,    0,
         2678,  777,  812,    6,    0, 3183,    0, 2011, 6504,    6,    8, 4222,
            0,   17,   66,  226, 3150,  111, 7225,   62,  199, 2116,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:5') tensor([57, 59, 64, 48, 66, 60, 60, 48], device='cuda:5') tensor([0.8740, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(100.2751, device='cuda:5', grad_fn=<MulBackward0>)
False False tensor([[ 347,  568,   21,  988,    0,  148, 1006,    6,    0,   26,   33,  116,
           13, 1678,   17, 9004,    6,    8, 6904,    6,  722,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13, 4746,   35,  803,   18,  249,   26,   13,  382,  783, 1522,  723,
            0,    8,   24, 5874,    8,  749,    6,  174,  589, 6010,  722,   55,
            7,  218, 1543,    0,    2,    1,    1,    1,    1,    1,    1],
        [  70,   33, 3565,  170,   26,    0,   33,  164,   15, 4482,  181, 2958,
            0,   21,   11,    6,  113, 2092,  365,    8,  883,  140,  365,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57, 1211,   17,   17,   11,    6,  204, 1031,  156,   62,
          199,  281,   94,   11,    6, 2702,   32,   54,   85,  185, 1648,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  103,   17,   11,    6,   86,  890,    0,   53,   11,  121, 2107,
           69,   10,  289,    0,  281,  490,   53, 4853,  747,  670,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1325,   53, 5415,   33, 1165,    0,   24,   11,  158,  492, 6290,  565,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  10, 1141,   17,  630,    0,   19,   11,   45,  142,   10,   66,   10,
          884,  486,   91,    0,  166,   26,    0,   70, 1148,  120, 6249, 8221,
            6,  415,  158, 1515,    0,    2,    1,    1,    1,    1,    1],
        [ 115,    0,  138,  323,   19,  367,   10,   33, 1760, 3140,   12,   13,
           48, 1184,  140, 3085,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 4268,   12,    7,  744, 6213,   55,   33, 1599,   34,   17,
           21,  220,   51,  619,   69, 8786,    6,    8,   91,   35, 1933,   35,
         1178,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [1008,   46,   25,   11,   57,    7, 3698,    0,    0,    0,  125,   25,
          618,  155, 1037,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,   13, 2198,  614, 1810,   34,    7, 4023, 2922, 2371,   56, 4654,
         2041,    9,    7, 1261,   12,    7, 1384, 1224,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  26,   21,  148, 1073,    7,  281,  839,  109,  148,   26,  281, 8064,
           10,  267, 3557,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   73,  876,  121,   10, 2697, 2360,    0,   10,  206, 7616,  568,
           86, 2705,  307, 1906,    0,    8,   26,  417, 1841, 1059,   62,   71,
         1894,  688,    6,    0,    2,    1,    1,    1,    1,    1,    1],
        [2018,  527,   93,   35, 2287,  234,  439,   12,    7,  179,   11,    6,
         1682,  931,    9,    0,  108,    0, 1884,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 4479, 1339,  783,  511,   37, 1405,   13,    0,  682,  247,   17,
          101,  434,    0,   38, 1879,    0, 2102,    3,  166,   26, 1223,    0,
         8044,   55, 3708,    6,    0,    2,    1,    1,    1,    1,    1],
        [  19,  321,   12,  100, 1060,  117, 1532,    8, 2346,   18, 1011,   71,
          565,  359,   13, 2555,  237,  866,    9,  321,   12,   13,  854, 4656,
            6,  737,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  24,   66,  255,  200,   22, 6339,   54, 2200, 2362,    0,    8,   24,
           66,  244,  168,  264, 8973,    6,   69,   13, 4481, 3489,   10, 1764,
          387,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [2805, 2219,   69,   70,   91, 1985,  150,    0,    7, 2805,   12,   17,
          107,   20,   15,    8,  291,  156, 3837,  457,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103,   17,   11,    6,   86, 4949,  890,    0,   25,   73, 5652,
           91,    8,   25,  175,   13, 2685, 8831,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 106,   17,  552,   13,    0,  325,   12, 5828,    6,    0,  109,  126,
            6, 2893,    6,    0,    0,   79,   19,  434,  134,    0,    0,  214,
           17,   19, 1385, 1313,    0,    0, 6263,   89, 6878,    0,    2],
        [  19,  154,  245,   21,   11,    6,  528,   55,  170,   10, 2613,   70,
           21,   26,   10,   51, 2639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3695,   12,  134,   63, 1387,   10,  206,    7, 4458,  362,    6,   63,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  744,   26,    9,    6,  234,   54,   17,   84,   26, 8902, 1666,
         2214,   10,  415,  500, 1783,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0, 6482,  215,  581,    0,  903,    0,  846,  315,   22,   18,
          380, 3089,   35, 3304,   62,  284, 3780,   11,    6,  874,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([23, 29, 25, 25, 24, 14, 30, 18, 28, 18, 22, 17, 29, 21, 30, 28, 28, 22,
        21, 35, 19, 14, 19, 24], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8584, 1.0000, 1.0000, 1.0000, 0.8706, 0.8208, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8242, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       dtype=torch.float16)
 > at.  tensor(41.0890, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:25:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
tensor([[  24,   66,   13, 7524, 4040,    9,  166,   24, 1393,   10,   91,  723,
           10, 8714,   39,  467,  982, 2884,   12, 1833,    0,   10,   51,   89,
         4023,  570,   59,    0,   89,  772, 1751,    0,    7,  772, 5370,    0,
           89, 2444,   62,  521, 1105,   48,  480,    0,   89, 2294, 1661,  221,
          369,    0,   89, 5930, 4888,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,   70, 1320,  335,  684, 4600,    0,    8,   25, 1510,   21,    9,
            7,  227,   93,  362,  715, 2378,    6,   12,  632,  237,    6,   15,
          593,    0,   34,   17,    7,  535,    0, 3139,    0,  227,  233, 2341,
         4267,    6,   12,  574, 2256,   12,    7,  564,  322, 1910,  162, 1028,
           10,   13, 1387,    0,    8,   17,   24,  162,   80,   10,  954,  199,
           91,   12,  251, 9200, 2760,    6,   12, 1261,  120,  768, 2317,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19, 7750,   25,   10, 1064,    0,   55,    7,  245,  183, 3983,    9,
            7,    0,  179,    0,  168,   69,    7, 1366, 2110,    0,   13,  475,
           35, 5203,    0, 2178, 2445,  266,    0, 6580, 1411,  271,    0,  629,
          110,    0,    8,   89, 1751,    0,  903,    0,    0,  728,  649,   57,
           93, 2734, 2003,    0,    0,  106,    0, 7002,   11,    6,  728,   18,
         3411,  300,    6,  369, 7312,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  17,   26,  120,   13,  150,   48,   26, 6788,   62,    9,   13, 7312,
           10,   87,  250,   86,    9,  191,   48,   62,  131, 1377,   46,  100,
         1520,    7, 2192,   12,   13, 1390,    8, 2985,   21,  199,    7, 2192,
           12,   13,   10,  424,  412,    0, 1995, 2083,    0,  192,   11,   18,
          175,  110, 1226,    0,   19,  100, 1390,    8,   10,  424,  412,   96,
            0,   67,   33,   26,  116,  775,   20, 1718,   93,    0,   68,  194,
           65,    7,  150,   48,    6,   63,  180, 2685,   62,    0,  180, 5847,
            0,    2],
        [   7, 2193, 1793, 2493, 7012, 1198,    6, 1361,    0,   13, 1361,   12,
           94,  148,    0,   69,   13, 1284,  383, 1663,    0, 3284,  199,   13,
         3508,  982,  964,    0,   85,  159, 3506,  160,  128,    6, 3696, 3836,
            8, 2187,    6,    9, 2828,   62, 7689, 1974,   22,   18,    0,  100,
           33,    0,    8,   53,  456,   80, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,   29,  115,    7, 2932, 1381,    0,  148,   55,   13,   87,  610,
          215,    0,  188,  923, 3150,  111, 6226,   62,    7, 2688,    0,   26,
          115,    9,    7, 3140,   12, 1057,   10, 4007, 1179,   10, 2156,  109,
         6406,  346, 1711, 4166,    0,  125,    7, 6105,   10,   13, 3147,  234,
         1827,   26,   29, 4126,   17,   53,   73,   11,   18, 1799,   71,   21,
          419,  218,  207,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1232, 5917,    6,  427,  362, 1011,  131,    7, 3022,   46,    8,
           19,   11,  121, 2138,  185,  798,  215, 2115,   54,  336,    7, 4959,
          401, 1221,   69, 8633, 1232,    6,    0,    8,   66, 3395,   62, 3585,
         1118,    9,  392,   10,  798, 1075,    9,   33, 4959,    0, 2386,   12,
         3585, 1118,   46,  752,   10,  661,   70,  513, 1226,   71,  108, 1232,
         5917,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:4') tensor([55, 73, 67, 86, 57, 65, 63], device='cuda:4') tensor([1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(135.8914, device='cuda:4', grad_fn=<MulBackward0>)
False False tensor([[1189,    0,    0,   29,  339,   11,    6,    0,  289,   21,   11,    6,
         1491,    0,    0,    8,  339,   11,    6,  289,   21,   11,    6,  384,
          372, 1761,   20,    0,    8,  115,    0,   25,   73,    0, 1948,    0,
          205, 1817,    8,  446, 1835,   13, 1484, 3762, 2293,   59,    0,    2,
            1],
        [   0,   67,   25,  135,    0,   19,  154,   25,   63, 4130,   71, 2084,
         1625,  128,    6,    0,   46,    7,   81, 2510, 3496,   18, 9875,    6,
            0,    8,   77,  117,  214,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  91,  383,    0,   19, 7262,   62,  126,   12,    7, 2657,    9, 3918,
         1118,  432,   13, 3040,   71,   13, 3532,   91,    0,    8,   19, 5429,
           62,  199,   13,  682,  181, 3435,    0,  206,   19, 1060,    7, 2859,
           57,    6,    6,   55,   13, 2705,    0,    2,    1,    1,    1,    1,
            1],
        [   8,  120,   19,  289,  923, 1738,    0,   21,  220,  499,   51,   29,
         4733,   17,  211,   91, 3485,   12,  282,  700, 6317,    6,  486,    0,
          166,   26,   13, 3462,  474,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  113,    7,  473,  203, 2470,  247,   55,    7, 3299,  267,   48,
           12, 2533,  382,  399,    0,    8,  113,    0,   12,  538,    0, 3547,
         6952,   55,  486,  733, 3609,   12,  218, 1369,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  94,   63,   29, 4493,   12, 6608,   17,   53,  780,    8, 2509, 1459,
            0,  276,   94,  148,  192,   11,   18,  213,   10,  109,   73,   11,
           18,    0,   10,  873, 1417,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 432,   77,    0,  305,   13,  274,   85,  117, 2792,    6,    0,   97,
          737, 1010, 1144,    8,  415, 2633, 2041,    0,  179,   82,    6,    0,
         6229, 1106,    0, 7050,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7,  240, 2120,   66, 2138, 4021,    0,  752,   10,    0,  661,  347,
           21,  188,    0,    0,   33,  133, 2035, 1236,  297,  293,  111, 3495,
           35,   18,  500,   62,  800,    0,    8,    0,   53,   11,  121,  367,
          132,   71,    0,    0,   13,  800,   12, 1254,    0, 7052,    6,    0,
            2],
        [ 125, 2392,   37,  109, 1065,    0,   24,   11,  158,   51, 7258,   62,
           71, 4012,   80,   33,    0,    8,   21,   11,    6,  509,  103,   24,
          154,  835,   80,   21,    0,  276,  103,   24,  213,   10,  154,  835,
           80, 2826,  347,   24,  451,  492,   87,   21,    0,    2,    1,    1,
            1],
        [   8,  168,   21,   26,   55,    7,  245,  183,  291,  121,  233,   62,
           85, 1366,   46,  108,  245, 4878, 1806,  469,  816,   24,  293,  457,
            0, 6712, 5831,    6, 1103, 3007,   54,   71,  282, 1117,   13,   24,
          293,  457, 7757,   54,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,   70,   24,  487,   71,   34,  172,   13,  264,  207,   12,
          826,   80, 1009,   39,    9, 3562,    0,  359,   13, 1039,  434,  229,
         1261,    0,  166,   24, 6532,    9, 5855,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   34, 1061,    0,  225,   34,  798,    0,  100,   13,  325,   12,
            7, 1323,    6,   24, 1618,    0,   53, 3994,   11,   18, 2603,   56,
         7038, 1568,   80,  159, 1666, 4388,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 391,  215,  581,   19,   34, 1074,    9,   13, 5200,    9,   13,   56,
         2561,   35,   45,  973, 2102,   54,  325,    0,    8,  440,   19,   11,
           45, 4379,   85, 1366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,  225, 3673,   62, 1406,  290,  125,  225,  692,   10,  229, 1123,
          225, 3673,   62,  250,  225, 1425,  267,  708,  169, 2034,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19, 1425,   17,   85,   17,  765,   17,   70,    0,   19,  451,
          508,    0,    0,   10,   89,   94,   26, 1370,    8,  958,    0,    0,
            8,   17,   11,    6,    0,   70,   19,  513,  432,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   70,   19,   11,   45, 2659,   55,   26,    7, 5708,   12,   13,
          264, 1329, 6813,   46,   19,   11,  158,  367,   10,   33,   13,  277,
         1065,   46,    8, 1645,    7, 4936,   12,   13,  264, 1455,  199,    7,
         2781, 1234,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:0') tensor([48, 31, 44, 31, 34, 31, 30, 49, 46, 42, 33, 32, 30, 24, 35, 40],
       device='cuda:0') tensor([0.8550, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8433, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(62.6279, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:25:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
tensor([[ 250,   17,  169,   51,  528,    0,   67,  113,  250,   17,    0,  169,
          305, 5542,   12,    0,   77,   12,  117, 7099,    6,   17,   91,    0,
            0,    0,  144,    0,   38,  511,    9,    7, 1165,   12,    7, 1827,
         2260,    0, 4619,   13,  325,   12,   94,    0, 4619,    0,   94,  148,
          162, 2129, 9829,    6,  109, 4488,   93,    0,    0,    0,    8,  113,
           86, 1057,    0,    0,    0,    9,   89,    0,    0,  447, 1165,    0,
           13, 3557,   10,    0, 4110,   80, 3903,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,  278, 1421, 3675,  126,   12,   21,   55,   89,  670,  125,
           19, 1831,   17,   24, 1591, 2637, 3181,    6,    8,  811,   35,  426,
          356, 5149,    8, 1127, 1848, 7178,   48,   10,  367,   10,   13,  670,
          206,   84,   34,  874, 2286, 5575,   62, 8517,   17,  877,   20,  307,
           62,  134,  333,  383,    0,  125,   21, 1017,    6,   13,   10,  375,
            8,   39, 7574,   80,  138,   25,  598,   80,   94,  535,  490,   25,
          508,  134,    7, 4340,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  432,  423,  215,   12, 7989, 1435,    8,   56, 2687,  240, 1032,
         6606, 6480,    0,  131,    0, 1655,   12, 5709, 2245,    0,  333, 2767,
         2931, 3182,    9,    7,  179,  188, 8152,   48,   17,    0,    7, 6929,
            6,    0,    0, 5914,   69,    7, 1232,   63, 2426,   10, 2034,    8,
           17,    7,  979,   12, 2753, 4429,   26,  211,    0,  143, 1565,   93,
          254, 3563, 4401,    6,   12, 2753,  824,   48, 4577,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   79,   25,  150,    7, 1819,  205,    9, 6714,  111,  359,   84,
            0,    8,  180,    0,    0,    0,   68,  386,   65,    8,    7,  281,
         4673, 1809,  411,   26,   17,   89,  874,  204, 1917,   62,  499,  535,
          890,   10,   87,   17,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   34,   89,  765,   10,  749,  221,    9,    0,   10, 2750, 1222,
         2433,   55,   70,   63,  172,  288,   13,  874, 1256,   12,    7,  133,
         1219, 5222, 3770, 3254,    0,    8,   19,  144,  116, 5893,   13,  488,
            0, 1061,   35, 1315,  322, 1039,   55, 2829, 1020, 1747,  160,  741,
            0, 3150,  111,    0,    8,   19, 1425,   19,  220, 6785,   13, 3028,
         1329,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  269,  290, 1543,  106,   10, 1059,  188,  691,    7,  291,   18,
         1397,  156,  457,    0,  391, 1543,    6, 4437, 2361, 1268,    0,  895,
         1101,  269,  484,    0, 1912, 2172,    0, 5087, 2172,    8, 1339, 1373,
          699,   57,   77,   85,    7,  370,  183,   46, 6835,  203,  620,  293,
            6,   62, 4437, 1395, 4781,    9,  166,   24,  205,  126,    0, 2213,
          132, 1546,  121,   22, 1691,  269,  484,    0,  388,    9,    7, 4437,
            6,   17,  204,   66,    7, 4273,    6,    0, 3020,  126,    7,  269,
          290,    8,  180,  339,  134,  205,    0,    2],
        [   7,  218, 8314,   12, 1780, 1119,    0,   38, 1865,   89,  227, 2704,
            7,  179,   34, 1621,    3,  166,   26,   10,  289,   21,   11,    6,
         1301,   17,   19,   73,   11,   18,   87,  778,    0,   67,   19,   73,
         1123,  111,   87,  250,    0,   19,   73,   55,  122,  366,    0,   19,
           73,  570,    0,   19,   73,  575,  132,    0,   19,   73, 6295,    0,
           19,   73,   51,   13,  461,   12,   33, 2469,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:4') tensor([81, 78, 71, 42, 63, 92, 70], device='cuda:4') tensor([0.8003, 1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(146.7911, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[ 220,   25,  367,  180,    0,   38,  235, 4639,  321,   12, 5832,   10,
           87,    0,    8,  204,    0,   17,  245, 3321,    0,   19, 1008,  826,
            3,   19,   66,   10,   51,    7,   91,   10,  884,    7,  558,  630,
            0,   38,  242, 1027, 1792,   19, 1425,   19,   34,  142,   10, 2184,
          649,    8, 3997,  649,   56, 1628,   33, 2469,    0,    2,    1,    1,
            1,    1,    1],
        [  84,   26,   39, 1248, 3046,   24,   63,  142,   69,   10,  873,  422,
          554,    0,   10,   51,  461,   12, 1237,  554,    0,   10, 1452,   12,
         1469,    0,   10,   51, 6610,    0,    8,   21,   11,    6,  133, 3315,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 101,  513,   10,  283,  333,  383,    9,   39, 6101, 6292,  100,   33,
           91,    0,  752,   10, 3067,   39,   96,  469,    6,  407,    8, 2572,
         1573,  138,   10,   87,   29,  802,   17,  370, 6629,   17, 1505,   29,
          291,   57,  297,  457,    0,    8, 7714,  107, 2398,   20,    0,    9,
          284, 2406,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 126,   12, 7680,    0,   51,  656, 6383,    6,    0,  148,   24,  144,
         1220,  226,  968,   34, 2429,    0, 3941,   45, 1755,   48,  199,    7,
         1953,   18,    0,  116,  100,   13, 1720,   45, 3328,    0,  101, 4311,
          199,    7, 1953,   18,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  84,   63,  453, 3254,    8,  453, 3557,    6,    0,    0,    8,  180,
           84,   63,    7,    0,  747,   35, 3455, 6758,    0,    0,    0,    0,
          747,   35,  119,   57,    6,    6,    0,    0, 2011,    6, 2083,   54,
            0,   29,  300,   35,   48,  608,  292,   93,   54,    0, 1587,    0,
           12, 3254,    0,    8, 6244,  111, 1155,    9,   35,  242,   18,  533,
           15,    0,    2],
        [1800,   17,   19, 1744,   89, 1113,    8, 1303,    6,   97,  652,  160,
           54,    7, 4615,   17,  220, 1085,    0,   21, 2220,   11,   18,   51,
         4822,  103,   89,  179, 6789,   34, 2264, 2304, 3328,    0,   68,  194,
           65,   21,   11,    6,   86,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  85,    7,    0,    0,  765,    0, 2627, 3188,    6,    0,   69,   97,
         1411,   54,    0, 2518,    0,   12,   10,   22,    6,   12,   29,   93,
          106, 2008, 1491,    0,  206,   21,    6, 4323, 6358,    6,   10,  913,
         6811,    0,   10, 9515,    0,   10, 9343, 4513,    0,    0,   10, 3018,
          931,   18, 1994,  168,    9, 2627,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [  67,   24,   11,   57,  168,   55,    7,  324, 1569,    0,    8,    7,
          324, 1569,   26,   17,   19,  154,   24,   73, 3522,  347,   33,  133,
          475,  422, 3916,    0,   33,  133, 7488, 2042,   12, 2070,    0,   26,
         4877, 1069,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([58, 38, 52, 42, 63, 43, 56, 40], device='cuda:4') tensor([[ 976, 1475,    0, 2184,  176,    0,   24,  154,   29,    0,   29,  115,
           17,   24, 2534,  117, 2522,    8,  591,  117, 3422,   17,  339,  170,
           87,  117,  214,    0,   24,  487,   10, 2252,   17,    0, 3423,    0,
          995,   17,   24,   73,   87,   71, 1780,    0,  995,   17,   24,   73,
           87,   71,   13, 1472,   12, 1780,    8,   13, 2705,   24,   73,  115,
           87,   71, 5414,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  499, 3008,  813,   69, 3634,    6,    0,   67,  115,   24,   73,
         3008,   13,  325,  143,  813,    0,  143,  254,  700,  490,    0, 2761,
           54,   21,   26, 4865,    0, 4585,   54,   21, 4865,    0, 4912,   21,
           26, 4865,    0,  979,   54,   21,   26, 4865,    0,    8,   70,   24,
           73,   87,   26,   24,   73,  203, 1792,   33,  813,   55,  368,    6,
           17,   24,  492,  276,  934,   48,  120,   24,  245, 2861,   62,    7,
          660,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21, 1518,  126,   17,   71, 1421, 1601, 2421,    6, 3195,   54,
            9,    7, 5199,    0,  432,  101,  976,  261,  442,  284, 3140, 2226,
           69,   33,   46,  101,  144,  284, 7986,    8,  101,  144,  284,   46,
          101,  258,   57,   33, 1266,   55,  908,  254,  248, 1551,    8,   34,
          529,   10, 7554, 4115,   13, 4058,    8,  278,  923, 2294,  244,    7,
          409,   17,    0, 5098,   46,  284, 5098,   46,   21,   11,    6,    7,
          245,  183,  101,   11,    6, 1831,  100,  101,   11,    6,  144,   39,
         1266,    9,  815, 1132,  215,    0,    2,    1,    1,    1],
        [ 108,  296,   55, 3051,    0,  108,  296,   55, 4029,  687,    0,  109,
          108,  296,   55, 3069,    8, 8464,    0,  109,  108,  296,   55,  540,
          687,    8,   55, 2848, 2050,   93,    0,    8,  103,   25,  154,   80,
            7,  277, 1920,  148, 2456,    6,   69,  155, 1067,  265,    8,  148,
           26,  415,  878,  111, 7307,   62,  168,    8,  133, 7759,    8, 6128,
            0,    8,   85,  185,  613,   77,   12,  170,  296,   10,  205,  126,
          199,    7,  179,   10, 3231,    8,   10, 3522,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 2983,   84,   11,    6,   13, 3024, 7927, 4719,    0,    8,   33,
          172,  132,    6,  307,  110,    0,  125,   13, 1027,  631,  407, 3565,
           25,  432,   13, 2767, 6204,    0,   84,   11,    6, 7008,   80,   39,
         1061,   35, 1315,  322,  183, 5848,   10,   46,   24,  321,  290, 4971,
            0,  498,    7, 4971,   93,  979,    0,   67,   70,  281,   94,  192,
           11,   18, 2252,   26,   17,   69, 2313,   21, 1847, 3497,   10, 1421,
         1113,  109,  143,   55,    7,    9, 1610,   45, 1032, 1809,  424, 1420,
          233,  373,   10,  276, 1772,   10,  575,  132,    0,    2],
        [5902,  589,    6,   69,  837, 1827,  162, 6386,    0,   38,  597,  269,
          340,  407,  323,   21,    0,  347,   73,   11,   18,   24,    3,   19,
         1850,   62,   39, 2792,   69, 5063,    8,  434,   21,   38,  290, 2078,
         1525, 6664,    0, 9413,    8, 9696, 2419,    3,   19, 8063,   48,   13,
          630,   10,    7,  640,    3, 6463,   12,    7, 2672,   85,    7,  183,
            0,   38,  412, 2366,   26,    7, 1802,  322,   12, 8338,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  203,  779,  675,  793,   26,  142,   10, 3128, 2129,   12,  248,
         3834,    0, 2129,   24,  164, 8896,  117, 1752,   35,    6,  335,   18,
         1617,  457,  183,   35, 2470,    6,   96,    9, 1764,   20,  484,  480,
         1081,   12,  108,  447, 2573,  131, 1520,  203, 7712,  237, 2332,    0,
          109,  994,  117, 3382,    6,   63,  142,   10,  175, 7154,   48,    9,
          291, 1076,   20,  484,  480, 1081,   86,   12,  108, 2573,   46, 1187,
          111,    0,  131,   82,    0, 1599,  109, 2145,  586,  271,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([66, 75, 91, 82, 94, 72, 84], device='cuda:5') tensor([1., 1., 1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
 > at.  tensor(153.3918, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[  29,    0,   25,  ...,    1,    1,    1],
        [   8,  180,    7,  ...,    1,    1,    1],
        [   7, 4821,   10,  ...,    1,    1,    1],
        ...,
        [ 540,   53, 3122,  ...,    1,    1,    1],
        [   8,   19,   11,  ...,    1,    1,    1],
        [ 635,   24,  296,  ...,    1,    1,    1]], device='cuda:5') tensor([13, 13, 20, 13, 14, 18, 18, 10, 16, 11, 16, 15, 17, 11, 19, 12,  6, 12,
         8, 17, 12, 21, 13, 12, 15, 18, 17, 15, 19, 24, 15, 14,  9, 18, 11, 16,
        15, 14, 12, 21, 14, 15, 15, 11, 21, 14, 20, 17, 23,  7, 17, 14, 12, 14,
        15, 13], device='cuda:5') tensor([1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8799, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 0.8286,
        1.0000, 0.8652, 1.0000, 1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8677, 1.0000, 0.8550, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8882, 1.0000, 1.0000, 1.0000, 0.8105, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 0.8521, 1.0000,
        1.0000, 1.0000], device='cuda:5', dtype=torch.float16)
 > at.  tensor(21.1390, device='cuda:5', grad_fn=<MulBackward0>)
False tensor(15.5532, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[   8,   21,   11,    6,   13,   24,  237, 1355,   54, 1909,    0,   19,
          154,    0,    9, 8041,   12, 4972,    0, 3982,  366,   12, 4972,    0,
            8,   19,  154,   33, 5510,   12, 1102,  639,    8,  998,    9,    7,
         1136, 5055,   26,   39, 1909,  206,    7,  832,    0,    6,    0,   73,
          172,  305,   13, 5247, 2883,    0,    8, 7419,   26,   91,  663,    0,
            2,    1,    1,    1,    1,    1],
        [  29,  168,   25,   66,   13,  567,    0,   25,   66,  250,   46,    8,
           84, 1631,  227, 8263,   12,   17,  993,    9, 3804,   46,  185, 8829,
         2385,   39, 4312,   22,    0,    8,    7, 4312,   22, 3060,   96,   10,
         3793,    0,    8,  288,  120,   13, 6737,  925, 1585,   17,  188,    7,
          230, 1051,  174, 1991,  568,    7, 5250, 1085,    0, 1239,    0,    2,
            1,    1,    1,    1,    1,    1],
        [  84,  188,  226,   39, 2244,   48, 2080,  199, 2627,  106, 1146,    0,
          106,    0, 5036,    0,   67, 5830,    0, 2288,    0,    8,   77,   12,
           13, 5827,   24,  144,   33,    0, 3024, 2244,    9,    7,  245, 1345,
           12,   33,    0,  464,    0,  347,    0,   19,  154,   84,    0,   63,
          391, 2826,    0,  248,  535,   35, 4935,    0,    0,    0, 1649,    8,
            7, 7678,    0,    2,    1,    1],
        [  19,  246,    0,   38,  187,   11,  158,   87,   21,  106,  384,  237,
          803,    3,   53,  246,    0,   38,  469,   57,   11,    6,  211,  207,
           25,   11,   57,  142,   10, 6785,   13,  759, 6052,   35, 3481,  176,
           12, 2811,  839, 2202,    9,  384,  237,  803,    3,   29,    9, 6769,
            0,   19, 5016, 1222,   13, 6478,  244,  699,  411,    8, 3122,   10,
          264, 1027,  119,  232,    0,    2],
        [ 101,  246,    0,   38, 3127,   19,  692,   10,   51,   13, 2182,  684,
            3,    8,  101,  246,    0,   38,  200,  969,   19,  278,   10,    7,
         8459,  464,   12,  670,    0,   89, 3540,  465,   11,   18,  305,   21,
         6064,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19, 1060,  267,   80,  267, 3013,    0,    8,  225,  246,    0,   38,
         1969,  135,    0,  635,   19,   11,  158,  618,    0,  635,   19,   11,
          158,   14,    0,   67,   19,  192,   11,   18,   66,   13, 3013,    3,
          225,   34, 1922,  200,   54,   51, 1795, 2073,   18,    0,  166, 6121,
          267, 7150,  244,    7,  215,   10, 1393,  133, 1665,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [ 476,   25,    0,   68,  386,    0,   65,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 101,  591,   17,   53,  162,   56, 8862,   10,  384, 1050,  140,   54,
            7, 7522,  469,   18, 1568,    0,   10, 9547,  945,   80,   70,  506,
           51,    0,    8,  101,  591, 1730,   17,   53,  465,   11,   18, 1799,
          238,   71, 6765, 1955,  109,  802, 6027,   69,  251, 6765, 1955,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:6') tensor([61, 60, 64, 66, 39, 59,  8, 49], device='cuda:6') tensor([1.0000, 1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 0.8110, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(103.0365, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[ 225, 1119,    0,   38, 1461,    0,   17,   11,    6, 1284,    0,   70,
           26,   21,    0,   26,   17, 7148,  650, 3727, 1994,    3,    8, 4069,
            0,   19,   73,   51,   13,  277,  593, 4187,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,   34,  133, 3315,    8,   19, 1385,  185, 4673,  214,    8,
          442,  185, 1968, 3051,    6,   17,   19,  213,   10, 1452,   71,   25,
          440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 294, 3940,    0,   68,  386,   65,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  251,  370,  215,    0,   89, 1970,    8, 3460, 1102,   14,   48,
           12, 1167,    0,    8,   19,   34,  914,  244,  221,  505, 1563,   80,
         6088,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  162,  461,   12,   33, 5673,    0,   70,  169,   25,  289,
            0,   63,   25,  850, 1203,  494,    0,    0,    0,  238,    0, 2859,
           46,  138,   87,   25,    0,  276,  135,    0,  125,   25,   11,   57,
            0,   86, 3531,   10,  456,   80,   21,    0,    2],
        [  29,   33,   26,   86,  116,   13,    0, 6429,    0,   80, 7291,    6,
            0,    0,   21,   11,    6,   13, 6429,   80, 3295,    9, 1146,   79,
          238,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  89, 2574,   26, 5205,    8, 4970,   63,    7,  473,  825,  120,    7,
         3022,  188, 2096,   10,   87,   21, 1574,    0,    8,   24, 2775,   11,
           18, 5768,   62,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 276,   13, 2900, 6815, 5707,   62,  392,  439,  143,    9, 7001,  254,
          225,  323,    9, 8979,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [8310,   54,   17,    0,    7, 2468,   12, 2573,    0, 1067,  525,  436,
            0,   17,   19,   34,  752,   10,  747, 2653,    0,  288,   51, 1217,
            6,    9,  248, 6874,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,    7,  207,    7,  227,   93,   45, 1604, 2334, 2895, 5070,  170,
           46,   24,   11,  121,  115,  278,   13, 1234,   10, 9375,  347,  117,
          227,   93,   45, 1604, 2334,   63, 7071,  341,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   53,  245,  487, 2980,   54, 2263, 1315,    0,   21,  220,  305,
           79,  294,   79, 1111, 6052,   12, 2533, 1390,   10,  229,   13, 1127,
         1116,  569,   12, 2263, 1315,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 1797,    6,    7,   39,   22, 1522, 3667,    8, 2322,   12, 7932,
         2673,   96,    8, 4157,    6,  142,  270,  490,    7,   69,    6,  307,
           12,    7,  473, 2217, 1137,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  961,  736,  439, 7692,  629, 2481, 2286,   62,  235,
            8, 5225, 5108,    8, 2288, 5225, 5108,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  388,    9, 3189,    0,    9,    7,    0, 1832, 2930,    0,   84,
            0, 7774,   62,   77, 1587,   12,  993,  106,    7, 1465,    0,   80,
         2192, 6631,  793,    0,    0,  281,   12,  166,    0,   19,  465,   11,
           18,  661,    0,    2,    1,    1,    1,    1,    1],
        [   8,   21,  875,    6,   25,  199,    7, 7771,   12,   70,   11,    6,
          226,  434, 1440,    0, 2243,   59,  827,  760,    0,  883,  424,    0,
         1579,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,    0,  154,  281,   12,  134,  169,   51, 8661,    8,  958,  756,
            0,    0,    0,    8,    0,   79, 5474,    0,  294,   12,  134,  169,
          914,   51, 7078,  706,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([34, 27,  7, 27, 45, 27, 29, 18, 30, 34, 31, 32, 21, 40, 28, 30],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8877, 0.8564, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 0.8223],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(56.7479, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[   9,   33, 1760, 1165,    0,    0,   24,  162,  802,   13, 2270,   45,
            0,  109,    0,    0,  339,  110,  116,  150,  103,    0,   19,   73,
          175,   33, 4481,    0,    0,   13, 9238,    0,  217, 1076, 5997,   56,
         8368,  922,    0,    0, 2806,  240, 2175,  249,    0,    2],
        [   7,  225,  158,   12,    7, 1052,  249,  781, 1158,   34,  561,   48,
          199, 1580, 6032,   85,   13, 2290,   17,   24,   11,   57, 2438,   54,
          131,    7,  467,   12,   33, 1910,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    9,  251, 1113,    0,   19, 1608,   11,   18,  283,   69,    7,
         6389, 6035,    0,   19, 1608,   11,   18, 1927,   13, 4446,  109, 2618,
           39, 5651,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  144,    7, 1666,  941,   35, 9748,  688, 9372,    6,   17,  842,
          785, 1601,   10,   82,   45,  132,    8,  180,   25,  162,  859,  665,
           13,  321,   12, 4594,  111, 1650,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,   77,   12,   13, 5827,    0,  168,   19,  217,    0, 6815,
          241,  829,  106,  747,  670,    9,  564,  957, 1132,    0,    8,   33,
         1148,    0,    8,   24, 2252,   17,   38,  290, 1408,  197,  513,  755,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 1043,   19,   11,   45,   29, 7105,   80,  467,   54, 3753, 3023,
         1382,   11,   18,  116,  125,   21,   11,    6,  999, 3023,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,  154,   80,   21,   79,   13, 1780,   35,   18, 1397,    0,
         1523, 1909,   12,  294, 2386,   12, 1655,   12, 2602,    0,  166,   26,
         2546,  829,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1831,   17,  103,   19,  220, 3665,  929, 7897,    0,   19,  220,
         4075,   17,    0,  432,   77,    0,   19, 1359,   11,   18,  172, 2702,
          111,   19,  158,    0,   21,   34,  185, 4416, 4113,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    9,   33, 1165,    0,   21,   11,    6,   39, 1663, 1039,  369,
           55, 2327, 1034,  240,   20,   69,  117, 4601,  825, 3275,   35, 3794,
         2885,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   56, 3168, 2445,   93,   11,    6,   13,  133,  528,  461,   12,
            7, 9655,    8,   24, 5150,  108, 1613,   10,  274,    9, 1820,   55,
         4455,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   69,    7,  218,    0,  874,    0,   24,   66, 1075,   17,
            0,   63,  244,  737,  891,   54,    0,   69,  837, 2694,    0, 5955,
           10,  159, 6768,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2072,  988,   26,  250,   17,  689,   11,   18,  100,   10, 2895,  133,
          261,    0, 3706,  359, 7036,    0,    8,   12,  538,   24,  169,  100,
           10, 1082,  143,   80,   21,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    7,  218, 2475,   12,   81,   18,  340, 8978,   26,    7,
          227,  424,    6,  560, 5824,  424, 1825, 1158,    0,    8,  117, 1816,
          985,  132,  227,  760,  233,    6,   55,   13, 1074,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1980, 1068,    0,   24,  296,   13,  574,  833,   12,  524,   35, 1105,
          505,   54,    0,   86,  116,  524,   35, 5001, 3635,  833,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   33,   34, 2861,   62,  131,   39,  786,  727,   46,  166,   26,
          100,    0, 1730,    9,    7,  467,    0,  101,  164,   86,   66,  995,
            0,  125,   21,  164,  417,  827,  603, 1906,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,   87,   25, 1005,    7,  909,  609,    6,  369,   12,    0,    7,
            0, 2018, 2510,  429,   12,    7, 1683,  724, 1025, 6433,   54,    7,
         8094,  429,    0,   12,    7,    0, 1478,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([46, 32, 28, 32, 38, 24, 28, 35, 28, 27, 29, 31, 35, 24, 34, 33],
       device='cuda:3') tensor([0.8242, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 1.0000, 0.8706],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(52.0802, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[   7, 3288,   63,  ...,    1,    1,    1],
        [  53,  113,  180,  ...,    1,    1,    1],
        [ 282,   26,   80,  ...,    1,    1,    1],
        ...,
        [4069,    0,   21,  ...,    1,    1,    1],
        [2869, 3540, 4712,  ...,    1,    1,    1],
        [  33,   26,    7,  ...,    1,    1,    1]], device='cuda:3') tensor([15, 15, 16, 13, 21, 10, 18, 12, 12, 14, 16,  9, 14, 19, 15,  8, 15, 16,
        15, 12, 10, 15, 19, 18,  8, 23, 16, 18, 13, 12, 11, 20, 19,  8, 15, 18,
        11, 23, 15, 13, 18, 15, 15, 11, 16, 13,  7, 14], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8711,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000], device='cuda:3', dtype=torch.float16)
 > at.  tensor(22.7148, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[   8,   46, 2307, 1008,   33,  283, 7772,    0,    0,   38,  160,  119,
          480,    0, 6880,   55,  155, 5223,  197,   46, 1363, 1850,   37,    0,
            0,    9,  545,    0, 1227,    0,   33,   34,  138, 7803, 1808,    0,
            0,  192,   11,   18, 3995,    0,    0,   13, 1192,  552,  126,    0,
           21,  144,  211, 3236,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,    7, 4174, 2078,  487,    0,    7, 1762,   12, 2718, 2202, 6611,
         6671,    9,    7,  774,   12, 5428,   34,   79,  488,   79,    7, 1762,
           12, 2718, 2202,  850, 3909,  607, 4357,  407,    9,    7,  774,   12,
         1947,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   85,    7,  467,   12,   21,    0,   24, 8152,   48,   17, 2878,
           12,  708,   73, 1082,   10,  368, 2930,    8,    7, 1465,   69,  159,
          447,    0, 5333, 2515,  366,   12,  148,  109,  206,   53,  162,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24, 1405,   13,  846, 4399, 7016,   12,   13,   56, 2515,  292,
         2404,    0,    8,   25,   73, 3684,  375,  688,  359,  341, 8829,    6,
           17, 2231,  341, 7125,    6,    0,   29,   17,   73,  601,   25, 5415,
           70,   11,    6,    9,    7,  563,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   11,    6,  916,   26,   53,  323,   21,    9, 2008, 1491,    8,
         6343,    0,  206,   21,   11,    6,   38,  626,   57, 6505,  128,   10,
          508,  561,  804,    6,    3,   53,  144,   10,  508,   21,  113,    9,
            7,  832,    0,    6,    0,   10,  175, 1719,  292, 2460,    0,   29,
           19,  154,   84,  162,  391,  832,    0,    6,    0, 2116,    9,  132,
         6034,  264, 1736,  148,  162,  461,   12,    7, 3677,    0,    2],
        [   0,   33,   26,   13, 1523, 7376,    0,    0,   21,   11,    6,   39,
          985,  687,    0,    0,   21,   11,    6,    0,   13, 4912,   12,   77,
         1587,   12,  813,    0,   86,  116,   80, 5807,    8,    0, 9827,    8,
         6689,    8,   29,   69,    0,   67,   80,  896, 2604,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   21,  169,  492,   66, 7747,   10,  110,   10,  154,   17,  116,
          125,   19,  144, 1157,   13, 5462,    9,  166,   13, 2232,   34,   13,
          227,   37, 1010, 2650,   37,   17,  101,   34, 3524, 2015, 1197,   12,
           77, 4039,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  169, 1017, 8788,   55,    7, 1732,    6,    0,    8,  780,   10,
         2895,   71,  134,    9,   13,  207,   17,   34,   79,    6,  762,  366,
         1094, 3835, 1256,    0, 6510,   54,   13, 2866,  979,   12, 3802,    8,
         7376,    9,  166,   24,  220, 1082,   10,  283,  540,    8, 1766,   91,
          486,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([54, 39, 37, 44, 71, 48, 40, 51], device='cuda:7') tensor([0.8584, 1.0000, 1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(86.2638, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[ 24, 498,  71,  ...,   1,   1,   1],
        [ 33,  34, 245,  ...,   1,   1,   1],
        [ 87,  21, 238,  ...,   1,   1,   1],
        ...,
        [886,   0, 476,  ...,   1,   1,   1],
        [ 29,  19, 415,  ...,   1,   1,   1],
        [596,  73,  66,  ...,   1,   1,   1]], device='cuda:7') tensor([ 6, 10, 10,  7, 10,  7, 10, 11, 10, 10,  6, 11, 10, 14, 10, 12, 14, 18,
        14,  8,  8, 14, 13, 11, 11, 15, 12, 11, 10, 14, 16, 10, 13, 20,  8, 17,
         9,  9, 21, 10, 17, 10, 12, 12,  8,  8,  8, 12, 14, 11, 10, 11,  9, 10,
        11, 12,  7,  9,  8,  4, 10, 12, 10, 13,  9, 14, 11, 15, 11, 10, 12, 14,
         8, 11,  7, 12,  9, 13,  9, 14, 13,  9, 13, 12,  8, 10, 10, 13],
       device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8633, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8174, 1.0000, 1.0000, 0.8384, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8755, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8091, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000, 1.0000, 0.8579,
        0.8691, 1.0000, 1.0000, 0.8516, 1.0000, 1.0000, 0.8652, 0.8286, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 1.0000, 0.8564, 1.0000,
        0.8223, 1.0000, 1.0000, 0.8276, 1.0000, 1.0000, 0.8394],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(15.8014, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([1.0000, 0.8086, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(86.8365, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[  25,  135,    0,   25,   11,   57, 2465,    0,   25,   66,   77,  117,
          214,  142,   55,   25,    0,   38,  511,   19,   11,   45,  100,    3,
          125,   19,   11,   45,  168,    8,   21,   11,    6, 3939,    0,    8,
           25,  135,    0,   19,  321,   12,  100, 2180,   18,   18,    6, 3884,
          176,    0,   38, 3794,    0,   17,   11,    6,    7, 9132,  608, 1043,
           19,   11,  121,  700, 1346,   55, 4432,   54,   10, 2240,  670,    0,
            2],
        [  29,  339,   11,    6,  498,  518,  131,  384, 2172,   54,  185, 2047,
            0,   70,   26, 7898,    0, 7898,   26,    7, 6408,   24, 1064,  120,
           24,  154,   17,  108, 2082, 2196,  220,   51,  509,  109, 8661,  103,
           24,  144,  691,  250,  341,    9,    7, 1406,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 103,   53,  169,    0, 1744,  116,   13,  277,  908,  183,    0,    0,
           69,  752,   10, 2509,  159,  207, 3298, 1573,    0,    8,   13,  277,
          523,  143,    0,    0,   69, 1880, 5917,   85,  637,    0,   53,  506,
            0,  508,  159, 3226,   13,  509, 2333,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,   66,   10,  229, 1123,   53,   11,   57, 3163,   10,   51,    7,
         3348,   12,   33,  453,  753,   12,  108,    6,    0,   13,  753,   17,
           26,  100,  211,  218,    0,   13,  753,   17,  217, 6167,   96,  110,
          333, 1127,  383,    0,   13,  753,   17,   11,    6, 2084, 1625, 1563,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [8084, 3531,  170,   10, 1103,  297,  362,    6,   20,  199,   13,  858,
           12,   70, 6204, 3071,  506,  274,  100,    9,   13, 5591,   35, 1218,
          375,  140, 1011,  179,  206,   94,   66, 2214,   10, 3901, 2465, 5259,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,  220,   51,  143, 9649,  254,   10,   51,   56,   15,  121, 3576,
           62,    9, 6462,    0,   10,   51,    7,  473,   12,  155,   94,   10,
         2056,  155, 1234,    0,   10,   66,  211,  207,   10, 2423,   69,    7,
         6266,   12,    7,   39,  430,  119, 1215,  109, 9237,    7, 4893,   12,
            7,  708,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    7, 5873, 5318,   48,   17,  244,    7, 1830,  215,   53,  162,
         2839,   54, 1723,    6,    0, 1061,  584,    3, 4039,   14,   48,  909,
         2965, 1049,  111,    0,   86,  106, 3611,    0,   67,  106,    7, 4699,
           17, 3611,   26,  999,   55,   25,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 5385,   71,    7, 2184,   59, 1775,  221,   20,    0,   71,
           33,   38, 3794,    3,    7, 4503,  465,   11,   18,   66,    7, 8511,
           12,  129, 2286, 1331,   54, 2184,   59, 1775,  221,   20, 4821,    0,
           38, 3794,  197,    9,   33,  841,   26,   13,  909, 4674,    0,    8,
           13,  909, 4674, 1847,   39, 2987,    0,  166,   26,   13,  211,  500,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:2') tensor([73, 46, 45, 50, 38, 52, 44, 62], device='cuda:2') tensor([1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(85.6748, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[ 125,    9, 2483,   35,  606, 5731,    0, 2932, 6930,    6, 5989,    8,
          447,  281,   12,    7, 2483,    6,   46, 4346,  737,    0,  742,  160,
          140,    0, 3213,   46,    8,   53, 5470,    7, 2791, 2008,    8, 3156,
          199, 2248,  713, 1777,    0,    2],
        [  13, 4495, 1435,    9,    7,  832,    0,    6,    0, 2721,   17,    0,
           12, 5990, 8459, 3665, 1118,    0,  248,   35, 8516,    6,   12,    7,
         5990,  889,  144,  708,    8,  288,   91,   35, 8516,   12,    7, 5990,
          596,  144,  708,    0,    2,    1],
        [ 168,   26,   13,   29,  300,  166,  188,  278,   77,    7, 8681,   12,
         7343,    0, 3280,    0, 1976,   25,  289,    0,   19, 3250,   21,    0,
           39,   19,  362, 1838,  235, 5384,    9,   25,    8,  180,   25, 3745,
           10,   21,    0,    2,    1,    1],
        [ 822, 2317,    9,  984,   11,    6, 6339,   17, 4877,  244, 1953,    6,
           10, 2386,   12, 1655,   12,  215, 4373,    7, 6484,   12, 7616,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  67,   70,  143,  220,  225,   66,  691,  103,  225, 1631, 1088,  144,
            7, 1885,  818,   12, 3802,    6, 2875,   10,  267,   10,  719,   13,
          841,   17,    7, 9413,   17,   94, 1095,  144,   10,   51, 1529,   62,
         3840, 4173,    0,    2,    1,    1],
        [  19, 1223, 2894,   55, 3101,  839,   69,    7, 2291,    0,   68,  194,
           65,    8,   19, 5429,   80,    7, 5986,    6,    0,    8,   19,   66,
          591,   13,  555,  214,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  66,   25, 1060, 9832,  347,    8,  138,   29,  294, 9696, 2419,    6,
           66, 6028,   48,  244,    7,  473,  640,  215,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [1223,   70,   21,  968,  110,   34,   46,   68,  194,   65,   68,  386,
           65,   46,    7, 1587,   12, 1075,   24, 5559,   63,  324, 1075,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([42, 41, 40, 27, 40, 30, 22, 25], device='cuda:1') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:1', dtype=torch.float16)
 > at.  tensor(72.0805, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[  21,  914,   26,    7, 1226,  279,   10,   87,   10, 4746,    8,  368,
            7, 3939,    8,  946, 2780,    0,   38,  511,    7, 1381,    0,   53,
         1346,  267,  546,    8,    7,  218,   94,    0,    8,   53,  246,    3,
         1768,    0,   25,   11,   57,  230,    0,   24,  451,  229,   13, 2240,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 2964,  169, 2154,  724, 2468, 7147,   69,    7, 2618,    8,  719,
          117, 1230,   35, 6937,   35, 7596, 1710,    6,  442,   12,   13, 1127,
         2468,    0,  736,  439,  203, 1255, 4711, 1755,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [7823,   94,   63,    0, 4820, 1256,   94,    0,    8, 4820, 1256,   94,
           46,    7,  143,    8,  143, 4820, 1256,   94,   84,   63,    0,    7,
          143,    8,  143,   24,   11,  158,    0,    0,   66,   13, 4820, 1256,
            0,  179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  84,   34, 9199,    0,  180,  552,  853, 3174, 4796,    0,  440,   24,
           66,  211, 3174, 4796,    0,    7, 1575,   35,  372,   59,  247,  119,
         1819, 2735,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 180,   19, 1095,  108, 2333,  158,  240, 1028,   10,  670,    9,   58,
          389,    6,    8, 2852,   59,   18, 1374, 2197,    6,    0,    8,  474,
            0,   21,   11,    6,   86,  230,   55,  110,   10, 1005,   33,  218,
          939,  668,  232,  834,   29,   19,  339,  134,  116,   46,   53,  144,
           10,   51, 1644,  411,    8, 2736,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17,  370, 4506,   17,   24,  116,  323,   71,    7,    0, 6144,  779,
          569,  164, 2676,   66,  475,   35, 2371,    0, 1683,  979,   54,    0,
            8,    0,    0,    7, 3180,  164,  289,    0,   38,  786,    0,  346,
            0,  859,    0,    0,  230,    0,   13,  176,    0,   95,  266,    0,
           17,   11,    6,    7,    0, 2636, 4057,   10, 1557,    0,   17, 1683,
          518,   10,  155, 3280,    3,    2],
        [  19,  513,  270,   10,    7, 1224,    8,  487,  665,  336,   10,  150,
          103,   19,  220,  446, 3302,  206,    0, 7490, 1390,   54, 7521,    6,
          144,  226,    0,    0, 2517,   48,    0,    8,   21, 1932,  126,   84,
          162,    0, 3695,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,  230,    0,   29,  339,  110,  508,    0,   25,   39,  663,   12,
         8681,    0,   12,   13, 1760,  321,    0,    8,   19,  213,   10, 7032,
           13, 1455,   17,   19,  154,   26,    0,    0,  133, 4501,    0,  166,
           26, 8164,   54,    0,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([50, 34, 40, 31, 56, 66, 41, 42], device='cuda:1') tensor([1.0000, 1.0000, 0.8799, 1.0000, 1.0000, 0.8735, 0.8794, 0.8555],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(77.8474, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[  67,   24,  192,   11,   18,  100, 1752, 7311,    6,   10,  368,  839,
           10,    0,    9, 2109,  673, 1115,   94,   10,    0,    0, 2231,  143,
            9,    0,  837,    0, 3762,    0,    2,    1,    1,    1,    1,    1],
        [ 117,   63,   13,  555,   12,    7, 5742,    6,   19, 1797,   62,   55,
          368,  120, 2346,  195,    8,   19, 2947,   62,   69,    7, 1868,   59,
         1029, 1997,  181,   57,   93, 1810,    0,    2,    1,    1,    1,    1],
        [   9,  409,    0,   91,   12,    7,    0,    0,  281, 8161, 1288,   34,
            0,   38, 6435,    0,    0,  339,   11,    6,   66,   13, 7862, 1109,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  474,   19,  169, 1157,    0, 5726,    6,   19,   66,   17, 7010,
           10,    0,    7,    0, 2860,   12,   25,  322,    8, 1137,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 9869,   17, 3142,  134,   77,    0,  108, 6535,  369,   10, 2718,
         2219, 3399,    6,    0,  100, 1593,  901, 5428,    8, 5222, 1947,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  71,  333, 2423,   54,  464,    0,   19, 2252,  138,  261,  143, 1894,
            7,  479,   12, 7652,  172,   26,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,    0,    9,  409,    0,    0,   21, 2947,   62,    9,   13, 1116,
           59,  679, 7778, 1227,    0,  166,   26,    7, 3715,   69,    7,  230,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,   11,   57,   86, 1449,  816,  214,    0,   53,  468,    0,   53,
         2154,  724,    0,   53, 7831,    0,    8, 4309,    6,   87,    7,  370,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,    0,   33,  546,   12,    7, 2569,   12, 3311, 2413, 3643,
          111, 4130,    0,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  274, 1387,  111,    0,   25,   11,  158,  150,   84,   11,
            6,   13,  277, 1950,    8, 7956, 1390,   17, 4974,    6,  126, 3019,
          134,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  333, 1237,    9,    7, 1384,    0,    0, 1224,    0,   25,  220,
           66,  155,  447, 1392, 1181, 1636,   12,   17,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 339,   11,    6,  274,   85,    0,  347,   53,  205,    0,   29,  238,
            0,  540,    0,    0,    7, 5462,    8,  728,  128, 1032,   93,    0,
            0,    0,  728,  128, 1032,   93,    8,  427,  335,   18,    0,    2],
        [  21,   11,    6, 5309,    8, 8509,   29,   21,   11,    6, 2426,    8,
           24,  192,   11,   18, 3188,   69, 7007,  203,  181,   59, 1159,  271,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67, 2816,    0,    7,    0, 4766, 8096,  690,  156,    6,    0, 5990,
           94,  229,  143,  839,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  296,  143, 3348,    0,    0, 3649,  188,    0,  388,   94,   69,
            7, 4241,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  162,   79,  324,   79,    7, 4059,    9, 1579,  187, 4534,  148,
           11,   48,  226, 3976,   55,  392,    8,   13,  854, 1345,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,  296,   13,  488, 1543,   12,  100,   35,  525,   48,   62,  807,
            0,    8,   29,   24,  774,   62,    7,  131,   20,  131,   20, 3434,
         4925,    6,  775, 2592,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   21,   26,    7,  245,  183,   17, 1160,   45,  188, 4691, 2101,
           18,  390,   11,    6,  874,    9, 1646,  215,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   53,  498,  952,    0,    0,    8,    0,   91,   12,  134, 1119,
            0,   38, 6435,    0,    0,  188, 2881, 2096,    0,   10, 2632,   55,
           33,  279,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  79,   13, 1747,   37,  122,   93,  684,    0,   25,   73,  934,  138,
          126,   12,  561,   19,  598,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  120,   19,  513,  270,   10, 4610,  117, 1830, 3676,    6,    0,
            7, 1470, 4272,  244,   89,  906, 4606,   62, 1360,  287,  247, 2071,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 143, 2523,    0, 3101, 4748, 1261,    0,  138,  294, 4748, 6214, 1118,
           66,  144,   13,  999,  383,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  204,    0,    7, 7990, 1726,    6,   24,   11,   57,  802,   63,
         1223,   80,    7,  370,   79,    7,  984,   11,    6, 7990, 1726,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   33,   26,  206,   94,  513,    0,   79,   18, 3304,  554,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([31, 32, 26, 24, 25, 20, 26, 26, 17, 27, 22, 36, 26, 18, 16, 24, 30, 22,
        28, 19, 26, 19, 25, 13], device='cuda:3') tensor([0.8105, 1.0000, 0.8794, 0.8315, 1.0000, 1.0000, 0.8560, 1.0000, 0.8667,
        1.0000, 0.8911, 0.8027, 1.0000, 0.8887, 0.8779, 1.0000, 1.0000, 1.0000,
        0.8398, 1.0000, 1.0000, 1.0000, 1.0000, 0.8779], device='cuda:3',
       dtype=torch.float16)
 > at.  tensor(40.8021, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[  21,   11,    6,  434, 6144,   35, 3326, 6184,    0,    8,  120,   25,
         4432,   21,   10,  419, 2468,    0,   21, 1518,  199,   13, 1629, 3351,
         1966, 7126,  174,  266, 3684,  356,   48,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 109,  728,   22, 1941,    0,  148,   26,   13, 1127, 1276,   12,  248,
            8,  213,    6,   10, 1320,  375,  267, 4159,   29,   17,  225,   73,
          205,  270,    8, 3768,  267, 4546,   11,    6, 5912,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   29,   70,   24,    0,  323,   34,   24,  842,   13, 3631,   12,
         5825,    6,    8,   24, 8425,    0,  134,   10, 6144,  879,  416,  307,
            0,  688,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   84,   11,    6, 1221, 3243,   17, 1880,  909, 3196,   48, 2658,
            6,   63, 6269,    8, 2183,   37,  254, 3753,  909, 3196,   48, 2658,
            6,    9,    7,  753,  115,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24,   11,   57,  142,   10,  274,   85,  185, 1636,    6,   12,
            7, 1406,    0,    7, 2082,    8,  185, 1636,    6,   25, 2775,   11,
           18, 1110,    9, 1296,   10,  175,   13,  841,   12,  206,  214,   63,
          142,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,   24,  824,   48, 3267,    7, 1868,  816,    6,   12,    7,
         2421,    9, 1296,   10, 8601,    7, 2610,   11,    6, 2326,   13, 1236,
          429,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 682,    0,   77,  230,    0,  339,   11,    6,  150,   17, 1228,  180,
            0,  125,   33,   26,  204, 1248,  120,   25,  154,   80,   21,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  86,   91,   11,    6, 1848,   11, 2332,    6,   69,   91,   11,    6,
           51, 6979,    0,   67,  120,   91,   11,    6,    0,    0,  447, 2332,
            6, 2141,    0,   10, 5204,    0,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,    0,   77,  401,   21,    0,  192,   11,   18, 1233,
            0,   79, 2392,   79,   25,  498,    0,  952,    0,    0,   25,  498,
          802,  155, 3200,   54, 1501, 4838,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 1808,   71,   13,  822, 3580,   37, 3085, 2092,   10, 1157,    7,
         1097, 1486,    0,  180,   13, 3603, 2092,    0,  180,   39, 2781, 2092,
            0,  180, 1192, 2092,   96,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  85,   13, 1905,  613,    0,   33, 6652,  369, 8314,    0,    9,  564,
          895,  895,   46,  939,  533, 7898,   17,   24,   63,  291,  457,   10,
          368,    7,   56,   15,  140,  849,   62, 2468,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  225,  188,  391,   29,   22,    6,  148,  225,    0,  388,  359,
         2108,  140,  271,  128,    0,  670,    0,    8,    0,  225,   11,    6,
          113,    7,    0, 5949,    0,   12,  267, 1749, 2346,  424,    0,  109,
         4586,    6, 1361,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   39, 8151,    0,   38,  779,   21,   11,    6,    0,
           55,  110,    0,   13,  453, 2913,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   11,   57,  142,  359,    7,  564,  868, 1047,    6,    0,
           84,   11,    6, 2154,  338, 2256,    0,   84,   63,  214,  142,   69,
            0,   67,   21,   11,    6,   77,  321,   12,    9,    7, 4579,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  244, 2371,    0,   29,   19,  116,  692,   10,  289,
           25,   73,    0,  103,   25,   11,   57,    9,  264, 1736,    0,   25,
           73, 2981,  126,   89,  283,   85,    7, 4656,   18, 2739, 2582,   15,
           22, 1010,  558, 1822,    0,    8,  113,   85,  523,  891,    6, 1492,
          158,   37,   93,    9, 1922,  237,    6,   20,  290,    0,    2],
        [   9, 4936,   10, 6461, 1234, 3707,   21,    0,    0,  113, 2828,   96,
         3238,    8,  415,  122,   22, 2278, 4340, 2806,   15, 1255,    8, 4340,
         4323,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([33, 35, 28, 31, 39, 27, 25, 33, 32, 31, 34, 41, 20, 37, 59, 27],
       device='cuda:1') tensor([1.0000, 1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000, 0.8589, 0.8755,
        1.0000, 1.0000, 0.8149, 1.0000, 1.0000, 1.0000, 0.8989],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(55.1722, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[  24, 1704,   11,   18, 6528,  419, 6669,    6,    0,   24, 1704,   11,
           18, 6528,  419, 3677,    6,    0, 5784,   53,   11,  121,  226, 9342,
           62,  490,   53, 1808,    0,   67,   53,  465,   11,   18, 2215,    7,
         1375,    0,    2,    1],
        [ 115,   21,   11,    6, 2076,  362,  945,   10,  213,   10,  175,    7,
          170,  443,  803,  607,    6,  929,    7, 1436,  237, 1027,   18,    6,
            0,  230,    0,   10,  175,    7, 3795,  993,  929,    7, 3172,  290,
         1408,  993,    0,    2],
        [  29,    0,   21,   11,    6,  100, 3457, 3539,    0,  700,  608,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  25, 1095,    0,    0,   19,  154,    0,    7,  456,    8, 3395,   17,
           56,   62, 1820, 5589,  303,    0, 1742,  168,    0,   13, 1323, 1113,
          581,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  67,    9, 4129,  409,    0,   19, 2775,   11,   18,  442,   33, 1677,
           29,   17,   19,   73, 1290, 5366,  214,   17, 1220, 1620,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   10, 6757,    0,   55,   13, 1720,   18,  480,    0,   84,    0,
           26,   13, 3955,   45,  502,   12,    0,    0, 5506,    0,   13, 3955,
           45,  502,   12,  227, 1411,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [1088,   45,    0,   55,  110,    0,   89, 1276,   34,  909, 1803,   54,
          939,  458,  834,  120,   19,   34,   13, 1269,   10,  873,   13, 3280,
            0,   67,   19,  172,  465,   11,   18,  213,   10,    0,    2,    1,
            1,    1,    1,    1],
        [  13,  488, 2020,   26,    0,  138,  451,    7,   19, 1203,   48,    6,
           51,  619,    0,    8,   70, 1719,    6,  451,   24,  388,   69,    7,
           19, 1203,   48,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   7,   94,  148,  162, 3045,   69,    7,  830,   57,  556, 1326,  158,
          499,  144,   13,  203,    6,  494, 1317, 2958,   12,    7, 3045,    0,
            8,   53,  162,  499, 3066, 8749,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,  180,  101,  169, 2153,   80,   21,    0,    8,  101, 1831,   17,
          101,   34,   13, 4083, 1262,    8,   17,  101,   34, 2914,   57, 3449,
            6,   15,  945, 4115,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   9,   77,  117, 3302,    0,   84,   11,    6,   91, 4040,  774,    0,
          125,  117,   63,  474,    9, 2047,   12, 1710,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  53, 1082,  261, 3592,    0,    0,   53,   63,  143, 8192,    0,   69,
            7, 3516,  440,   55, 6263,  203,  430,    6,    6,    0, 6288,    0,
          288,    0, 4478,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   55,  251,   12,   25,  148,   63,   86, 1848,    0, 1487, 1059,
           93, 6977,    6,   63,  251,  277, 3434,  214,   17,  155, 1269, 4853,
            6,   10,  432,  802, 5109,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 854,    7,  800,   12, 2028,   63, 4198,   62,    9, 6243,    9, 9022,
           10,   13, 5787, 8641,  595,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   8,   33,   26,    7, 4340,   17,  101, 1742,  120,  101,   34,   69,
         3677,   46,    8,   21,   11,    6,   13,  453, 4340,    0,   38,  242,
          119,   12,   77,    3,  101, 1119,    0,   38,  340,   10, 1997,    0,
            2,    1,    1,    1],
        [  70, 1878,  323,   21,  229,    9,  159, 2332,    6,    0,  159, 4388,
           10, 1487,  362,  297,  181,   93,    0, 3647,    0,   67,   70, 4877,
            6,   55, 3647, 2302,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1]], device='cuda:6') tensor([39, 40, 13, 27, 24, 31, 35, 30, 32, 30, 22, 29, 32, 19, 37, 30],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 0.8530, 1.0000, 0.8916, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(52.1639, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[   7,  370,  521, 6906,    0,    7,  370,   29,   59, 2405,    0,    7,
          370, 1790,   12,  378,   51,   18, 3304,   62,    8,   86, 5804,   54,
           10, 2307,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  154,    7, 1043,   33, 1429,   29,  238,   26,  125, 1308, 3006,
         1997,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  24,  296,   10,  446, 1081,   12, 9695, 3834,   12, 4619,   17,   63,
          143, 4414,    0,    8,  192,   11,   18,  735, 6430, 2423,   54,  518,
          108, 3916,  199,  108, 5627,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8, 5493,    0,  101,    0,  246,  101,   34,  142,   10,  175, 4532,
           12,    7,    0, 2902,    6, 4775, 1890,   46,   68,  194,   65,   46,
            0,    8,    7,    0, 2073,   45, 4775, 1890,    0,    0,  593,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 115,    0,    7, 3966, 1472,   12,   33,    0,    7,  305,   35,  176,
         2175, 2187,  106,   13,  740, 3238, 4774, 1109, 5412,   26,   33,    0,
           70,   24,  979,    0,   24, 1082,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   0,   67, 1223,    0, 1155,  261,   26,  142,   69, 1325,   25,  498,
            0,   10,  175,    0, 6509,  411,  266, 5408,    0,    0,    8,  131,
           17, 2110,    0,    0,   25,   11,   57,   86,  665,  453,    0,   25,
           11,   57,    0,   86, 1790,  453,    0,   25,   11,   57,   86, 1057,
           17,  261, 2314,    0,    2],
        [   7,  744,  279, 4673,   80,    7, 1579,  371, 1898,  424,  371,   26,
            0, 2027,  199,  801, 1137,   46, 1992,   10, 1536,  215,  801,   46,
          117, 1816, 2924,   11,   18, 2532,  886,  371,  322, 1921,    0,   53,
           11,   57, 2532,  110, 1488,   35,  424,  371,  322, 1921,    0,    2,
            1,    1,    1,    1,    1],
        [   7,  218,  461,   12,    7,  546,   63, 2720, 2261,   17,    7, 2932,
         1381, 1427,   69,   77, 1881, 6101,   69,    7, 2932, 1465,    0, 2134,
           79,   13,  567,   12, 1244,   35,  607,    6,  140,  763, 1386,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:6') tensor([28, 16, 31, 37, 32, 53, 48, 37], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 0.8457, 1.0000, 0.8691, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(73.1575, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[   7, 9449,   34,  142,   10,   66,  248, 6734, 2028,   10,   21,    0,
           91,   34,  206,   24,  175, 5744,   10, 3793,  518, 3632,  131,   13,
         3941,   22,   18,  684,  199,   39, 1404, 4925,    0,    8,    7,  744,
           34,    7, 6815,  241,  271,   10,    7, 1692,    0,   24,   11,   48,
         4938,   13,  316,  362,  532, 1722,   12, 2468,    8,   24,   11,   48,
         3793,  199,   21,    0,    2],
        [1979,    6,    0, 1189,    0, 6071,  371,    0,   67,   12,  538,    0,
         4059,  570,  159, 3181,   45,  693,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   79, 2392,   79,   53, 1095,  110,   53,  246,    0,   38,  533,
          296,   13, 3592,  979,  240,    8,   13,  509, 5991,    3,   68,  194,
           65,   19,   34,  475, 6011,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21, 1518,  126,   17, 1482, 1067,  362, 2304, 2871,  168, 2396,   10,
            7,  227,   22,  293, 4037, 1300,   17,  568,    7, 2696,    0,   86,
           71,   13, 3666, 3210,    0,   86,   71,   39, 2797,   12, 1703,  445,
            0,   67,  116,  246,    0,   38, 6435,    0,  169,   25, 1066,    3,
            7,  723,  246,    0,   38,  679,    0,   17,   11,    6, 3495,    3,
            2,    1,    1,    1,    1],
        [   9,   81,    6, 1955,  206, 1459,    9,   89,  775, 2592, 1559,  159,
          447,  964,   67,   84,   11,    6,  211, 8019,    0,    8,    9, 1052,
         2023,  227, 1996,   18,    6,    0, 1459,   69,    7, 3686,    9,   91,
          964,   71,  211, 6879,    6,   67,   71, 8019,    0, 3761,  961,   21,
            7,  509, 4387,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   33,   26,   46,   19,  150,   33, 3195,   54,  126,   77,  244,
           46,   33,   26,  108, 2696,    9, 1267,  158,  480, 2142,   18,  727,
          804,    0,  690,  241,    0,  717,  215,  581,    0,  206,   53,  245,
         1095,  159,  245, 2930,    0,  115,   53,   66, 2930,    9,  159, 5003,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,   24,   77,  229,   13, 5710,   30,  176,   10,   17, 2262,  620,
         1591,    0,  125,   24,  598,  100,  103,   24, 2924,   11,   18,  752,
          778,    0,   21,   11,    6,   79,  103,   24,   11,   57,  401, 1155,
            8,   24,   11,   57, 9796,   54,   69,  108,  403, 3034, 1428,   10,
          108,  807,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6, 3118,  528,   46,   33, 1382,   11,   18,   69,    7,
          179, 4038, 2688,    0,   26,   21,    0,   19,   11,   45,   80,   10,
          175, 2182,   48,   46,   21,   11,    6, 3118,  528,   17,   25,  492,
          388,    7, 3316,   12,  155,  282,    9,    7, 1860,   12,   13, 4890,
         8182,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:5') tensor([65, 20, 31, 61, 53, 51, 52, 51], device='cuda:5') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
 > at.  tensor(86.5499, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[   8,   29,  103,   19,  162,  142,   10,  508,   25,    7,  467,   54,
           12,   33,  546,    0,   21,  169,  205,  250,  100,   33,    0,    8,
           17,   11,    6,   70, 5017, 2912,  110,   10, 4379,   10,   25,  168,
           85, 1366,   80,  546,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,    7, 1043,   55,   33,    0,    7, 1043,   17,    7, 5367,
         2735,  689,   11,   18,   66,  419, 4585, 2836, 6672,   26,  125,    7,
         4910,    6, 1446,  535,  581,   17, 1719,  293,  356,   26,  593,  832,
           18,  233,  235, 4356,   10,   56, 2917,  597,   93,   55, 4585, 2836,
         6672,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1037, 4343, 6976,   34,   69,    7, 1832,    0,    0,    8,   89,
         3780,  246,    0,    0,   38,  779,    0,  995,  132,   84, 2320,   10,
         4432,    3,    8,    0,  225,  321,   12, 1260,  346,    7, 2884,    0,
            8,    0,    0,    0,  225,  246,    0,   38,  174, 1069,    0,    0,
           94,  540,    3, 3111,    0,   24,  144,   13,  207,  199,    7, 2469,
            0,    2],
        [ 115,   19,  217,  961, 1819,    6,    0,  473,  464,    0,   19, 5893,
           13, 1819,  434,   38, 2347,  588,  929,  889,    3,   38, 2347,  588,
          929,  889,  197, 3447,    6,   10, 1261,    0,   67,  486,  461,   12,
          108, 4990,  589, 1261,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19,  474,   19,   11,   48,  456,   13,  277,  523,   80,  185,  116,
          488, 1288,   80,   33,    0,    8,  180,  175, 3570,  270,  126,  168,
           29,   24,   73,  456, 2895,  366,  111,   13,  277,  523,  143,    8,
          154,    8,  884, 1532,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   70,    0,   19,  323,   26,    0,   19,  842,   77,   33, 1221,
            8,   19,  442,   21,  199,    0,   13,   46,  238,    0, 1223,   21,
           11,    6,    0,   13, 2913,  682,  455, 1434,    0,   12,   33,   91,
         4700,    0,    0,    8,   21,    0,  595, 2334,   13,  316, 1076,  266,
          436,    0,   12,    0,  284, 4896, 4437,   69,    7,  270,    0,    2,
            1,    1],
        [   9,  117, 2822,    6,    0,   25,  150,   39, 5020, 4373,   62, 4358,
            0,    8, 6013, 1708,  117,  824,  569,    6,   12, 1593,   18,   63,
          117, 3275,    0,  203,  140,   18, 1409, 2130, 1318,    6,  166, 2456,
         1646, 5494, 4653,    7, 2059,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   33, 1119,   17,   21,   11,    6, 3756,    0,  131, 5057,    0,
           10, 2032, 1456,    7, 1503,   46,   19,    0,   20,    0,    0,    7,
         3140,   46,    8,    7,  765,  502,   12,   13, 8683,    0,  125,    7,
         1529,   12, 8634,   21,    0,  131, 5057,    0, 2317,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:5') tensor([42, 51, 62, 42, 42, 60, 43, 48], device='cuda:5') tensor([1.0000, 1.0000, 0.8237, 1.0000, 1.0000, 0.8369, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(79.3042, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[   8,  225, 1321,  270,    8,  568,   33,    0,   38,  187,  135,    3,
           68,  194,   65,    8,  225,  246,    0,   38,  533, 6090,  155, 1366,
          456,    9,   89, 1227, 7005,    0,  180,   24, 1157,  155, 1227,    8,
           24,  203, 6360,   48, 1469,   11,    7, 2033, 6665, 1031,  242,    6,
            0,  501,  197,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 204,    0,   19, 1042, 4744,  100, 1421, 1086,   10,  150,   13,  133,
          453,   13, 2553,   10,   87,   13, 3479,   12,   38, 1778,  749,  293,
            3,    8,   19, 1831,  172, 6760, 3785,    0,  125,  131,    7,  183,
            7,   13, 2553,  487,  378, 4940,  749,  293,    0,  101, 4609,  378,
            7,  453,   13, 2553,   17,   19,  144, 4744,  839,   10,  150,    0,
            2,    1,    1,    1,    1,    1,    1,    1],
        [ 103,    7,    0, 2313,    0, 1137,    9,  155,  753,   26, 3497,    0,
           10, 7892,    0,    0,  115,    7, 2313,    0,  723,   26,  665,   85,
         8481,    0, 1666,   35, 1921,  307, 8481,    0, 1259, 4418,    0,  415,
         1252, 1020,  998,   37,   93, 1599,   46,  214,   17,    0,   63,    9,
          560,   15,   18,  111,  143,    0, 1623,   10, 2644,    0,    8,  261,
          143, 4167,   10, 2644,    0,    2,    1,    1],
        [  19, 8547,  111, 1621,   13, 5063, 2672,    8,  434,   21,   38,  533,
           63,   77,  421, 1054,   62,  246,    3,    9,  116,  391, 1113,    0,
            7, 2672,  144,  244,    3,   94,    0, 5996, 5902,  589,    6,  148,
         4240,    7,  370, 6429,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34, 6583, 6071,   57,  436,    0,   84,   19,  205,  554,
           46,   19,   73,   11,   18,  601,   21,    0,   21,   11,    6, 1301,
           46,   68,  194,   65,   19, 1621,   13, 6762, 5831,   69,  384,  237,
          455, 1404, 1386,    6,   17,  473,   62,   55,   13, 1323,   12,  215,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 830,   48,    0,  106,    7,  183,   19,   34,  133,  963,    0,   69,
         1820,    0,   19,   34, 1800,   13,  325,   12,  341,  203,  809, 1921,
         1331,  233, 1611,    0,    8,   21,  735, 4639,   10,  110,    0,  120,
           19,   34,  963,    0,   17,  778,   34, 1067,  494,  126,  490,  110,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  409,    0,  120, 1183, 3565,   25,   53,   11,   57,   13,  227,
         1147,  249,  440,    0,   53,  976,  261,  641,   13, 2291,  227, 1147,
          249,    0,  125, 1363,    6,  901,  232,    0,   21,  842,   80,  785,
          215,   55,   21,   10,   14,    0,    8,   85,   17, 2110,    0,   19,
           11,   48,  226,   13,   38,  311, 5296,  369,  197, 8823,   55, 1498,
          215,    0,  166,   46, 2290, 2592,    0,    2],
        [1552,  254, 2456,   71, 8760,   22,    0, 4456,    6,    0, 8393,  589,
            0, 1201,  588,    0,    0, 2008, 2593, 3284, 1727,    6,   54,    0,
         3348,    0,  108, 4341,    0,  446,   21,  143, 7553,    0,   10,  456,
           10,    7,   97,  181,    8,    7,  179, 2535,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:7') tensor([52, 61, 66, 42, 50, 50, 68, 46], device='cuda:7') tensor([1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 1.0000, 1.0000, 0.8506],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(104.0310, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[  33,   26,  204,   13, 3869,   17, 4974,    6,   85,    7, 2937, 1408,
         7174,   85, 6579, 2519,  670,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   33, 6735,   12, 1396, 2233,  106,  378,  116,   13,
         1466,   37,  199, 1692,   37,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9,  116, 1992,  215,    0,   24, 4311, 1469,  132,   10,    7, 4389,
           12, 6204,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,    9,  409,    0,   24,   66, 2134,   55, 7691,   80,    7, 9270,
           15,   18,  768,   12,   29,  237, 5997,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  33,   26,    7,   51, 4999,  741, 3647,  941, 2394,    9, 4517,    0,
          166,   26,   91,   12,    7, 1849,  608, 3632,    9,    7,  179,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  67,  735,   24, 1566,  371, 1993, 2482,   10, 3858,    0,    0,   71,
          267,  245,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,    9,    7, 1165,   12,    7, 1291,  821,    0,   84,  204,   66,
          226,   91,  109,  248,   17,   66,  226, 1110,    9,    7, 2533,    0,
            2,    1,    1,    1,    1,    1,    1],
        [   7,  279,   80,  283,    9, 2018,   22,  713,   26,   33,    0,   53,
          192,   11,   18,  403,    6,   96,    6,   80,  251, 7806,    6,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2884,   12, 7470,    6,  568,  450,   25,  250,    0,    8,
         1275,   21, 3565,   25,  250,   17,   11,    6, 1226,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 101,    0, 1918,   10,    0,    0,  468,  284,  427,    6, 5294,    6,
           46,   13,  264, 5931,   12,  427,    6, 5294,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   0,    0,    8,   19, 1917,   62, 1117,   12,   17,  563, 7373, 7443,
           89, 4764,    0, 4157,  169,  498,   10, 3424,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   33,   26,   13,  277, 1819,   19,  442,   80,    7,  961,   12,
            7,  417,  727,   96, 1436,  165,   20, 4879,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  25,   63, 4265,   54,   10,  575,   94,   17,   25,  661,  134,    0,
           17,   25, 2990,   71,  134,    0,   17,   25,   11,   57,  461,   12,
            7,  370, 1361,   79,  134,    0,    2],
        [  19,  213,   10, 3499,    9,   89, 1259, 5258,    7, 1890, 1455,   12,
            7, 8022,    6,   46, 4469,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  25,   73, 1157,  629,    7, 3744,    9,  251, 1532,    0,    8,    7,
         1141,  188, 1155,   10,   87,   71, 4289,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,    0, 2138,    7,  245,  785, 1601, 7396, 7833,  111,    0,  752,
           10, 2866,   89, 1259, 1882,  346,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   29,   24,  487,  826,   80,    0,  238,    0,  138,   87,   24,
          229,  251,  248, 4082,    6,    8, 1393,  134,  199,   91,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6,  347,   43,  588,    6,   26,  115,    9,    7, 4370,
           53,   63,    9,    8,   17,   11,    6,  347,   56,  525,   26,    9,
            7, 4370,   53,   63,    9,    0,    2],
        [ 903,    0,  853,   48,   22,  293,  552,   10,    7,  452, 1539, 1315,
           93,    8,  225, 1945,   62,   17,  225, 3367,  132,    9,  832,  156,
         3056,   20,    0,    2,    1,    1,    1],
        [  33,  987,  607,  445,   26,  736,  439,  442,  132,   12,    7,  225,
          158,    6,   12, 2481,    6, 5110,  266, 3529,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9,  409,    0,  108, 6180,   46,  131,   38, 3392,  197,   19,  641,
           24, 6814,    6,   46,   91,   12,  108,  488, 6180,    6,   34,    7,
          561,   22,  455,    0,    2,    1,    1],
        [  19,  220,  116,  450,   84,   34,   13, 3917,  694,  850,   21,   46,
            9,  333, 3489,    0,  333, 5082,    0,  333, 1375,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   26,  914, 3529,    0, 3706,   17,  117,   66,  226, 3572,   62,
            8, 9304,   62,  850, 4213,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [7894, 2071,  627, 5668,    6, 2027,    9,  108, 1402,  162, 2568,    9,
          564,  895,  895,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:7') tensor([19, 19, 16, 21, 25, 16, 25, 25, 23, 23, 22, 22, 31, 19, 21, 20, 24, 31,
        28, 22, 29, 23, 19, 17], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8286, 1.0000, 1.0000, 1.0000,
        0.8311, 0.8413, 1.0000, 1.0000, 1.0000, 1.0000, 0.8843, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(39.3382, device='cuda:7', grad_fn=<MulBackward0>)
False False tensor([[ 281, 1810,   45,  373,    9,  251, 1113, 1639,   69, 2248,  181, 3414,
           96,    0, 1009,  214,  100, 5032, 3157, 1794,    0, 1703, 2729, 1794,
            8, 3515,   35, 1203,   93,   54, 1794,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 138,  294,   12,   25,  168,    0,  103, 1482,   46,  185, 2465, 1339,
         2197,   46, 3336, 2432,   25,  138,   13, 1760, 3212, 4112,   26,  691,
            0,   25,  746,   12,  213,   10, 2477,  155, 4896,    6,    8,  289,
            0,   38,  679,    0,  211,    0,   19,  192,   11,   18,  213,   10,
          135,    0,    2,    1,    1,    1,    1],
        [  29,    0,   12,  538,    0, 1320,   45,  500, 1236,  675,    6,   26,
         6957,   86,   13, 8659,    0,   21,   11,    6,   25,    0,   21,   11,
            6,  110,    0,   21,   11,    6,  108, 5867, 1874,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  29,  276, 1445,   19,   34,   13, 1185,  501,  820, 1047,   38,    0,
         3938,   35,  603,  569,  415,  158,   20, 2415,  436, 5865,   37,    0,
            8, 1183,  148,  220,  467, 1049, 1230, 1551,   12, 1580, 1069, 7754,
         3454,   46,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 155, 8019, 2493, 1187,   26,   29, 3764,   17,   19,   34,  529,   10,
         5590, 5412,   21,  802,  660, 2875,  985,  111,   69,    7, 1465,   71,
          211, 4637,   54,  109, 5460,    0, 5460, 4112,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,    0,   19,   34,   13, 2988,    0,    9, 1416,    0,   12,    0,
          333, 1318,    9,    7, 4267,  276,   54,    6,    0,    0,   25,  169,
          446,  963,   94,  540, 2327,   54,    7, 3297,    6,   12,    7,  383,
            0,  109,    7,    0,  801, 3297,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   0,   21,   11,    6,  528,   17,   24,  991,   33, 5445,    0,   17,
           24,   66,    7, 3398,    0,  421,  678,  533,  153,    0,    8, 1573,
           12,    0,  108, 3348,  148,    0,    0,   63, 1211,    0,   38, 5034,
            0,   24,    0, 1412,   87,  250,  341,    3,  125,    0,   24,   63,
         7258,   62,   71,   13, 2020,    0,    2],
        [ 115,    0,   70,  103,   89,  964, 3147, 1425,   80,   89, 1751, 2079,
         1326,   19,   93,  511,  290,    0,   13, 1728,  982, 1365,  148, 5056,
            6,   13, 4158,  575,    9, 2891,  794,    0,    8,   26, 4962,   48,
           10,  450,    7, 1352,   17,   24, 5559,   10, 3995,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([33, 51, 35, 39, 35, 45, 55, 47], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8696, 0.8828, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(72.0429, device='cuda:0', grad_fn=<MulBackward0>)
False tensor([[ 166,   26,  347, 2290,  233,  824,   59,  338,  650,  583,    6, 1017,
          187,    0,   38,  469, 3920,  290,   20, 2992,   12,    7,  858,    3,
           21, 3565,  170,   80,  159, 1406,    0,   67, 3412,  369,   12,   13,
         3413, 3565,  170,   21,   11,    6, 1254,   55,  170,   10,   66,   13,
          535,  858,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [3761,    0,  103,   33,   26,  230,    0,   24, 1412,  203,  500, 1547,
            0,    7, 1880,    8, 1519, 7153,    6,    0,    8,  509,   87,    0,
           21,   71,   13,  102,  794,  378,    0,    9,    0, 1206,    0,    0,
          100,    9, 4748,   85,  969,    6, 3706,  929,    7, 8869,    6,    0,
          109,    7,  811,  140, 7710,   12,  596,    8,  848,  236, 1486,   18,
            6,    0,    2,    1,    1,    1,    1],
        [  67,   19, 1932,   10,   89,   13,  500,   18,  120,   19,  278,   17,
         3493,    8,  246,    0,   38, 1969,  135,    0,   19,  154,   33,  116,
          818,   17,   55,    7,  245,  183,    9,   89,  282,    0,  378, 1714,
          188, 4744,  172,  238,    3,   29,   19,  465,   11,   18,   66, 2214,
           10,   17,  813,  125,    7,  837, 5002,  336,  110,  465,   11,   18,
           66, 2214,   10,   17,  813,    0,    2],
        [ 115,    0, 5873,  296,   10,  229,  333, 3916,   10, 8711,   17,   39,
         7681,   17,  188,  226, 3498,   10,   51,   51,   22, 1987,  266, 1010,
           56, 1628,   13, 5895, 3677,   26, 8509,   10,    7,  461,  266,  763,
          480,    6,   12,    7, 3677, 1042,    7, 3677,  188,  226, 3768,   48,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  29,   55, 4212,    0,    7, 1650, 9787,   26,  923, 1052,  491,   18,
            0, 1120,  690, 2470, 3080,    0,    8,  419, 1289,   17,   19,   79,
          779,  609,  436,   71, 9787, 5953,    7,  370,  207,    0,  860,   79,
           38,    6,  500,    6,  307,  197,   46,   13,  133,  851,  266,   93,
         1455,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,  144,  860,   46,   21,   34,   39, 2576,   35,  781,   15,   54,
         1064,   10, 1510,  134,  456,   80,    7,  179,   17,   26, 1094,   10,
          367,  359,  639,    8,  948,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   48,    0,    0,  116,  570,   10,   51,  388,    9, 1416,
           12,  185,   12,    7,  281, 1248,  954, 1118,    0,    8, 6264, 1118,
            0,    9,    0, 1491,    0,    0,   38,  511,   13, 3087, 1065,    0,
         1366, 1583,   48,  110,  132,    8, 1742,  110,   33,   13, 1820,    0,
           19,   11,   45,    0,  168,    0,   29,    0,   89, 3013,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [3379, 5486,  412,   45, 1115, 1449,    6,  340,    0, 3379,   63, 5053,
           62,    9,    7, 2059,    9,   91,  561,   55,  294,  422, 2042,    6,
            0,   67,  103,   24, 3474,  108, 3064,  106,    7, 2591, 2023,   10,
            7,  830,  200,  236,    6,    0, 3379,  873,  133, 4889,  577, 1611,
            0, 1944,    8, 2288,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([52, 63, 67, 50, 51, 31, 60, 54], device='cuda:0') tensor([1.0000, 0.8687, 1.0000, 1.0000, 1.0000, 1.0000, 0.8716, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(108.1291, device='cuda:0', grad_fn=<MulBackward0>)
False tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8506, 1.0000, 0.8789, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(86.8561, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[   8,   24,   77,  135,   17,  103,   25,   66,   91, 6270,    0,   25,
           73, 2026,   91, 8971,  131, 4619,    7,  218,  248,  214,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  154,   17,   84,   26, 7033,   13,  264,  321,   12, 1478,   12,
         2895, 6053,   17,   11,    6, 2439,   10, 4504, 1817,  230,  115,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 473,  464,    0,   85,   33,  183,    0,   84,  162,  423, 6504,    6,
            0, 2697, 6214, 1118, 4691, 5056,  709,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 294,   94,  154,    0,   38, 1367,    0,   21,   11,    6, 2930,    0,
            8,   21,   11,    6,  691,  131, 3927,    8,  993,  100,   17,    3,
          238,    0,   53, 9736,   21,    0,   67,    7, 3573,    6,   63,   77,
          691,  131,  874,    0,    2],
        [   7,  218,  207,   26,   17,  155,  572,   26,   86,  288, 1520,    9,
         6445,   19,  362,  300,    6,   96,    0,   25,   11,   57,  113, 1557,
           54,  126,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 1511, 3316,    0,  109,    7, 1511,   12,    7, 1511,   26,  138,
           19,  100,   10, 4576, 1160,   45,  174,   57,    0,   10,  375, 1650,
           46,   53, 1985,  450,  117,  214, 9335,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  245,   12,   77,    0,    7,  245, 2110,   17,   24,   11,  121,
         1110,   12,    7, 1465,   34,   17,   21,   34,  142,   10, 3249, 2930,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,  162, 7037,    6, 4127,   62,   85, 1102,  467,    6,   12,  545,
          595,    0,   79,  103,   24,  162, 6959,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,  101,  492,  469,  982, 6953,  518,   91,   12,    7,  281, 2694,
          366, 6105,    6,  419,  753,  188,  700, 1110,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 101,  278,   13, 8429,    0,    8,  101,   34,  172,    0,  172, 4594,
            0,   29,  101,  172, 1608,   11,   18,  450,  110,   70,   10,   87,
          558,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   19,  154,   21,   11,    6,    9,    6,  221,   20,   10,  703,
           17,   24,  164,  700,  229,   13, 1504,   17,   26, 9519,  111,  110,
          338,   18, 1327,  371,  816,    0,   21,   11,    6,   39, 3756, 2484,
            0,    2,    1,    1,    1],
        [   7,  744, 3743,  106,    7, 1219,   26,   13, 1153,   12, 2060,  636,
            8,  284,  248, 3636,    6,  893,  126,   12,  159,  595,    9,  159,
          447, 1927, 1408,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,    7, 5003,    0,   19, 2020,   89, 1613,   10, 3522,    7, 6462,
            6,    9,  159,  447,  931,  359, 6762,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  17,  941,   26, 5823,   62, 1585, 7990, 1726, 3744,  346,  199,    7,
         6447, 2353,    6,    0, 2173,   54,    9, 2749, 1284,   13,  234,  240,
          290,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  552,  132,   71, 3900,  800, 2703,  432,   19,  204,  465,   11,
           18, 1757,   55,   13, 2322,    0,    8,  513,   80, 1016, 2360,    8,
         6356,   62,   13,  846, 1331,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   0,    8,  138,  261, 1747, 2611,  220,   53,  875,   10, 3848,  103,
           53, 5248,   10, 4996,    0,    0,   69,    7, 2517,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([24, 25, 21, 41, 28, 33, 26, 22, 22, 27, 38, 29, 21, 27, 31, 23],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(49.3800, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[ 29,   0,  19,  ...,   1,   1,   1],
        [ 29, 138,  63,  ...,   1,   1,   1],
        [ 91, 279,  19,  ...,   1,   1,   1],
        ...,
        [  8,  53,  77,  ...,   1,   1,   1],
        [ 68, 194,  65,  ...,   1,   1,   1],
        [ 33,  26,  13,  ...,   1,   1,   1]], device='cuda:4') tensor([19,  8, 26, 17, 18, 20, 15, 22, 22, 27, 20, 16, 36, 27, 19, 21, 26, 18,
        19, 23, 21, 15, 32, 20, 21, 22, 16, 28, 21, 22, 18, 20],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8721,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8657, 1.0000], device='cuda:4',
       dtype=torch.float16)
 > at.  tensor(35.5096, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[ 115,    0,  101,   34,   13, 6791,  903, 3902,   18,  365,    0,    0,
            0,  101,   34,   13, 6791,  903, 3902,   18,  365,    8,  101, 5523,
           84,   26,   13,  785,   35, 1625, 1710,    0,  166,  188,   39,  811,
         4674,    0,   13, 8092, 2332,    0,   13, 1747,  389, 7988,    0,   13,
         2322,   54, 2332,    8,   13,   58, 5594,   20,  445,    0,  166,   26,
            7,  291,  371,  121,  810,  109,    7,  203,  779,  675,  793,   12,
            7,  546,    0,    2,    1,    1,    1,    1,    1,    1],
        [  25,  659,   86,  154,   12, 3139,  424,  233,   79,  378,  198, 1558,
          300, 1032,    0,   67,   55,  110,   21,   34,  198, 1558,  300, 1032,
            0,  125,   19, 4776,    0,  116,   79,   19,   34, 1106,   57,  119,
          810,   71,   33,  524,   46,   19,   34,  740,    9, 1319,   35, 7469,
           22, 1146,    0, 4402,    0,   85,    7,  183,   46,   19, 4776,   17,
          333, 1319,   35, 7469,   22, 2593,  958,  283,   37,   17,   19,   34,
          740,   71,  144,   13, 3139,  424,  233, 3078,    0,    2],
        [ 115,   19,   11,   45,  142,   10, 2797, 2117, 2869,  143, 3676,    6,
           17,  164,   51, 4501,    9,   56, 9338,  155, 1370,    8, 3557,    0,
          109,  103,   25,   11,   57, 4313,    0,  138,   25,  506, 8863,  155,
          447, 4313,    8,  415,  500, 1783,   54,   12,  963, 2245,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2901,  245,    0,  180, 2455,    0,   24, 2859,   62,    0,  452,   48,
           35, 2917,  429, 1511,  244,    7, 3549,    0,  180, 2496,    6,  244,
            7, 1465,    0,  321,   12, 2285,    0,    8,  180,    7, 3901, 1583,
         7747,    0, 2901,    0, 2455,    0,   56, 7263,    0, 1228,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  172,  916,  120,   25,  780,    0,    8,    0, 1531,
            0,   13,  207,   10, 2872,   13, 2872,  371,  340,   54, 3182, 1405,
          518, 2288, 1412, 4117,    6,    0,   68,  194,   65,    0,  339,  110,
          450,    0,   25,   17,   84,   11,    6,   86,  593,  294,    0,   94,
            0, 2091,    9, 2516,   54,    9,   17,    0,    0,   86,  276,    7,
            0,  427, 6034, 1167, 4445,    0,  148,   24,   11,   48, 5443,   80,
            3,  584,  759, 1086,   55,   85,   17, 2110,    0,    2],
        [  89, 4856,    0,  255,  121,    0,    8,   19, 1570,  244,   10,    7,
         1396,    6,   12,    7, 1930, 3449,  458, 4910,   10, 6682,   17, 3486,
           71,   29,  294,  218,   94,    0,    8,   19, 1608,   11,   18,  601,
           67,  154,  138, 1019,   24,  552,  336, 1979,  122, 1630,  230,    6,
            8, 1094,  138, 1019,   24, 1918,   10,  205,  336, 2787,   12, 6535,
          369,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,  220, 1393,  134,  132,  109, 1393,  134,  346,    0,   94, 4600,
          134,    0,   67,    9,    7, 5970,    6,    0,    7,  941, 3741, 7602,
            0,    8, 4597,  373,  487,  826,   80,  138,   10, 2554,  941,    0,
           29,   70,  956,    0,    7,   59,  626,    6, 3668, 5125, 1446,   10,
         2699,   13,  264, 1396,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,   73,  154,   12, 2932, 5743,    6, 1645,   79, 1291,   22,  241,
          505,    0,  746,   12,   39,  985,   35, 4797,  279,    0,  230,    0,
           68,  194,   65,  206, 1288,  106,   91,  723,   73,   51,  415,  777,
           62,    8, 3411,  652,  922, 1004,    7, 1711,  567,    0,   17,   84,
           73,   51, 1986, 1181, 3479,    6,   12, 2932,  896,    0, 3188,   54,
           69,    7, 2353,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([76, 82, 48, 48, 82, 63, 54, 65], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8784, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(117.1517, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[  10,  388,   21,  133, 1948,    0,    7, 6730,   29, 1019,  188,  226,
           17,  103,   25,  508,   94,  890, 2273, 6053,    0,  103,   25,  508,
          134,  890, 5259,    0, 3226,  164,    9,   20,  879,   18, 3042, 2679,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,  391,  214, 3803, 1028,  132,   55,  333,  524,    0, 6664,    0,
         5691, 4140,    8,    7,   94,  148, 1202,  134,   46,    7, 3348,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,  120,   25,  175,   10,  227, 6085, 6106,    0,  204,    0,   21,
           73,   51,  250,   12,   39,   39,  816,  297,  424,  505,    0,  125,
           84,   63,    3,   94, 1074,   84,  148,   63,   86, 5607, 1719, 3475,
         1811,   25,   79,   25,   11,   57, 1028,  199, 3594,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  13,  325,   12,   94,   71,  452,  265,  192,   11,   18, 1570,    0,
           67,   89, 1848,  465,   11,   18,  703,    9,   38, 3760,   11,   18,
            3,   89, 1970,   11,    6,   81,  587,   34,    0,   38, 1969,   73,
           87,   21,    0, 1586,   25,   73,   73,    3,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   17,   26,  347,   19,  213,   10,  456,   10,   25,   80, 1729,
          290,  846, 2956,    0,  125,   19,  703,   24,   63,   13,  453, 8098,
            9, 8969,   54,   13, 2805,   55, 2394,  929, 4067, 3399,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,   11,  158,  229, 1123,   21,   11,    6, 3008,   48, 8289,    0,
           38, 9920,  159, 8460,    6,   66, 2107,  132,  248,   35,    0,  391,
           35,    0,  717,  181, 1178,    8,   53, 1531,   21,  126,    0,  125,
           21,   11,    6,    7,  245, 8370,   48, 2212,   53,   11,  121,  144,
            9,  159,  282,    0,    2,    1],
        [ 103,   25,  229,  143,    0,  254,    0,    0,  655,    3, 6479,    6,
           13,  464,    0,    9,    7,  832,    0,    6,    0,    0,    0,  419,
         2263, 1020, 2244,   25,   11,   57,  142,   10, 1064,  164,   66,  288,
            0,   13, 2768,    0,    0,    0, 2768, 4455,   69,  155,  244, 1021,
          238,   35,  242,   54,    0,    2],
        [9026,  131,   33,   89,  781,  266, 1466,    0,   89, 1751,    8,   19,
          144,   33, 3310,  479,    0,  339,   11,    6, 2033,  108,  743,   85,
           13,  341,  824,    6, 1090,    9,   13,  341, 1503,  545, 1303,   12,
          883,  424, 5089,    8, 1452,  251, 1352,   69,   13, 4813,    0,    2,
            1,    1,    1,    1,    1,    1]], device='cuda:3') tensor([38, 25, 47, 45, 37, 53, 54, 48], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8237, 1.0000],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(76.1079, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[ 103,  185,   12,   70, 5070,    6,   94,    9,  108, 4621,   10,  229,
           77,   12,    7, 4388,   24,  229,  162, 3474,   62,   10, 4621,    9,
          166,   94,   66,  593,  555, 4387,    6,    0,   86,  288,  169,  251,
           94,   11,    6,  931,   51, 3480,   48,    0,   67,  108,    6,  169,
           51, 3480,   48,  113,    0,  166,   26,   70, 5767,    6,  583,   13,
           38, 1803,   20,  412,   35,  389,  927,  586,   54,  954,    3, 4232,
         1665,  365,  338, 1337,  793,  164,  229, 1459,  509,  518,   46,   86,
          116, 1714,   94,   46,  125,   12,  138,   77,   33, 9733, 2573, 1764,
          652, 1317,    6,  170,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,   73, 8794,   20,  111,  450,   25,   17, 1267,  411,  458,  128,
            0,  728,  158,   35,  287,    0,  775,   20,  727,   12, 9779,    0,
         1492,  156,    0, 3997, 4999,    0, 1747, 2142,    0, 5183, 1353, 1027,
            0, 1487, 3174,  388,  901,    8,   10,  424,  412, 1406,   20,   87,
           86, 3715,  359,   13, 6069,   17,   11,    6,  415,  233,   62,  132,
          850,  155, 1729,  502,   96,   17,   11,    6, 3463,   10,  367,  126,
           39,  109,  597, 2658,    9,  155, 1922,  119,    8,  851, 3304, 2946,
            7, 1956,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,    7,  753,    9,    7,  179,   71,    7, 6100,  837, 2694,    0,
            7,  800,   91,  753,   69,  837, 2694,   26,  264,  742,   20,  128,
          511,    0,   68,  386,   65,  238,  691,    0,  492,  226,    0, 1412,
          205,    0,   68,  194,   65,    7,  753,   71,    7, 1686,  837, 2694,
            0,   19,   11,   45, 4701,   10,  289,    0,   26, 2346,   48,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7,  841,    7,  841,   12, 1057, 4311,  106, 2027, 1117, 1835,  126,
          199,    7,  203,  121,  237,  271,    0,   10,   66, 1565,   62, 1835,
           55,  250,   17, 4639,   10, 1109, 1102, 1117,   25,    8, 1019, 2816,
           25,    0,    8,   17,  434,   25,  270,    9,    7,  467,   10,    7,
          288, 2397,   25,  220, 2679,    0, 3045,   79,   25,  323,    0,    9,
          155,  883,  122,    6,   12,  570,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  24,  205,    9,    7, 7373,    6,   71,    7,  269,  290,   46,   24,
         2677,  159, 9060, 3183,   46,   21,   11,    6,  976, 1248,    0,   21,
          598,    6, 1968,    0,    8,  180,    0,  509, 1094,    0,   24,   11,
          121,  278,  108,  447, 3479,   12,  269,  290, 4656,    6,  737,  373,
            0,  108,  447, 3222, 2083, 1019, 1367,    0, 1339,  783, 2734,  741,
            0,  148,   73,  305,   13,  488,  269,  290,    8,    9,   91, 5563,
            0,  388,   21,  199,   39,   56,   15,  121, 3576,   20,   12,  563,
            0,   29,   17,   24,   73,  204,  283,   71,    7,  269,  290,    8,
         1082,    7, 4814,    6,   21, 1847,   10,   86,    9, 3196,   57,   33,
         1390,  148,  492,  150,    6,   13,  853,  569, 1020,    9,    7,  985,
         1580,    0,    2]], device='cuda:3') tensor([102,  88,  61,  67, 123], device='cuda:3') tensor([1., 1., 1., 1., 1.], device='cuda:3', dtype=torch.float16)
 > at.  tensor(187.6200, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[3111,    7, 1732,  465,   11,   18, 2320,  923,   29,   51,   22,  236,
           22, 3903,    0,    8,  120,  225,    9,    6,  365,   62,   17,   19,
          150,  156, 2519, 2303,    0,   19,  316,  111, 1661,  297,   62,    0,
            8,  166, 4075,   48,   10,   51, 4113,  800,  248,    0,    2],
        [   8,   29,  103, 2439,   71,   13,  555, 2960,   12,  170,    9,   33,
          964,    0,   24,   73, 7647,   10,  826,   80, 7053,   54,    9,   33,
          341,  688,    0,  180,   24,   73,   16,  287,   17, 1417, 2041,   19,
           34, 6037, 1069,   10, 3928,    0,    2,    1,    1,    1,    1],
        [  89, 3965,  440,   26,   10, 1914,  126, 1004, 7806,    6,    8, 7750,
          694,  826,  199,   33,  488, 2469,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101, 1559, 5990, 1323,    6,    0,  889,    8,  596,    0,  199,    7,
         2260,    0,    8,  101, 2700,  134, 3611, 1256, 2469,    6,   10,   66,
         1025,  101,   32,   96,  134,  132,   10,   13, 1116,  111, 8068,   29,
          101,   73,  150,  134, 4015, 3611,   62,    0,    2,    1,    1],
        [  53, 6960,   48,   17,    7,  207,   94, 5973,  185, 1760, 1361,   12,
         1573,    0,   34, 6027, 1490,    9, 1218, 6357,    0,    0,    0,   71,
            7,  207,   53,    9,    6,  365,   62,   69,  378, 5973, 1574,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115, 2283,    0,   79,   21, 2546,  436,    6,  270,    0,  138, 2117,
          155, 5897, 9023,    6,    0, 1239,  115,    0, 2283,   21,   79,   21,
         2546,  436,    6,  270,  554,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  432,   13, 2767, 9028,   12,    0,  155, 3612, 6385,  567,    0,
            7, 2116, 1149, 3320,   71,   13, 7069,  874,  266,    0, 1034,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101,  591, 2869,  133,  528, 4506,    6,    0,    8,   29,   24, 6631,
         1011,   33,    8,   24, 2721,   17,   33,   26,   13,  207,   17,  172,
         1429,   55, 3101, 3001,   35, 1966, 1265, 4506,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33, 1321,  133,  341,  254,  419, 3839,   69,  984,   17,   19,   11,
          121,  700, 1110,    0,    8,   19,   11,  121, 1110,   13,  325,   12,
         3839,   69,  984,    0,  703,  110,    0,   68,  194,   65,  274,   85,
           33, 3189,    9,    7, 1403,    0,    2,    1,    1,    1,    1],
        [  21,   34,    7,  403,    6, 3211,   71, 3039,    0,   13,  140, 1355,
         3471,  187,   62,  131,    7, 4401,   12, 1519, 3473, 2551,   54,  336,
            7,  179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,    0,  125,    7,  572,   26, 5802,   54, 2015,
         1428,   12,  214,   17,   63,    0,  415,   59, 8186,    9,  277,    0,
          765,    6,   12,    0,  183,    8,   17, 7010,    0,   10,   91,  486,
            9,  277,  765,    6,   12, 2786,  366,  183,    0,    2,    1],
        [  67,  244,    7,  501,  895, 1047,    6,    8,   11,    0, 1536,    6,
            0,    0,    7, 2008, 6248,    0, 1381, 1446,   53,  692,   10, 6896,
         4174, 1115,    0,    0,    8,   29,    0,   70,   53,  323,   34,    0,
           53,  487,   10, 2750,  596,  199,    7, 9824,    0,    2,    1],
        [   0,   13,  555,    0,    0, 2960, 2602,    0,  755,    0,    9,  383,
          741,    0, 1851, 1353,    0,    0,  109, 6893,    8, 1106,  233,  174,
          234, 1106, 2836,    0,   53,  144, 3695,   12,   70,   24,    0, 2807,
           10,   51,    7, 8968,   55, 2786,    0,    2,    1,    1,    1],
        [  19,   34,  133, 6011,  120,   19,  487,   10,   87,   33,   10,  150,
           17,    0,    9,  409,    0,  276,    7,   94,  148, 9235,   48,    7,
          368,   12, 1752, 2856, 1054, 5786,  131,    7, 3973,  144,   22,   11,
           18,  204,  691,   17,    0,    2,    1,    1,    1,    1,    1],
        [  21,   26,  108,  688,    8,   86,  108, 2072,  687,   17,  664, 2836,
           15,    6,  170,    0,   38, 3328,  583,   10, 2332,  440,   26,   17,
           24,  296,   10,  175,  244,  108, 1728,   12,  138, 2583, 9646, 1894,
           24,   73,   51,    9,  545,  218,   11,    6,  931,    0,    2],
        [  91,   12,    7,  214,   17,  956,  432,   17, 1064,   26,   17,   19,
          474,   80,    7,  218,  889,  148,  162,    9, 2536,  140,   37,  922,
         1585, 1763,   12,  110,    0,    8,  138,  261,   19,  692,   10, 1452,
           33,   71,  134,    0,    2,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([47, 43, 20, 45, 37, 31, 25, 35, 43, 28, 46, 46, 44, 42, 47, 41],
       device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8789, 1.0000, 0.8970, 1.0000, 1.0000,
        1.0000, 0.8662, 0.8765, 0.8071, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(66.5558, device='cuda:0', grad_fn=<MulBackward0>)
False tensor([[   0,   19,  144,    7,    0, 2212,   10, 2115,  336,    0,    7, 1291,
           45,  603,  603,    0,  427, 2984,  430,    9, 2008, 1146,   71,    0,
           39, 3698,    9, 1031, 1967,  541, 3379,    0,    2,    1,    1,    1,
            1,    1],
        [   8,   12,  538,    0, 1042,   25,   66,  264, 1893,    0,  276,    7,
          801, 6878,    6,    0,  276,  155,  637,  873,  250,  341,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,    7,    0, 1043,  159,  813,  188,  419, 2070,   26,    0,    0,
          238,    0,  125,  211,   91,  994,  188, 2214,   10,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   0,    0,    8, 3108,  144,  246,    9,   39, 3836,   17,   24,  451,
            0,  172,  875,    0,  250,  264,    0,   10, 1366,    0,  250,   17,
           24, 2775,   11,   18, 3498, 2307,    0,    2,    1,    1,    1,    1,
            1,    1],
        [  69,   33, 1472,   12, 1780,   26,    7, 1261,    7,  207,   19, 1385,
           21,  106,   89, 1067,  971,  455, 1431,    8, 1037,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  108,  630,  172,   34,    0,   87,  108, 1752,  352, 4041,    6,
         7420,  138,   24,  154,    8,  598,   80, 1469,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 169,  378,   13, 1396, 4988,   48,  106, 7801,   55,   13,  555, 4299,
          131,  378, 4744,  131,   10,  861,  229,   13, 1878,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  101, 2770,   33,  567,    0,   67,   21,  172,  465,   11,   18,
          283,    0,  125,  284, 7393,  162, 5785, 4777,   54,  545,  218,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19,   11,  121, 1621,   13, 2695,  434,   38, 2366,   10, 1303,  197,
            8,   19,  703,   21,   11,    6,  142,   10,  468,    7,  207,   25,
          274,   85,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2993,   51, 7989,    0,   38,  511,  180,    0,   12,  538,    0,    7,
          558,  279,   19, 1510,   26,    3,   19,   11,  121,  442,   21,   10,
            7, 1219,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  103,   25,   11,   57,    0,    9,    7,  417, 1236,  556,  240,
          589,    0,    0,    0, 1052, 8374,  371,  626,    0,   25,   11,   57,
          142,   10, 2034,   13, 1335, 1273,  303,   18,  434,   13, 2973,   93,
            0,    2],
        [  29,   12,  538, 1976,   25,  192,   11,   18,  172,   87, 1835,  689,
           11,   18,  172,  175,  691, 8289,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  79,   13, 5425,   59,    0,   19,   34, 8784,   54,    0, 5184,    0,
            8,   19,  692,   10,   51,   39, 2420,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  276,  115,    0,  432, 1450,  215,    0,   53,   11,   57,   86,
          132,   84,    0,   53,   66,   86,  203,  699, 2633, 1181, 1501,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 103,   25,  204,  274,   85,   13,  453, 1201,    0,   25,   11,  158,
         4712,  735,  150,   77,   12,  117,  391,  214, 1028,  199,  722,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 719, 6382, 8193,    6,   17,  811,  265, 1782,    0,   94,   10,    7,
         4620,    6,    0,   12,  159, 2332,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,    7, 5410,   26,  172,    0,  101,  954,    6,  199,    7, 3140,
           12,  378,   39, 9005, 1198,   12,   77,   12,  117, 6710,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   34,  391, 1113,    0,    0,  717,    0,  110,  128,    6,    0,
            8,   85, 5826,   69,  227, 2124, 2366,    0,    0,    0,   24, 5623,
            7,  501,  887,  895,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  665,   85, 1228, 6529,    6, 1850,   62,  131, 5543,    6,    0,
           13,  179,   12, 5407,  985,    6,  132,   55,  565,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24,  144,  521,  870,   45,    6,    9,   77,    7,  203,  181,   59,
         1159,  866,    6,    9,    7, 6870,    6,    8,    7, 3061,    0,  125,
         8939,   97, 1203,  551,    6, 7955,    0,    2,    1,    1,    1,    1,
            1,    1],
        [  29,  154,   80,    0,   17,  765,    0,  270,    9, 8085,  584,    0,
            0,  554,  206,   13,  664,   37, 1327, 1563,    0, 3848, 1505,   13,
         1366, 1251, 3848,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  281,   12,    7, 1583,    6,   84,   63,   86,  744,   35,
         2514,   85,   77,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0,  108,  288,  818,   12, 3802,  169,   51,  359,    7,
          801,   35,  181, 3488,  369,   62, 1850, 2100,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,    0,    7,  207,  117, 2032, 2261,    0,    0,    0,  162,  691,
            0,   84,   11,    6,  226,   13, 1017,   12, 4795,    6,    0,    8,
           33,    0,   26,  206,   25,  175,   10,  150,    0,    2,    1,    1,
            1,    1]], device='cuda:5') tensor([33, 24, 24, 32, 23, 22, 23, 25, 30, 28, 38, 20, 21, 25, 25, 21, 24, 30,
        23, 32, 29, 17, 22, 34], device='cuda:5') tensor([0.8315, 1.0000, 0.8818, 0.8154, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8682, 1.0000, 1.0000, 1.0000, 1.0000, 0.8813, 1.0000, 0.8257,
        1.0000, 1.0000, 0.8022, 1.0000, 1.0000, 0.8340], device='cuda:5',
       dtype=torch.float16)
 > at.  tensor(41.2409, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[  24, 1621,   86,  288,   39, 2987,    0,    0,   67,   85,    7, 1740,
           12,   33, 2987,   24,    0, 1621,   13, 4601,    0,    0,  672,    0,
            0,    8,   33,  672,   34,   80,    7,    0, 1064,    0,   12,   13,
         4328,    0,    7, 1064,   12, 7376,    8,   12,  540,  687,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,    7, 8088,   54,  279,   26,   17, 4623, 2050,  373, 2523,   66,
          226, 5037, 4031,   17,   63, 2414,   10,  108, 1715,    0,    8,   53,
           11,  121,  591,   17,   13,  800,   12,  134,    0,  120,   53,   11,
           57,   80,    7, 1137,   12,  108, 1715,    0, 5368,   15,  131,   13,
          409,  240,   12,   79,  261,   79,  423,    0,  689,   11,   18,  473,
           55,  133,  535,    0,    2],
        [ 545,  188,  822,    0, 2683,  964,    6,    0,  206,    7, 8869,    6,
            0,  596,    0, 1585,   71,  963, 2352,    8, 4478,    0,  185,   79,
          963,   79, 1646,  215,  801,    0,   63, 6324,   10, 3284,   18, 1617,
            7, 7009,    6,    0, 9695,  134,   10, 1713,  143,  896,    8, 8939,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,    7, 5407,   12,  961,   13, 2688, 3860,   59,   55,   13, 6049,
         3367,  199,   70, 1505, 2134,   79,    7, 1465,   12,  214,    0,  166,
           26, 2318,   39, 2735,  115,   71, 4960,   19,  362, 1838, 1428,   55,
          958, 1006,    0,  941, 7824,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 185, 1075,   46,  305, 6124,    0,   39, 4174,  768, 6503,   71,   13,
         2127,   86,   17,  341,  106, 5200,  699,  241,  352,   11,    6,    0,
          131,    7,  207,   46,   91,  383,  473, 9472,    0,  278, 1759,  820,
          439,   12,   77,   21,    6,  941,  106, 8307, 2791,    0, 2248,  111,
         2626,    8, 2678,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,   63,  133,  555,  214,    0,  133,  555,  214,   17,   25,   73,
          172,   87,   17,  164,  468,    7,  207,   17,   25,   73, 2644,  117,
         1587,   12, 4390,    8, 1064,   70,   19,  169,  583, 3849,   13, 1395,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   39,  276,  143,  528,  774,   12, 9063,  271,   55,  565,    0,
           17,   56,  111,   22, 4014, 3570,  650,  492,  220, 3890,    0,   34,
           13,  570,   12,   46, 3524,   46, 7683,    0,    8, 1790,  126,   70,
         2275,  372,   59, 1563, 2028,   12,  282,   73, 2231,   79,   13,  926,
         2653,   10,    7, 3462,  687,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 148,  220,   66,  934,   48,   17,   13, 1127, 1369,   12, 2951,   57,
          200,   35, 3794, 6814, 1074,    9,    7, 6356,    6,   12,    7, 5495,
          179,  169, 7831,  199, 2955,  634,   18, 3772, 5869,   12, 2232, 2399,
            8, 2434,    7,  133, 5495,    6,   53, 1412,   66,  903,   20,  556,
           62,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:1') tensor([48, 65, 50, 43, 53, 38, 55, 51], device='cuda:1') tensor([0.8169, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(95.2938, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[ 148,   63,   77,   12,    7,  218,  596,   69,  117,  255,  945, 2696,
            6,    0,   19,  591, 6087,   93,  122,  551,  237,  820, 1132,  895,
          828,    0,  225,  246,  225,   34,   13,   38, 2470,   22, 2088,  148,
           26, 1767,    8,  126,  606,   54,    3,  225, 2884,   62,  267, 1329,
           79,   38,  153, 1040,   37,    3,  225,  246,  225,   26,   38,    6,
          233,  111,    0, 2292,    8, 1751,  111,    3,  225,  100,    6,   10,
          229,   94, 4265,   38,  128,  959,    3,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   85,    7, 1565,   12, 5604,   54, 1222,   13,  277,  523,    0,
          120,   19,  274,   85,   70,   11,    6,  142,   69,   71, 1578,  639,
          117, 1113,    0,   24,   63,   86, 3983, 3019,  359,   71,   33, 3046,
            0,    8,  120,   19,  274,   85,   70,   26, 1592,   10,  108, 8281,
            8,  108, 4621,    0,   89, 1127, 7034,   26,   17,   24,   13,  160,
           11,   18, 1110, 1155, 1094,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  17,   26,  347,   25,    0,    9,  155,  294, 4159,    8, 9034,    0,
           63,   13, 3764, 1986,  365,    0,   13, 1986,  365,   17,   11,    6,
         4690,  111,  341,    9,  155, 2395,  572,    9, 3201,  254,    7,  572,
           12,   39, 2216,  736,  215,  581,    0, 4126,  111,  341,    9,    7,
         3201,    6,  106,    7,  572,   12,    7, 2313, 2216,    3,  215,  581,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,   19,  213,   25,   10,  934,   17,   33, 2088, 1603,   26,
         2952,    0,    8,   21,   11,    6, 6487,    0,    8,   21,   11,    6,
         4469, 1584,    0,    8,   21,   11,    6, 9101,    0,    8,   21,   11,
            6,  985,  687,    0,    8,   21,   11,    6,    9,  191,    6,  429,
            0,    8,   21,   11,    6, 9059,    0,    8,   21,   11,    6, 2549,
            0,    8,   21,   26,    9,   18, 2738,  366,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3524,  109,  218,   46,  635,   91, 9737,   62,  486,    0,   24,   11,
          158,  492,  135, 1456,  347,   46,   67, 1042,   53,  278,   13, 9745,
          336,  134,    0,   53,  162,   77,    9,    7,  370, 9745,    0,  115,
           77,    7, 4488,   35,  140,   57,  922,  876, 5521,   12, 4618,    0,
           77,    7,  453,  687, 1621,  131, 8218,    0, 1917,    6, 4775,   62,
         1117,    7, 9745,    8,   24,   11,  121,  278,   13, 1629, 3438,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   26,  824,  174,  779,  527,    0,    0,   91,   12,    7, 1881,
           17,   33, 8223,  140,  338,  525,  128, 6688,    0,  447,   62,    0,
            0,    8,   39,  916,  279,   80,  824,  174,  779,  527,    0,    0,
           26,    7,  655,   35,  737, 2109,  447,   37,   12,   33, 1850,   62,
            0,   13, 1329,   13,   48,  352,   18,    0,    8,   33, 1329,   13,
           48,  352,   18, 4223,   62,   91,   12,    7, 7577,    0, 2348,  106,
            7, 3141, 3928,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,   66,    0,   13,  277, 1964,   69,   89,  664, 6729,    0,
            8,    0,    0,   21, 1119,    0,   38,  242, 1027, 1792,   19,   11,
           45,    0,    7, 3806, 1251,    0,   17,   11,    6,  347,    3, 4701,
            0,  211,    0,  143, 7052,    0,    0,   38,  187,   11,   45,    0,
            0,    0,    7, 3806, 1251,    0,    0,   25,   11,   57,   86,    0,
         2456,    3,    0,    0,    0,    8,   84,   11,    6,    0,  825,    0,
           55,  663,    0,    0,    0,  103,   89,   29,   22,   11,    6, 1431,
         1516,    7, 2657,  985,    0,    7, 2918,    6,   66,   10,  135,   25,
          192,   11,    0,   18, 1396, 1004,   33, 1375,    0,    2],
        [  70,   33, 1221, 3556,  170,   46,   86,  116,   89, 2260,    0,   67,
          108, 6142,    0,    8, 6579, 2519,  670,  143, 6902,   46,   26,   17,
           24,   66, 3764, 2791,    9,   13, 1027,  631,  407,   55, 3001, 5605,
            0,   17,  108, 1740,    0,  166,  188, 1108,   62, 1645,  143, 1167,
         5152,    9,   13, 2931,  207,  254,  419,  218,    0,  492,  442,   91,
           12,   21,    6,  447,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([ 80,  67,  62,  70,  73,  77, 106,  66], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8936, 0.8052, 1.0000],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(119.0856, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[5082,    6,   12, 2422, 7713,    0, 2422, 7254,   54,    8, 2422, 8365,
          271,    0,   84, 1859,  961,   17,    9,   18, 1625, 6483,  119,  724,
           12,    7,   73,  781,   93,  291,  457,   10, 2443,    9,    7, 9370,
         1032, 1081,   17,   21,  188,  120,   21,   26,   86,  838,   18,  234,
         3785,  131, 2048,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  91,   17,   24,  939,    6,   20,   20,  834,  168,    0,  886,    6,
         1558,  609,  287,    0,    7, 5694,   12, 1954,  455, 1943,  407, 5462,
          372,    9, 5556,   59, 1068,    0,   29,   24,  116,  246,    0,  238,
          635,    0, 1051,    6, 3287,  188,  691,  250,  100,   17,    9,    7,
         1165,   12,   33,  453,  283,   12,  998,  131,  749,  399,  918,  287,
            0, 1313,  101,   34,   13,  453,   13,   48,   45,  551,   37,   12,
          749,  399,  918,  287,  255, 1051,   22,  609,    0,    2],
        [  29,    0,   24,  278, 6713,    0,   55,    7, 1793, 1956,    0,   77,
          230,    0,    8,  168,   26,   33, 1284,  422, 1603,    0,    8,   25,
           73,  934,   17,  168,  113,    0, 3317, 1143,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,   13,  555, 2168,    0, 1459,    9,    7, 4857, 3539,   34,    9,
          108, 2092,   96,    0,   38,    6, 2704,  760,   26,   13, 3700,   85,
           13,  183,  120,   10, 7077,  596,   26,   13, 3975, 1201,    9, 4970,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101, 1772,    6,   33, 4340,    0,  166,   34, 6483, 1797,   62,    9,
         9479, 1706,  111,  490,  101,   14,   48,   46,  101, 1772,    6,  131,
         1945,   54,  138,  948, 1429,  359,    7, 6501,   12, 7522, 5294,    6,
           17,   63, 5848,   48,   10,   51, 6610,   10,  838,  927, 3149,    0,
            8,  180,  101, 1143,   69,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   25,   11,   57,   86,  172,  142,   10,    0,    9,    7,  501,
         7015,    6,    0,  467, 7886,  131, 1766,   54,   91,   12,  117, 2878,
            0,   21,   11,    6,   13, 6410,   12, 1752,    6,   15,    6,   20,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 336,    7,  370,  183,    0,   19,   34,  142,  359,   39, 8087,  369,
           46,    7,  245,  700,  985, 8087,  369,  131, 1553, 4528,    9, 1777,
           46,   71,  486, 3619, 2900, 2352,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  509, 6406,  132,    0,   68,  386,   65,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([53, 82, 34, 38, 55, 38, 32,  9], device='cuda:2') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:2', dtype=torch.float16)
 > at.  tensor(95.0997, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[  24,  175,  700,  ...,    1,    1,    1],
        [ 851,    0, 1598,  ..., 1262,    0,    2],
        [  29,  339,   11,  ...,    1,    1,    1],
        ...,
        [ 125,   21,  188,  ...,    1,    1,    1],
        [  70,  169,   25,  ...,    1,    1,    1],
        [  19,  116,  465,  ...,    1,    1,    1]], device='cuda:2') tensor([ 7, 16, 12,  7,  9,  8, 10,  7, 14, 15,  9,  9,  9, 11,  8,  6,  9,  7,
         7,  8,  9, 13,  7, 10,  9,  8,  7,  9,  7, 11, 11, 11,  8, 15, 10,  9,
        11,  8,  9,  8, 15,  7,  8, 10,  8, 12,  9, 10,  9,  6,  8, 11, 14, 10,
         8, 12, 10,  8, 14, 14, 12,  9, 10,  8,  8, 11,  6, 12, 10, 13,  8,  8,
        10, 10, 10, 12,  8,  9,  7,  8,  8,  9, 11, 11, 12,  9,  9,  7, 10,  7,
        10, 10,  8, 11,  8,  8, 10,  8,  9,  8, 11,  8,  8,  7, 12, 10, 13,  7,
        14,  9, 15,  9,  8,  8, 10, 10, 10,  7,  8, 10], device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8130, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8696, 1.0000, 1.0000, 0.8672, 0.8652,
        1.0000, 1.0000, 1.0000, 0.8687, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8955, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8750, 1.0000, 0.8003, 1.0000, 1.0000, 0.8154,
        1.0000, 1.0000, 1.0000, 0.8145, 0.8315, 0.8403, 1.0000, 1.0000, 0.8857,
        0.8267, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8125, 1.0000,
        1.0000, 1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8857, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 0.8867, 1.0000, 1.0000, 1.0000,
        0.8481, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
 > at.  tensor(11.5136, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[  17,   11,    6,  ...,    1,    1,    1],
        [  86,   85,   77,  ...,    1,    1,    1],
        [ 120, 8121,  728,  ...,    1,    1,    1],
        ...,
        [  53,   11,   57,  ...,    1,    1,    1],
        [   8,   86,  288,  ...,    1,    1,    1],
        [  24,   63,   86,  ...,    1,    1,    1]], device='cuda:2') tensor([12, 18, 15, 13, 14, 14, 16, 14, 15, 17, 11, 11, 14, 16, 14, 13, 13, 13,
        13, 10, 18, 12, 12, 15, 13, 18, 16, 15,  9, 16,  9, 17, 12, 15, 13, 11,
        13, 15, 11,  9, 17, 20, 14,  9, 11, 15, 11, 15, 13,  9, 13, 12, 11, 10,
        21, 15, 10,  7,  9, 19, 14, 16, 15, 11], device='cuda:2') tensor([0.8955, 1.0000, 0.8086, 1.0000, 0.8545, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8662, 0.8711, 0.8091, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8262, 0.8496, 1.0000, 1.0000,
        1.0000, 0.8403, 1.0000, 1.0000, 1.0000, 1.0000, 0.8154, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8760, 0.8003, 1.0000, 1.0000, 1.0000, 0.8896, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:2', dtype=torch.float16)
 > at.  tensor(18.6326, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[  29,   70,   10,   87,    0,  238,    0,   39, 1141,  659,  367,  106,
         1560,  121, 3165,    8, 2185,  502,    6, 1574,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    0, 1296,   10,    0,   87,   33,    0,   21,   11,    6,   13,
          325,   12, 8512,    8, 3236,   17,    0, 1202,    6,    9,   13,    0,
          133,  747,  203,  779,  675,  793,    0,    2,    1],
        [  19,  246,    0,   38,  242, 1865,   20,   25,  162, 2188,    0,   21,
           34,  116,  155, 3181,    8, 3806,    0,  109, 3181,    8, 3181, 3328,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [ 440,    0,   21,   26, 1970,    6,    0,   86, 1276,    6,    0,  148,
         2508,    7,  281,  283,   35, 5203, 3382,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,   13,  324,  279,    0,  125,    9, 1296,   10,
          229,  995,   13, 1874,    0,   25,   66,   10, 2484,   80,   21,  245,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6,   13,  341,  561,   10,   51,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 4460,   12,   13,  384,  372,   93,   62, 8093, 8187,   26,   86,
          142,   10, 6264, 4032, 5099,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 6532,    7, 5408,  338, 1344,   71,   39, 6291, 5108,   12,  655,
            3, 1086,    9, 1565, 3979,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   11,    6,  276,  143, 1248,   26,   21,  164, 3162,  126,   13,
         1850,  709, 6649,   69,    7, 4057,    0,  125,   21,  135,    6,    7,
          207,   12,    7,   21,  510,    0,    2,    1,    1],
        [   8,   79,    7, 9448,  480,    6,   12,  117, 3522, 1118,    0,   24,
           66,  159,  211,  424,   48,  266, 2011,  415,  234,    6,   54,  359,
          108,  447, 2200, 2374,    0,    2,    1,    1,    1],
        [ 238,    0,  274,   85,  185,  801, 1081,    0,  185, 1081,   17,   24,
           11,   48, 1220, 6356,   62,    0, 1696,  140,  292, 4635,    0, 1696,
          140,  292, 4635,  172, 2244,    6, 4232,    0,    2],
        [  24,  144, 3181,    6,  148,  169,  367,    9,    0, 3636,  188,   79,
          322,  424,    0, 6108,    6,  132, 6686,    9,  415,  656,  292, 4117,
            6,  333, 2223,    0,    2,    1,    1,    1,    1],
        [ 115,   17,   19,   11,  121,  691,   17,   46,   19,   11,  158, 1945,
           21,    9,   13,  765,   46,   19, 1412,  450,   25,   17,   19,  217,
           39,   13, 2553,    0,    2,    1,    1,    1,    1],
        [  67,    7, 7552,   17,    7,  942,  188,  442,   10,    7, 1136, 1909,
           26, 4050,    0,    8, 1120,   39, 1670, 1478,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   67,  218,  825,   53,   11,   57,    0,   86,    0,    8,    0,
           25,   66,  288,  427,  174,  541,  233, 1611,   55,    0,  341, 5204,
            6,    0,    2,    1,    1,    1,    1,    1,    1],
        [5385,   13, 3495,  998,    0,  860,   79, 7286,  109, 3717,    0, 2475,
         6414,    6,   21,    6, 4401,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  703,   17,    7, 1880, 5917,    6,   63,   13, 1412,   55, 1777,
           10, 6433,   21,    6, 2247,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  279,   73,  388, 4743,    3, 4997,    6,  690, 3350, 3298,   39,
         5414,    6, 3516,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    0,   53,  246,   10,    7, 5410,    6,    0,   38, 5034,    0,
          108,  415,   59,  338, 4482,    6,    0,   63,   39,    0,    0, 5441,
         1303,  424,   57,    0,    2,    1,    1,    1,    1],
        [   7,  288,    0,  279,   17,   53,   73,   11,   18,    0, 4585,   26,
            7,    0, 4129, 3585, 2908, 6649, 1241,   17, 1472,   12, 1719,  293,
          356,    0,    2,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  162,  142,    9,    7,  595,  270,   10,    7, 8145,    0,
            0,   24,  162,  826,    0,  347,   34,   33,    0,   29, 1894,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   86, 5424,   62,    0,   17,   11,    6,   13, 1931,
            8,   21,   11,    6, 1897,   69,    7,  876, 6106,   22,  128,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   53,  144,   39, 1248, 3473,    0,  166,   34,   53,  465,   11,
           18,   66,   10,  229,  419,  839,  106,   21,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  229, 6730,    6,    8, 1560,  300,  901, 8018,    6,   12, 6027,
            0,   24, 3065, 3285, 6443,    0,   24,  135,   17,   24,   63,  230,
            0,    8,   53,   63, 1226,    0,    2,    1,    1]],
       device='cuda:7') tensor([22, 32, 26, 21, 26, 10, 19, 19, 31, 30, 33, 29, 29, 22, 27, 20, 19, 17,
        29, 27, 25, 25, 22, 31], device='cuda:7') tensor([1.0000, 0.8369, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8042, 1.0000, 1.0000, 1.0000,
        0.8306, 0.8911, 0.8936, 1.0000, 1.0000, 1.0000], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(42.2640, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[ 125,   70,   25,  296,   26,    7, 3189,    0,   86,    7, 7619,    0,
           68,  194,   65,   68,  386,   65,   29,  347,  192,   11,   18,   25,
          203,   22,   18,    7, 7619,    0,  109,    0,  276,  509,    0,  203,
           22,   18,  126,  155,  447, 7619,   10,  218,   94,    8,  229,  185,
          839,  106,   21,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,  103,   25,   87,  296,   10,  456,   80,  250,    0,   25,   73,
         1503,   21,    9,   13,  207,   17, 2700,   25,  211, 2693,  340,  181,
         1625,  369,    0,  860,   79,    0,   38,  187,  172,  213,   10, 1202,
           33, 1943,  411,  715,   22,    0,   29,   19,  296,   10, 2365,  785,
          825,   13, 1822,    8, 6015,   89,   79,    6,  103,   19,  192,   11,
           18,    0, 1189,    3,    2,    1,    1,    1,    1,    1,    1],
        [ 432,  392,  215,    9,  749, 3333,  399,    0,  106, 1553,  886,  371,
          322, 1921,  109,  106, 1553, 2792,    6,   10, 4100, 2353,  128, 3753,
            6,    0,   24,   11,  121, 1110,   17,   94,  213,   10, 1202,   55,
           13,  509,  858,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  103,   24,  487,  665,  199,   17, 2753,  813,    0,   24,   11,
           48,  150,   13,  800,   12,  170, 1522, 7777,    6,  126,   84,   46,
           12,  538,    0,   13,  325,   12,  422, 2753,  813,    0,   67,  113,
         3343,  237,    8, 8430,  813,    0, 4402,  106,  214,   17,   63, 1501,
         3919,  982, 1241,  155,  211,    6,   20,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,   77,    7, 2826,   25,  150, 2884,   62,  168,    0,   24,  154,
           84,   11,    6,   13,  453, 2212,   55, 6904, 1740,    6,   10, 6757,
            9,   33, 9648,    0, 2695, 1522,  111, 4112,   93,    8, 3066, 7806,
           12, 7016, 3001, 5605,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   53, 5164,  248,   12,  134,  132,    0,    8,   53,   66,  248,
          205,   59,  247, 1032, 1830,   35, 1859,   35,  820, 1047, 1103,  849,
            6,  693,   12, 7097,  214,   10,  134,    0,    8,   24,  289,    0,
           38,  200,  176, 1328,   91,  169,   25,  100,   10,  508,  132,    3,
           38,  187,   66,   10,  508,   91,  132,    3,   38,   93,   96,    0,
           24,  296,   91,   79, 2808,   12,    7, 2092, 1039,    0,    2],
        [  77,   17,   19,  217, 5290,    6,  106,  120,   19,  278, 3298,   13,
         2365,    9, 1051,   15,  760,    0,  461,   12,    7,  321,  762, 1486,
            6, 1411,   17, 6609, 4712,    3, 8867,  708,  106, 1122,  878, 2627,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  294,    0, 2076,   15,    6,    0,    0, 2638,   18,  810,    0,
         9344, 2544, 4513,    0,  159,  205,   35,  412, 5492,   10, 1661,   15,
            6,  436,   26,  521, 2391,   54, 1335, 1336,  480, 1611,    0,    0,
           12,  682,  649,  539,   20,    0,    9,    7,  774,   12, 5668,  187,
         2084, 1371,  241,  140,  609,  679,    6,    0,    0,  109,  941, 4058,
            6,    8, 3489,    6,    0,    2,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([53, 65, 41, 57, 42, 71, 38, 66], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8560],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(87.7319, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:26:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
tensor([[   9,   56,  525,   11,    6,    0,  179,    0,    7,  415, 1040,    0,
         4925,    6,  144,   13, 6149, 9417,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2129,   21,  689,   11,   18, 1620,   85,   77,    0,  109,   21,   11,
            6,  250,  994,    0,   13, 1192, 1810,  109,  185,  255,   45,   22,
         1974,  416,  279,    0,   67,    9,  419, 1165,   21,   11,    6,   86,
          461,   12,  948,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   80,   39, 1759,   35, 6937,   35, 7596, 6069,   12,
         5933,    0,   21,   11,    6,  278,   13, 2960,  109,   29, 3189,    6,
           69, 1219,    0,   69,   17,  926,   26,    7, 6342,    0,    8,  168,
           26,  185, 2260, 2852,  174,   54,    0,    8,   21,   11,    6, 3364,
           10,   33, 7373,   12, 3411,  221,   20,    0,    2],
        [ 554,    0,   85,   80,  423,    3,  215,   79,   13, 5030,   35,  181,
         2482, 2361,  170,    0,    8,   24,  288,  859, 1146,   80, 1992,    3,
          215,  581,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,  323,  101,   86,  135,   46,   21,   11,    6,   70,   24,   11,
          121,   77,   55,  606, 2490,   46,   17,   24,   11,   57,   86,    7,
          245, 1649,   10,  991,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 339,  110, 7032,   25,   10,   89, 4149,  264, 1039,    0,   38,  712,
          287, 1090,    9, 3164,    3,  166, 1505,   13, 3213,   15, 3634,  281,
         2523,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 432, 1655,   12,  203,    6,  856,    6,    9,  203,  372,   93,    0,
            7, 1269,   34,  591,    0,    8,   24, 6853,   62,    7,  203,  500,
          369,   12,    7, 1037,  359, 2481,  174, 1434, 1395,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 109,  305, 6908,    0,  440,    0,  149,    6,   96, 1744,   39, 2313,
           12, 2199, 1601,  690, 3474,  665,   55, 2519, 6629,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  71,   33,    9, 1066,    0,   24, 2534,   13, 1501,  264, 6129,  567,
           10, 1766,    7, 3708,    9,  419, 3440,   12,  672,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  274,   85,    7, 1912, 1375,    0,    7, 2384,   71, 1962,
         4266,   12, 5894,    0,   25,   73,  150,    9,  251, 2384,    0,  204,
            0,    7,  800,   12, 4224, 3883,    6,   26, 2585,   57,  484,   54,
           13,  325,   79, 3538,  909, 2460, 1068, 1143,  132,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,  185,   94,    9, 7419,  552, 1585,    8,   53,  246,    0,
           38,  160, 2402, 5448,    0,    7,  134,   20,   12,    7, 7419,  422,
         1611, 9797,   26,  913, 6811,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1239,    0,   19,  213,   10,  575,   25,  321,   12,   13, 7306, 6143,
            0,   67,    9,   21,   26,   89, 7143,   71,   70,   11,    6,  142,
           10, 1085,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  79,   25,   73,  934,    0,   19, 2138,   13,  325,   12,  183,   85,
         8145, 3069,    0,   68,  194,   65,   89, 3780, 2220,   11,   18, 2744,
            6,    6,  110,   55,    7,  473,  248, 1345,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 1321,  453,    0,    8,   25,  169,  205,  199,   13,  384, 1838,
          436, 3084,    8,   77,   12,   13, 5827,  150,   17,   69,    7,  225,
         1591,    0,    8,   21,   11,    6, 1968,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 7602,  110,   17,   21,  169,   66,  226, 3643, 3296,    0,   55,
          728,    6,  335,   10,  367,  270,  554,    8,  554, 2219,   69,    7,
          341,  183, 5331,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 276, 1445,   24,   11,  121,  367,   10,   33,  106,  341, 3773,    0,
           24,   77,   66,   10, 1529,   69,  108, 2410,   10,  468,    7,  207,
           17, 1459,  154,    6,   80,  896,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([20, 41, 57, 28, 30, 27, 35, 23, 23, 47, 31, 28, 34, 33, 30, 32],
       device='cuda:5') tensor([0.8584, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(65.0935, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[ 339,   11,    6,    0, 3522,    0,    0,    7,  179,   77,  336,  170,
           71,   33,  264, 7260,    0,    8,    0,  661,    7, 1192,  179,  106,
           13, 4149,  264, 3064,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   13,  277,  523,  143,  567,  411, 1181,    0,   89,
         5199,   26, 6224,  660, 1740,    6,    9, 2406,    6,   12,   77,  214,
            0,   29,   19,  135,   13,  277,  523,   80, 2567,   13, 8192, 1422,
            0,    2,    1,    1,    1,    1,    1],
        [   8,  103,   21,  144, 3548,   48,    0,    7,  858,   12, 6895,  169,
          914, 1220,   51,  168,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 225,  246,    0,   38, 3127,   21,   11,    6,   86, 3718,    0,  125,
           84,   26,  288,   91,  282,    0,    8,   84,  451,  288,   51,   91,
         1723,    3,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1732, 1107,    0,   77,    7,  126,  603,  234,   54,   12, 3429,  369,
            8, 6408,   17,  552,  106,  108,  753,   34,  250,  172,   17,  164,
         5258,    0,  700, 1917,   71,  110,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  434,   56, 1949, 3287,    9, 8393,    0,  206,   19,
         5747,   21,    0,   21,   11,    6,  434,   81, 1027,  372,  168,   69,
            7, 3156, 4710,    0, 1031,  287,    9, 5712,    0,   29,  606,  994,
         6228,    0,    2,    1,    1,    1,    1],
        [  29,  103,   24,  274,  336,  554,    0,   84,   63,   80,  717,   94,
          499, 3642,    0,    8,  204,   19,   11,   45,   86,  142,   10,  388,
           25,   69,    7, 4057,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1250,   12,   13, 8690,    0,  116,   33,  225,  265,  560,   48,   11,
            6, 7304,    0,    8,  101, 1119,   46,  101,   11,    6,    9,    6,
          300, 1011,   46,   38,  727,   19,   13, 2918,   17,   25,  169,  367,
           10,  110,   71, 3020,    6,    3,    2],
        [  24,  113, 5332,   62,  131, 3158,  959, 6037,  856,  502,    7, 2100,
           12, 9380, 8428,    6,   55,    7,  942,   12,  264, 1736,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   12,   77,    7,   94,    6,   17,   19,   11,  121,  700,
          226,   71,    0,    7,  281, 3586,   63,    7, 1097, 2415,   12,    7,
           43,   59,  371, 1644,  827, 1151,  384, 1954,  455, 1943,  455,    9,
         6869,  415,  237, 4599,  407,    0,    2],
        [  19,   34, 4022,  215,  801,   85,    7,  183,    0,    8,   19, 1425,
           17,   17, 3463,   24,  144,   13, 3286, 1565,   12, 1057,   13, 1269,
           71,   13, 4460,  384,  616,  140,   18,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,    0,  278,   10,   87,   70,  294,  596,   66,   46,    7,    0,
          728, 3174,   69,    7,   51, 3174,    0,    8,    7,    0,   38,  174,
          195,  195,  195,  195,    3,   89, 3780,    0,  552,   71,  110,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  29,    0,    7,  245, 1017,   12,    0, 2296, 3403,    6,    0,   53,
          467,  132,    0,    9,    7,    0, 3020,   81,   69,  284,    0,   19,
         1218,    0,   67,    7, 1590,   12,    0,  134,   63,  172,   70,   11,
            6,  528,  168,    0,    2,    1,    1],
        [  67,  120,   24, 2982,   10,   94,    0,   21, 2947,   62,   17,   13,
         2079,   22,  356, 2958,  144,  956,    9,  117, 6786,    6,   77, 1004,
            7, 1384, 1224,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1436,   59,  760,    8, 1546,  607,  322, 1873,   17,  145,   37,  144,
          226,   79, 1226,   80, 7103,  378, 4733,   79,  101,  144,  226,   80,
         1848, 7993,   21,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    0,    7, 1793,  958, 1006,  567,    0,    0,  188,  143,
            0,  254,   21,    6, 3718, 1452,   12,   56, 1251,    6, 2470,   22,
          140,  793,   46,   10, 4223,   21,    6,    0,  862,  233,  297,  833,
            0,    0,   10,   51, 1123,    0,    2]], device='cuda:7') tensor([30, 38, 18, 27, 32, 39, 30, 43, 24, 43, 33, 37, 41, 29, 29, 43],
       device='cuda:7') tensor([0.8560, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8857, 0.8438, 1.0000, 1.0000, 0.8413],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(55.1328, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[  67,    0,   21,  188,   10,    0,   51, 7118,    0,   29,   19,   11,
          158,  456,   80, 4312, 1098, 1994, 7289,    0,   68,  194,   65,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1057,  453, 4997,    6,   26,   86,  890,    0,    8, 1094,   24,   11,
          121,  226, 8160,    9, 3023,   71, 4997,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   26,  324,   10,   51, 1751,  111,    0,    8,   21,   11,    6,
          324,   10, 1082,  120,   86,   10,   51,    0,   67, 3695,   12,   17,
          818,   24,   66,   10,   51, 4493,    0,    2,    1],
        [  21,   11,    6,   13, 8308,    0,    8,   25,  154,    0, 8308,    0,
         1768,    0,   67,   33,   34,   86,  116,  419, 8308,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7,  245, 1289,   17,   19, 4415,  162,    9, 6062,    0, 1088,
            0,  140,    0,    0,   69,    7,  423,  322,   39, 2227, 1305, 1020,
           12,  984,  383,    0,    2,    1,    1,    1,    1],
        [   7,   94,  148, 5523,    9,    7, 1106, 2836, 3460,    6,   11, 2484,
         1639,   71,  134,   71, 2011,    8, 7889,    8, 3918, 1118,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 682,    0,   19,  641,    0,   70,  169,   21,  305,   10,    0,  690,
            6,  241, 2427,  108, 1507,    0,   10,  346, 7378,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 440,   19,  213,   10,  521,  616,    6,    6,  250,   10,   25,    0,
           67,  245,   12,   77,   19,   11,   45,  142,   10,  884,   25,   13,
         1323,   12, 1532,    0,    2,    1,    1,    1,    1],
        [  55, 4212,    0,   89,  277, 3460,    0,  101,   11,    6,  133, 7069,
          111,   13,  387, 2459,    0,  101,   11,    6, 1752,  352, 4041,    0,
          101,   73,   11,   18,  456,   85,   77,    0,    2],
        [   8,  120,   25, 2215,  251, 2636,  277, 4059,    9,  155,  874,    0,
          108, 1329,   26,   86,   10,  289,    0,   38, 5034,   85,  267,    0,
          225,   11,    6, 2636,    0,    2,    1,    1,    1],
        [  21,   11,    6,   86,  116,   17,   25,   11,  158,  492, 3121,  155,
         1832,  307,  109,  155, 1890,    6, 3903,    0,  109,  155, 1269,  120,
           25,   11,   57,   85,  838, 2739,  713,    0,    2],
        [  39,   19,  371, 4676,   34, 3489,    0,    8,  284,  595, 1764, 1949,
           62,  199,   94,   69,    7,  926,   12,    7, 2291,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  185,  841,    0,   17,   11,    6,   70,  956,    0, 1456,    0,
           10,    7, 1017,   12,   94,   24,  162,  665,   85,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1482, 2700,   25,   13,    0,   51,   18,    0,   25,  164,    0,  274,
           85,   13, 9096, 3158,    0,    8,   25,  164,    0,  150,    7,  858,
            0,    7,  858,  164,   51, 6461,    0,    2,    1],
        [   8,   19,  278,  172,  324,   85, 3044,   54,  138,  294,  955, 4925,
            6,   25,  169,  296,   55,    7,   94,  148,  162,  142,   10,   14,
            9,  117, 3539,    6,    0,    2,    1,    1,    1],
        [ 238,    0,   21,   34, 1968,    0,  853, 8637,    6,   89, 1066,    0,
           33,   26,   70,   24,  278,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,    0, 1005, 2432,   89, 1629,   20,  606,   10,    0,  270,  518,
            0,    8,  339,  110, 3890,   70,   19,  499,   66,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,   91,  854,   12,    7,   94,    0,   19,  388,    0,    9,   13,
          572,   17,   11,    6,    0,  321,   12,    7, 3505,    0,    0, 7518,
           48, 6129,  572,    0,    2,    1,    1,    1,    1],
        [  63,  155,  451,  373, 3163,    0,    8,   53,  162,    0,   53,  162,
         7070,    0,  211, 5060,  859,  291,   18,  234, 2871,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 244, 3455,   62,    0,  244,  335,   62,    0,  244, 4014,   20,    0,
         3572,   62,  126,    0,    7, 3349, 4609,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  409,  552,  126,   12,   60,    0, 1323,   12,  215,  581,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,   73,   21,   51,   17,   86, 1891,    7,  179, 6461,  111, 2700,
          170,   13, 6166, 5542,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   34, 2291, 6292,    0,  985,   35,  290,  551, 5588,    6,    0,
          743,   54,    8, 1809,  484,  945,    8,   10,  484,  945,    0, 2994,
         1344,    8,  853,   22, 1105,   57,    6,    0,    2],
        [  67,   33,   26,  113,    0,  103,   19,  305,   13,  488, 1396,  270,
          106,   33,    0,   33,   26,  113,   13,  133, 1392, 3046,   55,  110,
            0,    2,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([24, 22, 32, 23, 29, 24, 24, 29, 33, 30, 33, 23, 23, 32, 30, 19, 23, 29,
        23, 21, 13, 18, 33, 26], device='cuda:3') tensor([0.8950, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8906, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8774, 1.0000, 1.0000, 0.8340, 0.8726,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
 > at.  tensor(42.5344, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[ 115,  103,   19,  ...,    1,    1,    1],
        [  55,  663,    0,  ...,    1,    1,    1],
        [  55,    7,  473,  ...,    1,    1,    1],
        ...,
        [  33,   26,   13,  ...,    1,    1,    1],
        [  29, 4232,  818,  ...,    1,    1,    1],
        [ 251,   63, 3744,  ...,    1,    1,    1]], device='cuda:2') tensor([29, 22, 29, 26, 20, 21, 33, 26, 32, 27, 36, 30, 40, 35, 23, 34, 24, 45,
        38, 14, 20, 28, 16, 25], device='cuda:2') tensor([1.0000, 0.8721, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8198, 0.8774, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8545,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(46.3150, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[ 421,  156,   45,    0,    8,   19,   73,   11,   18, 3858,    0,   21,
           26,  172,    0,  172,  835,    0,    2,    1,    1,    1,    1,    1,
            1],
        [   0,   67,   33,  183,    0,    0,   21,   11,    6,   86,   13,  830,
         1159,   85,    7, 5491, 5522,    0,    2,    1,    1,    1,    1,    1,
            1],
        [  29,   84,   34,   86,   13,  475, 2212,   55,  488, 3012,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   55,   17, 1043,    0,    0,   21,   11,    6,    0,    0,  133,
            0,  133,  835,   10, 4971,  106,    0,    2,    1,    1,    1,    1,
            1],
        [   0,   71,  170,    0,   33,   34, 5655,   89, 2838,   37,    0,   10,
         3078,   46,    8,  294, 1573,    0,    2,    1,    1,    1,    1,    1,
            1],
        [  21,   11,    6,   13, 1244, 9484,   62,    0, 2963,    9,  131,  267,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  180,   86,  288, 2048,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 853,  174,    8, 2591, 1262,   93, 1618,   69,   13, 2639, 5207,  120,
          225,   34,  499,    9,  747,  670,    0,    2,    1,    1,    1,    1,
            1],
        [  38,  242, 1027, 1792,    0,  101,   11,    6,    9,    0, 7053,    3,
          225, 7348,  111,    0, 6181, 1011,    0,    2,    1,    1,    1,    1,
            1],
        [  21,   11,    6,   10, 4808, 5423,  131, 7376,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  66,   53,  226, 5747,   71, 3835,   10,  159, 2703,   35, 1408, 4506,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,  168,   24,   11,   57,  665,  106,    7, 2008,    0,  274, 2193,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 170,  443,  803,  607,    0,  131, 7692,    0,   26, 9475, 2070,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,  150,    6,   17,    7, 2918,  188,   33,  488,  258,  569,   69,
           21,    6, 3622,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   34, 3163,   10,  970,  565,  120,  101,   14,   48,   85,    7,
         1137,   12,  815, 1132,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [   9,    7,  832,    0,  156,    0,    0,   17,   11,    6,  143,  254,
          392,  439,   12,    7,  670, 1682,    0,    2,    1,    1,    1,    1,
            1],
        [ 116,  498,   71,    7,  800,   12,  943,    6,   84,    0,   63,    0,
            9,  108, 6249,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,   29,   24,   11,   57,  665,  168,   85,   91,   12,    7,  245,
          214,   24,    0,  487,   10,   87,    0,    2,    1,    1,    1,    1,
            1],
        [1573,  154,   17,   21,  188,  143,   12,   13,  837, 2883,    0,   17,
           21,   11,    6,  619,   10,  313,   48,    7, 1361,    0,    2,    1,
            1],
        [  69,  824,   22, 2366,    6,    8, 1911,  234,    6, 2366,    6,    0,
           19, 1082,  138,   10,   14,    0,    2,    1,    1,    1,    1,    1,
            1],
        [  25,   11,  158,  498,   10, 2613,  117,  214,  106,   70,   24,   11,
          121, 2861,   62,  490,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [4701,    0,   33,   34,   89, 1276,    0,  131,    7,  207,    0,    9,
          267,   51,  556, 4659,    9, 4517,    0,    2,    1,    1,    1,    1,
            1],
        [  24,  474,   24,  144,   13, 6785,   69,   77,    7, 5834, 1390, 4972,
           46,  417,  879,  303,   18,  111,   86,    0,    2,    1,    1,    1,
            1],
        [ 206,   87,   24,   56, 2150,  391, 2168,  581,    9,  486, 2508,  131,
            7, 1267,   20,  140,   48,    0,    2,    1,    1,    1,    1,    1,
            1],
        [   8, 1042,   25,   11,  121, 5424,   62,   21,  199,   13, 1780, 5651,
            0,   19,  213,   25,   10,  305,  185,   39,  322,  371,  505,   46,
            2],
        [   8, 6487,  188,  248, 9371,    0,   91,    0,   26,    7,    0, 2434,
          461,   12,   21,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,   24,  150,    0,    0, 3610,    0,   63,    0,  248,  244,  372,
         4635,    0, 9795,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [   7,    0, 1339,  265,  443, 3360,  258,  569,    0,  132,  378,  417,
         2862,  140, 1011,    0,  106,  284,  447, 1037,    0,    2,    1,    1,
            1],
        [   8,   19,   34,  116, 3122,    0,   19,   34,  204,  775,   93,   54,
           56, 1628,  284, 7004,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [1250,   12,   91,  524,  629,    7,  270,    8,    7, 1416,    0,    0,
          115,   19,   66,    0,  248, 1195,    0,    2,    1,    1,    1,    1,
            1],
        [ 225, 1359,   11,   18,   80,   10,  229,  132,   55,   33, 6673,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19,   11,   45,   86,  378, 5347,  119,  109, 1244,   35,  262,
         3449,  140,  829,   46,   19,  217, 1248,    0,    2,    1,    1,    1,
            1],
        [  29,  339,   11,    6,  305,   39,  663,    0, 2032,    7,  214,   17,
           63,  528,    9,  155, 1201,    0,    2,    1,    1,    1,    1,    1,
            1],
        [  17,   26,   77, 1995,   20,  135,   69,  984,    0,    8,   77, 1995,
           20,  296,   10,  135,    0, 2537, 3439,   65,   68,  386,   65,    2,
            1],
        [  70,  568,   21,   87,    0,   21,  583,    6,    0,   21,    6,    0,
          245, 3745,  373,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,   11,    6,   80, 4986,   79,  747,   79, 2593,   35, 7717,    6,
           63, 2015,   62,    9,    7, 1682,    0,    2,    1,    1,    1,    1,
            1],
        [  85,    0,    7, 1913,   12,    0,  419, 7004,    0,   25,  296,   10,
         6510,   70,   26,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 238,    0, 2574,   70,    0,   19, 1704,    0, 1320,  240, 2142,    0,
           68,  386,   65,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,   19,   34,  401,   17,    0, 1644,  606,   18,  187,  829,   71,
          134,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19,   11,   45, 3263,   10,   87,   77, 1587,   12, 5930,  722,
           29,   53,   11,  158, 1754,  132, 2465,    0,    2,    1,    1,    1,
            1]], device='cuda:2') tensor([19, 19, 12, 20, 19, 14,  7, 20, 20, 10, 14, 14, 13, 17, 18, 20, 17, 20,
        23, 19, 18, 20, 21, 19, 25, 17, 18, 22, 18, 20, 13, 21, 19, 24, 17, 20,
        17, 16, 15, 21], device='cuda:2') False tensor([[ 670,   34,  453,    0,    8,  635,  142,   10,  670, 4327,  110,  175,
           10,   33, 1827, 2260,  561,    0,   19,   11,   45,   86, 1123,    0,
           68,  194,   65,  476,   25,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  169, 4512,  132,    7, 7373,    6,    8,   56, 3430, 1004, 8425,
           51,  727,    6,   79,  103,   19,  513,  270,    9,  183,    8, 1505,
           13, 1269,  554,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 108, 1661,   57,  969,    6,  369,    0,    0,    0,   12,    7,  207,
            7, 1479, 1429,   26,  172,   46,    0,  188,  116, 4624,   62, 2447,
          297,   20,  586, 3042,    9, 4495,  215,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   77,  103,    0,   29,   70,  103,   24,  162, 1074,
            9, 7755, 1318,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   24,   66,   13,  325,   12, 6627,    8, 1064,    0,   67,   21,
           11,    6,   13, 4112,    0,   29,  138,   63,   25, 3263,   10, 9202,
           10,  175,  359,   17, 3805, 8487,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  837, 2245,   66, 2138,   13,  325,   12,  183,  665,   85,    7,
         4608,   12,  108,  955, 1234,    0,  109,  218,   94,   11,    6,  955,
         1234,    0,   69, 7955,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 109,   66,   53, 6510,   62,    0,  159,  447,  746,   12, 7372, 2243,
         1150,   17,   26,    0, 5709,   12,  639,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 846, 1026,  156,   26,   13,  133, 3818, 8043,   22,  148,  568,   39,
         4126, 1762,   12,  283,   71, 5200,   48,  128, 1144,    0, 1320, 7988,
           96,    8, 4269,    6,   55,  384,  232,  793,    0,    2,    1,    1,
            1,    1,    1,    1,    1],
        [   9,   77,    0,    7,  664,  726,  188, 8095,   62,    0,  143,  254,
         1998,  852,   94,    0,    0,    9, 7176,   20,    0,    0,  850,  699,
          352,  521,  249,  249,   59,  240, 1144,  574,   54,    6,    0,    2,
            1,    1,    1,    1,    1],
        [  29,    7,  546,    0,    0,   17,    0,    7, 1827,  968,   80,    7,
         6900, 1532,   34,   17,   84,  506,   51,   13, 1706, 4562,   10, 2322,
           54,    9,  570,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,   11,    6,   33, 4673,  639,  434,   13,    7,   59,  626,    6,
         3668,    0,   25, 9588,   21,    0, 2546,  436,   21,   10,    7,  859,
            0,    8,  155,  941, 6650,    9,    7,  637,  164, 8463,    0,    2,
            1,    1,    1,    1,    1],
        [1241, 6671,    0,   84,   26,   13, 6815,  634,   18,   12,  958,   46,
            8,   33,   26,   70,   11,    6,  434, 3057, 1181, 7347,    0, 1223,
          155, 2333,    6,   12, 4836,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 3570,  650, 1580,   35, 1533,  156,   46,   68,  200,  803,   59,
         1069, 4579,   65,   46,  166,   25,   73,  914,  116, 1598,   20,  111,
         1510,  168,    0,  368,    6, 5992, 1911,  668,  532,    6,   46,  133,
            0,  133, 5606,    0,    2],
        [1979,  122,    0,   21,   34,  432,    7, 2639,  181, 1178,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7,  245,   26,   17,   77,  117, 2671,   63,  116,   13, 3954,   35,
         2502,   12, 2248, 5561, 2519, 1221,   17, 1308,  213,    6,   10,  150,
         1085,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 164,   25,  601,   51,  159, 3806,    0,   38,  511,   19,  246,   10,
         1222,   19,  169,  583,   33, 1361,   12,  889,  197,    7, 8195,   12,
         3806,    6,    0,   38,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:0') tensor([31, 29, 33, 18, 32, 31, 21, 34, 36, 29, 36, 31, 41, 12, 27, 29],
       device='cuda:0') tensor([1.0000, 1.0000, 0.8594, 1.0000, 1.0000, 1.0000, 0.8677, 1.0000, 0.8862,
        0.8623, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(52.8034, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:26:28 | INFO | train_inner | epoch 001:    105 / 1474 loss=20.057, trans_loss=5.871, nll_loss=4.679, w2v_ctc_loss=22.349, task_loss=1.781, contrastive_loss=3.263, total=4211.65, n_correct=124.31, ppl=25.61, accuracy=2.952, wps=18152.1, ups=1.45, wpb=12566.9, bsz=472.1, num_updates=100, lr=4.098e-06, gnorm=2.875, clip=0, loss_scale=4, train_wall=83, gb_free=19.4, wall=142
2023-08-17 09:27:34 | INFO | train_inner | epoch 001:    205 / 1474 loss=16.63, trans_loss=5.866, nll_loss=4.699, w2v_ctc_loss=17.133, task_loss=1.712, contrastive_loss=3.236, total=4114.86, n_correct=114.36, ppl=25.97, accuracy=2.779, wps=18698.6, ups=1.52, wpb=12286.8, bsz=458.8, num_updates=200, lr=8.096e-06, gnorm=7.244, clip=17, loss_scale=4, train_wall=65, gb_free=19.3, wall=208
2023-08-17 09:28:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-17 09:28:40 | INFO | train_inner | epoch 001:    306 / 1474 loss=9.929, trans_loss=5.841, nll_loss=4.706, w2v_ctc_loss=6.897, task_loss=1.653, contrastive_loss=3.18, total=4088.29, n_correct=110.55, ppl=26.1, accuracy=2.704, wps=18357.4, ups=1.5, wpb=12212, bsz=441.8, num_updates=300, lr=1.2094e-05, gnorm=2.137, clip=0, loss_scale=2, train_wall=66, gb_free=18.6, wall=274
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 1 terminated with signal SIGTERM
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 131 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-08-17 09:30:00 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:14577
2023-08-17 09:30:00 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:14577
2023-08-17 09:30:00 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:14577
2023-08-17 09:30:01 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:14577
2023-08-17 09:30:01 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14577
2023-08-17 09:30:01 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:14577
2023-08-17 09:30:01 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:14577
2023-08-17 09:30:01 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14577
2023-08-17 09:30:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-17 09:30:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-17 09:30:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:30:02 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:30:02 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-17 09:30:02 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:30:02 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:30:02 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:30:02 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:30:02 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:30:02 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-17 09:30:02 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-17 09:30:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14577', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-17 09:30:06 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-17 09:30:06 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-17 09:30:06 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-17 09:30:06 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-17 09:30:06 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 09:30:11 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-17 09:30:11 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-17 09:30:11 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-17 09:30:13 | INFO | root | load pretrained hubert
2023-08-17 09:30:16 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 09:30:17 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 09:30:21 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 09:30:21 | INFO | root | share the sematic adapter and textual encoder
2023-08-17 09:30:21 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-17 09:30:21 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-17 09:30:21 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-17 09:30:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-17 09:30:21 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-17 09:30:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-17 09:30:21 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 09:30:21 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:30:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:30:21 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:30:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-17 09:30:25 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-17 09:30:25 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-17 09:30:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 09:30:26 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:30:26 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:30:26 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:30:26 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:30:26 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:30:26 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:30:26 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:30:26 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:30:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 09:30:26 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-17 09:30:26 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-17 09:30:26 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 09:30:26 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 09:30:26 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-17 09:30:26 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 09:30:26 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:30:26 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:30:27 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:30:29 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:31:19 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-17 09:31:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 09:31:19 | INFO | fairseq.trainer | begin training epoch 1
2023-08-17 09:31:19 | INFO | fairseq_cli.train | Start iterating over samples
False tensor([[   7,  422,  874,    0,    0,    9, 6788,  369,   12, 3648,    0,   26,
            7,  874,    9, 2761,   12,   13,  572,    0,    7,  572,   26,    0,
            0,    9, 2761,    0,   12,   13,  874,    0,    8,  722,   26,    7,
         7465,  131,  166,  251,    0,  248,   63, 3249,   62,    9,    7,  772,
          207,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [1179,   25,  135,   21,  109,   86,    0, 1335, 2348,   12, 1613,   71,
          747, 2693,   11,    6,    8, 1335, 2348,   12,   13,    0,  265,    0,
         6466,    6,  148,  205,   10,    7,  281,  909,  119,  236, 1563, 8562,
            9,  108,  753, 1772,    9,  909,   35, 3080,  109,  909,   35, 9376,
           54,    8, 4429,    0,    8,   53,  467,  132, 2762,  159, 2767,    6,
            0,    2],
        [  67,   21,  465,   11,   18,  468,    7, 2932, 1880,  567,    0,    8,
          113,    7, 2932, 3612, 1381,  832,   18,  233, 1181,   33, 3612, 1181,
         3860,   59, 1710,   10, 5111,   15,   21,    6,  768,   10, 5574,    7,
         1749, 1381,    8,    7,  341,  409, 1955,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   56, 1285,    0,  347,  192,   11,   18,   25,  450,  117, 6010,
           70,   21,   11,    6,  100,  378, 7000,   71,   33, 4716,  434,  427,
          821,  407,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0, 2753,  427, 1105,  810, 3565,   17,   21,  659,   51,
         1953,    6,   12, 1655,   12,  215,  801,    0,  166,  818,   17,   33,
         1167,  659,   66,  245,   13, 2003,   15,  106,    7, 1247,   12,   13,
          258, 1591,   17, 2963, 1585, 1763,    7, 1644,  511,  762, 1054,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,   13,  563,   93, 2484, 9473,   13, 4866,   10, 2266,
          389,  552, 1252,   11,    6,   38, 5001,  411,  293,    3,   24,   73,
          205,   10,   91,   12,    7,  822,  608,   56, 9694, 1075,    0, 3109,
         3902,    0,    8,  150,  113,   13, 1523, 3609,   12, 2718, 7719,    0,
          106,  747,    9, 1665,   10, 1962,    9, 1912,    0,    2,    1,    1,
            1,    1],
        [  33, 2780,   34, 6291,  111, 2770,   55,    7, 2932, 1232,    0,   67,
          115,   21,   11,    6, 2823,   54,  100, 3139,  140, 2704,    6,    9,
            7,  832,    0,    6,    0,    8, 2627,    0,  206, 2406,    6,   63,
         4213,   48,   10, 3067, 3316, 1006,   85, 3646, 1729,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 108, 2476,   26,   10, 8711,   17,   53, 1516,   71,   13,  509,  841,
           12,  138,   10,  229,  214,  254,  120,   53, 6565,    0,    8,    7,
         2027, 6936,  475, 2041,   17,   25,   73, 1531,  214,  126,  131, 1974,
          416,   54,  336,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([51, 62, 45, 28, 50, 58, 47, 41], device='cuda:0') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(90.0032, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:31:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
False tensor([[   9,   13, 1880, 4745,    0,   25,  498,   71,  211, 2837,    8,  211,
         4774,   10, 1393,   17,  199, 2332,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,  131, 3696,    7, 4221,    0,   24,   73,  446,  126,  261,
          143,   80,  155, 1037,  254,   25,  914,  135,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   11,    6,    7, 6099,   12,  958,    9,    7, 6869, 8521, 2353,
           12, 5205,    0,    9,  421,  234,   48,  365,  221,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,  162,   86, 2841,    9, 1979,    0,  290,    0,   69,    7, 5225,
           38, 2862,  781,  918,   93,  197, 1017,    0, 2503,   18,  650,  568,
           86, 2115,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [3398, 7936,    8,  284, 1543, 1948, 1985,   87,   21, 2373,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   84,   11,    6,  288,   91,  227,   15,    6, 2158,    0,    0,
          207,   10,    0,  205,  106,   84,    0,    0,   25,  175,  717,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,   10,  101,  128,    0,    0,   10, 1008,    0,   10,  289,    0,
           38,  533, 2056,   13, 1738, 1234,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 206,   11,    6,    7, 2808,    0,   38,  533, 1412, 2215,  159, 2360,
           10,    7, 2182,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 473,  111,    0,   33,   26,   13, 2322,   35, 5444, 2595,  856,  293,
            0,  125,   19,   66,   29,  294, 1516,    6,    9,   89, 1995,  918,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [7494,  185, 1118,  307,   34,   39, 1830,   35, 1933,   35, 1178, 2988,
          120,  101,   34, 1920,   22, 1371,   62,  106, 3022, 1146,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   33,   26,  116,  133,  946,   46,  183,   26,   69,    7, 3089,
           35, 7988,  340,    8,    7, 2509,   26,   69,    7, 1995,   35, 7988,
          340,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,    0,  169,  154, 1827, 1794,    0,  162, 4491,    0,  166,  818,
           53,  451,  601,   25,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,    8,   10,   87,   17,    0,   25, 1275,    0,   66,    0,   10,
         4988,   13, 1472,   12, 2066,  416, 1374,  572,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 1220,   25,  659,   51, 1211,   12, 1835,    0,   38,  187,
           11,   45,   86,    9,   18, 2687,  241,   54,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,   11,    6,    7,  613,   12,  117, 3742, 3228,    6,   17,   25,
           11,  121,  591,    0,   38,  511,  101,  246,    3,  238,    0,   19,
           11,  121,  211,  479,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [  33,  188, 8166,   19,  362, 1838, 1428,   55,   70,    7, 1479,   26,
          142,   10,   87,    9,    7,  858,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,   11,    6, 5653,   10, 1719,   59, 5292, 3147,  111, 1830,  439,
           55, 2677,    8,  248,   10,  391,  439,   55, 5031,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,  180, 2568,  347,   19,  305,    0,    0,   29,  261, 7003,  106,
         4990,  589,  596,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7, 2403,   26,   17,   84,   11,    6,   13,  785,   35, 9058, 3900,
            0,   86,    7, 1953,  322,   35, 3149,   35,  290,   35, 8856, 3900,
          100,   10, 4482, 2147,    0,   67,   13,  785,   35, 9058, 3900,    0,
            2],
        [   8,    7, 1714,  608,  423,  439,    0,   53,  305,   80,  248,  439,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 244, 2869, 4021,    0, 2008, 6248, 1405,   91,   12,    7,  281, 3861,
          338, 3232,  128, 4621,   24,  135,   80,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,   11,   45,  142,   10,  575,   25,  138, 7218,  204, 2895,    6,
           71,  108, 5967,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 110,  153,  240, 1547,    6, 3067,   62, 6873,    0,    8, 1645, 9745,
            6,    0,    0,    0,  593,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,  103,   25,  192,   11,   18,   66, 6759,   35,    6, 2174,  297,
           62, 3867,    0,  180,    7,  630,   12, 1703, 6230,   26,   13,  341,
           91, 5020,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:6') tensor([20, 22, 23, 28, 12, 25, 20, 17, 26, 24, 27, 18, 22, 22, 30, 20, 23, 17,
        37, 14, 21, 18, 19, 28], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8721, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8916, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000, 0.8887,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000], device='cuda:6',
       dtype=torch.float16)
 > at.  tensor(38.1071, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[1250,    0,   53,  ...,    1,    1,    1],
        [ 832, 1337,   22,  ...,    1,    1,    1],
        [  67,   25,  113,  ...,    1,    1,    1],
        ...,
        [  24,  278,    0,  ...,    1,    1,    1],
        [  24,  321,   12,  ...,    1,    1,    1],
        [  19,   11,   45,  ...,    1,    1,    1]], device='cuda:6') tensor([11, 11,  9, 13, 12, 12, 12, 12, 16,  9, 10, 11, 10, 11, 11, 12, 13, 11,
        11, 10,  9, 13, 17, 15,  7,  9, 11, 12, 17,  6,  7,  9, 10,  8, 10,  8,
        10, 11, 14, 13, 11,  9,  8, 19, 12,  9,  9,  6,  8, 16,  8, 12, 11, 13,
        10, 12, 10, 13, 10, 15,  7, 12, 11, 12, 13,  9, 12,  8, 13, 20,  8,  8,
         8,  7,  9, 11, 12, 11, 11, 12], device='cuda:6') tensor([0.8091, 1.0000, 1.0000, 0.8789, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000,
        0.8555, 1.0000, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8218, 0.8652, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 0.8984, 1.0000,
        0.8286, 1.0000, 1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8838, 0.8677, 1.0000, 1.0000, 1.0000, 0.8369,
        0.8652, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  False tensor([[  55,  663,    0,    9,  264,  109,  232,  221,    6,   24,   66,  682,
         3196,   22, 2932,  896,    0,  206,   53, 3860,   75,  241,  221,   77,
          236,  866,    8, 6387,    8,   29,  234,  452, 6029, 7401,    0,    2,
            1,    1,    1],
        [ 115,   24,   11,  121,  492, 1110,   13, 7123,  100,   33,   12,   13,
         1470, 3189,    0,   67, 1470, 3189,    6,   73,   51, 1346,    0,  276,
          103,   53,   11,   57,   86, 1110,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [ 168,   26,  138,  488,   12,   13,  775,  411,   37,   34,  859,    0,
           21,    0,   34, 1523,    0,    0,   10,    0,    0,  508,   25,   13,
          841,   12,    7,    0, 1823,    0,    0,    0,   84,   25,  205,    0,
            2,    1,    1],
        [ 115,    7, 1885, 5055,    0,   12,  538,    0,   26,  206,   13,  325,
           12,    7, 2394, 4793,    8, 2791,  513,  199,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,   21,  818,   17,   51,   20, 7931,  373,   63,  133,  324,   85,
          203, 4030,  140,   54, 2429,   35, 2611,    6,    0,    8,   29,   53,
           11,  121,  226,  529,   10, 3258,  251, 4513,   96,    0,    2,    1,
            1,    1,    1],
        [  24,   66,    7, 2216,  768,   10,  205,    0,   38, 5276,   26,   70,
           19,  213,   10,   87,    3,    8,    9,   13,  613,  982, 1479,    0,
           17,    0,   10,  110,    0,   26,   13, 1968,  279,    0,    2,    1,
            1,    1,    1],
        [  25,   73, 6265,  281,   12,   17,   13, 1674,    6,    0,  125,  103,
           25,   11,   57, 4298,  336, 3594,    0,    8,    7,  595, 3336, 1233,
         2233, 4457,  111,    0,   25,   11,   57,  492,  142,   10, 1713,   17,
          595,    0,    2],
        [  29,    0,   85,   33,  613,    0,   19,   87,  213,   10,  450,   25,
           17,  185,   12,    7, 2767, 4131,   24,   66,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 101, 1119,    0,   38,   18, 2704,  518,  155,  227,  715,   96,    3,
          125,   33,   26, 1320,  111, 2059,    0,   77, 2808,   10,    7, 9510,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  131,   13,  316,  287,    0, 7492,  136,  760,  322,   93,
            8,  697, 1262, 2178,  713,    0,    8,   53,   11,   57, 2091,    9,
         2567, 1507,    0,  116, 5000,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   0,    9,  264, 1884,  166,    0,   63,  142,   10,   51, 1405,    0,
           33,  169,   86,   51, 3118, 1623,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  67,   21,  465,   11,   18,  229,    0,   13, 1878,   10,  565,    0,
          103,    0,  284,  708,  162,  142,   10,   51,    0,   13, 2988,  109,
            0,   13, 2088,    0,   53,  162,    0,  142,   10,  205,   10,  670,
            0,    2,    1],
        [  67,   24,  499,  213,   10,   51, 1123,   17,   33,   26,    7,  281,
         4089,    8, 2426, 3180,   17,   24,   73,   51, 2985,  199, 2406,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  768,   62,  131, 1962,   35,  699,  119, 3050,    6,  411,
          639,    0, 7176,   20,  111,  752,   10, 1914, 4907,   10,  875,  134,
          199,    7, 1232,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,    7, 5714,   17, 7076,    9,    7, 3227,   12,  423,  759,
         2270, 1098,   18,  500,    6,  148,  618,   69, 1102,  926,    6,   12,
            7, 8783,    8, 6907, 4895,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  213,   25,   10,  150,  138,   33, 7223,   26, 5802,   62,    0,
            8,   21,   11,    6,  142,   10, 2546,  436,   29,   25,  150,   17,
           21,   11,    6, 1117,  126,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([36, 32, 37, 22, 35, 35, 39, 22, 26, 31, 20, 38, 26, 29, 31, 31],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8418, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8721, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(50.3735, device='cuda:4', grad_fn=<MulBackward0>)
False False tensor([[  67,    7,  143,   24, 1260,   85,   21,    0,    7,  143,   24, 1873,
           17, 1057,  453, 3540,   34,    7,  133, 1890,  279,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,    0,   24,  552,  132,   71,   13, 4445,    0,    8,    0,  204,
           70,  956,   34,   91,    0, 2767, 2923,   37,    9,  264, 1736,   46,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 211,    0,    0,   19,  641,    0,    0,   19,  154,   46,    0,  682,
            0,   67,   26,   17,  155, 4234,    0,  846,  174,    0,    0,  211,
            0,    0,  682,    0,   70,   34,   17,   91,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,   89, 9264,    0,   86,  860,   13,  488,   91,    0,   19,
          213,   10,  289,  116,   13, 1455,   80, 2263,  556, 5610,   54,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  142,   10, 4431,   35,  140, 4076,   69,  832, 1486,
          335,    0,    8,   24,   73,  150,  832, 1486,  335, 2546,  829,   69,
           21,    6,  926, 1585,   71,   21,    6, 4241,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [   9, 1296,   10, 2572,   25,   33, 1678,    0,   19,   11,   45,  142,
           10,  296,  185, 5825,    6,   10,  367,  132,   69,    6, 5072,  172,
         2117,    0,    8,   24,   11,   57,  142,   10,   87,   13,  277, 1860,
           35,  399,  102,  287,    0,    2],
        [  24,  465,   11,   18,   87,   33,  131,  204, 2905, 3141,    0,  100,
           25,   87,    9,   13, 1192,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17, 1073,   13,    0, 1878,  629, 1891,    7,  595,    0,    9, 1416,
           12,   25,    8,    0, 4958,   54,    7, 4292,    0,  109,  893,  199,
           39, 4292,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 115,   26,   33,  250,   17,   24,  451,   51,  101,   62, 1256,   12,
            0,   12,  538,    0,   21,   11,    6,  528,   17,   24,  135,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,   12,   33,   10,  289,   17,    7, 4719,  629,   70,   24,   73,
         7039,    8,   70,   24,   73, 2032,   26,  288,  142,   10, 4038,   22,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,  106,   38,  469,  291,   18,  187,   62, 1224,   12, 1491,
            3,    9,  564,  868, 1047,    0,   84,  162, 1107,  820, 5149,  690,
         7653,   20,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  60,  116,  188,   13,  264, 3742, 6211,   17,  368,    6, 1019,  908,
          768,  254,  108, 6211,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 5346,   12, 1009,   17, 1241,   13, 1127,  567,   26,    0,  554,
            0, 2816,  419, 1661,   57,  969,    6,  369,  109,  419, 1620,   54,
         5974,  440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 214,   63,  341,  125,  115,   19,  135,   17, 4751,   26,    7, 1037,
         2829,   17, 1459,  188,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 103,   25, 1260,   85, 2264,  241,  140, 1782, 5152, 1160,  158,   25,
          162, 2639,    0,   25, 2220, 1631,  830,  150,  347,   53, 5953,   48,
         6387,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,    0,  692,   10,   66,   13,  740,  672,   71,    0,    7,  230,
            0,  688,    0,   71,    7,  230, 4157,    0,   71,    7,  230, 1404,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([23, 25, 34, 25, 35, 42, 19, 28, 26, 26, 28, 19, 28, 18, 27, 26],
       device='cuda:0') tensor([1.0000, 0.8755, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8677],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(49.9238, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:31:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
False tensor([[  67,  108, 8830,   48,  589,    0, 7557,    6,    0,  166,   63,    7,
         7557,    6,   17,  601,  170,   10, 6108,    8, 2544,    8,   51,    0,
            0,   13,  562,   18,    0,    8, 9063,   62,    8,   29, 4404,    8,
           29,   69,    0,   53,    0,   63,  261,  143, 7678,   62,  131, 1912,
          688,    0,    2],
        [  24,  116,  144, 6985, 2421,    6, 4502,   80,  854,   39, 3350,  581,
           77, 1004,    7, 6869, 9271,    0, 3381,   70,   11,    6,  956,    0,
            8,   24,   11,  121, 2138,   13,  325,   12,  183,    9,    7, 1726,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 185, 2193, 6248,   22,    6,    9, 1777,  150,  156,   79,   93,  675,
           45,    9, 5222, 1290, 1373,    6,    6,  693,    0,   67,  294,   73,
           51, 5853,  131,    7, 2932, 3288,    0,    8,  203, 1203, 1015,  922,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 106, 1244,   35, 1966,  673,   54, 2409,   10, 2465, 3602,    6,   10,
           33, 2178, 2445,  266, 1192,   69,   89,  906,    0, 3927,   63, 4015,
         5869,   12, 2434,  108,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  154,   21,   26, 2497, 1226,   55, 1221,   10, 1772,    9,    7,
          245,  561,  929,   13, 2226, 1757,   55,   70,  169, 1085,   10,    7,
          461,  266,  763,  480,    6, 1042,    7, 3677,  188, 3699,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7, 4100, 1649, 1085,  143, 1149,    0,   67,  914,   69,    7, 1296,
           12,   13, 4400,   15,   22,  996,    0,  333,  555, 7691,  109,  333,
          555, 3619,  215,    0,   67,   21,   11,    6,  499,  250,   10,   51,
         4837,   12,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7745,  659,   51,    0,   67,   21,  689,   11,   18, 4432,
           10,  452, 1539,  128,    6,    0,   17,   24,   66, 5020, 6083, 5475,
            6,   46,    7, 1320,   59,  816,  300, 1079, 1643, 1794,  230,   85,
            7, 1219,  859,   35, 2514, 4209,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   9,  409,    0,    7,   29,   70,   26,    0,    9,    7,  572,    0,
         2317,    9, 7418, 3782,    0,    7,  214,   17,  229,  155,  572,  283,
            0,    7,  214,   17,  229,  155, 3236,  283,    9,  155,  572,    0,
           63, 8220,  111,  415,   59, 8186,   71, 2317,    9, 2011, 3043,    0,
            2,    1,    1]], device='cuda:7') tensor([51, 38, 38, 31, 36, 40, 44, 49], device='cuda:7') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(71.0548, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[ 131, 1183,  148,   26, 1948,  664,   62,  132,   71,    7, 4617,  292,
         2109, 1775, 4040,    6,   12,   33, 6536,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   19,   11,   48,  100,   10,  227, 2174,  232,  445,  159,  283,
           71,   13,  277, 7074, 4112,    0,    8,    7, 4112,   26,   33,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1425,   19,  144,   10, 1336,  235,    0,   19,  442,   17, 3486,
            8,   79, 2392,   79,   19,  442,   21,    0,   39, 2285, 1790,   12,
         2922,  552,  244,  110,    0,    2,    1,    1,    1,    1],
        [ 877,  290, 2689, 2345, 1212,  188, 2277,   48,   10, 2699,  143,    8,
          143, 5593,   10,   33,    0, 5413, 3301, 5690,    6,  199,  459,  510,
           96,   17,  719, 2070,    0,    2,    1,    1,    1,    1],
        [  29,   24, 6103,  154,   12, 4781,   79,  378, 3816,  214,   17,   24,
          991,   55,  133, 3926, 3965,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  12,  538,    0,  108, 4734,  192,   11,   18,  116,  468, 1004,  183,
            0,   53,  113,  468,  106,  561,   10,  561,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,   66, 4595, 1122,   59,  699,   35,    6,  819,  424, 1942,   96,
            8, 4515,    6,   17,   63,   86, 3412,   62,  131,  883,   48,  293,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103, 2263,   96,  498,    0,   10, 3424,    0,  180,    7,  723,
           73, 5415,    0,    0,    7,  524,    8, 3536,    7, 2517,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 427,  412,  430,  158,   13,  188,   13, 1905,  321,   12, 6873, 1117,
           17,    0,  120, 8491,   48,    0,    7,  427,  412,  430,  158, 3336,
           10, 1051, 1672,  153,  336,    0,  116, 7743,    0,    2],
        [  29,  417,  140, 2362,    6,   93,   26, 3423,   13, 1396, 2233,  199,
           39, 3993, 1874,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  188,  226,    7, 3841,    6,   20,   12,  333, 1670, 2127, 9746,
            6,    0, 3448, 7414,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  225, 4576,    6,   10,  110,    7, 1790,  225, 1831,   79,   13,
          785,   35, 1933,   35, 1178, 3642,    9,   17, 6460, 4057,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  116,  323,   13, 4454, 2761,   69, 5335,   80,    7, 5169,    8,
         1095,  248, 8099,    6,  558,   10,  545,  218,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 4639,   17,   24,  220,   51, 1155,  143,  254, 3857,    8, 1064,
            0,   67,  138,   10, 3918,    6,   20,  126,    7,   91,  106,    7,
          218,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2551,   12,  324,    8, 6790,  214,   26, 2720,   48,   10,
         7572,    8,  211,  234, 1410,  837, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  101,   11,    6,    0,   13, 4961, 1531,    0,    0,   79, 3708,
           18, 3304,    0,  169,   66,  246,    0,   79,  238,   79,   13, 1880,
         1531,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   10, 4637,  373,    0, 2192,   26,  116,  486, 6101,  567, 4111,
           10,   51, 4637,   62,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 8900,   62,  250,   19,  172,  144,   13, 2184,   22,  311,   69,
            0,   26,   17,    7, 1956,  204,  213,    6,   10,  283,   55,  159,
          110,  128,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,  593, 1706,   12,   13,  183, 2760,   55,   77,
            7,  341, 1565,  409, 1215,   17,   24,  135,   10,  468,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1777,  188, 1220, 6380,   48,   17, 2439,  558,  464,    0,   53,   11,
           57, 5332,   54,   13, 4369,  200, 1515, 2902,    8, 3585,  567,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   63,  423,   12,  117, 3357,    0,    8,   53,   63, 4673,    9,
          159, 4268,    8,  159, 4972,    0,    8,   53,   63, 2762,   94,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  976, 8372,   17,   24,   66,  535, 1313, 5260,    7,
          613,  206, 4387,    6, 3480,  108,   24, 1591, 2637,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  717,   35, 1315,  322,  801, 5725,    6,  221, 1339, 5061,  292,
            6,    6, 3222, 1026,   14,   48,   71,   33,    9,   21,    6, 9442,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  69,    7,    0,  939, 2836,  834,   26,    0,    7,   56,  111,    6,
         3620,  266,  207,   17,  368,    6, 2829, 7337, 7393,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([20, 25, 30, 30, 20, 22, 26, 24, 34, 17, 18, 24, 22, 27, 21, 27, 18, 28,
        24, 25, 25, 23, 26, 23], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8550, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8975], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(42.7609, device='cuda:7', grad_fn=<MulBackward0>)
False False tensor([[ 800,   91,    0,  103,  886,  424, 3848,   11,    6,  853,  200,  237,
           26,  593, 3401,  120, 3213,  187, 2044,    6, 1570,    6,  199,    7,
          964,    0,  568,   17,  641,   21,   11,    6,  735,  226,  593, 3401,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24, 8396, 1469,  106,    0,  108, 2914,   37,   93,    0,  131, 2532,
          336, 3494,    0,    0,   67, 1223,   24,   77,   66,   33, 1738,    0,
         2914,   37,   93,   17,   24,   63,  746,    0,   12,    0, 5472, 1117,
          108, 3183,    6,    8,    0, 1459,  994,   26,  126,   84,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2937,  982,  218,  943,    6,    9,  108, 6249,  451,   66,  774,   62,
         3928,    0,    8, 1800,  282,   13, 2333,   10,  175,  850, 1408, 1072,
            6,    0,  109, 3119,  294, 2518,   12,  215, 3928,  254,  956,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  125,   29,  294,   12,    7, 2787,    9, 1370, 2924,   11,   18,
          116,    9,  670,    0,   53,   11,   57,    9, 1037,    8, 1237,    0,
           70,   25,  113,  296,    0, 6957,    0,   26,  143,   69,    7,  230,
          874,  926,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  103,   25, 1816,   66,   13, 5590,   20,  521,   20,  874,
           93,    0,  115,   11,    6,   13,  324,  183,   10,  388,   21,    9,
          155,  874,    8,  598,   21,    0, 2603,   69,    7,  467,    0,   17,
           11,    6,  916,    0,   25,   11,  158,  446,  126,  347,   21,   11,
            6,  434,   13,  862,  365,  232, 1218,   20, 5590,   20,    0,    2,
            1,    1],
        [ 248,  215,  581,   85, 1366,    0,   19,  154,   46,   19,   11,  121,
          367,   10,   33, 7034,   46,   19,  154,   19,  659,   66,  226, 5187,
          106,   13, 3643,  384, 7710,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,    0,    0,    0,    0,    9,    0,   89, 6525,  230,
          115,    0,   38,  187,  144,   33, 1850, 2536,   48,    0, 1850,   62,
           69,    0,    7, 1850,    6,   20, 2286,  307,    0,    0, 4813,  248,
          215,  581,   69, 2154, 1374,   18,  881,   11,    6,  383,    0,   21,
           34,    7,  133, 2354,    0,    7,  473, 2829,    9,    7,  535, 8311,
            0,    2],
        [  29,  115,    0,  103,   24,  274,  270,   85,    7, 3253,    0,   85,
          108, 8196,    6,   17,  162, 5690,   54, 1004,    7, 3253,    0,  117,
         8196,    6,   63,  100,    7,  384,  174, 2003,   12,  108, 2626,  567,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:2') tensor([38, 48, 39, 40, 60, 31, 62, 38], device='cuda:2') tensor([1.0000, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(78.3692, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[ 225,    0,  509,  ...,    1,    1,    1],
        [ 101,  876,  140,  ...,    1,    1,    1],
        [1979,  265,    0,  ...,    1,    1,    1],
        ...,
        [ 109,   63,   24,  ...,    1,    1,    1],
        [ 117,   63, 4700,  ...,    1,    1,    1],
        [   8,   33,  321,  ...,    1,    1,    1]], device='cuda:2') tensor([14, 12, 21, 10, 15, 16, 11, 12, 18, 11, 13, 17, 11, 18,  9, 13, 12, 24,
        19, 14, 20, 21, 20, 19, 17, 18, 19, 24, 10, 15, 10, 13, 18, 14, 10, 15,
        17, 17, 19, 22, 21, 17, 19, 18, 20, 14, 12, 12], device='cuda:2') tensor([1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740, 0.8652, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8677, 1.0000, 1.0000, 1.0000, 0.8979, 1.0000, 0.8882, 1.0000, 0.8799,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
 > at.  tensor(24.2805, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[   7,  245,   91,   26,    0,   25,  305,   17,  524, 4474,  199,   21,
            6,  523,    6,    8, 3675,   29,   17,   25,   73, 5382, 7394,  251,
          523,    6,    8, 3675,    0,    8,  180,   12,  538,   25,   87,    7,
          744,  461,    0,   25,  388,   77,   12,  117,  523,    6,    8, 3675,
          270,  540,  554,   10,  367,   10,  155, 7034,    0,    2,    1,    1],
        [  24,  169,  492,   87,  860,   13, 3310,  279,    0,    0,   67,   46,
            9,  419,    0, 1165,    0,   24,    0,  591,  103,   24,  116, 1223,
         2396,    0,  134,    0,   77,  346,    8, 1296,   62,    0,  134,    0,
            0,   17,  281,    0,   94,  169,  204, 2990,   70,    7, 1296,   54,
          451,   51,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1141,   26, 1111, 6504,    6,    0,    8, 1008,    0,  423,
           12,  117, 3049, 4515,    6,  162, 1028, 1004,    7,   13, 1966,  407,
          816,  333, 1127, 1303,    0,   29,   70,  162,  117, 1816,  401,   71,
           77,    7,  839,   53,  162,  961,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [7616,    8,  415,  584,   26,   91, 4401,    0,    0,    0,   68,  386,
           65,   67,    9,  108, 5605,  336,    7,  179,    0,   24,   66,   77,
         1587,   12,  218, 4401,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,  278,   13, 1335, 1793, 7470,   97, 1411,   37, 2091,  890,
           10, 1557,   13, 1543,   12, 9829,    6,   10, 3022, 1146,   71,  170,
           10, 3522,    7, 4749, 5242,   11,    6, 1051, 2256,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   21,   11,    6,  321,   12,  100,    0,   25, 1057,   13, 1329,
            9, 7419,    0,    8,  180,   25,  175, 6899, 1314,    0,   10,    0,
          574,    0, 9784,    0,    0,    0,    8,  155,  886,  153,   26,   86,
         1767,   80,   33,  125,   25,   11,  121,  278,   10, 1703, 1404, 1610,
           57,    0,    0,  125,   25,   11,   57,    0, 2107, 1978,    0,    2],
        [  29, 8093, 8187,    6, 1452,  736,  439,   12,  159, 1422,    8,  736,
          439,   12,  159, 3857,    0, 6872, 1752,   35,  494,   15,   18, 1568,
         8187,    6, 1452,  736,  439,   12,  159, 1422,    0,   67,  116,  100,
          419, 3460,    8, 4197,    0, 1452,  288,  655,  439,   12,  159, 3857,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53, 1918,   10, 2554,  392,  759, 1086,    0,   29,    0,  100,   29,
          294, 1881,  440,    0,    7, 3516,  278,  540,    8, 6732,   62, 5725,
         2502,    6,    0,    8,  853,  174, 8227,   48,    0,   25,  150,    0,
          853,  174,  689,   11,   18,  703,    9,  906, 2937,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([58, 52, 44, 31, 35, 60, 50, 48], device='cuda:2') False tensor([[  67,  490,   24,   87,   17,    0,   19,   11,   48,  100,   25,   10,
          154,   17,   85,  419, 1800,  183,  120,   25,   11,   57, 2202,   84,
            0,    9,  155, 1992, 5896, 1247,    0,    7, 2583, 8748, 9169, 3413,
           54,   10,  155, 9879,    0,    7, 9879,   26, 3413,   54,   10,  155,
         2583, 8748, 9169,    8,   33,   26,  138,  155, 4941,   26, 3803,    8,
         8965,   48,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [5609,   10,    7, 1793,  183,  368, 5673,    0, 1276,    6,  499,   87,
         4986,   79,  261, 1269, 2536,   20,   79, 1970,    6,    0,  166,   26,
          509,  254,   21,   34,    9,   95,  424, 3400, 2197,   11,    6,  383,
            0,   67,   19,  499,  154,   17,  250,  225, 2396,   26, 4803, 5040,
            0,   38,  187,   66,   86,  226, 2373,    9,    7, 9091, 1313, 9094,
            3,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  56,  525,   11,    6, 4197,  246,   10,  267, 1848,    0,   38,  160,
            0, 1491,    0,    0,   33, 4925, 2823,    6,   55,  815, 6091, 1086,
            3,  267,    0, 1848,    0,  148,   63, 1102, 4907,    0, 1260,   69,
            0, 4340,  982,    0,    0,   38,  511,   17,   11,    6,   86,   77,
           46,  415, 1040,    0,   26, 1028,  126,    0,   71,   13,  264,    0,
         1375,    0, 2199, 1132,  820,    3,  225,    0,    0,  246,    0,    0,
           38, 2207, 4925,  164,    0, 2823,   55, 1581,    3,    3,  225,    0,
         8640,   48,    8,  246,    0,    0,   38,  187,  192,   11,   18,  135,
            0,  103,   17,   11,    6,    0, 1581,    3, 1995,  241,  221,  109,
            0, 1581,    3, 1793,    0, 1086,    0,    0,   67,    0, 3296,    0,
           21,   11,    6, 1581,    3,    3,   68,  194,   65,    2],
        [   8,    9,    7,  370,  207,  294,   12,  170,   63,  115, 5954, 4273,
            6,   17, 3412,  108, 1259, 1882,    0,  108,  203,    6,  777, 1991,
            0,  108, 3857,    0,   69,    7, 1080,    6,   17,   33,  659,  601,
          170, 3560, 4390,    0,   24,   73,  884, 1179, 5625,   54,    8,   39,
          128,   93,  195,   54,    7, 1289,   24, 2056,    0,   24, 8099,    0,
           24, 3836,    0,   24, 2153,    0,   73,  450,  170, 4324,   12,  183,
         1179,  250,  659,  205, 1226,   71,  108, 3227,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   71,   33,  822, 5108,    9,   13, 3708, 4774,    0,   24,   66,
         6960,   48,    9,  824, 2723,   45,  726, 1090,   17,   24,   73, 3814,
            7, 1729,   35, 1727,  430,  690, 3275,  110,  249,  132,   10, 1421,
          439,   12,   70,   26, 5914, 1417,   46,  248, 1086,  690, 3275,  110,
          249,    0,   24,   87,   21,   85, 7738,    0,  820,  828,    0,    8,
           24,   73,  499,  875,   17, 3151,  346,    0,  630,   12, 1823,    0,
          103,   25,   73,  875,    9,  143, 3708,    6,    0,   24,   73,  204,
          229,    7,  126, 2446,  276, 3028,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 875,   54,   94,  970,   10,  970,   71,  108, 3648,   26,   13,  207,
           12,  875,   54,  134,  970,   10,  970,   71,   94, 1004,  183,    0,
         1004,  672,    0, 4355,  931,  659,   66,  226,  133,  341,   10,  108,
          447,    0,   67,  148,    0,  100,  170,    0,  144, 1080,    6,    8,
         4989,    0, 2084,  335,   18, 1991,    6,    8, 3128, 2261,    9,  159,
          931,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([ 64,  65, 130,  82,  92,  63], device='cuda:3') tensor([1.0000, 1.0000, 0.8271, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
 > at.  tensor(163.2629, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[  67,   25,  296,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        [  29,   70,  321,  ...,    1,    1,    1],
        ...,
        [  29,   24,   73,  ...,    1,    1,    1],
        [ 229, 1118,   63,  ...,    1,    1,    1],
        [ 103,   25,   11,  ...,    1,    1,    1]], device='cuda:3') tensor([ 9,  9, 13, 11, 14, 10, 14, 16, 15, 10, 13, 13,  9, 18, 11, 17, 20, 14,
        17, 14, 13, 11,  7, 14,  7, 14, 13, 13, 16, 10, 12, 14, 13,  9, 11, 13,
        12, 14, 11, 12, 15, 10, 11, 11, 14,  9, 10, 11, 15, 13,  7, 15, 15, 13,
         8, 17, 13, 20,  7, 16, 14, 13,  7, 13], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 0.8105, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8555, 1.0000,
        1.0000, 0.8579, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000,
        1.0000, 0.8984, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8369,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8882,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:3', dtype=torch.float16)
 > at.  tensor(18.6333, device='cuda:3', grad_fn=<MulBackward0>)
False False tensor([[  67,   24,  150,   17,  545,   91,   12,  117,   26,  204, 4637,   54,
            0,    8,  188,   13,  341,  207,   12, 3101,  126,  138,   10,   87,
          282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   89, 1970,   34,   13, 3090,  756,    0,    8,   89,  291, 3769,
            0,    0,   34,   13, 1390,   37,  684,    8,    0,  113,   13, 1116,
         1040,   37,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  164,  450,   25,    0,   13,  546,   12,  248,  341,    0, 2352,
            0,    0,  248, 1284, 2352,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  391, 1551, 1065,    0,   17, 2182,  144, 3814,   48,   89,  637,
            8,  333,  473,  279,    9,   21, 3706,   55,  110,   10,   13, 1098,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 3195, 6482,   35, 6937,  227, 3134,  221,    0,   19,    0,   20,
            0,  108, 2805,    0,  132,   10,  108, 7009,    6,   11, 1492, 1485,
           12, 3402,   93,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  246,   53,  169,  780,   29,  835,   10, 1008,    7, 8365,    6,
           12,    7, 3311,    0, 7443,   17, 1573,  162, 3754,    8,  401,    7,
          370,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  289, 1228, 3374,    0,  281,   12,   25,  154,   80,  708,
            0,   21,   11,    6, 1301,    0, 2703,  901,  439,   12,  708,   87,
          722, 1228, 3374,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 291, 1105,   18, 3396, 1095,    0,    7, 4480,   79, 1019, 2071,    0,
         1095,    7,    0,    0,    0, 5640, 1375,   79, 1019, 2071,  755,    0,
          254,   94,  148,  162,    9,  509, 1862,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  116, 2213,   54,    8, 9171,    0,   38, 3127,   89,
         3397,   26,    3,   70,   11,    6, 1226,   71,  682,  616,  249,  407,
            6,    0,   38,  187,   11,  121,  144,  185,  453,  110,  128,    6,
           85,  682,  616,  249,  407,    6,    0,    2],
        [ 101, 5523,   17, 1381,  220,   51, 1405,  131,    7,   94,    0,    8,
           24,  583,  251,   94, 9475, 4637,  373,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  162, 1052, 1563,   94,    0,   67,   53,  162,  113,  172, 7176,
           20, 3979, 2120,    0,    8,   53,  162, 9231,   62,   12, 4391, 3834,
           12, 3716,   35, 7305, 2527,   15,  609,   96,    0, 5653,   10,    7,
          218,  415, 2633, 2120,    0,    2,    1,    1],
        [  85, 4615,    0, 9785,  369,   46,   17,  101,   11,   48,  226,  987,
         1181,  131,   39, 6651, 2266,  160,   22,    0,   13, 5289,  126,   10,
         2585,   20,  366,  565,    0,  276,   10, 9304,    7,  282,  126,   12,
          565,    0,    2,    1,    1,    1,    1,    1],
        [   0,    8,   53,  276,    0, 2982,   80, 1790, 8661,    0,    0,  276,
            0, 1445,   53,  162,    9, 3454,    0,  276, 1445,   53,  162,  830,
         1785,  810,    7, 5167,  608, 2020,   12,  159,  931,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  117, 4312,   22,    6,    0, 1025, 3043,   54,    0,   53,  415,
          158, 1515,   71,    7, 7224,    0,    8,    9,  117,  415,  158,  340,
         1955,   53, 3121,   13, 1905, 1762,   12,  941,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   85, 8195,   12, 2931, 2735, 1221,   46, 1016, 4618,
          866,  693,   46,  120,  248, 4618,  866,  693,  162,   86,  952,   10,
          545,  218,    0,   19,  169,  939, 8966,  834,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 131,    0, 5114,  235, 2399, 1006,    0,  889,   63,    0,    0,    9,
           13,    0,  841,    0,    0, 1449, 1778,  159, 5186,   10,    7, 1722,
         3609,   12,  422, 6222,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([27, 28, 19, 26, 29, 27, 29, 33, 44, 21, 42, 39, 35, 34, 34, 30],
       device='cuda:1') tensor([1.0000, 0.8887, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000, 0.8208],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(58.5012, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[  53, 2828,  987,  ...,    1,    1,    1],
        [6938,   25,   66,  ...,    1,    1,    1],
        [   8,   12,  538,  ...,    1,    1,    1],
        ...,
        [  29,   24,   66,  ...,    1,    1,    1],
        [  67,  985,  687,  ...,    1,    1,    1],
        [   0,    8,  432,  ...,    1,    1,    1]], device='cuda:1') tensor([14, 13, 27, 18, 21, 17, 19, 15, 18, 17, 24, 12, 15, 15, 33, 26, 20, 11,
        22, 14, 15, 22, 24, 13, 20, 16, 15, 17, 15,  9, 16, 21, 15, 13, 16, 26,
        19, 13, 24, 13], device='cuda:1') tensor([1.0000, 1.0000, 0.8550, 0.8975, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8848, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8677], device='cuda:1', dtype=torch.float16)
 > at.  tensor(28.6373, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[3929,  155,  874,  103,   25,  154,   25,  135,    0,   84,   63, 1230,
            0,   91,   26,  850, 6566,  131,    7, 2932,    8,    7,  218, 1498,
         5804,   10,  170,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    7,  467,    0,    7,  453, 6578,  148,   34,    7, 1951,   12,
           13, 1318,    8, 5555,   55,   24, 6868,    7, 1406,    0, 2082,    8,
          858, 4007,    6,   10,  367, 4404,    8,   51,   13,  461,   12,    7,
          546,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   21,   11,    6, 3849,    0,   21, 3560,    6,    0, 1259, 1599,
            0,    0,   21, 2293,    6, 1890,   13,  525,  287, 7195,    6,    8,
         1890, 3577,  901, 7195,    6,    0,  100,    0,   56, 2175, 1488,   35,
          868,    6,    0,  166,   26,  133,  341,  106,    0, 1120,  419,  218,
         2475,   12, 6293,    0,    2,    1,    1,    1,    1],
        [  21,  204, 1847,  214,   24,  169, 2807, 3301,   46,  214,  100,  150,
           48, 2184,    6,  156,    6,  109, 4377,   93, 2058,  424,    6,    6,
           46,    8,   73, 2575,  134,  199,   13, 4458,   18,  160, 1032, 1116,
          111,   45,   37,    0,  166,   25,   73,  774,  199, 1120,  419, 1862,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1384, 1224,  368,    6,   80,  423,  759, 9162,    6,   13,
          383,    0,  166,   26,   80, 1450,  439,   12,   77,    7, 1947,  619,
         5948,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1291,   22,  140,  416,   22,  110,  556, 1949,   11,    6,   13,  391,
           35,  511,   35,  290,   35, 6979,   35,  715,  234, 1927, 3156,   12,
         1954, 6935,    9,    7,   43,   59,  371, 1644,  827, 1151, 3144,    6,
            0,   85,   80,    3, 5494, 1360, 5997,    0,    8,   19,   11,  121,
          226, 1797,   54,   84,   55,  294,  215,    0,    2],
        [  94,  506,  204,  498, 1557,   54,  126, 8282,    6,    0,  934,    0,
            0,  109,    0,   53,  506,   86,   66,    0,   13, 4561,  583,   71,
         1230,   94,   10,    0,  456,   80,   13, 6711,  120,   53,  220,  116,
           87,   13,    0, 4454, 3836,    8,    0,  175,   21,  691,    0,   71,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   7,  853,   22,  335,   26,   17,   25,   73,  446, 2533, 5203,  336,
           21,    0,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,   11,  121,  116, 3498,   25,   63, 3302,   12,    7,  133,
          946,    8, 9743, 1081,   17, 5577,    6,    8,  896,  227, 2174,  232,
          445, 6442, 1052,   62,   48,  562,    6,    8, 1122, 1079,  292, 3751,
            6,   73, 9478, 2808,   55,  159,  447, 3965,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  274, 6269,    8, 6269,  199,    7, 1710,   12, 5554,    0,    8,
            9,   17,  207,   24,  175,  914, 4616,    8, 4616,   10,   33, 2570,
         2240,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  218, 2604,    6,   12,  813,    0,   79,   25,   11,   57,  211,
         6449,    0,    0,    0, 4837,   63, 2875,   80,  860, 4825,    6,    0,
          106, 3836, 4506,    6,    0,    0, 1817, 4506,    6,    0, 1817,  837,
         5002,    8,   29, 4404,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   77,   87,   33,   13,  277, 4838,    0,  166,    0,   26,
          347,   24,    0,   73,    0,   77,  274,  132,   85,    7,  370, 1303,
            0, 3253,    8,  150,    0,   33,    8,  113,   33,    8,  113,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 225, 1639,   85,    7, 1749,  747,  670,   79,   13, 2829, 1020,    9,
            7,  384,  221,   11,    6, 2100,    0,   29,  225,  278,   10,  150,
           77,    7,  807,   17,  278, 3172,   22,  126,   12, 2092,    0,   55,
         1976, 1043,    0,  148,  162, 4111,   10,   51, 7806,   48,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 116,  336,   17,  183,  206,   19,   34,  961,   17, 7374,    0,   19,
         1260, 1004,    7, 4895,   12, 5205,    8, 4776,   84,   34,  486,  546,
           24,  162, 5054,    0,    7,   82,    9, 7723,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   34, 3642,   84,    0,  116,   13,  110,   57, 1802,   35, 1933,
           35, 1178,    0,    8,   19,  934,   48,   13,  422,  378,    9,   13,
         2414, 2196,    0,  125,   33, 5353,  220,   86,  601, 1584,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45, 2464, 1778,    0,   68,  194,   65,  211,    0,   53,
          162,  172,  488,  184, 2148,    6,    0,    8,   53,  465,   11,   18,
          135,  261,   80,  998,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:4') tensor([29, 39, 53, 50, 30, 57, 50, 18, 47, 27, 42, 38, 48, 34, 36, 30],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8652, 1.0000, 1.0000, 1.0000, 0.8394, 1.0000, 1.0000,
        1.0000, 0.8887, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(71.0567, device='cuda:4', grad_fn=<MulBackward0>)
False False tensor([[3706,   55,    7,    0, 3210,   12, 2058,   35,  249,   59,  240, 1144,
            0,   53,   11,   57,  133,  946, 2787,    9, 2047,   12,    0,    0,
           73,    0,   24,    0,  694,  214,   10, 2231, 2736,  941,    0, 1645,
         2078, 2399,   70, 2892, 1075,    0,   73,   87,    8, 2293,  359, 4531,
          946, 6710,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  13,  143, 4089,  207,   10,  154,   80,   89, 6674,    6,  506,   51,
            9, 2047,   12, 1387,  687,    8, 4480,    0,  206,   85,  419, 1800,
          613,    9,  183,    0,   71,  419, 1127,  723,    0,   19,  217, 1102,
         1387,    8, 8219,  106,   17, 2216,    0,   77,   79,   13, 2443,   12,
           70,   19,  296,   10,   87,  230,  115,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   7, 7374,   17, 5290, 1247,    0,   79, 4546, 1247,    0,  220,  508,
         3667,   10,   13,  733, 6608,   12,  341, 1603, 4345,   46, 1259, 1247,
            0,  618,   59, 1247,    0, 3109,  140,   57,  411,  266,   26, 2856,
         1247,   46, 4574,   48,    7, 2303,   12,    7, 1827,    8,    7, 4715,
           12,    7, 1136,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [4157,    0, 7957, 6266, 1119, 1017,   21, 1962,    0,   85,  109, 3019,
          964, 4157,    0,    8,  180, 4587,   13, 1206,  567,   10, 1005,   21,
           84,    0, 4958,    7,   59,  627, 1202,  290, 1408,    0, 6712, 5933,
         6962,   26, 2770,   10, 5989,   85, 1362,   20,  586,  922, 4157,   71,
         3955,   45,  502, 8366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   39,    9, 1324,    9, 1954, 6935,    0,   19, 1346,
           80,   13, 1361,   12, 8188, 4039,  148,  144, 1828,  244, 1339, 1027,
          587,  195, 3485,    0,    8,   13, 8188, 1793,  148,  692,   10,  508,
         4460,   69,   17, 3485,    0,    8,  211,  218, 3280,  692,   10,  205,
            8,  601,  267,  508, 4460,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 103,    0,   84,   11,    6,    0,    0,    0,    0,   91,  759, 4039,
           46,    8,   19,  172,  135,   33,   46,  148,   63,    0, 3163,   10,
          229,    0, 1583,  583,    6,    0,  148,   63,    0,    0, 3163,   10,
           51,   69, 3836,    0,   19,  217, 2497, 1123,   17,   24,   73,    0,
          204,  468,    7,  538,   12, 1261,    0, 2318,    0,   55,    7, 4959,
           12, 1146,    0,    2,    1,    1,    1],
        [   8,   29,   19,  246,    0,   38,    6, 1049,    3,    0,   29,   19,
            0,  175,  518,    0,    8,  225, 1559,   69,    0,    8,   71,   13,
          277,  523,   12,    7,  170, 1522,    0,    0,    0,   13,  176,    0,
            0,  180,  225, 1518,  336,    0,    8,  225, 1143,    0,   80,  423,
         2360,    0,    8,    0,    0,  225, 1518,  270,  336,    0,    8,  225,
           11,    6,   77, 6087,    6,    0,    2],
        [  85,    7,  183,   19,   34, 1931,   54,    9,  117,  341, 1427,    0,
           19,  474,   19,   34, 3258,   54, 4029, 1352,    0,   67,   69, 1858,
         2007, 1498, 1261, 9096,  237, 1181,    0,    8,   19, 4600,   19,   11,
           48,  204,  226, 3258,   54,   13, 1127,  546,   55,  143,  254,  423,
          215,    0,    8,    7, 2838,   69,  264, 1736,   34,   21,    6, 2402,
          119, 8197,  271,    0,    2,    1,    1]], device='cuda:5') tensor([52, 57, 53, 54, 55, 64, 67, 65], device='cuda:5') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8584, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(113.0635, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[  21,   11,    6,    0,   86,   79,  103,   19,  144,    0,  419,    0,
          479,   70,   21,  169,  274,  100,    0,    0,   67,   84,   34,   13,
          841,   17,   24,  169,   66,   10,   87,  250,   71,  264, 9520,    0,
           12, 5149,    8,  250, 1057,   10,   87,   71,    0, 1588,  468,    8,
          250, 1057,   10,   87,   71, 1136, 1370,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  116,  226, 4174,    0,  166,  818,  251,  148,  162,
         7220,  144,   10,   66,   13,  207,   12, 8254,  839,   10, 1703,  251,
          248,    8,   13,  854,  759, 1086,    0,    8, 1065,    0,  143,   55,
            7, 6580, 8068,    0,    8,    7, 3742, 7856,  249,    0,    8,    7,
         4528,    0,    8, 3097,    7, 2248,  181, 3414,   20,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  67,    7,  630,   13, 2003,   96,    0, 1189,    0,   29,  138,  261,
           87,   56,  234, 1265,  241, 2142,    0, 2994,  122,  241, 2142,    0,
           13,   59,  380,   18, 3110,    8, 3098,    7, 1503,   12,  886,  412,
         3276,  287,    9, 4928, 1703,   55,   17, 7752,    9, 2446,   10,   17,
         2340,   10,    7, 1503,   12, 7121,  484,    0,  166, 2231,    6,   17,
         3833, 1548,    0,    2,    1,    1],
        [  21,   11,    6,  434, 3919, 8570,    0,    8,   21, 6430,    6,    0,
         2584,  218,  214,    0, 7753, 2736,  296, 1212,    8,  227,   93, 1069,
           96,    0, 2797,   54, 1868,  187, 3635, 1319, 3562, 7289,    8,  218,
         2808,   35, 5444, 2840,    6,   10, 3814, 3001, 3188, 1068,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,  258,  636,  132,    9, 5950,   20, 1006,    0,    8,    7, 3564,
          162,  172, 3633,   17,    7, 5317,  144,  226,   13, 2786,    0,  125,
           85,   17, 2110,    0,   19,  144,   13,  277,  523,   12, 2080,    9,
           91,   12,   89,  488,   10,   96,    0,    8,   19,  474,    0,   38,
          122,   57,  411,    0,  125,   19,   11,   45,  142,   10,    7, 8022,
            6,    3,   68,  194,   65,    2],
        [   8,  115,  339,  110,  873,  908, 2955,  187,  445,  128,    0,    8,
          289,   17, 4586,  931,   46,  166,  185,   12,    7, 4793,   24,  175,
          568,   69,   33, 4959,   46,  120,   25, 2554,    7,  282,   12, 2307,
            0,   13, 2980,   37,    0,   13, 3700,    0,   13, 1276,    0,   53,
           63,  521, 1015, 3127,   54, 7553,  111,  199,    7, 2340,    0,    2,
            1,    1,    1,    1,    1,    1],
        [   0,   19,  113,  213,   10, 2678,   33,   91,  132,   13,    0,    0,
          523,    0,  138,   73,    0,   53, 4477,    0,   19,   11,   45,    0,
          142,   10,  388,  134,    0,   69,   13,  824,  586,  457, 5445,    0,
            0,  166,    0,   26,    7,   38,  122,  241, 1515,   10, 4853, 1435,
           85, 8251, 1243,    3, 1189,    0,   29,  168,   21,   26,    0,    2,
            1,    1,    1,    1,    1,    1],
        [1220,    9, 3023,    0, 2058,  181,  541, 1775,  271, 4814,    6,   66,
          226,  619,   10, 1754, 7371,  955, 2028,    0,  100, 4896,    6,    0,
         2678,  777,  812,    6,    0, 3183,    0, 2011, 6504,    6,    8, 4222,
            0,   17,   66,  226, 3150,  111, 7225,   62,  199, 2116,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:5') tensor([57, 59, 64, 48, 66, 60, 60, 48], device='cuda:5') tensor([0.8740, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(100.2751, device='cuda:5', grad_fn=<MulBackward0>)
False False tensor([[ 347,  568,   21,  988,    0,  148, 1006,    6,    0,   26,   33,  116,
           13, 1678,   17, 9004,    6,    8, 6904,    6,  722,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13, 4746,   35,  803,   18,  249,   26,   13,  382,  783, 1522,  723,
            0,    8,   24, 5874,    8,  749,    6,  174,  589, 6010,  722,   55,
            7,  218, 1543,    0,    2,    1,    1,    1,    1,    1,    1],
        [  70,   33, 3565,  170,   26,    0,   33,  164,   15, 4482,  181, 2958,
            0,   21,   11,    6,  113, 2092,  365,    8,  883,  140,  365,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57, 1211,   17,   17,   11,    6,  204, 1031,  156,   62,
          199,  281,   94,   11,    6, 2702,   32,   54,   85,  185, 1648,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  103,   17,   11,    6,   86,  890,    0,   53,   11,  121, 2107,
           69,   10,  289,    0,  281,  490,   53, 4853,  747,  670,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1325,   53, 5415,   33, 1165,    0,   24,   11,  158,  492, 6290,  565,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  10, 1141,   17,  630,    0,   19,   11,   45,  142,   10,   66,   10,
          884,  486,   91,    0,  166,   26,    0,   70, 1148,  120, 6249, 8221,
            6,  415,  158, 1515,    0,    2,    1,    1,    1,    1,    1],
        [ 115,    0,  138,  323,   19,  367,   10,   33, 1760, 3140,   12,   13,
           48, 1184,  140, 3085,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 4268,   12,    7,  744, 6213,   55,   33, 1599,   34,   17,
           21,  220,   51,  619,   69, 8786,    6,    8,   91,   35, 1933,   35,
         1178,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [1008,   46,   25,   11,   57,    7, 3698,    0,    0,    0,  125,   25,
          618,  155, 1037,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,   13, 2198,  614, 1810,   34,    7, 4023, 2922, 2371,   56, 4654,
         2041,    9,    7, 1261,   12,    7, 1384, 1224,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  26,   21,  148, 1073,    7,  281,  839,  109,  148,   26,  281, 8064,
           10,  267, 3557,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   73,  876,  121,   10, 2697, 2360,    0,   10,  206, 7616,  568,
           86, 2705,  307, 1906,    0,    8,   26,  417, 1841, 1059,   62,   71,
         1894,  688,    6,    0,    2,    1,    1,    1,    1,    1,    1],
        [2018,  527,   93,   35, 2287,  234,  439,   12,    7,  179,   11,    6,
         1682,  931,    9,    0,  108,    0, 1884,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 4479, 1339,  783,  511,   37, 1405,   13,    0,  682,  247,   17,
          101,  434,    0,   38, 1879,    0, 2102,    3,  166,   26, 1223,    0,
         8044,   55, 3708,    6,    0,    2,    1,    1,    1,    1,    1],
        [  19,  321,   12,  100, 1060,  117, 1532,    8, 2346,   18, 1011,   71,
          565,  359,   13, 2555,  237,  866,    9,  321,   12,   13,  854, 4656,
            6,  737,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  24,   66,  255,  200,   22, 6339,   54, 2200, 2362,    0,    8,   24,
           66,  244,  168,  264, 8973,    6,   69,   13, 4481, 3489,   10, 1764,
          387,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [2805, 2219,   69,   70,   91, 1985,  150,    0,    7, 2805,   12,   17,
          107,   20,   15,    8,  291,  156, 3837,  457,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103,   17,   11,    6,   86, 4949,  890,    0,   25,   73, 5652,
           91,    8,   25,  175,   13, 2685, 8831,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 106,   17,  552,   13,    0,  325,   12, 5828,    6,    0,  109,  126,
            6, 2893,    6,    0,    0,   79,   19,  434,  134,    0,    0,  214,
           17,   19, 1385, 1313,    0,    0, 6263,   89, 6878,    0,    2],
        [  19,  154,  245,   21,   11,    6,  528,   55,  170,   10, 2613,   70,
           21,   26,   10,   51, 2639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3695,   12,  134,   63, 1387,   10,  206,    7, 4458,  362,    6,   63,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  744,   26,    9,    6,  234,   54,   17,   84,   26, 8902, 1666,
         2214,   10,  415,  500, 1783,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0, 6482,  215,  581,    0,  903,    0,  846,  315,   22,   18,
          380, 3089,   35, 3304,   62,  284, 3780,   11,    6,  874,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([23, 29, 25, 25, 24, 14, 30, 18, 28, 18, 22, 17, 29, 21, 30, 28, 28, 22,
        21, 35, 19, 14, 19, 24], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8584, 1.0000, 1.0000, 1.0000, 0.8706, 0.8208, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8242, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       dtype=torch.float16)
 > at.  tensor(41.0890, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:31:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
tensor([[  24,   66,   13, 7524, 4040,    9,  166,   24, 1393,   10,   91,  723,
           10, 8714,   39,  467,  982, 2884,   12, 1833,    0,   10,   51,   89,
         4023,  570,   59,    0,   89,  772, 1751,    0,    7,  772, 5370,    0,
           89, 2444,   62,  521, 1105,   48,  480,    0,   89, 2294, 1661,  221,
          369,    0,   89, 5930, 4888,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,   70, 1320,  335,  684, 4600,    0,    8,   25, 1510,   21,    9,
            7,  227,   93,  362,  715, 2378,    6,   12,  632,  237,    6,   15,
          593,    0,   34,   17,    7,  535,    0, 3139,    0,  227,  233, 2341,
         4267,    6,   12,  574, 2256,   12,    7,  564,  322, 1910,  162, 1028,
           10,   13, 1387,    0,    8,   17,   24,  162,   80,   10,  954,  199,
           91,   12,  251, 9200, 2760,    6,   12, 1261,  120,  768, 2317,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19, 7750,   25,   10, 1064,    0,   55,    7,  245,  183, 3983,    9,
            7,    0,  179,    0,  168,   69,    7, 1366, 2110,    0,   13,  475,
           35, 5203,    0, 2178, 2445,  266,    0, 6580, 1411,  271,    0,  629,
          110,    0,    8,   89, 1751,    0,  903,    0,    0,  728,  649,   57,
           93, 2734, 2003,    0,    0,  106,    0, 7002,   11,    6,  728,   18,
         3411,  300,    6,  369, 7312,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  17,   26,  120,   13,  150,   48,   26, 6788,   62,    9,   13, 7312,
           10,   87,  250,   86,    9,  191,   48,   62,  131, 1377,   46,  100,
         1520,    7, 2192,   12,   13, 1390,    8, 2985,   21,  199,    7, 2192,
           12,   13,   10,  424,  412,    0, 1995, 2083,    0,  192,   11,   18,
          175,  110, 1226,    0,   19,  100, 1390,    8,   10,  424,  412,   96,
            0,   67,   33,   26,  116,  775,   20, 1718,   93,    0,   68,  194,
           65,    7,  150,   48,    6,   63,  180, 2685,   62,    0,  180, 5847,
            0,    2],
        [   7, 2193, 1793, 2493, 7012, 1198,    6, 1361,    0,   13, 1361,   12,
           94,  148,    0,   69,   13, 1284,  383, 1663,    0, 3284,  199,   13,
         3508,  982,  964,    0,   85,  159, 3506,  160,  128,    6, 3696, 3836,
            8, 2187,    6,    9, 2828,   62, 7689, 1974,   22,   18,    0,  100,
           33,    0,    8,   53,  456,   80, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,   29,  115,    7, 2932, 1381,    0,  148,   55,   13,   87,  610,
          215,    0,  188,  923, 3150,  111, 6226,   62,    7, 2688,    0,   26,
          115,    9,    7, 3140,   12, 1057,   10, 4007, 1179,   10, 2156,  109,
         6406,  346, 1711, 4166,    0,  125,    7, 6105,   10,   13, 3147,  234,
         1827,   26,   29, 4126,   17,   53,   73,   11,   18, 1799,   71,   21,
          419,  218,  207,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1232, 5917,    6,  427,  362, 1011,  131,    7, 3022,   46,    8,
           19,   11,  121, 2138,  185,  798,  215, 2115,   54,  336,    7, 4959,
          401, 1221,   69, 8633, 1232,    6,    0,    8,   66, 3395,   62, 3585,
         1118,    9,  392,   10,  798, 1075,    9,   33, 4959,    0, 2386,   12,
         3585, 1118,   46,  752,   10,  661,   70,  513, 1226,   71,  108, 1232,
         5917,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:4') tensor([55, 73, 67, 86, 57, 65, 63], device='cuda:4') tensor([1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(135.8914, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[   8,   46, 2307, 1008,   33,  283, 7772,    0,    0,   38,  160,  119,
          480,    0, 6880,   55,  155, 5223,  197,   46, 1363, 1850,   37,    0,
            0,    9,  545,    0, 1227,    0,   33,   34,  138, 7803, 1808,    0,
            0,  192,   11,   18, 3995,    0,    0,   13, 1192,  552,  126,    0,
           21,  144,  211, 3236,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,    7, 4174, 2078,  487,    0,    7, 1762,   12, 2718, 2202, 6611,
         6671,    9,    7,  774,   12, 5428,   34,   79,  488,   79,    7, 1762,
           12, 2718, 2202,  850, 3909,  607, 4357,  407,    9,    7,  774,   12,
         1947,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   85,    7,  467,   12,   21,    0,   24, 8152,   48,   17, 2878,
           12,  708,   73, 1082,   10,  368, 2930,    8,    7, 1465,   69,  159,
          447,    0, 5333, 2515,  366,   12,  148,  109,  206,   53,  162,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24, 1405,   13,  846, 4399, 7016,   12,   13,   56, 2515,  292,
         2404,    0,    8,   25,   73, 3684,  375,  688,  359,  341, 8829,    6,
           17, 2231,  341, 7125,    6,    0,   29,   17,   73,  601,   25, 5415,
           70,   11,    6,    9,    7,  563,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   11,    6,  916,   26,   53,  323,   21,    9, 2008, 1491,    8,
         6343,    0,  206,   21,   11,    6,   38,  626,   57, 6505,  128,   10,
          508,  561,  804,    6,    3,   53,  144,   10,  508,   21,  113,    9,
            7,  832,    0,    6,    0,   10,  175, 1719,  292, 2460,    0,   29,
           19,  154,   84,  162,  391,  832,    0,    6,    0, 2116,    9,  132,
         6034,  264, 1736,  148,  162,  461,   12,    7, 3677,    0,    2],
        [   0,   33,   26,   13, 1523, 7376,    0,    0,   21,   11,    6,   39,
          985,  687,    0,    0,   21,   11,    6,    0,   13, 4912,   12,   77,
         1587,   12,  813,    0,   86,  116,   80, 5807,    8,    0, 9827,    8,
         6689,    8,   29,   69,    0,   67,   80,  896, 2604,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   21,  169,  492,   66, 7747,   10,  110,   10,  154,   17,  116,
          125,   19,  144, 1157,   13, 5462,    9,  166,   13, 2232,   34,   13,
          227,   37, 1010, 2650,   37,   17,  101,   34, 3524, 2015, 1197,   12,
           77, 4039,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  169, 1017, 8788,   55,    7, 1732,    6,    0,    8,  780,   10,
         2895,   71,  134,    9,   13,  207,   17,   34,   79,    6,  762,  366,
         1094, 3835, 1256,    0, 6510,   54,   13, 2866,  979,   12, 3802,    8,
         7376,    9,  166,   24,  220, 1082,   10,  283,  540,    8, 1766,   91,
          486,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([54, 39, 37, 44, 71, 48, 40, 51], device='cuda:7') tensor([0.8584, 1.0000, 1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(86.2638, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[ 24, 498,  71,  ...,   1,   1,   1],
        [ 33,  34, 245,  ...,   1,   1,   1],
        [ 87,  21, 238,  ...,   1,   1,   1],
        ...,
        [886,   0, 476,  ...,   1,   1,   1],
        [ 29,  19, 415,  ...,   1,   1,   1],
        [596,  73,  66,  ...,   1,   1,   1]], device='cuda:7') tensor([ 6, 10, 10,  7, 10,  7, 10, 11, 10, 10,  6, 11, 10, 14, 10, 12, 14, 18,
        14,  8,  8, 14, 13, 11, 11, 15, 12, 11, 10, 14, 16, 10, 13, 20,  8, 17,
         9,  9, 21, 10, 17, 10, 12, 12,  8,  8,  8, 12, 14, 11, 10, 11,  9, 10,
        11, 12,  7,  9,  8,  4, 10, 12, 10, 13,  9, 14, 11, 15, 11, 10, 12, 14,
         8, 11,  7, 12,  9, 13,  9, 14, 13,  9, 13, 12,  8, 10, 10, 13],
       device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8633, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8174, 1.0000, 1.0000, 0.8384, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8755, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8091, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000, 1.0000, 0.8579,
        0.8691, 1.0000, 1.0000, 0.8516, 1.0000, 1.0000, 0.8652, 0.8286, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 1.0000, 0.8564, 1.0000,
        0.8223, 1.0000, 1.0000, 0.8276, 1.0000, 1.0000, 0.8394],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(15.8014, device='cuda:7', grad_fn=<MulBackward0>)
False False tensor([[1189,    0,    0,   29,  339,   11,    6,    0,  289,   21,   11,    6,
         1491,    0,    0,    8,  339,   11,    6,  289,   21,   11,    6,  384,
          372, 1761,   20,    0,    8,  115,    0,   25,   73,    0, 1948,    0,
          205, 1817,    8,  446, 1835,   13, 1484, 3762, 2293,   59,    0,    2,
            1],
        [   0,   67,   25,  135,    0,   19,  154,   25,   63, 4130,   71, 2084,
         1625,  128,    6,    0,   46,    7,   81, 2510, 3496,   18, 9875,    6,
            0,    8,   77,  117,  214,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  91,  383,    0,   19, 7262,   62,  126,   12,    7, 2657,    9, 3918,
         1118,  432,   13, 3040,   71,   13, 3532,   91,    0,    8,   19, 5429,
           62,  199,   13,  682,  181, 3435,    0,  206,   19, 1060,    7, 2859,
           57,    6,    6,   55,   13, 2705,    0,    2,    1,    1,    1,    1,
            1],
        [   8,  120,   19,  289,  923, 1738,    0,   21,  220,  499,   51,   29,
         4733,   17,  211,   91, 3485,   12,  282,  700, 6317,    6,  486,    0,
          166,   26,   13, 3462,  474,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  113,    7,  473,  203, 2470,  247,   55,    7, 3299,  267,   48,
           12, 2533,  382,  399,    0,    8,  113,    0,   12,  538,    0, 3547,
         6952,   55,  486,  733, 3609,   12,  218, 1369,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  94,   63,   29, 4493,   12, 6608,   17,   53,  780,    8, 2509, 1459,
            0,  276,   94,  148,  192,   11,   18,  213,   10,  109,   73,   11,
           18,    0,   10,  873, 1417,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 432,   77,    0,  305,   13,  274,   85,  117, 2792,    6,    0,   97,
          737, 1010, 1144,    8,  415, 2633, 2041,    0,  179,   82,    6,    0,
         6229, 1106,    0, 7050,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7,  240, 2120,   66, 2138, 4021,    0,  752,   10,    0,  661,  347,
           21,  188,    0,    0,   33,  133, 2035, 1236,  297,  293,  111, 3495,
           35,   18,  500,   62,  800,    0,    8,    0,   53,   11,  121,  367,
          132,   71,    0,    0,   13,  800,   12, 1254,    0, 7052,    6,    0,
            2],
        [ 125, 2392,   37,  109, 1065,    0,   24,   11,  158,   51, 7258,   62,
           71, 4012,   80,   33,    0,    8,   21,   11,    6,  509,  103,   24,
          154,  835,   80,   21,    0,  276,  103,   24,  213,   10,  154,  835,
           80, 2826,  347,   24,  451,  492,   87,   21,    0,    2,    1,    1,
            1],
        [   8,  168,   21,   26,   55,    7,  245,  183,  291,  121,  233,   62,
           85, 1366,   46,  108,  245, 4878, 1806,  469,  816,   24,  293,  457,
            0, 6712, 5831,    6, 1103, 3007,   54,   71,  282, 1117,   13,   24,
          293,  457, 7757,   54,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,   70,   24,  487,   71,   34,  172,   13,  264,  207,   12,
          826,   80, 1009,   39,    9, 3562,    0,  359,   13, 1039,  434,  229,
         1261,    0,  166,   24, 6532,    9, 5855,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   34, 1061,    0,  225,   34,  798,    0,  100,   13,  325,   12,
            7, 1323,    6,   24, 1618,    0,   53, 3994,   11,   18, 2603,   56,
         7038, 1568,   80,  159, 1666, 4388,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 391,  215,  581,   19,   34, 1074,    9,   13, 5200,    9,   13,   56,
         2561,   35,   45,  973, 2102,   54,  325,    0,    8,  440,   19,   11,
           45, 4379,   85, 1366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,  225, 3673,   62, 1406,  290,  125,  225,  692,   10,  229, 1123,
          225, 3673,   62,  250,  225, 1425,  267,  708,  169, 2034,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19, 1425,   17,   85,   17,  765,   17,   70,    0,   19,  451,
          508,    0,    0,   10,   89,   94,   26, 1370,    8,  958,    0,    0,
            8,   17,   11,    6,    0,   70,   19,  513,  432,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   70,   19,   11,   45, 2659,   55,   26,    7, 5708,   12,   13,
          264, 1329, 6813,   46,   19,   11,  158,  367,   10,   33,   13,  277,
         1065,   46,    8, 1645,    7, 4936,   12,   13,  264, 1455,  199,    7,
         2781, 1234,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:0') tensor([48, 31, 44, 31, 34, 31, 30, 49, 46, 42, 33, 32, 30, 24, 35, 40],
       device='cuda:0') tensor([0.8550, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8433, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(62.6279, device='cuda:0', grad_fn=<MulBackward0>)
False tensor([[ 125,    9, 2483,   35,  606, 5731,    0, 2932, 6930,    6, 5989,    8,
          447,  281,   12,    7, 2483,    6,   46, 4346,  737,    0,  742,  160,
          140,    0, 3213,   46,    8,   53, 5470,    7, 2791, 2008,    8, 3156,
          199, 2248,  713, 1777,    0,    2],
        [  13, 4495, 1435,    9,    7,  832,    0,    6,    0, 2721,   17,    0,
           12, 5990, 8459, 3665, 1118,    0,  248,   35, 8516,    6,   12,    7,
         5990,  889,  144,  708,    8,  288,   91,   35, 8516,   12,    7, 5990,
          596,  144,  708,    0,    2,    1],
        [ 168,   26,   13,   29,  300,  166,  188,  278,   77,    7, 8681,   12,
         7343,    0, 3280,    0, 1976,   25,  289,    0,   19, 3250,   21,    0,
           39,   19,  362, 1838,  235, 5384,    9,   25,    8,  180,   25, 3745,
           10,   21,    0,    2,    1,    1],
        [ 822, 2317,    9,  984,   11,    6, 6339,   17, 4877,  244, 1953,    6,
           10, 2386,   12, 1655,   12,  215, 4373,    7, 6484,   12, 7616,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  67,   70,  143,  220,  225,   66,  691,  103,  225, 1631, 1088,  144,
            7, 1885,  818,   12, 3802,    6, 2875,   10,  267,   10,  719,   13,
          841,   17,    7, 9413,   17,   94, 1095,  144,   10,   51, 1529,   62,
         3840, 4173,    0,    2,    1,    1],
        [  19, 1223, 2894,   55, 3101,  839,   69,    7, 2291,    0,   68,  194,
           65,    8,   19, 5429,   80,    7, 5986,    6,    0,    8,   19,   66,
          591,   13,  555,  214,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  66,   25, 1060, 9832,  347,    8,  138,   29,  294, 9696, 2419,    6,
           66, 6028,   48,  244,    7,  473,  640,  215,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [1223,   70,   21,  968,  110,   34,   46,   68,  194,   65,   68,  386,
           65,   46,    7, 1587,   12, 1075,   24, 5559,   63,  324, 1075,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([42, 41, 40, 27, 40, 30, 22, 25], device='cuda:1') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:1', dtype=torch.float16)
 > at.  tensor(72.0805, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[  21,  914,   26,    7, 1226,  279,   10,   87,   10, 4746,    8,  368,
            7, 3939,    8,  946, 2780,    0,   38,  511,    7, 1381,    0,   53,
         1346,  267,  546,    8,    7,  218,   94,    0,    8,   53,  246,    3,
         1768,    0,   25,   11,   57,  230,    0,   24,  451,  229,   13, 2240,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 2964,  169, 2154,  724, 2468, 7147,   69,    7, 2618,    8,  719,
          117, 1230,   35, 6937,   35, 7596, 1710,    6,  442,   12,   13, 1127,
         2468,    0,  736,  439,  203, 1255, 4711, 1755,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [7823,   94,   63,    0, 4820, 1256,   94,    0,    8, 4820, 1256,   94,
           46,    7,  143,    8,  143, 4820, 1256,   94,   84,   63,    0,    7,
          143,    8,  143,   24,   11,  158,    0,    0,   66,   13, 4820, 1256,
            0,  179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  84,   34, 9199,    0,  180,  552,  853, 3174, 4796,    0,  440,   24,
           66,  211, 3174, 4796,    0,    7, 1575,   35,  372,   59,  247,  119,
         1819, 2735,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 180,   19, 1095,  108, 2333,  158,  240, 1028,   10,  670,    9,   58,
          389,    6,    8, 2852,   59,   18, 1374, 2197,    6,    0,    8,  474,
            0,   21,   11,    6,   86,  230,   55,  110,   10, 1005,   33,  218,
          939,  668,  232,  834,   29,   19,  339,  134,  116,   46,   53,  144,
           10,   51, 1644,  411,    8, 2736,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17,  370, 4506,   17,   24,  116,  323,   71,    7,    0, 6144,  779,
          569,  164, 2676,   66,  475,   35, 2371,    0, 1683,  979,   54,    0,
            8,    0,    0,    7, 3180,  164,  289,    0,   38,  786,    0,  346,
            0,  859,    0,    0,  230,    0,   13,  176,    0,   95,  266,    0,
           17,   11,    6,    7,    0, 2636, 4057,   10, 1557,    0,   17, 1683,
          518,   10,  155, 3280,    3,    2],
        [  19,  513,  270,   10,    7, 1224,    8,  487,  665,  336,   10,  150,
          103,   19,  220,  446, 3302,  206,    0, 7490, 1390,   54, 7521,    6,
          144,  226,    0,    0, 2517,   48,    0,    8,   21, 1932,  126,   84,
          162,    0, 3695,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,  230,    0,   29,  339,  110,  508,    0,   25,   39,  663,   12,
         8681,    0,   12,   13, 1760,  321,    0,    8,   19,  213,   10, 7032,
           13, 1455,   17,   19,  154,   26,    0,    0,  133, 4501,    0,  166,
           26, 8164,   54,    0,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([50, 34, 40, 31, 56, 66, 41, 42], device='cuda:1') tensor([1.0000, 1.0000, 0.8799, 1.0000, 1.0000, 0.8735, 0.8794, 0.8555],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(77.8474, device='cuda:1', grad_fn=<MulBackward0>)
False tensor(15.5532, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[   8,   21,   11,    6,   13,   24,  237, 1355,   54, 1909,    0,   19,
          154,    0,    9, 8041,   12, 4972,    0, 3982,  366,   12, 4972,    0,
            8,   19,  154,   33, 5510,   12, 1102,  639,    8,  998,    9,    7,
         1136, 5055,   26,   39, 1909,  206,    7,  832,    0,    6,    0,   73,
          172,  305,   13, 5247, 2883,    0,    8, 7419,   26,   91,  663,    0,
            2,    1,    1,    1,    1,    1],
        [  29,  168,   25,   66,   13,  567,    0,   25,   66,  250,   46,    8,
           84, 1631,  227, 8263,   12,   17,  993,    9, 3804,   46,  185, 8829,
         2385,   39, 4312,   22,    0,    8,    7, 4312,   22, 3060,   96,   10,
         3793,    0,    8,  288,  120,   13, 6737,  925, 1585,   17,  188,    7,
          230, 1051,  174, 1991,  568,    7, 5250, 1085,    0, 1239,    0,    2,
            1,    1,    1,    1,    1,    1],
        [  84,  188,  226,   39, 2244,   48, 2080,  199, 2627,  106, 1146,    0,
          106,    0, 5036,    0,   67, 5830,    0, 2288,    0,    8,   77,   12,
           13, 5827,   24,  144,   33,    0, 3024, 2244,    9,    7,  245, 1345,
           12,   33,    0,  464,    0,  347,    0,   19,  154,   84,    0,   63,
          391, 2826,    0,  248,  535,   35, 4935,    0,    0,    0, 1649,    8,
            7, 7678,    0,    2,    1,    1],
        [  19,  246,    0,   38,  187,   11,  158,   87,   21,  106,  384,  237,
          803,    3,   53,  246,    0,   38,  469,   57,   11,    6,  211,  207,
           25,   11,   57,  142,   10, 6785,   13,  759, 6052,   35, 3481,  176,
           12, 2811,  839, 2202,    9,  384,  237,  803,    3,   29,    9, 6769,
            0,   19, 5016, 1222,   13, 6478,  244,  699,  411,    8, 3122,   10,
          264, 1027,  119,  232,    0,    2],
        [ 101,  246,    0,   38, 3127,   19,  692,   10,   51,   13, 2182,  684,
            3,    8,  101,  246,    0,   38,  200,  969,   19,  278,   10,    7,
         8459,  464,   12,  670,    0,   89, 3540,  465,   11,   18,  305,   21,
         6064,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19, 1060,  267,   80,  267, 3013,    0,    8,  225,  246,    0,   38,
         1969,  135,    0,  635,   19,   11,  158,  618,    0,  635,   19,   11,
          158,   14,    0,   67,   19,  192,   11,   18,   66,   13, 3013,    3,
          225,   34, 1922,  200,   54,   51, 1795, 2073,   18,    0,  166, 6121,
          267, 7150,  244,    7,  215,   10, 1393,  133, 1665,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [ 476,   25,    0,   68,  386,    0,   65,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 101,  591,   17,   53,  162,   56, 8862,   10,  384, 1050,  140,   54,
            7, 7522,  469,   18, 1568,    0,   10, 9547,  945,   80,   70,  506,
           51,    0,    8,  101,  591, 1730,   17,   53,  465,   11,   18, 1799,
          238,   71, 6765, 1955,  109,  802, 6027,   69,  251, 6765, 1955,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:6') tensor([61, 60, 64, 66, 39, 59,  8, 49], device='cuda:6') tensor([1.0000, 1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 0.8110, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(103.0365, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[ 225, 1119,    0,   38, 1461,    0,   17,   11,    6, 1284,    0,   70,
           26,   21,    0,   26,   17, 7148,  650, 3727, 1994,    3,    8, 4069,
            0,   19,   73,   51,   13,  277,  593, 4187,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,   34,  133, 3315,    8,   19, 1385,  185, 4673,  214,    8,
          442,  185, 1968, 3051,    6,   17,   19,  213,   10, 1452,   71,   25,
          440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 294, 3940,    0,   68,  386,   65,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  251,  370,  215,    0,   89, 1970,    8, 3460, 1102,   14,   48,
           12, 1167,    0,    8,   19,   34,  914,  244,  221,  505, 1563,   80,
         6088,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  162,  461,   12,   33, 5673,    0,   70,  169,   25,  289,
            0,   63,   25,  850, 1203,  494,    0,    0,    0,  238,    0, 2859,
           46,  138,   87,   25,    0,  276,  135,    0,  125,   25,   11,   57,
            0,   86, 3531,   10,  456,   80,   21,    0,    2],
        [  29,   33,   26,   86,  116,   13,    0, 6429,    0,   80, 7291,    6,
            0,    0,   21,   11,    6,   13, 6429,   80, 3295,    9, 1146,   79,
          238,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  89, 2574,   26, 5205,    8, 4970,   63,    7,  473,  825,  120,    7,
         3022,  188, 2096,   10,   87,   21, 1574,    0,    8,   24, 2775,   11,
           18, 5768,   62,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 276,   13, 2900, 6815, 5707,   62,  392,  439,  143,    9, 7001,  254,
          225,  323,    9, 8979,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [8310,   54,   17,    0,    7, 2468,   12, 2573,    0, 1067,  525,  436,
            0,   17,   19,   34,  752,   10,  747, 2653,    0,  288,   51, 1217,
            6,    9,  248, 6874,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,    7,  207,    7,  227,   93,   45, 1604, 2334, 2895, 5070,  170,
           46,   24,   11,  121,  115,  278,   13, 1234,   10, 9375,  347,  117,
          227,   93,   45, 1604, 2334,   63, 7071,  341,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   53,  245,  487, 2980,   54, 2263, 1315,    0,   21,  220,  305,
           79,  294,   79, 1111, 6052,   12, 2533, 1390,   10,  229,   13, 1127,
         1116,  569,   12, 2263, 1315,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 1797,    6,    7,   39,   22, 1522, 3667,    8, 2322,   12, 7932,
         2673,   96,    8, 4157,    6,  142,  270,  490,    7,   69,    6,  307,
           12,    7,  473, 2217, 1137,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  961,  736,  439, 7692,  629, 2481, 2286,   62,  235,
            8, 5225, 5108,    8, 2288, 5225, 5108,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  388,    9, 3189,    0,    9,    7,    0, 1832, 2930,    0,   84,
            0, 7774,   62,   77, 1587,   12,  993,  106,    7, 1465,    0,   80,
         2192, 6631,  793,    0,    0,  281,   12,  166,    0,   19,  465,   11,
           18,  661,    0,    2,    1,    1,    1,    1,    1],
        [   8,   21,  875,    6,   25,  199,    7, 7771,   12,   70,   11,    6,
          226,  434, 1440,    0, 2243,   59,  827,  760,    0,  883,  424,    0,
         1579,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,    0,  154,  281,   12,  134,  169,   51, 8661,    8,  958,  756,
            0,    0,    0,    8,    0,   79, 5474,    0,  294,   12,  134,  169,
          914,   51, 7078,  706,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([34, 27,  7, 27, 45, 27, 29, 18, 30, 34, 31, 32, 21, 40, 28, 30],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8877, 0.8564, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 0.8223],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(56.7479, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[ 976, 1475,    0, 2184,  176,    0,   24,  154,   29,    0,   29,  115,
           17,   24, 2534,  117, 2522,    8,  591,  117, 3422,   17,  339,  170,
           87,  117,  214,    0,   24,  487,   10, 2252,   17,    0, 3423,    0,
          995,   17,   24,   73,   87,   71, 1780,    0,  995,   17,   24,   73,
           87,   71,   13, 1472,   12, 1780,    8,   13, 2705,   24,   73,  115,
           87,   71, 5414,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  499, 3008,  813,   69, 3634,    6,    0,   67,  115,   24,   73,
         3008,   13,  325,  143,  813,    0,  143,  254,  700,  490,    0, 2761,
           54,   21,   26, 4865,    0, 4585,   54,   21, 4865,    0, 4912,   21,
           26, 4865,    0,  979,   54,   21,   26, 4865,    0,    8,   70,   24,
           73,   87,   26,   24,   73,  203, 1792,   33,  813,   55,  368,    6,
           17,   24,  492,  276,  934,   48,  120,   24,  245, 2861,   62,    7,
          660,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21, 1518,  126,   17,   71, 1421, 1601, 2421,    6, 3195,   54,
            9,    7, 5199,    0,  432,  101,  976,  261,  442,  284, 3140, 2226,
           69,   33,   46,  101,  144,  284, 7986,    8,  101,  144,  284,   46,
          101,  258,   57,   33, 1266,   55,  908,  254,  248, 1551,    8,   34,
          529,   10, 7554, 4115,   13, 4058,    8,  278,  923, 2294,  244,    7,
          409,   17,    0, 5098,   46,  284, 5098,   46,   21,   11,    6,    7,
          245,  183,  101,   11,    6, 1831,  100,  101,   11,    6,  144,   39,
         1266,    9,  815, 1132,  215,    0,    2,    1,    1,    1],
        [ 108,  296,   55, 3051,    0,  108,  296,   55, 4029,  687,    0,  109,
          108,  296,   55, 3069,    8, 8464,    0,  109,  108,  296,   55,  540,
          687,    8,   55, 2848, 2050,   93,    0,    8,  103,   25,  154,   80,
            7,  277, 1920,  148, 2456,    6,   69,  155, 1067,  265,    8,  148,
           26,  415,  878,  111, 7307,   62,  168,    8,  133, 7759,    8, 6128,
            0,    8,   85,  185,  613,   77,   12,  170,  296,   10,  205,  126,
          199,    7,  179,   10, 3231,    8,   10, 3522,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 2983,   84,   11,    6,   13, 3024, 7927, 4719,    0,    8,   33,
          172,  132,    6,  307,  110,    0,  125,   13, 1027,  631,  407, 3565,
           25,  432,   13, 2767, 6204,    0,   84,   11,    6, 7008,   80,   39,
         1061,   35, 1315,  322,  183, 5848,   10,   46,   24,  321,  290, 4971,
            0,  498,    7, 4971,   93,  979,    0,   67,   70,  281,   94,  192,
           11,   18, 2252,   26,   17,   69, 2313,   21, 1847, 3497,   10, 1421,
         1113,  109,  143,   55,    7,    9, 1610,   45, 1032, 1809,  424, 1420,
          233,  373,   10,  276, 1772,   10,  575,  132,    0,    2],
        [5902,  589,    6,   69,  837, 1827,  162, 6386,    0,   38,  597,  269,
          340,  407,  323,   21,    0,  347,   73,   11,   18,   24,    3,   19,
         1850,   62,   39, 2792,   69, 5063,    8,  434,   21,   38,  290, 2078,
         1525, 6664,    0, 9413,    8, 9696, 2419,    3,   19, 8063,   48,   13,
          630,   10,    7,  640,    3, 6463,   12,    7, 2672,   85,    7,  183,
            0,   38,  412, 2366,   26,    7, 1802,  322,   12, 8338,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  203,  779,  675,  793,   26,  142,   10, 3128, 2129,   12,  248,
         3834,    0, 2129,   24,  164, 8896,  117, 1752,   35,    6,  335,   18,
         1617,  457,  183,   35, 2470,    6,   96,    9, 1764,   20,  484,  480,
         1081,   12,  108,  447, 2573,  131, 1520,  203, 7712,  237, 2332,    0,
          109,  994,  117, 3382,    6,   63,  142,   10,  175, 7154,   48,    9,
          291, 1076,   20,  484,  480, 1081,   86,   12,  108, 2573,   46, 1187,
          111,    0,  131,   82,    0, 1599,  109, 2145,  586,  271,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([66, 75, 91, 82, 94, 72, 84], device='cuda:5') tensor([1., 1., 1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
 > at.  tensor(153.3918, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[  29,    0,   25,  ...,    1,    1,    1],
        [   8,  180,    7,  ...,    1,    1,    1],
        [   7, 4821,   10,  ...,    1,    1,    1],
        ...,
        [ 540,   53, 3122,  ...,    1,    1,    1],
        [   8,   19,   11,  ...,    1,    1,    1],
        [ 635,   24,  296,  ...,    1,    1,    1]], device='cuda:5') tensor([13, 13, 20, 13, 14, 18, 18, 10, 16, 11, 16, 15, 17, 11, 19, 12,  6, 12,
         8, 17, 12, 21, 13, 12, 15, 18, 17, 15, 19, 24, 15, 14,  9, 18, 11, 16,
        15, 14, 12, 21, 14, 15, 15, 11, 21, 14, 20, 17, 23,  7, 17, 14, 12, 14,
        15, 13], device='cuda:5') tensor([1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8799, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 0.8286,
        1.0000, 0.8652, 1.0000, 1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8677, 1.0000, 0.8550, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8882, 1.0000, 1.0000, 1.0000, 0.8105, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 0.8521, 1.0000,
        1.0000, 1.0000], device='cuda:5', dtype=torch.float16)
 > at.  tensor(21.1390, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[ 250,   17,  169,   51,  528,    0,   67,  113,  250,   17,    0,  169,
          305, 5542,   12,    0,   77,   12,  117, 7099,    6,   17,   91,    0,
            0,    0,  144,    0,   38,  511,    9,    7, 1165,   12,    7, 1827,
         2260,    0, 4619,   13,  325,   12,   94,    0, 4619,    0,   94,  148,
          162, 2129, 9829,    6,  109, 4488,   93,    0,    0,    0,    8,  113,
           86, 1057,    0,    0,    0,    9,   89,    0,    0,  447, 1165,    0,
           13, 3557,   10,    0, 4110,   80, 3903,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,  278, 1421, 3675,  126,   12,   21,   55,   89,  670,  125,
           19, 1831,   17,   24, 1591, 2637, 3181,    6,    8,  811,   35,  426,
          356, 5149,    8, 1127, 1848, 7178,   48,   10,  367,   10,   13,  670,
          206,   84,   34,  874, 2286, 5575,   62, 8517,   17,  877,   20,  307,
           62,  134,  333,  383,    0,  125,   21, 1017,    6,   13,   10,  375,
            8,   39, 7574,   80,  138,   25,  598,   80,   94,  535,  490,   25,
          508,  134,    7, 4340,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  432,  423,  215,   12, 7989, 1435,    8,   56, 2687,  240, 1032,
         6606, 6480,    0,  131,    0, 1655,   12, 5709, 2245,    0,  333, 2767,
         2931, 3182,    9,    7,  179,  188, 8152,   48,   17,    0,    7, 6929,
            6,    0,    0, 5914,   69,    7, 1232,   63, 2426,   10, 2034,    8,
           17,    7,  979,   12, 2753, 4429,   26,  211,    0,  143, 1565,   93,
          254, 3563, 4401,    6,   12, 2753,  824,   48, 4577,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   79,   25,  150,    7, 1819,  205,    9, 6714,  111,  359,   84,
            0,    8,  180,    0,    0,    0,   68,  386,   65,    8,    7,  281,
         4673, 1809,  411,   26,   17,   89,  874,  204, 1917,   62,  499,  535,
          890,   10,   87,   17,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   34,   89,  765,   10,  749,  221,    9,    0,   10, 2750, 1222,
         2433,   55,   70,   63,  172,  288,   13,  874, 1256,   12,    7,  133,
         1219, 5222, 3770, 3254,    0,    8,   19,  144,  116, 5893,   13,  488,
            0, 1061,   35, 1315,  322, 1039,   55, 2829, 1020, 1747,  160,  741,
            0, 3150,  111,    0,    8,   19, 1425,   19,  220, 6785,   13, 3028,
         1329,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  269,  290, 1543,  106,   10, 1059,  188,  691,    7,  291,   18,
         1397,  156,  457,    0,  391, 1543,    6, 4437, 2361, 1268,    0,  895,
         1101,  269,  484,    0, 1912, 2172,    0, 5087, 2172,    8, 1339, 1373,
          699,   57,   77,   85,    7,  370,  183,   46, 6835,  203,  620,  293,
            6,   62, 4437, 1395, 4781,    9,  166,   24,  205,  126,    0, 2213,
          132, 1546,  121,   22, 1691,  269,  484,    0,  388,    9,    7, 4437,
            6,   17,  204,   66,    7, 4273,    6,    0, 3020,  126,    7,  269,
          290,    8,  180,  339,  134,  205,    0,    2],
        [   7,  218, 8314,   12, 1780, 1119,    0,   38, 1865,   89,  227, 2704,
            7,  179,   34, 1621,    3,  166,   26,   10,  289,   21,   11,    6,
         1301,   17,   19,   73,   11,   18,   87,  778,    0,   67,   19,   73,
         1123,  111,   87,  250,    0,   19,   73,   55,  122,  366,    0,   19,
           73,  570,    0,   19,   73,  575,  132,    0,   19,   73, 6295,    0,
           19,   73,   51,   13,  461,   12,   33, 2469,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:4') tensor([81, 78, 71, 42, 63, 92, 70], device='cuda:4') tensor([0.8003, 1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(146.7911, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[  29,  180,   24,  388,  117,  427,  412,  430,  158,    6,  540,    9,
           13, 1127, 1692,   10,  150,   70,   53,  169,   87,    0,    8, 3188,
           54,   69,    7, 3530,    0,   24,   66,  185,  427,  412,  430,  158,
            6,   69,    7,  859,   17,   63, 1944,  336,    8,   21,  100,    6,
           10, 2677,    7,  218, 1710,    6,    9,   21,    6, 1422,    0,    2,
            1,    1],
        [  25,   11,   57, 3487,   62,   71, 7863,   54,   21,  346,  461, 1010,
          111,   35,    6,   20,   15, 2397, 1408,    6,    0,    9,    8, 2584,
          119,  218, 5933, 4601,    6,    0,   85, 1629,   35, 1898,  684, 3049,
            6,    0, 1189,    0,    8,   77,   25,   66,   10, 5971,   25,   63,
          155,  248, 1893,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,  169,   51,  100,   13, 3019,   35,  262,  411,  176, 1064,  206,
           25,  150,    7,  688,   85,    7,  467,   12,    7, 7781,    0,   67,
           21,   11,    6,   13, 2701, 1723, 1064,    0,   68,  194,   65,    8,
           84,   11,    6,  211,  207,   12, 2432, 2881,   80,    7,  688,   85,
            7,  467,   12,    7, 7781,    0,    2,    1,    1,    1,    1,    1,
            1,    1],
        [  19,  144,  288,   33, 6629,   46,   89, 4795, 1583,    0,   89, 1192,
            8,   13, 3434, 2626, 6951,   46,   17,   19, 6414,   21,    9,    7,
         2422,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   0,  225,   15,  290,   19,   93,   15, 4835,    0,    0,  238,    0,
           21,   11,    6, 4810,   17,   25,  451,  884,   17,  125,   91,   12,
            7,  214,   17,   11,    6,    0,    0,  916,   80,  378, 2639,    0,
           26,   25,  204,  175,   13,  341, 5200, 5072,    0,  613,  120,   25,
         6337,    7,    0,  207, 6878,   62,   94,  229, 4388,    0,    2,    1,
            1,    1],
        [   8, 5348,  203,   45,   22, 1026,    0,    0,  148,   34,  172, 1968,
           80,   21,    0,    0,    0,  144,   91,  524,    0,    0,  101,  465,
           11,   18,    0,  100, 1339,  742, 2919,  373,    0,  125,  101,  474,
           21,  169,    9,    6,  300,   18,   94,   71, 7576,   11,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1141, 4433,  270,    9,    7,  832,    0,    6,    0,    0,    9,
         6848,  518,    7, 4139,    0,   38,  779,  180,   19,  205,  270,  637,
            8,   19,  456,   10,   94, 3006,    9, 2667,   35, 1966, 1265, 3916,
            6,   84,    0,    8,   53,   11,   48,  289,    3,   25,  135,    0,
          417, 2847,    0,   25,   73,   11,   18,  172, 1897,  518,    7, 4139,
            0,    2],
        [  70,   11,    6, 4767,   55, 1136,  958,   26,  113, 4767,   55, 2216,
         2314, 5149,  125,   21,   11,    6,  172,  999,   55, 1201,   10,   66,
          486,  574,  187,    0,  211,   91,  213,    6,   21,    0,    8,    0,
          204,    0, 7009,    6,  192,   11,   18,  213,   10,  205,  637,   71,
           13,  903,  763, 2129,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([60, 53, 55, 27, 59, 49, 62, 54], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8911, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(82.9655, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:31:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-17 09:31:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
tensor([[ 115,   33,  415,    6, 3784, 2410,   35, 5444, 1466,  659,   46,    8,
           19,  154, 7674,   10,   46,  229,  170,  598,  133, 1986,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,  409,    0,   33,  479,   12,  203,    6,  234,   57,  140,  945,
            7,  733,  479,   26,   29,  528,   17,   19, 1808,   10,  154,   80,
           70, 1148,   71,  801,  639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,  154,   80,    7, 2695,   12, 6105,    9,    7,  270,   12,  155,
         1066,  125,  214,   73,   51, 1932,  336, 1552, 2117,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 126,  168,  440,    0,   84,   63,   13, 3631,   12,   94,   46,   13,
         2045,  556,  881, 1341,   48,   55,  663,   46,  148,  169,  100,   86,
          288,   10,  991,   13, 3400,    0,   67, 1108,   13, 3400,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 1043,    0,   19,  154,    0,   26,   17,    7, 2705,    6,  369,
          373,  703,   53,   11,  121, 5248,   22,   10,   51, 2705,    6,  369,
          373,    0, 6872,    7,  963,  291,   20,  362,  614,   93,   62,  598,
           21,   11,    6,  226, 1911, 1965,   18, 3840,  134,    0,    2,    1,
            1,    1,    1,    1,    1],
        [ 339,   11,    6,  368,    7, 1232,   79,    7,  772, 3976, 3180,   24,
            0,   66,    0,    0,    8,  661,   85,   70, 3151,   94,  169, 1703,
           55,   33,    0,   29,   53,    0,  175,    7, 8568,   12, 2573,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  79,   13, 1920,    0,   19, 6090,   13,  325,   12, 4528,    8,  144,
         1771,   12,   56,  706, 1251, 1320, 1993,  693,  100, 2483,  371, 1434,
           93,    8, 2481,  726, 2992,    8,    7,  672, 1810,    8,   13,  277,
          523,   12, 4289,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  53,   11,   57,  434,    7, 5875, 4460,  415, 3729,   18,    6,    0,
            8, 2245,   66, 2107,  270,    8, 1797,   62,  143,  813,   69,   77,
           12,  117,   94,  333,  555,  215,  700, 1313,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 103,   25,   11,   48, 2107,  199,    7, 3227,   12,    7,   94,    0,
          238,    0,   25,  169,   66, 1110,   13,  325,   12, 6146,    0,    8,
           25,  169,   66, 1110,   13,  325,   12, 1259, 4117,    0,    8,   25,
          169,   66, 1110,   13,  325,   12,  214,   17,  169,  305,   13,  535,
          183,   10, 4971,    0,    2],
        [ 245,    0, 3942,   53,   11,   57,   86, 6902,    9,    6,   15,    6,
          235,  366,   10,  218,   94,   11,    6, 4734,    0,   53,   63,    9,
            6,   15,    6,  235,  366,   10, 1964,    6,   17,  218,   94,   63,
            9,  838,   18,   57,    6,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6, 1223,  952,  100,   25, 2153,    0,    8,   29,    0,
           55,  663,    0,   24,   11,   57,  826,   29,  261,  117, 1113,   80,
         1291,   22,  140,  416,   22,  125,   12,    7, 2496,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   19,  591, 4822,  765,    6,   12, 6950,    9,    7, 7343,   17,
           89, 6792, 1290,  804,  607,   62,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  24,  220,  154,    0,   38, 1871,    0,  220,   24,    0,   87,   10,
          229,  749,  532,  713,  509,    3,   21,   11,    6,   85, 1686, 1254,
            7,  749,  532,    6,  169, 1529,   55,    7,    0,    0,  324,   12,
          749,  532,  713,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   86, 1525,  596,    0,   21,   11,    6,   55,  170,
            0,   55,  170,  596,    0,   55,  108,  708,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 276,    7,   73,  287,   96,  873, 6608, 3008,    6,  206, 7400, 2270,
           48,   48,  232,  106, 1318,   10, 1318,    0, 2823,   54,  995,  106,
          593,  322, 1203,  426,   10, 5966, 6926,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,  564,  895,  584,    0,   56,   62, 1251,  110,   59,  656,  505,
         1017,    7, 1797,   55,    7,  535,  608, 4480, 5147,   48,    9,   91,
         3350,   85,  640, 2602,    0,  815,    0,  895,  895,  887, 2360,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([24, 31, 23, 36, 47, 37, 41, 34, 53, 44, 35, 20, 41, 22, 34, 37],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8945, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8901, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(61.7179, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[   9,   33, 1760, 1165,    0,    0,   24,  162,  802,   13, 2270,   45,
            0,  109,    0,    0,  339,  110,  116,  150,  103,    0,   19,   73,
          175,   33, 4481,    0,    0,   13, 9238,    0,  217, 1076, 5997,   56,
         8368,  922,    0,    0, 2806,  240, 2175,  249,    0,    2],
        [   7,  225,  158,   12,    7, 1052,  249,  781, 1158,   34,  561,   48,
          199, 1580, 6032,   85,   13, 2290,   17,   24,   11,   57, 2438,   54,
          131,    7,  467,   12,   33, 1910,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    9,  251, 1113,    0,   19, 1608,   11,   18,  283,   69,    7,
         6389, 6035,    0,   19, 1608,   11,   18, 1927,   13, 4446,  109, 2618,
           39, 5651,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  144,    7, 1666,  941,   35, 9748,  688, 9372,    6,   17,  842,
          785, 1601,   10,   82,   45,  132,    8,  180,   25,  162,  859,  665,
           13,  321,   12, 4594,  111, 1650,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,   77,   12,   13, 5827,    0,  168,   19,  217,    0, 6815,
          241,  829,  106,  747,  670,    9,  564,  957, 1132,    0,    8,   33,
         1148,    0,    8,   24, 2252,   17,   38,  290, 1408,  197,  513,  755,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 1043,   19,   11,   45,   29, 7105,   80,  467,   54, 3753, 3023,
         1382,   11,   18,  116,  125,   21,   11,    6,  999, 3023,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,  154,   80,   21,   79,   13, 1780,   35,   18, 1397,    0,
         1523, 1909,   12,  294, 2386,   12, 1655,   12, 2602,    0,  166,   26,
         2546,  829,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1831,   17,  103,   19,  220, 3665,  929, 7897,    0,   19,  220,
         4075,   17,    0,  432,   77,    0,   19, 1359,   11,   18,  172, 2702,
          111,   19,  158,    0,   21,   34,  185, 4416, 4113,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    9,   33, 1165,    0,   21,   11,    6,   39, 1663, 1039,  369,
           55, 2327, 1034,  240,   20,   69,  117, 4601,  825, 3275,   35, 3794,
         2885,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   56, 3168, 2445,   93,   11,    6,   13,  133,  528,  461,   12,
            7, 9655,    8,   24, 5150,  108, 1613,   10,  274,    9, 1820,   55,
         4455,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   69,    7,  218,    0,  874,    0,   24,   66, 1075,   17,
            0,   63,  244,  737,  891,   54,    0,   69,  837, 2694,    0, 5955,
           10,  159, 6768,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2072,  988,   26,  250,   17,  689,   11,   18,  100,   10, 2895,  133,
          261,    0, 3706,  359, 7036,    0,    8,   12,  538,   24,  169,  100,
           10, 1082,  143,   80,   21,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    7,  218, 2475,   12,   81,   18,  340, 8978,   26,    7,
          227,  424,    6,  560, 5824,  424, 1825, 1158,    0,    8,  117, 1816,
          985,  132,  227,  760,  233,    6,   55,   13, 1074,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1980, 1068,    0,   24,  296,   13,  574,  833,   12,  524,   35, 1105,
          505,   54,    0,   86,  116,  524,   35, 5001, 3635,  833,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   33,   34, 2861,   62,  131,   39,  786,  727,   46,  166,   26,
          100,    0, 1730,    9,    7,  467,    0,  101,  164,   86,   66,  995,
            0,  125,   21,  164,  417,  827,  603, 1906,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,   87,   25, 1005,    7,  909,  609,    6,  369,   12,    0,    7,
            0, 2018, 2510,  429,   12,    7, 1683,  724, 1025, 6433,   54,    7,
         8094,  429,    0,   12,    7,    0, 1478,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([46, 32, 28, 32, 38, 24, 28, 35, 28, 27, 29, 31, 35, 24, 34, 33],
       device='cuda:3') tensor([0.8242, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 1.0000, 0.8706],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(52.0802, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[   7, 3288,   63,  ...,    1,    1,    1],
        [  53,  113,  180,  ...,    1,    1,    1],
        [ 282,   26,   80,  ...,    1,    1,    1],
        ...,
        [4069,    0,   21,  ...,    1,    1,    1],
        [2869, 3540, 4712,  ...,    1,    1,    1],
        [  33,   26,    7,  ...,    1,    1,    1]], device='cuda:3') tensor([15, 15, 16, 13, 21, 10, 18, 12, 12, 14, 16,  9, 14, 19, 15,  8, 15, 16,
        15, 12, 10, 15, 19, 18,  8, 23, 16, 18, 13, 12, 11, 20, 19,  8, 15, 18,
        11, 23, 15, 13, 18, 15, 15, 11, 16, 13,  7, 14], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8711,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000], device='cuda:3', dtype=torch.float16)
 > at.  tensor(22.7148, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[ 125,    7,  688,  ...,    1,    1,    1],
        [1228,    0,  155,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        ...,
        [   7,  214,   24,  ...,    1,    1,    1],
        [   8,   21, 4333,  ...,    1,    1,    1],
        [   8,   29,    0,  ...,    1,    1,    1]], device='cuda:3') tensor([ 8, 10, 11, 15, 12,  9,  8, 12, 10, 10, 17, 16, 12, 10, 10, 11, 12, 18,
         6, 11, 12,  8, 10, 10, 10, 11, 11,  9, 10, 10, 12, 18, 16, 12, 14, 14,
         7,  9, 12,  9, 12, 11,  9, 10,  9, 10, 10,  7,  9, 11, 14,  9,  8,  8,
         6, 12, 10, 12,  8,  9, 11,  8, 11,  8, 12,  8,  7, 13, 11, 15, 17,  8,
        13, 10, 12, 13, 11,  9, 11,  6], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8872, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8379, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8101, 1.0000, 0.8486,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8057, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8257, 1.0000, 1.0000,
        1.0000, 0.8301, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(15.6052, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[  25,  116,  175,    7, 8570,    0,  347,    0,  125,    7, 1922,  181,
           85, 1922,  195, 2270, 3956,  188,   13,   56, 4030,  741,  266, 5126,
           80, 1665,   35, 5489, 3153,    6,  803, 1326,    0,   38, 5276,   26,
            7,  207,   21, 7674,   10,   51,    3,    8,  225, 3860,    6,   21,
           17,  207,  183,    8,  183,  554,    0,    8,  103,   25, 1336,  293,
           57,  237,   71,  267,    0,  225,  164,  289,    0,   38, 1969,  135,
           70,    0,   25,   11,   57, 1226,    0,   33,   26,    7,  772,  207,
           21, 7674,   10,   51,    9,   33, 5743,    3,    2],
        [  53,   66, 2314,   93,   35,  779,  569,   54, 1187,    6,  100,   38,
            6,  786,  373,   93,   45, 1604,  724,  197,  109,   38,  372,   59,
          247, 2583, 6874,    3,   19,   11,   45,   86,  142,   10,  205,  199,
            7, 3201,    6,   12,  117, 1288,  115,    0,   67,    7, 1890,  613,
           26,   33,    0,  103,  419,   12,  134, 1945,   62,   33, 4949,  111,
         3495,   35,   18,  500,   62, 2070,   12,    7, 8575, 1726,    0,  180,
           24,  451,  150,  264, 5554,  378, 1621,   85,    7, 1979,  176,  140,
         1585,   71,    7, 8575,  853,  650,    0,    2,    1],
        [  13, 2672,   34, 1621,    0,   39, 8547,   13,   48,  525,  365,   59,
          866,   34, 1223,    9,  879,  945,   94,   10, 2802,    7, 2672,    0,
            8,   84,   34,  211, 1757,    0,   38, 1871,   63,   24,  142,   10,
           87,    3,   38,  187,  192,   11,   18,  135,    3,    9,   13,  555,
         1113,    0, 1953,    6,   12, 1655,   12,   94,   84,   46, 7417, 5902,
          589,    6,  148,  162, 2659,    7, 3955,  119,  724,   12, 9657, 8428,
            6,    0,   38,   15, 4399,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  120,   19,  316,  122,    0,  359,   89,   86,   96,    0,   70,
           19, 4776,   19,  144, 3924,  346,   26,   17,    0,    7,    0, 5638,
          415,  829,    0,   69,  108, 1247,   26,  100,    7,    0, 5638,  415,
          829,    0,    0,   69,   13, 2035,  221,  387,    0,  848,    8,  848,
            0,    8,   94,  474,    7, 5638,  415,  829,   69,  108, 1247,    0,
            0,   34,  100,   13, 2103,  366,    0,  415,  829,   17, 3524,  442,
          108, 1247, 2183,   37,  109, 5167,   37,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   24,  316,  122,  108,  101,  356,    6,    8,  246,    0,   38,
          533,   11,   57,   86,    9,    7,  800, 1678,    0,   24,  213,   10,
          305,   91, 1269,   85,   13,  183,    8,  305,    7, 1269,  230,  359,
          670,    0, 2955,   10, 2900,    0,    8,  175,  134, 7070,   55,  509,
         1074,    0,   13,  747, 2070, 1329,    3,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   19,   11,   48,  100,   25,   10, 1683,   13, 3955,  930, 2735,
            9,   13,  207,   17,   91,  188,   22,   11,   18, 1620,   62,  490,
            0,  934,   13, 3955,  930, 2735,   17,  689,   11,   18,  641,  384,
         1105,  810,    7,  984,    0,  934, 3343, 4867,  170,   87,   33,  131,
           13,  140, 1236,   45,  300,  829,    8,  909,  609,  777,   18,  829,
            8,  987,  607,  445,   54, 2483, 1643,    6,  126,   12,  129,  128,
          160,  271,  862,  881,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 3699,  132,  378,  529,   10, 1157,   79, 1029,  187, 3684,   22,
         1337,    0,   79, 1029,  187, 6073,    0,   80, 1111, 1345, 1065,   46,
           29,   13, 2701,   12, 1498, 1345, 1065,   46,    8,  513,  106, 6871,
           19,   10, 6871, 1051,    0, 3699,  132,  401, 8899,  283,   85, 1137,
         1839,  120,   19, 3447,   62,   10,    7,  832,    0,    6,    0,    0,
            8,   66, 2277,   48,   10, 4432,   33, 2468,  244, 4401, 2205,   10,
         1387,   10,   13,   87,  610, 1234,    6,  115,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([93, 92, 79, 81, 56, 78, 82], device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 0.8413, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(138.0204, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[  91,   12,    7, 3373,   19, 3395,   62,    0,   10,   45,    8, 6819,
           15, 6760,  918,    6,    0,  162, 1828,   13, 6230,  120,    0,   79,
          963,    8, 3150,  264, 1736,  373,    0,  159,  245, 1269,   34, 8465,
           71,  346, 9414,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  25,   13,   59, 2315,   20,    0,    0,    7,  415, 1124,  160,  271,
            9,    7, 1361,    0,    8,  131,  401,   17,    0,    0,   25,  175,
            7,  370, 5204,    0,  929,    7, 8820, 1623,  693,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 3757,  457, 1006, 1529,   26,    7, 3187, 9225,   12, 8829,  368,
         5465, 2840,    9,   13, 2042,    0,    8,   21,  113,  583,    6,   55,
            7,   56, 9826,   12, 2840, 4166, 1241, 7202, 1006,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  276,   71,   33, 4673, 2394,    0,   24, 6182,   10, 3121,    0,
            8,   24,  323,    0,  746,   12,    0,  467,    6,  132, 7461, 3486,
            0, 1056,   10,    3,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,   24,   66,   10, 1031,  235,    7,   56, 2687,    0,  125,   24,
          296,   10,  175,    7, 4233,  128,  800,   12, 4709,    6,  359,   10,
          175,    7, 2931, 2808,    0,    8,  131, 1031,  235,   54,    7,   56,
         2687,    0,   24,   11,   57, 2983, 2762, 4581, 1588,    0,    2,    1,
            1,    1,    1,    1,    1],
        [1377, 2293,    6,   13,  245,  903, 5575,    0,  166, 1064,  180,  203,
         2329,   96,    0, 1405,   35,  160,  689,   11,   18,  641,  291,  627,
          232,  457,    0,   21,  818, 7013,    9, 6390,   12, 1064,    0,   38,
         1824,    0,   29,   70,   11,    6,   69,    7,  245,  903, 5575,   12,
            7, 2645, 1066,    0,    2],
        [1723, 6068,   26,    7, 3139, 2719,    0,  903,  693,   18,  561,    9,
         1491,    0,    8, 1155, 1754,    6,   84,    0, 1155, 1754,    6,   84,
          125,   21,  689,   11,   18, 3833,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   13,  546,   12,    7, 1644, 1327, 1632,  783,    0,
          166,  818,   38,   22, 2592,  846, 2845,    3,   29,    9,  117, 1666,
         6814,    6,    0,  125,  288, 6814,    6,   66,   13, 1644, 1327, 1632,
          783,    0, 1273,  303,   18,   35, 3794, 5558,    0,    2,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:7') tensor([41, 35, 35, 29, 47, 53, 32, 46], device='cuda:7') tensor([1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(74.7355, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[1042,   24,  468,  251,  248,  214,   46,  103,   25,  289,    7, 2593,
            6,   63, 1714,    8,   53,  296, 3039, 8570,    0,   25,   66,    7,
         1670,  595, 1795,   12,  324, 7226,    6, 1944, 3298,    7, 4959,    0,
           71,   70,    0, 3023,    6,   55,    7, 1714,    0,  896,  203, 3870,
           55,  251,  148,   63, 8422,    0,    8, 2922, 7931,  373,   55,  251,
          148,   63, 6499, 4186,   82,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   0, 2266,  140,    0,    0,    0,    0,    0,   13, 6014,   12,   56,
          386,    0,    0,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,   21, 1847,   13,  670, 3516, 6467,  148,   26,  142,   10, 1436,
         1993,   93,   55,   25,    8,  289,    0,   38,  156, 3837,    0,    7,
         8260,   26,  752,   10,   19,  362, 1782,   33,    0,   67,   25,   66,
            7, 3473,   10,   87, 5422,    3,    8,   21, 1847,   39, 4414, 5370,
         1740,  148,   26,   86,  288,   84,    0, 3243,   13, 7771,  333,  383,
            0,   67,  148,   26,  461,   12,  108, 8755,    0,  961, 4012,   55,
          159,  807,    0,  108,  807,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33, 1760,  203,  356,   46,   21,   11,    6,   39, 4174, 1228,   46,
           67,   21,   34, 3498,    9, 1927,   35,  160, 6292,    6,    9,  564,
          957,  820,   46,    9,    7,  384,   18,  292,  235, 1909,    0,    9,
          409,   46,    8,   21,  909,  140,   62,   62,   13, 1591, 1314, 1988,
          311,  140, 1994,   11,    6,   38, 9103,    3,   68,  194,   65,   29,
           19,  100,   10,  154,   84,   34,  250,  142,   69,   84,   12,    7,
          264, 2042,   12,  229, 1118, 1028,  126,   12,   33,    0, 5493,   38,
         9103,    3,    2,    1,    1,    1],
        [   0,   19, 1704,   11,   18,    0,  205,  199, 3201,    6,   80,    0,
            0,    0,   70, 2912,   10,    0,   13, 3486,   19,  442,    0,   67,
          339,   11,    6,    0,  116,    0,  289,   21, 3006, 8939,    0,    0,
         9621,    6,    0,    0,  218, 8829,    6,    8,   13, 1365,    0,    0,
           68,  194,   65,   19, 1223, 1446,   17,   21,   34,   19,    0,    0,
           86,    7, 2421,  109,    7, 2493,    0,  109,    0,  995,   17, 5725,
         1663, 1222,    0,   17,   34,    7,    0,  288,    0, 4802,    9, 9480,
         3584, 2539,  269,   54,    0,    2],
        [  19,   34, 1060,  131, 1106,  233,  650,  973, 1670,    0,   13, 3434,
         1067,  525,  436, 1484,    0,  166,   26,    7, 3299, 3434, 1067,  525,
          436, 1484,    9,    7,  179,   46,   53, 1060,  110,   10,  694,   13,
         3585,  575, 5667,  322,   55, 7996,   85,    7, 1670, 9302, 8517, 3718,
            9,  264, 1736,    0,    9, 3697,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:7') tensor([67, 17, 79, 87, 90, 56], device='cuda:7') tensor([1.0000, 0.8477, 1.0000, 1.0000, 0.8018, 1.0000], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(157.0840, device='cuda:7', grad_fn=<MulBackward0>)
False tensor([[   9,    7,  248,  215,  490,   89, 7102,    0, 4743,  439,   12,    7,
         8224, 1580, 2217, 3258,  116,  110,  543,   62,  755,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   33,  842,   77,   12,    7, 3677,    6,   17,  144,  700,  226,
         5823,   62,   69, 2667,  262, 2651,  480,    6,   17,  162, 1719,  292,
          121,   48,  244,   13,  798,   35, 1933, 2760,  131,    7,  664, 1151,
            0,    2,    1,    1,    1,    1],
        [   9,  409,    0,  276,    7, 4273,   93,  813,   17,  155, 1893,   63,
         9830,    0,  155, 4896,    6,   63, 9830,    0,   26, 1501, 3012,  982,
          125,   21,  220,  641,  995,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [2266,   45,    0,   39, 5486,  140, 1997,   26,   39, 5204,   29, 2583,
         9646, 2010,    0,   25,  465,   11,   18,  276,  135,   21,   34, 1254,
         1325,   25, 3128,   48,   21,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 473, 1822,    0,   19,   34,    9, 8393,   71,    7, 6099,   12,  958,
            0,  125,  103,   25,  192,   11,   18,  135,    0,   84,   11,    6,
           13,  452, 1534,   20,  371,  126,  174,   57, 1161,    9, 8393,   85,
            7,  765,    0,    2,    1,    1],
        [   9,  409,    0,    9,    7, 1909,  206,  225,  925,  106,    0,  143,
          254, 1737,  439,   12,    7, 1579,    6,  684,  589, 9447, 1682,  188,
         1220,   14,   48,   12,   33, 1599,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   66,  185, 2468,   80,    7,   29,   35, 8309, 5308, 5874, 8282,
            0,  166, 3336,  518,  131, 2659,    0,  138, 5308,   26,    7, 5874,
         8282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  25,   66,    7, 2375,   82,   45,  322,   12,  155, 1037, 2202,  230,
          558,   10,   25,    0,   67,   25,   63,  113, 1363,   10,  205, 1273,
          727,   54,  336,    7, 8464,  713, 1117,  155,  447, 1066,    0,    2,
            1,    1,    1,    1,    1,    1],
        [  25,  175,   10,  780,  134,  126,    0,  722,   71,  134,    0,  417,
         6127,  436,  134, 1325,    7, 5462,  901,   24,  293,    6,  126,    0,
          490,   25,   66,   10, 1557,  134,  270,    0,    8,   25,   11,  158,
          175, 4744,   55,   21,    0,    2],
        [   7,  747, 2653,   12, 7151,    0,   29, 3761,    0,   34,    7, 5605,
           12,    7, 8575,  853,  650,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  53,   63,  401,   21,  125,    0,   12,   91, 7857,  288,    0,   53,
           66,    0,   13,    0, 4960, 4469,   10, 1082,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   24,  172, 7827,    7, 2918,   10,  213,   10,   87,   21,    0,
          860,   17,    7,  296,   55, 7935,  445,   56, 1783,  870,   45,  925,
          132,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 639,  188,  367,   12, 1137,   17,   11,    6, 2156,   54,  170,   10,
          150,    7,  984,  106,  672,    8,  205, 2027,  199,    7, 1580,    6,
         5309,  111,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [3088,   46,   24,   11,   57,  133, 3926,   46,   17, 1455,   38, 1355,
          777,  232,    3,    7, 8750,   26,   86,  595,  121,   48,  126,   12,
           13, 1472,   12,  877,  221, 1547,    0,  126,   12,   13, 1886,  362,
           12, 3164,    0,    2,    1,    1],
        [   8,  180,    0,   13, 7685,    0,   13, 1838,   20,   26,   15,    0,
          323,   33, 1968, 1692,    0,  225, 2393, 1613,    9,   10, 2026, 1195,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,  120,   19,   34,   13, 2145,  586,   54, 1850,   35,   48, 1327,
           85,  415,  675,   45,  174,  407, 2811,    0,   19,   34, 2027,    9,
           13, 6528,   35,  240,   35,  737, 1410, 7424,   12,   89, 3557,    0,
            2,    1,    1,    1,    1,    1]], device='cuda:1') tensor([23, 38, 31, 31, 40, 32, 27, 36, 42, 19, 22, 27, 28, 40, 26, 37],
       device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8276, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(52.8364, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[ 251, 1051,  174,  ...,    1,    1,    1],
        [   7,   10, 3464,  ...,    1,    1,    1],
        [  67,   19,   11,  ...,    1,    1,    1],
        ...,
        [  29,  117,  126,  ...,    1,    1,    1],
        [  17,   11,    6,  ...,    1,    1,    1],
        [  17,   11,    6,  ...,    1,    1,    1]], device='cuda:1') tensor([19, 30, 24, 19, 25, 25, 30, 15, 23, 12, 21, 26, 24, 25, 19, 40, 11, 24,
        23, 21, 21, 12, 24, 11, 19, 14, 17, 28, 20, 26, 22, 21],
       device='cuda:1') tensor([1.0000, 1.0000, 0.8955, 1.0000, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8232, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8896,
        1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 1.0000, 0.8418, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:1',
       dtype=torch.float16)
 > at.  tensor(36.8091, device='cuda:1', grad_fn=<MulBackward0>)
False tensor([[  84,   11,    6,  288,  391,  214,   25,  296,   10,  135,    0,  138,
          261,   94,  229, 1117,    7, 1484,    0,  138,  261,   94,  229, 3494,
          994,    9,   13, 2414, 1201,    8,  138,  261,   24,  229,    9, 2737,
           10,  150, 1179,   24,   73, 3757,   21,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1313, 6249, 8221,    6,   63, 2015, 1197, 2185, 2658,    6,   12,    7,
         1479,    0, 1823,   48,   35, 6665, 3479,    6,    0,   53,   63, 4402,
          442,  132,   12, 2072,  988,    0,    8,   17,   11,    6,   70,   25,
          150,    9,   33, 5674, 1410, 9787,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  487, 1692,   54,    0,  961, 3555,    0,  952,   10, 3698,    6,
            9,    7, 1726,    0, 1520,  159, 6382,    0,    8,  203, 2172,   54,
            0,    8,   19, 3803,   69,  203, 2172,   54,    8,  203, 2172,   54,
           55, 1303,    6,    8, 1822,  856,    6,   55,  244,  785,  215,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,  217,   26,   13, 3280,   12, 6936, 3023,  148, 1505, 7105,
          111, 2091,    9,   33, 6120,   80,  392,  215,  581,  120,   13, 2745,
         1060,  110,   13,  630,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 125,   21,   34,  288,  131,   86,  378,  529,   10, 4075,   17,  225,
           34, 1226,    0,   17, 6229,  220,  508,   13, 1838,   20,    7, 7343,
          225, 1918,   10,  135,   17,  225,   34,  230,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,  288, 3699,  125,    7, 2781,  775, 1026,  307,  373,  144,
           10,  205,  106,  316,   59, 3333,   10, 2902,   20, 3594,    0,  166,
           26,   13,  248,   35,  715,  234, 2365, 3046,    0,   10, 4342,    7,
         3432,   17,  859,   69,    7, 1998,  322,    0,  125,    7,  558, 3432,
         1359,   11,   18,  336,   55,   13,  535,  183,    0,    2],
        [  24,  591,   13, 8319,   22,  148, 1202,    6,   13, 1201,   17,   11,
            6, 2875,   10, 2156,    7,   25,  322,   10,  722, 1192, 3374,   69,
         8549,   48, 3374,  521,  779, 1212,    8, 8549,   48, 4528,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9, 1760,    0,   38,  469,  227, 2174,   37,   85, 1290,   45,  904,
          197,  166,   34, 1466,   62,   79,  171,  458,   37,   11,    6, 3495,
          119, 4546,  777,   20,  430,    0,  284,  772,  283,   46,   94,  169,
          367,  939,  181,  292,   45,  834,   77,  244,    7,  179,   10,  150,
           21,   46,   34,  204,   13,   55,  821,   93,    0,    2]],
       device='cuda:6') tensor([45, 44, 49, 30, 34, 58, 37, 58], device='cuda:6') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:6', dtype=torch.float16)
 > at.  tensor(76.0430, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[   8,  143,  254,   13, 4430, 1065,    0,  120,   19,  456,   10, 2299,
          606, 5996,    6,    0, 1179,    9, 2591,   15,  741,    0,  264, 2266,
          373, 2383,  109,    7, 2100,   12,    7, 1950, 1318,    0,    8,   24,
          456,   80,    9,  122,  338,   48,    0,   53,   77,  289,   17,   53,
           11,   57,  752,   10, 7369,  267, 6266,    8,  267, 5289,    8,  172,
          991,   69,    7,  291, 1256, 1105,  158,   62,  283,   12,  267,  282,
           11,    6, 4343,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   24,  192,   11,   18,  276, 2036,  160,  156,  120,   24, 1510,
         1482,  289,   33,    0,  125,   24,   11,  121,    0, 1346,    0,   17,
          321,   12,    0,  993,    0,   55,    0,   29,  535,    8, 3524,   24,
           11,  121, 1501, 6936, 1181,    8, 3250,   62,    0, 4328,  111,    0,
            0,   33, 5126,   17, 4564,    8, 5187,   63, 3524,    9,  560,   15,
           18,  111,    0, 3249,   62,    8,   17, 2420,  724,    0,    9,    7,
            0,  467,    0,  164,    0,  735, 5017, 2141,   10,   13, 5295, 1410,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 131, 6763,    7, 1880, 4948,  366,  687,   12,    7, 1503,    0, 2603,
            9,  108, 8977, 1490, 8365,   62, 4621,    9, 1146,    0, 4793, 2527,
            6,   10,   13,  140, 2109,  241,  436, 8977, 1953,    6, 1955,   79,
          333, 1127, 8977, 1361,  115, 1772,    6, 8569,   10, 3284,    7, 1503,
            9, 1296,   10,  175, 2214,   10,    7, 5222, 4793, 2180,   20,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   33,   34,   13, 6954,    9,    7,  264, 1736,  825,  206,
            7, 8189,   34,  133, 1665,    0,    8,    7,  264, 1736, 1503,   55,
           15,    6,  266,    6, 5670,  552,    9,    8,   53,  204,  323,  185,
          948,   10, 4223,    7, 1665,   56, 1251,   20,   71,   17,  591,    9,
           13, 1943,  484, 2846,  679, 1922, 3832, 6412,  346,    7, 2291,    0,
           68,  194,   65,   29,   25,   73, 6212,  240,  155, 8189,   10, 5953,
         3610,   25,  213,  131, 2685,   54,   51,   20,   35, 6523,  111, 5464,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0, 2275,   84,    0,   19,   11,   45,  188,  221,    0,    0,   19,
           11,   45,    0,    0,   39, 2420,    0,    8, 3465,    0,    0,  120,
           19,  450,   94,   19,   11,   45,   39, 2420,    0,    0,    0,   53,
          116,  274,   85,  110,    8,  289,    0,   38,  870,    0,   25,    0,
         3765,    3,  109,   38, 1871,  321,   12, 7465,   87,   25,  283,    9,
            3,  238,  281,   12,   89,  283,   17,   19,  283,   71,   26,  172,
           13,  277,  523,   80, 4401, 3403,    6,   12,  740, 1552,    0,    0,
          254,  204,   13,    0, 3926, 7806,  109,   13, 3926, 4814,    0,    2],
        [4290, 6142,    6,    8, 1370,  128,  567,    0,   71,   13,  555, 6850,
            6,    0,   63,    9, 6063,  457,   12, 7220, 1613,    9,    7, 1336,
          480,  429,    8, 3316, 1918,   10, 1005,   33, 2462, 4718,  142,    0,
           29, 1881,   63, 3101, 5442, 1081,   10, 7992,   33,    0,   67,    9,
            7,  467,   21,  568,   86,  318,    6,  416,  121,    7, 1381,   12,
            7, 4937,   55, 2567,   33, 1370,  128, 1710,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    0,  120,    0,   17,  689,   11,   18,    0,  283,    0,  120,
           21,    0, 1518,  126,   17,    0,    0,   94,  148, 8523,    0,   71,
          170,   66,   77,    0,    7,  370,  409,    6,   24,   87,    8,   63,
          204,  976, 2465,    0,  180,   24,  954,   69,   10,   13, 1575, 6730,
            0,   53,  135,    7, 2403,    0,    8,   53,   63, 9700,  111, 9478,
           54,   21,   55,  159,  447, 3360, 1184, 1374,   18, 3965,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([77, 86, 61, 87, 96, 70, 73], device='cuda:6') tensor([1.0000, 0.8491, 1.0000, 1.0000, 0.8389, 1.0000, 0.8726],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(136.7229, device='cuda:6', grad_fn=<MulBackward0>)
False tensor([[  53,   73,  618,   10,   51, 1421,  215,  109,  143,    0, 3942,   24,
          154,  281,   12,  134,  914,  192,   11,   18,  229,   21,   10, 1421,
            9,    7, 2533,    0, 5349,    0,   56, 9172,    0,    7, 3726,  188,
          267,  245, 2610,  120,  225,   11,    6, 1498,  109, 1230,    0,   84,
         5575,   37,    0,  225,  188,   91, 2610,  288,  333,  785,  109, 1111,
          215,    0,   13,  535, 2760,   12, 5671, 3188,   15, 1255,  120,    7,
         1269,   26,  149,    6,   54,    0, 2544,   54,   71,    7, 1276,   85,
         1303,    0,    8, 4532,   54,   69,  267,  270,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,   11,   45, 1120, 1123,  281,   12,   25,   66,  474,    0,
           38, 1461,    0,  367,   69,    0,   73,   11,   18,   25,   87,  250,
          143, 3021,  254, 4502,   54,   85,  742, 4599,  693,    3,   19,   11,
           48,  100,   25,   10,  388,   33,  321,   12, 7754,   35, 1899,   37,
          156, 5250,    9,    7, 3317,   12,   70,   25,  169,   66,  474,  103,
           25,  144,  591,  155, 2088, 2841, 1930,  870, 1537,  109,  155, 2988,
         3696, 6264,    6,  812, 2637,    0,  230,    0,  281, 1848,  169,  446,
           17,  453,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13, 5573,   12,   94,    9,  108,  753,  440,  148,   63, 1880,  111,
         4414,  703,   17,   53,   63, 7827,   48,  131,  570,   67,    7,  218,
          926,   26, 7827,   48,  131,  130,   20,    0,  154,   80,   21,    0,
          154,   80,   21,    0,  281,   94,   63, 3045,  336, 1211,    0,   38,
         1969,  135,    0,   89,   19,  262, 2992,   26, 2219,   69, 2827,   51,
          375, 1184,  237, 1068,    0,   19,  213,   10,  601,   94,    0,   67,
            7,  218, 1816,    0,   53,   11,   57, 6651,    8,  126,   10,  175,
          110,    3,   25,   73,   11,   18, 2694,   79,   13, 1504,  120,   25,
           66,   33,  321,   12,   79,   93,   45, 1604,  724,    0,   21,   11,
            6, 3756,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,   21,  164,   66,   70, 5767,    6,  583, 8153, 4608,   69,
           13, 1749, 2340,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,   11,   45,  142,   10, 2156,   25,   10, 1797,    7, 8574,
            8,   19,   11,   45,  142,   10,  508,   25,    7, 1298,    0,    8,
          103,   25,  700, 3265,   10, 2823,   21,    0,   25, 1412, 1964,   39,
         2990,  445,   17, 1119,    7,  839,  164,  367,  270,    8, 1766,    7,
          670,    0,   38,  511,   19, 1797,   62,  876,  195,  195,   93,    0,
            8,  101,   14,   48,   13,  464, 1065,    0,   67,   86,  490, 2432,
           13, 5996, 3335, 5829,  699,   93,   56,  901,  706,   70,   24,  162,
          401,    0,    8,  101, 2721,  132,    8,  246,    3,  876,  195,  195,
           93,  952,   80,   25,   77,  244,    7,  753,    0,   81,    0,    8,
           19,  213,   10,  601,   25,    0,   38,  511,  180,   13, 1300, 3335,
         1106, 1806,  741, 2961,  128,  340, 2721,  132,    0,    2]],
       device='cuda:5') tensor([ 94,  88, 112,  17, 130], device='cuda:5') tensor([1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
 > at.  tensor(185.6168, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([1.0000, 0.8086, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(86.8365, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[  25,  135,    0,   25,   11,   57, 2465,    0,   25,   66,   77,  117,
          214,  142,   55,   25,    0,   38,  511,   19,   11,   45,  100,    3,
          125,   19,   11,   45,  168,    8,   21,   11,    6, 3939,    0,    8,
           25,  135,    0,   19,  321,   12,  100, 2180,   18,   18,    6, 3884,
          176,    0,   38, 3794,    0,   17,   11,    6,    7, 9132,  608, 1043,
           19,   11,  121,  700, 1346,   55, 4432,   54,   10, 2240,  670,    0,
            2],
        [  29,  339,   11,    6,  498,  518,  131,  384, 2172,   54,  185, 2047,
            0,   70,   26, 7898,    0, 7898,   26,    7, 6408,   24, 1064,  120,
           24,  154,   17,  108, 2082, 2196,  220,   51,  509,  109, 8661,  103,
           24,  144,  691,  250,  341,    9,    7, 1406,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 103,   53,  169,    0, 1744,  116,   13,  277,  908,  183,    0,    0,
           69,  752,   10, 2509,  159,  207, 3298, 1573,    0,    8,   13,  277,
          523,  143,    0,    0,   69, 1880, 5917,   85,  637,    0,   53,  506,
            0,  508,  159, 3226,   13,  509, 2333,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,   66,   10,  229, 1123,   53,   11,   57, 3163,   10,   51,    7,
         3348,   12,   33,  453,  753,   12,  108,    6,    0,   13,  753,   17,
           26,  100,  211,  218,    0,   13,  753,   17,  217, 6167,   96,  110,
          333, 1127,  383,    0,   13,  753,   17,   11,    6, 2084, 1625, 1563,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [8084, 3531,  170,   10, 1103,  297,  362,    6,   20,  199,   13,  858,
           12,   70, 6204, 3071,  506,  274,  100,    9,   13, 5591,   35, 1218,
          375,  140, 1011,  179,  206,   94,   66, 2214,   10, 3901, 2465, 5259,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,  220,   51,  143, 9649,  254,   10,   51,   56,   15,  121, 3576,
           62,    9, 6462,    0,   10,   51,    7,  473,   12,  155,   94,   10,
         2056,  155, 1234,    0,   10,   66,  211,  207,   10, 2423,   69,    7,
         6266,   12,    7,   39,  430,  119, 1215,  109, 9237,    7, 4893,   12,
            7,  708,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    7, 5873, 5318,   48,   17,  244,    7, 1830,  215,   53,  162,
         2839,   54, 1723,    6,    0, 1061,  584,    3, 4039,   14,   48,  909,
         2965, 1049,  111,    0,   86,  106, 3611,    0,   67,  106,    7, 4699,
           17, 3611,   26,  999,   55,   25,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 5385,   71,    7, 2184,   59, 1775,  221,   20,    0,   71,
           33,   38, 3794,    3,    7, 4503,  465,   11,   18,   66,    7, 8511,
           12,  129, 2286, 1331,   54, 2184,   59, 1775,  221,   20, 4821,    0,
           38, 3794,  197,    9,   33,  841,   26,   13,  909, 4674,    0,    8,
           13,  909, 4674, 1847,   39, 2987,    0,  166,   26,   13,  211,  500,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:2') tensor([73, 46, 45, 50, 38, 52, 44, 62], device='cuda:2') tensor([1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(85.6748, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[  21,   11,    6,  ...,    1,    1,    1],
        [ 778,    9,    7,  ...,    1,    1,    1],
        [4701,   80,   17,  ...,    1,    1,    1],
        ...,
        [  29,   70,   26,  ...,    1,    1,    1],
        [  53,  192,   11,  ...,    1,    1,    1],
        [  21,   26,  593,  ...,    1,    1,    1]], device='cuda:2') tensor([11, 10, 11, 12, 10, 15,  7,  8,  9,  9, 12, 10, 14, 14, 16, 12,  9, 15,
        21,  9, 12,  8,  6, 10, 11,  4,  7, 15, 14, 10, 10, 10, 11, 10, 12, 12,
        10, 18, 10, 10, 10, 11,  5, 14, 13, 11,  9,  6, 12, 10, 11, 10, 11, 12,
        10, 10, 11, 24, 12, 12, 10, 11, 10, 13, 15, 12, 15,  9, 10, 10,  8,  8,
        10, 10,  9,  9, 10,  8, 14, 11, 10, 10, 10, 13,  7, 12, 10, 13, 13, 10,
        16, 15, 13,  9, 10,  9], device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8301, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8901, 1.0000, 0.8003, 1.0000, 1.0000, 0.8989, 1.0000, 1.0000, 0.8726,
        0.8867, 0.8872, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8013, 1.0000,
        1.0000, 0.8872, 1.0000, 0.8774, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8062,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8101, 1.0000, 1.0000, 1.0000,
        0.8711, 1.0000, 0.8911, 0.8647, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8564, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 0.8848, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8501, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(15.3440, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[  77,  159, 6084,  ...,    1,    1,    1],
        [   8,   21,   11,  ...,    1,    1,    1],
        [  13,  122,    0,  ...,    1,    1,    1],
        ...,
        [  56, 6906,   26,  ...,    1,    1,    1],
        [ 284, 1187,   26,  ...,    1,    1,    1],
        [ 115,   33,   26,  ...,    1,    1,    1]], device='cuda:2') tensor([10, 11,  8,  9,  7, 11,  9, 18, 11, 13, 13,  8, 13,  7, 11, 14,  7, 10,
        10,  7,  7, 15,  6,  9,  9, 10,  7,  9, 11, 11,  9,  7, 11, 11,  9, 11,
        11,  8,  6, 14, 10,  8,  9,  8,  8,  8, 10, 10,  9,  9,  9, 13, 11,  9,
        12,  8, 12,  8, 18,  9, 10,  8,  9, 13,  7, 11, 11,  8, 11,  9, 11,  8,
        14, 13,  8,  9, 10, 12,  9,  8,  9, 10, 11, 10, 14, 10, 12, 13, 11, 10,
        10, 16,  9, 11,  6,  9,  8,  8, 12,  9,  9,  9, 11, 10],
       device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8647, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8340,
        0.8486, 1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8726, 1.0000, 0.8213, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8506, 1.0000, 1.0000, 1.0000, 0.8096,
        1.0000, 1.0000, 0.8198, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8970, 1.0000, 0.8149, 1.0000, 1.0000, 1.0000, 0.8765, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8579, 0.8594, 1.0000,
        0.8560, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8042, 1.0000, 1.0000, 1.0000, 1.0000, 0.8022, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8076, 0.8984, 1.0000, 1.0000,
        0.8604, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(12.9093, device='cuda:2', grad_fn=<MulBackward0>)
False False tensor([[ 115,    0,   77,  244,    7,  179,    0,   84,   26,   91, 1455,   17,
           94,  148,   66, 8428,    6,  735,  450,  110,    0,   53,  598, 3754,
            0,    8,   53, 1149,  164,  450,  110, 1352,   12, 4495, 4513,   96,
           46,   12,   13, 5370,  148,   14,   48,    0,    8,   13, 1751,   17,
          513,  593, 2392,    0,    8,  999, 1569,   85,    7, 3280,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8, 1004,   13,  733, 3609,   12, 1726,    6,   46,  106, 3236,    0,
         4623, 2050,   93,    0, 1745,  948,    6,    0, 4690,    0, 2384,   12,
          749,  340, 1049,    0,    8, 1507,  100, 2744,  153,   35,    6, 4201,
           54,    0,    0,   29,   69,    8,   29,    0,    0, 4404,   46,   25,
          446,    0,   94,  148,  213,   10,   87,  214,  125,   53,  570,    0,
           21,    0,   67,   53,  213,   10,   87,  117,  214,   10,  133,  747,
         3057,    6,    0,    2,    1,    1,    1,    1,    1,    1],
        [  29,    0,   55,  185,   12,   25,    0,    7, 5054, 6985,  439,   26,
           39,  479,   55,   25,   10,  388,  199, 2332,    0,    8,   19, 1080,
           17,    0,   55,   77,   12,   25,    0,    0,   25,  164,  150,   21,
           79,   39,  479,    0, 2539, 2551,   54,    9, 1296,    0,   10,  601,
         4491,   51,    0,  143, 4089,    0,   10,    0,    0,  601,  596,  719,
         3557,    6,    0,   17,   29,  293,    0,    0,    8,   10,  601,    0,
         1387,    7, 5086,    0, 4719,   85,    7, 1219,    0,    2],
        [  71,    7, 4057, 2653,   69,    0,  143, 3537,  340, 5188,    0, 6214,
         1508,  860,   79,    7,    0, 5019,    6,    8, 3943, 5834,    6,    0,
            0,    0,   21,   26,    0,    0, 1829,   10, 3995,   17,  218, 6214,
          282, 1833,  108,  601,    0,  593,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  207,   19, 7777,   21,  164, 1085,   26,  802, 6102, 8452,    6,
            8, 2583, 8452,    6,    0,   29,   24,  205,  106, 1107,  957,   10,
         1107,  895,  109, 1107,  828,    0,    8,   91,   17,   26,   86,  267,
          235,  457,  125,  148,  169,  213,   10, 2423,   69,   10,  159,  708,
            7, 3920,  290,  266, 8863,  445,   56, 8368,   96,   17,   53,  278,
         1450,  215, 3928,  106,  159, 1848,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  368,    7, 1455,   38, 3232, 1547,  140,   18,  197,  109,
           38, 4596,   37,    3,   70,   24, 3465,  641,   26,   13, 4405,    0,
         1183,  148, 1559, 4744,    0,    8,   24, 2527,   10, 5899,   17,   21,
           11,    6,  251, 4405,    6,  148,   63,  142,   10,   51,    7, 1649,
           10,  601,  170, 2026,    7,  172,  488,    0,  567,  266,  694, 4131,
           17,   24,  970,  100, 2127,  468,    0, 3135, 2041,    8,  837, 8226,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9, 7034,    0,   19,   11,  121,  367,   10,  703,   17,    7, 7470,
            6,   55,    7,  558, 3109,  469, 1921,   63,   77,  336,  170,    0,
          116, 4111,   55, 2805, 1020,   94,   71,    7, 5168, 2410,    0, 2541,
          607,    6,  609, 1076,  160, 1020, 4159,    0,    8, 7488, 4469,   10,
         3065,  687,  134,   10,  229,  159, 4989,   13, 1874,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([60, 76, 82, 44, 68, 74, 59], device='cuda:0') tensor([1.0000, 0.8975, 0.8291, 0.8066, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(134.9926, device='cuda:0', grad_fn=<MulBackward0>)
False tensor([[  19,   34,  851,  ...,    1,    1,    1],
        [  29,  103,   25,  ...,    1,    1,    1],
        [1065,    0,   19,  ...,    1,    1,    1],
        ...,
        [   8,    9,    7,  ...,    1,    1,    1],
        [  53,  132,    6,  ...,   18,    0,    2],
        [   0,   19,   11,  ...,    1,    1,    1]], device='cuda:0') tensor([22, 22, 22, 27, 21, 17, 24, 32, 13, 24, 25, 22, 24, 23, 26, 18, 30, 25,
        19, 22, 17, 21, 28, 24, 21, 26, 11, 25, 20, 22, 33, 22],
       device='cuda:0') tensor([1.0000, 1.0000, 0.8608, 1.0000, 1.0000, 1.0000, 1.0000, 0.8662, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8022, 1.0000, 0.8618, 1.0000, 0.8726,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8516], device='cuda:0',
       dtype=torch.float16)
 > at.  tensor(35.5439, device='cuda:0', grad_fn=<MulBackward0>)
False tensor([[   8,   25,   73,  150,  168,    0,  101,   11,    6, 2439,   10,  175,
          143,  941,  199,    7, 1416,  128, 1436,  242,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101,   11,    6,   80,  717,  109,  785,  825,   79,  743,   71,   33,
          946,  277, 3899, 1618, 1775,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 800,  248,    0,   13, 2585, 1533,   54, 1682,  818, 4028,  430, 4618,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1988,  562, 1425,   21,    0,  101,  619, 6031,  706,   10,  229,   77,
            7, 1122,  878,    6,  598, 1968,    8,   13,  724,  221,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 238,    0,   79,   71,  995,    9, 3804,    0, 1155,  172, 1073,  841,
         3706,    9,    7,  688,   12, 2569,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   33,  993,   26, 1944,  133, 2117,    0,    8,  131,    7,  207,
            0,   17,  279,   73, 3499,   80, 8255, 6052,   12, 4172,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2248,  630,    0,   38, 2637,    0,   24, 2373,    3,   19,
          154,   21,  164,   86,  367,  106,  832, 2287,    0,    0,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   46,   25,  135,    0,  440,   19,   11,   48,  100,
           10,  289,   21,   11,    6,   13, 3647, 1072, 6049, 1201,    0,   67,
            0,    9,  409,    0,   21,   11,    6, 3600,    0,    2],
        [ 948,  188,  291, 1090,    6,  793, 3042,  442,  108,  931, 5368,   37,
            8, 1978,    8,  958,  756,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24,  735,  164,  175,   70,   24, 2885,    0,   55,    0,    8,
          461,   12,   70,   24,    0, 2885,   55,   26,    9,  108,    0,    0,
         6501, 4401,    6,    0,    2,    1,    1,    1,    1,    1],
        [  21,   11,    6,   13,  546,   17,  931, 2027,    9,   89, 1259,    0,
            8,   19,   11,   45, 5592,   10, 1452,   33,   71,   25,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   29,    0,  168,   25,    0,   77,   63,    0, 1267,  381,    6,
         1331,  111,    0,    0, 7077,   48,   94,    0,  281,   12,   25,  274,
         2465,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,  203,   35,  160, 7261,   62,   77,   12,    7, 7755,    6,
            0,  388,  134,  132,   69,   13, 6381,    0,    8,   56, 3728,  134,
          132,   84, 2584,    7, 6316,   37, 5714,    6,    0,    2],
        [ 108, 3555,   66, 2386,   12, 1655,   12, 6759, 2179,   96, 2595,  140,
          300,  829, 2386,   12, 8971,    6,  545,    0,   69, 3752,  183, 6753,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [1559,  132,    0, 1570,    6,   10,    7, 4209,    0,   21,   11,    6,
         1336,  366, 1069,    0,    8,   21, 4433,  346,    8,   21,   11,    6,
            9, 4514,  838,   18,   57,    6,    6,    0,    2,    1],
        [  79,   24,  289,    9, 3060,   22,  494,  556,    8,   10, 1373,  606,
            0,  148,   26,  148,    8,   70,   26,   70,    0, 1189,    0,   24,
          213,   10,  780,   10,  203, 4434,  140, 5366,    0,    2],
        [   8,   79,   53, 5604,   62,  111, 7928,   21,  346,    8,  498,  554,
            0, 1609,   26, 1592,   79,   13,  746,   12,  131,   35, 7614,   12,
          722,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24, 1873,  230,  755,   17,    7, 1192,  144,   13,  488,  637,
         4910, 5542,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  120, 9300,   26, 7014,    9,    7, 3611, 3071,    0,   21,   26,
           56, 7617,  829,   25,   10,  150,  156, 1766,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 281,   12,    7,  183,    0,    0,    0,  103,   53,   66,   39,  811,
          235, 5492,    0,   53,  451,  305,   21,    0,   38, 1967,  586, 1563,
          111,    0,    0,   25,   11,   57, 3021,    0,    0,    2],
        [ 391,    0, 2168, 1065,    0,   19, 5692,    0,    0,   13, 3768, 1017,
           12, 5184,    6, 1211,   24,   11,   48, 1704,    7, 5423,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8, 2394,  116, 1223,  142,   51, 1118,   37,  156,    0,   55,   13,
         4431, 1531, 2247,    9,   48,  783,    0, 1223,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24,  246,    0,   38, 5034,    0,  596,  274,  100,   53,   11,
           57,  976,  324,   80, 2985,  321,   12,    7, 9057,  755,    0, 8864,
          111, 2985,   21,  755,    0,    2,    1,    1,    1,    1],
        [ 120,   19, 1505, 2091,    9,  117, 1532,   80,  392,  215,  581,    0,
         2245,  474,   53, 1425,   70,  341, 3257,  162,  442,   12,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([22, 19, 14, 24, 20, 24, 25, 34, 19, 29, 24, 27, 34, 27, 33, 34, 27, 16,
        22, 34, 24, 22, 30, 24], device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8462, 1.0000, 1.0000,
        0.8428, 1.0000, 0.8008, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8452, 0.8281, 1.0000, 1.0000, 1.0000], device='cuda:5',
       dtype=torch.float16)
 > at.  tensor(41.0648, device='cuda:5', grad_fn=<MulBackward0>)
False tensor([[   7,  409,  240,  281, 5555,   55,   17, 2182,   26, 5387,   58,    6,
          429,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [1269,  726,   59,  322,   26,    0,    8,  735,  164,   51,    0,   39,
         4126, 7074, 2792,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [2769,  649,  187, 3603,   37,    0,   19,  116,   66,   10,  175,  359,
            7, 6291, 1113,  206,   19,  192,   11,   18,  175, 9001,   48,    0,
            2,    1,    1],
        [   8, 1120,    0,   19,  169, 1120,  583,   21,  378,  946,   35,  525,
           48,   62,    0,   67,    9,    7,  772,  841,   12,    7, 1455,    0,
            2,    1,    1],
        [  67,  593,  294, 3002, 3560, 7253,   62,  566,  195,  195, 7309,    6,
          106,   97,  927, 2329,   54,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21,  451,   66,  211, 6850,    6,   55,  822, 6183,    0, 7031,   12,
         5388,  445,  109, 4456,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21,   11,    6,    7,  264,  741,   11,    6, 4271,    0,  103,   25,
          213,    0,   12, 8683, 4141,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 117,   63,    0,    0, 2465,    0, 1816,   46,  889,    8,  596,   46,
         4623,  760,  387,    6,    0,  100,   19,  246,    0, 5553,    0,    2,
            1,    1,    1],
        [ 169,   25,  100, 2744,   45, 2464,  930,   35,  233,    0,  148,   11,
            6, 3561,   80,  248,  759,   94,    0,  169,   25,  100,  565, 2107,
            0,    2,    1],
        [3943,    6,   63,    0,   25,  135,    0,    7,  637,   10,   80,   13,
         6897,   12,   77, 6214, 1369,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21, 1073,  110, 2749,    0, 6667,   10,  618,   85,   33,    0,  765,
            9, 1261,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  84,   11,    6,  113,   13, 6383, 1440, 1344,    6,  148, 1206,    6,
           13, 4457, 6389, 1232,    8, 4457, 6383,    0,    2,    1,    1,    1,
            1,    1,    1],
        [  67,   25,  150, 8747,    6, 6123,    0, 7168,  221,    0,    0,   86,
           13,  488, 1878, 2219,   69,    0,  148,  155, 4563,    6,   63,    0,
            2,    1,    1],
        [  85,    7,  467,   12,    7,  276,   54,    0,   19, 5853,    7,  473,
         8313, 2365,  270,   10,    7,  218,  926,   12, 4517,    0,    2,    1,
            1,    1,    1],
        [1771,   12, 2808,  106,  987,  607,  445, 4764,    6,    8,    7, 3727,
           15, 4345,    0, 4834,   26,  287, 1825,   96,    8,   29,   69,    0,
            2,    1,    1],
        [   8,  432,   13,  555, 1345,  513,  131,    0,   19, 1873,   17,  101,
           34,  341,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  55,   91,   12,   89, 2262,    0,   19, 1260,   85, 9548,  660,    9,
         1185,  828, 4621,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [1645, 1440,   26, 6501,  170,    0, 6501,  108, 3537,  429,    0,  109,
          108, 5384,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [7520,   97,   45,  236, 1991, 2734, 3967,  266, 2834, 1144,   63, 3756,
           10, 1799,   71,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  67,  120,   19, 1095,   33,    0,   19,   34,  172, 8288,    0,  125,
           21, 4639,  282, 3794,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21, 1847,   33,   56, 7101,  248, 1113,   10, 1137,   79,    0,  261,
            0,   79,    0,    7, 1417,    0,   56, 7101, 1137,    6,    9,   91,
          383,    0,    2],
        [  68,  194,   65,  168,  367,    7,  488,  211,   35,  679,    6,   12,
         5714,  694,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21, 1143,   79, 1019,  270,  199,    7, 1406,   79, 1061, 1101,    0,
          125,   25,  492,  135,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 413,    8,   24, 7774,   77,  108, 1298,  413,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [7714,    0,   19,   34,  172, 4876,   62,   10,  446,  126,   17, 7002,
          188,   33,  524, 1639,  126,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19, 3008,  134,   79, 1289,    0,    8,    0, 1065,   69,    0,  203,
         1015,   20,  121,  134,   79, 2348,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  246,   24,   73, 1706,   15,    7, 8311,    0,   24,   73, 2551,
           21,  126,   10, 3258,    7,  415, 1231,    0,    2,    1,    1,    1,
            1,    1,    1],
        [ 115,    0,   69,    7,  218,  926,   12,    7, 2705, 2374,  300,  290,
         4433,  682,  804, 1052,  300,  626,    0,   13, 2544,   93, 1390,   54,
         2822,    0,    2],
        [ 138,   87,   24,  204, 3560,    7, 4690, 5573,   12, 2833,  106,  700,
         4877, 1069,    9,    7,  245,  561,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [1149,    0,    0,   24,  144,   10, 3070,  214,    0,    0,  126,   79,
            0,   13, 2173,  125,   12, 1160,   45,   54,  109, 8929,    6,  109,
         1976,    0,    2],
        [   7, 2636,  570,  282,   46, 3891, 1181,  131,    7, 2636, 8373,  372,
          430,    0,    7, 2636, 9303, 5369,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7650,   96,    0,    7,   86,   96,    0,   79,   25,  135,
            0,   63,  116, 1051,  174, 1991,    6,    0,    2,    1,    1,    1,
            1,    1,    1]], device='cuda:2') tensor([15, 17, 25, 25, 19, 19, 19, 24, 26, 19, 16, 21, 25, 23, 25, 16, 17, 16,
        17, 18, 27, 16, 18,  9, 19, 20, 21, 27, 20, 27, 20, 21],
       device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8623, 1.0000,
        1.0000, 0.8677, 1.0000, 0.8491, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8203, 1.0000, 1.0000, 1.0000, 1.0000, 0.8862, 1.0000,
        1.0000, 1.0000, 0.8228, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(33.0279, device='cuda:2', grad_fn=<MulBackward0>)
False tensor([[6984,    6,   66,  968,  170,   55,   13,  535,  183,   17,    7, 5933,
           12,   17, 2365, 1321, 5461,   67,  172,   21,   11,    6, 4402, 6465,
          672,   71, 2481,    6, 5110,  266, 5554,   56,  878, 4635,  336,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 120,   25,   11,   57,  401, 3144, 7082,    0,   25,  192,   11,   18,
          305,   13, 3280,  436,    9, 3144, 7082,    0,   25,  274,   55, 1482,
          148,  135,    6,    7,   56,  249, 3056,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  21,   34,  384,  827,  119,  829,   10,  150,    7,  946, 1052,  111,
         4796, 5445,   71,   13,  883,  233,  336,   21,    0,  206,    7, 3373,
           12,    7, 5244,    6,  144,  859,   86,   96,   10,  134,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 225, 8227,   48,    0,  225, 6295,   62,    0,  225, 4415,  132,  120,
         3326,  225, 1831,  225,   34,  378,  384, 1727,  121,   48,   12, 2214,
            0,   12, 1070,    0,   17,  746,   12,  279,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 185,  215,  581,    0,   19,   34, 3006,   79,   13, 1560,  609, 3685,
          866,    9,    0,  185,  133, 5167,  456,    6,    0,  629,    0,    7,
         3348,   12, 5071,    0,    8,    7, 3348,   12, 1922,  311,   22, 4128,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 108, 4597,  373,   66,  619,  159, 1436,  221,    6,   55, 1037,  811,
         2461,    6,   96,    0, 4504,   22,  609,   96,    0, 2115,    8,   55,
         2516,   54,  270,  199, 2288,  159, 6183,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 963,    0, 1665,   35, 6905,   48,   71,  284, 2185,   20,   20,  121,
            6, 3195,   62,  132,    0,  664, 2120,   12, 3763,    6,  100, 3577,
         3195,    6,   12,  415, 2374,    0,  101, 1260,  100, 3674, 2383,   20,
            7, 7615,  240,   81,  367,   10,  282,    0,    2,    1,    1,    1,
            1],
        [  84,   11,    6,  486, 8752,   12,    7,  452, 1539,  242,  158,  502,
           46,   17,   11,    6,  143,  254,  854,    7, 4168,    9,    7,  572,
           46,  554,    0, 1429,  133, 2414,  111,   10,  422, 6627,  774,  271,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8, 6293, 6650,    9, 2008,   20,  484,   18, 5036,    8, 1777,    9,
         1760,   26,   70, 1927,    6,    7, 3151,    6,   12,  452, 1539,  128,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  55,  663,    0,    7, 6769, 1362,   20, 2553,  128, 1051, 2553,   93,
           12,    7, 3003,  608,  160,  589, 1333,  424,    6, 1985,   51, 4600,
          929,   13,  656, 3837, 2482, 1395,    7, 1361,   11,    6,  837,  283,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,    9,   33,  207,    0,    0,    0, 1976,   24, 2770,    0,   24,
          162,  204,  529,   10,    0, 6754,   21, 1004,   77,  798,    3,    0,
         3061,  106,  383,   91,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   9, 1015,   22,    6, 3358, 2426, 1422,    6,  100,   33, 3380,  292,
         1150,   45, 1568, 2685,   46,   25,   73,   11,   18,  368,  846,  181,
            0,   21,  659, 4489,   39,  191,  760,  227, 4811,    6,    0,   67,
           25,   73,  368,  688,   46,   25,  150, 8263,   12,  688,   84,    0,
            2],
        [  67, 3097,    0,  211,  988,  138,  324,   25,   63,   85, 4101, 1395,
          132,    7, 5694,    0, 1797,    6,    0,    8,   77,    0,   17,    0,
          993,    0,   25,    0, 1988,   70,    0,    7, 2372,  290, 1434, 2120,
            0,    0,  583,   13,  862, 1026, 1832,    0,    2,    1,    1,    1,
            1],
        [   8,  120,   25,  175, 1387,    0,    7, 7035,   25,   11,   57, 2202,
            0,    0,    9,  384,   18, 4117,    6,    8, 1244,   35, 1966,  366,
            6,   25,    0,  230,   10,  155, 2657,  426,  265,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  55,  663,    0,  110,  484, 1212,   34,  717,  759,   12,    7, 1723,
            6,  270,   79, 2523,   79, 5611,    8,  115,   26,  850, 1016,    3,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   84,   63, 2869,  218, 3885,    9,  166, 1749,    0, 6012,  292,
          959,    6, 3382,  203,  779,  675,  793,  188,  442,   13, 8166, 1878,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:4') tensor([37, 33, 36, 34, 38, 33, 45, 38, 27, 38, 30, 49, 45, 35, 26, 26],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8916, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8486, 1.0000, 0.8218, 0.8955, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(63.6269, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[ 486,  723,    0,  ...,    1,    1,    1],
        [1732,    0,  567,  ...,    1,    1,    1],
        [ 488, 1075,    0,  ...,    1,    1,    1],
        ...,
        [  17,   34,  159,  ...,    1,    1,    1],
        [  29,   84,   11,  ...,    1,    1,    1],
        [   8,  333, 2291,  ...,    1,    1,    1]], device='cuda:4') tensor([16, 12, 18,  9,  9, 13, 11, 11,  9, 13, 14, 19, 16, 18, 12, 11, 22, 19,
        15, 16, 18, 21, 11, 14,  9, 15, 19, 17, 16,  7,  9, 16, 11, 14, 14, 20,
         8, 16, 11, 13, 13, 13, 18,  9, 17, 12,  8, 17, 18, 11, 15, 20, 11, 15,
        19, 13], device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8545, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8916, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8271,
        1.0000, 0.8423, 1.0000, 0.8335, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8950, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8135, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8960, 1.0000, 0.8335, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000], device='cuda:4', dtype=torch.float16)
 > at.  tensor(19.8179, device='cuda:4', grad_fn=<MulBackward0>)
False tensor([[1008,   70,   19,  ...,    1,    1,    1],
        [ 115,   19,   11,  ...,    1,    1,    1],
        [ 476,   25,  133,  ...,    1,    1,    1],
        ...,
        [  24,   11,   57,  ...,    1,    1,    1],
        [   0, 1573,  289,  ...,    1,    1,    1],
        [   8,   29,   19,  ...,    1,    1,    1]], device='cuda:4') tensor([ 9, 16, 11, 12, 13, 13,  7, 16, 13, 12, 10, 10, 27, 13, 14, 11,  9, 12,
        20, 11, 18, 13, 10, 12, 11, 11,  8, 11,  8,  7, 11, 11,  8, 10,  7, 14,
        10, 15,  7, 10, 13,  7, 15, 10, 15, 11,  9, 12,  8, 12, 14, 19, 12, 20,
        13, 11, 11, 52, 10, 13,  8, 11, 13, 12, 14, 12, 11, 15, 19, 14,  9, 11,
         8, 10, 11, 10, 11, 16, 19,  9], device='cuda:4') tensor([[   8,  180,   19,  278,  172, 8930,    0,  125,   19,  474,   21,   34,
         6756, 1829,   85,   17, 1137,   10,  116,  508,   94,    7, 7998,    6,
           12,    7, 5086,    9,  166,   53,  162,   13,  649,  551,   45,   54,
            0,    2,    1,    1,    1,    1,    1,    1],
        [  68, 4077,   65, 3173,    0,   29,  108, 1678,   26,   80,  116,  160,
         2582,   20,  571,    0,  125,  101,   11,    6,  226, 3049,   54,    0,
            8,    7, 2987,   26,   10, 3560,  565,  106,  893, 5853,  131,    7,
         1067,  265,   48,   46,    2,    1,    1,    1],
        [  17,   11,    6,    7,  800,   55,  170,   10, 4530,    0,   38,  511,
           24,  116, 6083, 1319, 2921,    6, 8338,  392,  322,    0,    8,  108,
         3966, 6407,   93,   34, 1056,    0, 1047,  852,  820, 1228,    6,  106,
         1185,  828,  341, 1075,    0,    2,    1,    1],
        [   8,   85,    7,  370,  183,    0,   19,  474,   80,   33, 3024, 1361,
           12,   94,   19, 1425,    0, 5538,    6,    0, 7621,    6,    0, 5577,
            6,    0, 4853, 1613,    0,   79,    6,  365,  480, 4479,    6,    0,
           25, 1187,   21,    0,    2,    1,    1,    1],
        [ 103,   24,  162,   10, 5831,    8,  203,  607,   57,  140,   18,   77,
          264, 2394,  336, 2555,  235,    0,   24,  220,  204, 6785,   17, 1682,
         2244,    0,   24,  474,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  67,    7,   91, 4272,   17,  772, 4490,   96,  347, 4272,    6,  603,
           18,  945,   26,  143, 6790,  440,  254,  700,   26,   33,   91,    0,
            7, 2973,   45,  300,  335, 4272,    0,  230,    0,   21, 3834,   69,
           13, 1715, 1941,  383,    0,    2,    1,    1],
        [ 115,   84,   34,   13,  133,  916, 5100,  131, 1747, 2142, 3684,   59,
         4037,   17,  278,   13,  325,   12, 2303,   80,   13, 3087,  581,    0,
            8,   33,   26, 1223,    7, 6484,   12, 3142,   69,    7, 2688,   10,
           77,  117, 4531,  341, 4813,    6,    0,    2],
        [  24,   11,  121,  278, 2869,  341, 1369,   12, 6534, 1028, 1585,    0,
            8,   19,   11,   45, 7443,   17, 2392,   24,   11,  158,   51,  529,
           10,  175,  185, 2872,   54,  540,   29,   24,   73,  175,  270,    8,
          498,  665,   85, 5894,    0,    2,    1,    1]], device='cuda:3') tensor([38, 41, 42, 41, 30, 42, 44, 42], device='cuda:3') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:3', dtype=torch.float16)
 > at.  tensor(70.7355, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[   8,   21,   11,    6,    0,   13,    0,    0,  976,  488, 1009,    0,
          138,    0,  488,    0,    0,  238,    0,   19,    0,   73,  508,   25,
            7, 2348,   46, 1802,    3, 3275, 5494,   46,   67,   17,  689,   11,
           18,  172,  450,   25,  133,    0,  261,    0,    2],
        [  19,   11,   48,  492, 1110,   21,  619,   79,   13, 3102,   71,   33,
          851, 1243,   54,    0,    8,   19,   34,  838,  587, 4476,   18,   10,
          154,   17,   38, 3784,   11,   62,  197,  169,  175,  199,    7, 4868,
           69,   89, 2283,    0,    2,    1,    1,    1,    1],
        [ 120,    7, 1868, 4674, 4657, 1446,   10, 6406,  346,  461,   12,    7,
          942, 1740,    0,   24, 1644,  606,   18,  187,  922, 3993, 7018,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 248,  215,  581,    0,   19,   34, 6036,   79,   39, 2420,   10, 6757,
            9,   39, 7996,  415,   45,  458,   45,  240,  829,  736,  215,   12,
         3967,  266,  998,    9, 2627,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,  188,    7,   21,  234,  187, 2422, 1584,   46,   80, 1268,
            0,  868, 1101, 1369,   12, 3529,    0,   29, 1019, 2134,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  17, 3098,    0,   13, 2239, 6050,  369,   34,  850, 1408,    9,  108,
         1726,    6,    0,    9,  108, 8633,  567,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,  818,  250, 7678,   54,   13,  488, 2294, 3071,    0, 1149,  923,
           13, 3462, 2294, 3071,    0,   67,   21,   11,    6,  461,   12,   70,
           24,   87,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  103,   25,   11,   57,   13, 8159,    0,   51,    7, 8159,  148,
           26,  735,  142,   69,   80,    0, 3039,    8,    7, 2645,  403, 3034,
          271,   10,   51,   13,    0,   82,  338,  240,    0,   55,    7,    0,
         1714,    0,    2,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6,   13,  279,    0, 2991,   21,    0,   67,    7, 1590,
           12,  170,  135,   17,    7, 1465,   73,   51,   13,  172, 8121,  561,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   24,  229,    7, 4389,    6,   12,  251, 3380,  128,    6, 1456,
          230,    0,  103,   24, 1206,  159, 1862,    0,   24,   73, 1206,  876,
          649,   59, 1625,  369,    0,    8,  115,   24,   66,   13,  453, 7123,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  29,  339,   11,    6,  289,   17,   24,  283,  835,    0,    8,   24,
          204,  192,   11,   18,  116, 5183,    7,  862, 2704,    6,    0,   67,
           24, 1396,  835,   69,    7,  862, 2704,    6,    8,  172, 3814, 6927,
            8, 3097, 3814, 5748, 1955,    0,    2,    1,    1],
        [  21, 1518,  126,   25,   73, 2365,    7,  572,   12,  860,   39, 2216,
           46,   33,   26,   13,  822, 1682,   12,  860, 3396,   46, 2365, 7573,
         5950,   20,  111,   55,   80,  640, 1551,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   63, 1230,  341, 3058,    0, 1498,  341, 1167, 4345,    0,   67,
            7,  475,  630,   26,    0,  138,  238,   87,  117,  283,    9, 2894,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   71,  117,  370,  572, 6710,    0,   24,   11,  121, 2107,
          106, 1609,   10, 3548,   10, 2318, 6355, 1469,   71,  117, 6201,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33, 1599,   26,  434, 5827, 4488, 9414,    0,   68,  194,   65,   21,
           11,    6,   39, 1248, 5764,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,  659,   51,   17,   84,   34,   13,  453,   82,   10,   51, 9003,
          629, 3022,    8, 3156,    0,    8,   21,   34, 5763,   69,    7,   13,
         1674,    6,   12, 3226, 1525, 7531, 1144,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([45, 41, 26, 31, 24, 21, 28, 39, 26, 38, 43, 33, 26, 26, 19, 33],
       device='cuda:3') tensor([0.8086, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8340, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(56.2259, device='cuda:3', grad_fn=<MulBackward0>)
False tensor([[ 670,   34,  453,    0,    8,  635,  142,   10,  670, 4327,  110,  175,
           10,   33, 1827, 2260,  561,    0,   19,   11,   45,   86, 1123,    0,
           68,  194,   65,  476,   25,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  169, 4512,  132,    7, 7373,    6,    8,   56, 3430, 1004, 8425,
           51,  727,    6,   79,  103,   19,  513,  270,    9,  183,    8, 1505,
           13, 1269,  554,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 108, 1661,   57,  969,    6,  369,    0,    0,    0,   12,    7,  207,
            7, 1479, 1429,   26,  172,   46,    0,  188,  116, 4624,   62, 2447,
          297,   20,  586, 3042,    9, 4495,  215,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   77,  103,    0,   29,   70,  103,   24,  162, 1074,
            9, 7755, 1318,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   24,   66,   13,  325,   12, 6627,    8, 1064,    0,   67,   21,
           11,    6,   13, 4112,    0,   29,  138,   63,   25, 3263,   10, 9202,
           10,  175,  359,   17, 3805, 8487,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  837, 2245,   66, 2138,   13,  325,   12,  183,  665,   85,    7,
         4608,   12,  108,  955, 1234,    0,  109,  218,   94,   11,    6,  955,
         1234,    0,   69, 7955,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 109,   66,   53, 6510,   62,    0,  159,  447,  746,   12, 7372, 2243,
         1150,   17,   26,    0, 5709,   12,  639,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 846, 1026,  156,   26,   13,  133, 3818, 8043,   22,  148,  568,   39,
         4126, 1762,   12,  283,   71, 5200,   48,  128, 1144,    0, 1320, 7988,
           96,    8, 4269,    6,   55,  384,  232,  793,    0,    2,    1,    1,
            1,    1,    1,    1,    1],
        [   9,   77,    0,    7,  664,  726,  188, 8095,   62,    0,  143,  254,
         1998,  852,   94,    0,    0,    9, 7176,   20,    0,    0,  850,  699,
          352,  521,  249,  249,   59,  240, 1144,  574,   54,    6,    0,    2,
            1,    1,    1,    1,    1],
        [  29,    7,  546,    0,    0,   17,    0,    7, 1827,  968,   80,    7,
         6900, 1532,   34,   17,   84,  506,   51,   13, 1706, 4562,   10, 2322,
           54,    9,  570,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,   11,    6,   33, 4673,  639,  434,   13,    7,   59,  626,    6,
         3668,    0,   25, 9588,   21,    0, 2546,  436,   21,   10,    7,  859,
            0,    8,  155,  941, 6650,    9,    7,  637,  164, 8463,    0,    2,
            1,    1,    1,    1,    1],
        [1241, 6671,    0,   84,   26,   13, 6815,  634,   18,   12,  958,   46,
            8,   33,   26,   70,   11,    6,  434, 3057, 1181, 7347,    0, 1223,
          155, 2333,    6,   12, 4836,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 3570,  650, 1580,   35, 1533,  156,   46,   68,  200,  803,   59,
         1069, 4579,   65,   46,  166,   25,   73,  914,  116, 1598,   20,  111,
         1510,  168,    0,  368,    6, 5992, 1911,  668,  532,    6,   46,  133,
            0,  133, 5606,    0,    2],
        [1979,  122,    0,   21,   34,  432,    7, 2639,  181, 1178,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7,  245,   26,   17,   77,  117, 2671,   63,  116,   13, 3954,   35,
         2502,   12, 2248, 5561, 2519, 1221,   17, 1308,  213,    6,   10,  150,
         1085,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 164,   25,  601,   51,  159, 3806,    0,   38,  511,   19,  246,   10,
         1222,   19,  169,  583,   33, 1361,   12,  889,  197,    7, 8195,   12,
         3806,    6,    0,   38,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:0') tensor([31, 29, 33, 18, 32, 31, 21, 34, 36, 29, 36, 31, 41, 12, 27, 29],
       device='cuda:0') tensor([1.0000, 1.0000, 0.8594, 1.0000, 1.0000, 1.0000, 0.8677, 1.0000, 0.8862,
        0.8623, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(52.7117, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:32:42 | INFO | train_inner | epoch 001:    105 / 1474 loss=20.068, trans_loss=5.876, nll_loss=4.685, w2v_ctc_loss=22.36, task_loss=1.781, contrastive_loss=3.265, total=4215.28, n_correct=125.37, ppl=25.72, accuracy=2.974, wps=18196.9, ups=1.45, wpb=12577.2, bsz=473, num_updates=100, lr=4.098e-06, gnorm=2.847, clip=0, loss_scale=4, train_wall=75, gb_free=19.4, wall=136
2023-08-17 09:33:48 | INFO | train_inner | epoch 001:    205 / 1474 loss=16.553, trans_loss=5.862, nll_loss=4.694, w2v_ctc_loss=17.018, task_loss=1.719, contrastive_loss=3.236, total=4114.86, n_correct=114.14, ppl=25.88, accuracy=2.774, wps=18631.5, ups=1.52, wpb=12286.8, bsz=458.8, num_updates=200, lr=8.096e-06, gnorm=7.243, clip=17, loss_scale=4, train_wall=65, gb_free=19.3, wall=202
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 2 terminated with signal SIGTERM
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 131 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-08-17 09:34:02 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10032
2023-08-17 09:34:02 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10032
2023-08-17 09:34:02 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10032
2023-08-17 09:34:02 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10032
2023-08-17 09:34:03 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10032
2023-08-17 09:34:03 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10032
2023-08-17 09:34:03 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10032
2023-08-17 09:34:03 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10032
2023-08-17 09:34:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-17 09:34:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-17 09:34:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-17 09:34:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-17 09:34:03 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:34:04 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-17 09:34:04 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-17 09:34:04 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-17 09:34:04 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-17 09:34:04 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:34:04 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:34:04 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-17 09:34:04 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 09:34:04 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-17 09:34:08 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10032', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-17 09:34:08 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-17 09:34:08 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-17 09:34:08 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-17 09:34:08 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-17 09:34:08 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 09:34:12 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-17 09:34:12 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-17 09:34:12 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-17 09:34:14 | INFO | root | load pretrained hubert
2023-08-17 09:34:17 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 09:34:18 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 09:34:21 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 09:34:21 | INFO | root | share the sematic adapter and textual encoder
2023-08-17 09:34:21 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-17 09:34:21 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-17 09:34:21 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-17 09:34:21 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-17 09:34:21 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-17 09:34:21 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-17 09:34:21 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 09:34:21 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:34:21 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:34:21 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:34:28 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-17 09:34:28 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-17 09:34:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-17 09:34:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 09:34:29 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:34:29 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:34:29 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:34:29 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:34:29 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:34:29 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:34:29 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:34:29 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 09:34:29 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 09:34:29 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-17 09:34:29 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-17 09:34:29 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 09:34:29 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 09:34:29 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-17 09:34:29 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 09:34:29 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:34:29 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 09:34:31 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:34:32 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 09:35:22 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-17 09:35:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 09:35:22 | INFO | fairseq.trainer | begin training epoch 1
2023-08-17 09:35:22 | INFO | fairseq_cli.train | Start iterating over samples
True tensor([[   7,  422,  874,    0,    0,    9, 6788,  369,   12, 3648,    0,   26,
            7,  874,    9, 2761,   12,   13,  572,    0,    7,  572,   26,    0,
            0,    9, 2761,    0,   12,   13,  874,    0,    8,  722,   26,    7,
         7465,  131,  166,  251,    0,  248,   63, 3249,   62,    9,    7,  772,
          207,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [1179,   25,  135,   21,  109,   86,    0, 1335, 2348,   12, 1613,   71,
          747, 2693,   11,    6,    8, 1335, 2348,   12,   13,    0,  265,    0,
         6466,    6,  148,  205,   10,    7,  281,  909,  119,  236, 1563, 8562,
            9,  108,  753, 1772,    9,  909,   35, 3080,  109,  909,   35, 9376,
           54,    8, 4429,    0,    8,   53,  467,  132, 2762,  159, 2767,    6,
            0,    2],
        [  67,   21,  465,   11,   18,  468,    7, 2932, 1880,  567,    0,    8,
          113,    7, 2932, 3612, 1381,  832,   18,  233, 1181,   33, 3612, 1181,
         3860,   59, 1710,   10, 5111,   15,   21,    6,  768,   10, 5574,    7,
         1749, 1381,    8,    7,  341,  409, 1955,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   56, 1285,    0,  347,  192,   11,   18,   25,  450,  117, 6010,
           70,   21,   11,    6,  100,  378, 7000,   71,   33, 4716,  434,  427,
          821,  407,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0, 2753,  427, 1105,  810, 3565,   17,   21,  659,   51,
         1953,    6,   12, 1655,   12,  215,  801,    0,  166,  818,   17,   33,
         1167,  659,   66,  245,   13, 2003,   15,  106,    7, 1247,   12,   13,
          258, 1591,   17, 2963, 1585, 1763,    7, 1644,  511,  762, 1054,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,   13,  563,   93, 2484, 9473,   13, 4866,   10, 2266,
          389,  552, 1252,   11,    6,   38, 5001,  411,  293,    3,   24,   73,
          205,   10,   91,   12,    7,  822,  608,   56, 9694, 1075,    0, 3109,
         3902,    0,    8,  150,  113,   13, 1523, 3609,   12, 2718, 7719,    0,
          106,  747,    9, 1665,   10, 1962,    9, 1912,    0,    2,    1,    1,
            1,    1],
        [  33, 2780,   34, 6291,  111, 2770,   55,    7, 2932, 1232,    0,   67,
          115,   21,   11,    6, 2823,   54,  100, 3139,  140, 2704,    6,    9,
            7,  832,    0,    6,    0,    8, 2627,    0,  206, 2406,    6,   63,
         4213,   48,   10, 3067, 3316, 1006,   85, 3646, 1729,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 108, 2476,   26,   10, 8711,   17,   53, 1516,   71,   13,  509,  841,
           12,  138,   10,  229,  214,  254,  120,   53, 6565,    0,    8,    7,
         2027, 6936,  475, 2041,   17,   25,   73, 1531,  214,  126,  131, 1974,
          416,   54,  336,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([51, 62, 45, 28, 50, 58, 47, 41], device='cuda:0') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(86.9355, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:35:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
True tensor([[  55,  663,    0,    9,  264,  109,  232,  221,    6,   24,   66,  682,
         3196,   22, 2932,  896,    0,  206,   53, 3860,   75,  241,  221,   77,
          236,  866,    8, 6387,    8,   29,  234,  452, 6029, 7401,    0,    2,
            1,    1,    1],
        [ 115,   24,   11,  121,  492, 1110,   13, 7123,  100,   33,   12,   13,
         1470, 3189,    0,   67, 1470, 3189,    6,   73,   51, 1346,    0,  276,
          103,   53,   11,   57,   86, 1110,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [ 168,   26,  138,  488,   12,   13,  775,  411,   37,   34,  859,    0,
           21,    0,   34, 1523,    0,    0,   10,    0,    0,  508,   25,   13,
          841,   12,    7,    0, 1823,    0,    0,    0,   84,   25,  205,    0,
            2,    1,    1],
        [ 115,    7, 1885, 5055,    0,   12,  538,    0,   26,  206,   13,  325,
           12,    7, 2394, 4793,    8, 2791,  513,  199,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,   21,  818,   17,   51,   20, 7931,  373,   63,  133,  324,   85,
          203, 4030,  140,   54, 2429,   35, 2611,    6,    0,    8,   29,   53,
           11,  121,  226,  529,   10, 3258,  251, 4513,   96,    0,    2,    1,
            1,    1,    1],
        [  24,   66,    7, 2216,  768,   10,  205,    0,   38, 5276,   26,   70,
           19,  213,   10,   87,    3,    8,    9,   13,  613,  982, 1479,    0,
           17,    0,   10,  110,    0,   26,   13, 1968,  279,    0,    2,    1,
            1,    1,    1],
        [  25,   73, 6265,  281,   12,   17,   13, 1674,    6,    0,  125,  103,
           25,   11,   57, 4298,  336, 3594,    0,    8,    7,  595, 3336, 1233,
         2233, 4457,  111,    0,   25,   11,   57,  492,  142,   10, 1713,   17,
          595,    0,    2],
        [  29,    0,   85,   33,  613,    0,   19,   87,  213,   10,  450,   25,
           17,  185,   12,    7, 2767, 4131,   24,   66,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 101, 1119,    0,   38,   18, 2704,  518,  155,  227,  715,   96,    3,
          125,   33,   26, 1320,  111, 2059,    0,   77, 2808,   10,    7, 9510,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  131,   13,  316,  287,    0, 7492,  136,  760,  322,   93,
            8,  697, 1262, 2178,  713,    0,    8,   53,   11,   57, 2091,    9,
         2567, 1507,    0,  116, 5000,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   0,    9,  264, 1884,  166,    0,   63,  142,   10,   51, 1405,    0,
           33,  169,   86,   51, 3118, 1623,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  67,   21,  465,   11,   18,  229,    0,   13, 1878,   10,  565,    0,
          103,    0,  284,  708,  162,  142,   10,   51,    0,   13, 2988,  109,
            0,   13, 2088,    0,   53,  162,    0,  142,   10,  205,   10,  670,
            0,    2,    1],
        [  67,   24,  499,  213,   10,   51, 1123,   17,   33,   26,    7,  281,
         4089,    8, 2426, 3180,   17,   24,   73,   51, 2985,  199, 2406,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  768,   62,  131, 1962,   35,  699,  119, 3050,    6,  411,
          639,    0, 7176,   20,  111,  752,   10, 1914, 4907,   10,  875,  134,
          199,    7, 1232,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,    7, 5714,   17, 7076,    9,    7, 3227,   12,  423,  759,
         2270, 1098,   18,  500,    6,  148,  618,   69, 1102,  926,    6,   12,
            7, 8783,    8, 6907, 4895,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  213,   25,   10,  150,  138,   33, 7223,   26, 5802,   62,    0,
            8,   21,   11,    6,  142,   10, 2546,  436,   29,   25,  150,   17,
           21,   11,    6, 1117,  126,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([36, 32, 37, 22, 35, 35, 39, 22, 26, 31, 20, 38, 26, 29, 31, 31],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8418, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8721, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(48.4796, device='cuda:4', grad_fn=<MulBackward0>)
True True tensor([[   9,   13, 1880, 4745,    0,   25,  498,   71,  211, 2837,    8,  211,
         4774,   10, 1393,   17,  199, 2332,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,  131, 3696,    7, 4221,    0,   24,   73,  446,  126,  261,
          143,   80,  155, 1037,  254,   25,  914,  135,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   11,    6,    7, 6099,   12,  958,    9,    7, 6869, 8521, 2353,
           12, 5205,    0,    9,  421,  234,   48,  365,  221,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,  162,   86, 2841,    9, 1979,    0,  290,    0,   69,    7, 5225,
           38, 2862,  781,  918,   93,  197, 1017,    0, 2503,   18,  650,  568,
           86, 2115,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [3398, 7936,    8,  284, 1543, 1948, 1985,   87,   21, 2373,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   84,   11,    6,  288,   91,  227,   15,    6, 2158,    0,    0,
          207,   10,    0,  205,  106,   84,    0,    0,   25,  175,  717,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,   10,  101,  128,    0,    0,   10, 1008,    0,   10,  289,    0,
           38,  533, 2056,   13, 1738, 1234,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 206,   11,    6,    7, 2808,    0,   38,  533, 1412, 2215,  159, 2360,
           10,    7, 2182,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 473,  111,    0,   33,   26,   13, 2322,   35, 5444, 2595,  856,  293,
            0,  125,   19,   66,   29,  294, 1516,    6,    9,   89, 1995,  918,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [7494,  185, 1118,  307,   34,   39, 1830,   35, 1933,   35, 1178, 2988,
          120,  101,   34, 1920,   22, 1371,   62,  106, 3022, 1146,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   33,   26,  116,  133,  946,   46,  183,   26,   69,    7, 3089,
           35, 7988,  340,    8,    7, 2509,   26,   69,    7, 1995,   35, 7988,
          340,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,    0,  169,  154, 1827, 1794,    0,  162, 4491,    0,  166,  818,
           53,  451,  601,   25,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,    8,   10,   87,   17,    0,   25, 1275,    0,   66,    0,   10,
         4988,   13, 1472,   12, 2066,  416, 1374,  572,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 1220,   25,  659,   51, 1211,   12, 1835,    0,   38,  187,
           11,   45,   86,    9,   18, 2687,  241,   54,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,   11,    6,    7,  613,   12,  117, 3742, 3228,    6,   17,   25,
           11,  121,  591,    0,   38,  511,  101,  246,    3,  238,    0,   19,
           11,  121,  211,  479,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [  33,  188, 8166,   19,  362, 1838, 1428,   55,   70,    7, 1479,   26,
          142,   10,   87,    9,    7,  858,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,   11,    6, 5653,   10, 1719,   59, 5292, 3147,  111, 1830,  439,
           55, 2677,    8,  248,   10,  391,  439,   55, 5031,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,  180, 2568,  347,   19,  305,    0,    0,   29,  261, 7003,  106,
         4990,  589,  596,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7, 2403,   26,   17,   84,   11,    6,   13,  785,   35, 9058, 3900,
            0,   86,    7, 1953,  322,   35, 3149,   35,  290,   35, 8856, 3900,
          100,   10, 4482, 2147,    0,   67,   13,  785,   35, 9058, 3900,    0,
            2],
        [   8,    7, 1714,  608,  423,  439,    0,   53,  305,   80,  248,  439,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 244, 2869, 4021,    0, 2008, 6248, 1405,   91,   12,    7,  281, 3861,
          338, 3232,  128, 4621,   24,  135,   80,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,   11,   45,  142,   10,  575,   25,  138, 7218,  204, 2895,    6,
           71,  108, 5967,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 110,  153,  240, 1547,    6, 3067,   62, 6873,    0,    8, 1645, 9745,
            6,    0,    0,    0,  593,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,  103,   25,  192,   11,   18,   66, 6759,   35,    6, 2174,  297,
           62, 3867,    0,  180,    7,  630,   12, 1703, 6230,   26,   13,  341,
           91, 5020,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:6') tensor([20, 22, 23, 28, 12, 25, 20, 17, 26, 24, 27, 18, 22, 22, 30, 20, 23, 17,
        37, 14, 21, 18, 19, 28], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8721, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8916, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000, 0.8887,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000], device='cuda:6',
       dtype=torch.float16)
 > at.  tensor(34.7195, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[1250,    0,   53,  ...,    1,    1,    1],
        [ 832, 1337,   22,  ...,    1,    1,    1],
        [  67,   25,  113,  ...,    1,    1,    1],
        ...,
        [  24,  278,    0,  ...,    1,    1,    1],
        [  24,  321,   12,  ...,    1,    1,    1],
        [  19,   11,   45,  ...,    1,    1,    1]], device='cuda:6') tensor([11, 11,  9, 13, 12, 12, 12, 12, 16,  9, 10, 11, 10, 11, 11, 12, 13, 11,
        11, 10,  9, 13, 17, 15,  7,  9, 11, 12, 17,  6,  7,  9, 10,  8, 10,  8,
        10, 11, 14, 13, 11,  9,  8, 19, 12,  9,  9,  6,  8, 16,  8, 12, 11, 13,
        10, 12, 10, 13, 10, 15,  7, 12, 11, 12, 13,  9, 12,  8, 13, 20,  8,  8,
         8,  7,  9, 11, 12, 11, 11, 12], device='cuda:6') tensor([0.8091, 1.0000, 1.0000, 0.8789, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000,
        0.8555, 1.0000, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8218, 0.8652, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 0.8984, 1.0000,
        0.8286, 1.0000, 1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8838, 0.8677, 1.0000, 1.0000, 1.0000, 0.8369,
        0.8652, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  True tensor([[  67,    7,  143,   24, 1260,   85,   21,    0,    7,  143,   24, 1873,
           17, 1057,  453, 3540,   34,    7,  133, 1890,  279,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,    0,   24,  552,  132,   71,   13, 4445,    0,    8,    0,  204,
           70,  956,   34,   91,    0, 2767, 2923,   37,    9,  264, 1736,   46,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 211,    0,    0,   19,  641,    0,    0,   19,  154,   46,    0,  682,
            0,   67,   26,   17,  155, 4234,    0,  846,  174,    0,    0,  211,
            0,    0,  682,    0,   70,   34,   17,   91,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,   89, 9264,    0,   86,  860,   13,  488,   91,    0,   19,
          213,   10,  289,  116,   13, 1455,   80, 2263,  556, 5610,   54,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  142,   10, 4431,   35,  140, 4076,   69,  832, 1486,
          335,    0,    8,   24,   73,  150,  832, 1486,  335, 2546,  829,   69,
           21,    6,  926, 1585,   71,   21,    6, 4241,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [   9, 1296,   10, 2572,   25,   33, 1678,    0,   19,   11,   45,  142,
           10,  296,  185, 5825,    6,   10,  367,  132,   69,    6, 5072,  172,
         2117,    0,    8,   24,   11,   57,  142,   10,   87,   13,  277, 1860,
           35,  399,  102,  287,    0,    2],
        [  24,  465,   11,   18,   87,   33,  131,  204, 2905, 3141,    0,  100,
           25,   87,    9,   13, 1192,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17, 1073,   13,    0, 1878,  629, 1891,    7,  595,    0,    9, 1416,
           12,   25,    8,    0, 4958,   54,    7, 4292,    0,  109,  893,  199,
           39, 4292,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 115,   26,   33,  250,   17,   24,  451,   51,  101,   62, 1256,   12,
            0,   12,  538,    0,   21,   11,    6,  528,   17,   24,  135,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,   12,   33,   10,  289,   17,    7, 4719,  629,   70,   24,   73,
         7039,    8,   70,   24,   73, 2032,   26,  288,  142,   10, 4038,   22,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,  106,   38,  469,  291,   18,  187,   62, 1224,   12, 1491,
            3,    9,  564,  868, 1047,    0,   84,  162, 1107,  820, 5149,  690,
         7653,   20,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  60,  116,  188,   13,  264, 3742, 6211,   17,  368,    6, 1019,  908,
          768,  254,  108, 6211,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 5346,   12, 1009,   17, 1241,   13, 1127,  567,   26,    0,  554,
            0, 2816,  419, 1661,   57,  969,    6,  369,  109,  419, 1620,   54,
         5974,  440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 214,   63,  341,  125,  115,   19,  135,   17, 4751,   26,    7, 1037,
         2829,   17, 1459,  188,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 103,   25, 1260,   85, 2264,  241,  140, 1782, 5152, 1160,  158,   25,
          162, 2639,    0,   25, 2220, 1631,  830,  150,  347,   53, 5953,   48,
         6387,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,    0,  692,   10,   66,   13,  740,  672,   71,    0,    7,  230,
            0,  688,    0,   71,    7,  230, 4157,    0,   71,    7,  230, 1404,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([23, 25, 34, 25, 35, 42, 19, 28, 26, 26, 28, 19, 28, 18, 27, 26],
       device='cuda:0') tensor([1.0000, 0.8755, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8677],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(46.5689, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:35:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
True tensor([[  67,  108, 8830,   48,  589,    0, 7557,    6,    0,  166,   63,    7,
         7557,    6,   17,  601,  170,   10, 6108,    8, 2544,    8,   51,    0,
            0,   13,  562,   18,    0,    8, 9063,   62,    8,   29, 4404,    8,
           29,   69,    0,   53,    0,   63,  261,  143, 7678,   62,  131, 1912,
          688,    0,    2],
        [  24,  116,  144, 6985, 2421,    6, 4502,   80,  854,   39, 3350,  581,
           77, 1004,    7, 6869, 9271,    0, 3381,   70,   11,    6,  956,    0,
            8,   24,   11,  121, 2138,   13,  325,   12,  183,    9,    7, 1726,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 185, 2193, 6248,   22,    6,    9, 1777,  150,  156,   79,   93,  675,
           45,    9, 5222, 1290, 1373,    6,    6,  693,    0,   67,  294,   73,
           51, 5853,  131,    7, 2932, 3288,    0,    8,  203, 1203, 1015,  922,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 106, 1244,   35, 1966,  673,   54, 2409,   10, 2465, 3602,    6,   10,
           33, 2178, 2445,  266, 1192,   69,   89,  906,    0, 3927,   63, 4015,
         5869,   12, 2434,  108,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  154,   21,   26, 2497, 1226,   55, 1221,   10, 1772,    9,    7,
          245,  561,  929,   13, 2226, 1757,   55,   70,  169, 1085,   10,    7,
          461,  266,  763,  480,    6, 1042,    7, 3677,  188, 3699,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7, 4100, 1649, 1085,  143, 1149,    0,   67,  914,   69,    7, 1296,
           12,   13, 4400,   15,   22,  996,    0,  333,  555, 7691,  109,  333,
          555, 3619,  215,    0,   67,   21,   11,    6,  499,  250,   10,   51,
         4837,   12,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7745,  659,   51,    0,   67,   21,  689,   11,   18, 4432,
           10,  452, 1539,  128,    6,    0,   17,   24,   66, 5020, 6083, 5475,
            6,   46,    7, 1320,   59,  816,  300, 1079, 1643, 1794,  230,   85,
            7, 1219,  859,   35, 2514, 4209,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   9,  409,    0,    7,   29,   70,   26,    0,    9,    7,  572,    0,
         2317,    9, 7418, 3782,    0,    7,  214,   17,  229,  155,  572,  283,
            0,    7,  214,   17,  229,  155, 3236,  283,    9,  155,  572,    0,
           63, 8220,  111,  415,   59, 8186,   71, 2317,    9, 2011, 3043,    0,
            2,    1,    1]], device='cuda:7') tensor([51, 38, 38, 31, 36, 40, 44, 49], device='cuda:7') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(69.2369, device='cuda:7', grad_fn=<MulBackward0>)
True tensor([[ 131, 1183,  148,   26, 1948,  664,   62,  132,   71,    7, 4617,  292,
         2109, 1775, 4040,    6,   12,   33, 6536,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   19,   11,   48,  100,   10,  227, 2174,  232,  445,  159,  283,
           71,   13,  277, 7074, 4112,    0,    8,    7, 4112,   26,   33,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1425,   19,  144,   10, 1336,  235,    0,   19,  442,   17, 3486,
            8,   79, 2392,   79,   19,  442,   21,    0,   39, 2285, 1790,   12,
         2922,  552,  244,  110,    0,    2,    1,    1,    1,    1],
        [ 877,  290, 2689, 2345, 1212,  188, 2277,   48,   10, 2699,  143,    8,
          143, 5593,   10,   33,    0, 5413, 3301, 5690,    6,  199,  459,  510,
           96,   17,  719, 2070,    0,    2,    1,    1,    1,    1],
        [  29,   24, 6103,  154,   12, 4781,   79,  378, 3816,  214,   17,   24,
          991,   55,  133, 3926, 3965,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  12,  538,    0,  108, 4734,  192,   11,   18,  116,  468, 1004,  183,
            0,   53,  113,  468,  106,  561,   10,  561,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,   66, 4595, 1122,   59,  699,   35,    6,  819,  424, 1942,   96,
            8, 4515,    6,   17,   63,   86, 3412,   62,  131,  883,   48,  293,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103, 2263,   96,  498,    0,   10, 3424,    0,  180,    7,  723,
           73, 5415,    0,    0,    7,  524,    8, 3536,    7, 2517,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 427,  412,  430,  158,   13,  188,   13, 1905,  321,   12, 6873, 1117,
           17,    0,  120, 8491,   48,    0,    7,  427,  412,  430,  158, 3336,
           10, 1051, 1672,  153,  336,    0,  116, 7743,    0,    2],
        [  29,  417,  140, 2362,    6,   93,   26, 3423,   13, 1396, 2233,  199,
           39, 3993, 1874,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  188,  226,    7, 3841,    6,   20,   12,  333, 1670, 2127, 9746,
            6,    0, 3448, 7414,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  225, 4576,    6,   10,  110,    7, 1790,  225, 1831,   79,   13,
          785,   35, 1933,   35, 1178, 3642,    9,   17, 6460, 4057,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  116,  323,   13, 4454, 2761,   69, 5335,   80,    7, 5169,    8,
         1095,  248, 8099,    6,  558,   10,  545,  218,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 4639,   17,   24,  220,   51, 1155,  143,  254, 3857,    8, 1064,
            0,   67,  138,   10, 3918,    6,   20,  126,    7,   91,  106,    7,
          218,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2551,   12,  324,    8, 6790,  214,   26, 2720,   48,   10,
         7572,    8,  211,  234, 1410,  837, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  101,   11,    6,    0,   13, 4961, 1531,    0,    0,   79, 3708,
           18, 3304,    0,  169,   66,  246,    0,   79,  238,   79,   13, 1880,
         1531,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   10, 4637,  373,    0, 2192,   26,  116,  486, 6101,  567, 4111,
           10,   51, 4637,   62,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 8900,   62,  250,   19,  172,  144,   13, 2184,   22,  311,   69,
            0,   26,   17,    7, 1956,  204,  213,    6,   10,  283,   55,  159,
          110,  128,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,  593, 1706,   12,   13,  183, 2760,   55,   77,
            7,  341, 1565,  409, 1215,   17,   24,  135,   10,  468,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1777,  188, 1220, 6380,   48,   17, 2439,  558,  464,    0,   53,   11,
           57, 5332,   54,   13, 4369,  200, 1515, 2902,    8, 3585,  567,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   63,  423,   12,  117, 3357,    0,    8,   53,   63, 4673,    9,
          159, 4268,    8,  159, 4972,    0,    8,   53,   63, 2762,   94,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  976, 8372,   17,   24,   66,  535, 1313, 5260,    7,
          613,  206, 4387,    6, 3480,  108,   24, 1591, 2637,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  717,   35, 1315,  322,  801, 5725,    6,  221, 1339, 5061,  292,
            6,    6, 3222, 1026,   14,   48,   71,   33,    9,   21,    6, 9442,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  69,    7,    0,  939, 2836,  834,   26,    0,    7,   56,  111,    6,
         3620,  266,  207,   17,  368,    6, 2829, 7337, 7393,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([20, 25, 30, 30, 20, 22, 26, 24, 34, 17, 18, 24, 22, 27, 21, 27, 18, 28,
        24, 25, 25, 23, 26, 23], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8550, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8975], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(40.5196, device='cuda:7', grad_fn=<MulBackward0>)
True True tensor([[ 800,   91,    0,  103,  886,  424, 3848,   11,    6,  853,  200,  237,
           26,  593, 3401,  120, 3213,  187, 2044,    6, 1570,    6,  199,    7,
          964,    0,  568,   17,  641,   21,   11,    6,  735,  226,  593, 3401,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24, 8396, 1469,  106,    0,  108, 2914,   37,   93,    0,  131, 2532,
          336, 3494,    0,    0,   67, 1223,   24,   77,   66,   33, 1738,    0,
         2914,   37,   93,   17,   24,   63,  746,    0,   12,    0, 5472, 1117,
          108, 3183,    6,    8,    0, 1459,  994,   26,  126,   84,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2937,  982,  218,  943,    6,    9,  108, 6249,  451,   66,  774,   62,
         3928,    0,    8, 1800,  282,   13, 2333,   10,  175,  850, 1408, 1072,
            6,    0,  109, 3119,  294, 2518,   12,  215, 3928,  254,  956,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  125,   29,  294,   12,    7, 2787,    9, 1370, 2924,   11,   18,
          116,    9,  670,    0,   53,   11,   57,    9, 1037,    8, 1237,    0,
           70,   25,  113,  296,    0, 6957,    0,   26,  143,   69,    7,  230,
          874,  926,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  103,   25, 1816,   66,   13, 5590,   20,  521,   20,  874,
           93,    0,  115,   11,    6,   13,  324,  183,   10,  388,   21,    9,
          155,  874,    8,  598,   21,    0, 2603,   69,    7,  467,    0,   17,
           11,    6,  916,    0,   25,   11,  158,  446,  126,  347,   21,   11,
            6,  434,   13,  862,  365,  232, 1218,   20, 5590,   20,    0,    2,
            1,    1],
        [ 248,  215,  581,   85, 1366,    0,   19,  154,   46,   19,   11,  121,
          367,   10,   33, 7034,   46,   19,  154,   19,  659,   66,  226, 5187,
          106,   13, 3643,  384, 7710,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,    0,    0,    0,    0,    9,    0,   89, 6525,  230,
          115,    0,   38,  187,  144,   33, 1850, 2536,   48,    0, 1850,   62,
           69,    0,    7, 1850,    6,   20, 2286,  307,    0,    0, 4813,  248,
          215,  581,   69, 2154, 1374,   18,  881,   11,    6,  383,    0,   21,
           34,    7,  133, 2354,    0,    7,  473, 2829,    9,    7,  535, 8311,
            0,    2],
        [  29,  115,    0,  103,   24,  274,  270,   85,    7, 3253,    0,   85,
          108, 8196,    6,   17,  162, 5690,   54, 1004,    7, 3253,    0,  117,
         8196,    6,   63,  100,    7,  384,  174, 2003,   12,  108, 2626,  567,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:2') tensor([38, 48, 39, 40, 60, 31, 62, 38], device='cuda:2') tensor([1.0000, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(75.0274, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[ 225,    0,  509,  ...,    1,    1,    1],
        [ 101,  876,  140,  ...,    1,    1,    1],
        [1979,  265,    0,  ...,    1,    1,    1],
        ...,
        [ 109,   63,   24,  ...,    1,    1,    1],
        [ 117,   63, 4700,  ...,    1,    1,    1],
        [   8,   33,  321,  ...,    1,    1,    1]], device='cuda:2') tensor([14, 12, 21, 10, 15, 16, 11, 12, 18, 11, 13, 17, 11, 18,  9, 13, 12, 24,
        19, 14, 20, 21, 20, 19, 17, 18, 19, 24, 10, 15, 10, 13, 18, 14, 10, 15,
        17, 17, 19, 22, 21, 17, 19, 18, 20, 14, 12, 12], device='cuda:2') tensor([1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740, 0.8652, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8677, 1.0000, 1.0000, 1.0000, 0.8979, 1.0000, 0.8882, 1.0000, 0.8799,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
 > at.  tensor(22.4525, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[   7,  245,   91,   26,    0,   25,  305,   17,  524, 4474,  199,   21,
            6,  523,    6,    8, 3675,   29,   17,   25,   73, 5382, 7394,  251,
          523,    6,    8, 3675,    0,    8,  180,   12,  538,   25,   87,    7,
          744,  461,    0,   25,  388,   77,   12,  117,  523,    6,    8, 3675,
          270,  540,  554,   10,  367,   10,  155, 7034,    0,    2,    1,    1],
        [  24,  169,  492,   87,  860,   13, 3310,  279,    0,    0,   67,   46,
            9,  419,    0, 1165,    0,   24,    0,  591,  103,   24,  116, 1223,
         2396,    0,  134,    0,   77,  346,    8, 1296,   62,    0,  134,    0,
            0,   17,  281,    0,   94,  169,  204, 2990,   70,    7, 1296,   54,
          451,   51,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1141,   26, 1111, 6504,    6,    0,    8, 1008,    0,  423,
           12,  117, 3049, 4515,    6,  162, 1028, 1004,    7,   13, 1966,  407,
          816,  333, 1127, 1303,    0,   29,   70,  162,  117, 1816,  401,   71,
           77,    7,  839,   53,  162,  961,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [7616,    8,  415,  584,   26,   91, 4401,    0,    0,    0,   68,  386,
           65,   67,    9,  108, 5605,  336,    7,  179,    0,   24,   66,   77,
         1587,   12,  218, 4401,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,  278,   13, 1335, 1793, 7470,   97, 1411,   37, 2091,  890,
           10, 1557,   13, 1543,   12, 9829,    6,   10, 3022, 1146,   71,  170,
           10, 3522,    7, 4749, 5242,   11,    6, 1051, 2256,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   21,   11,    6,  321,   12,  100,    0,   25, 1057,   13, 1329,
            9, 7419,    0,    8,  180,   25,  175, 6899, 1314,    0,   10,    0,
          574,    0, 9784,    0,    0,    0,    8,  155,  886,  153,   26,   86,
         1767,   80,   33,  125,   25,   11,  121,  278,   10, 1703, 1404, 1610,
           57,    0,    0,  125,   25,   11,   57,    0, 2107, 1978,    0,    2],
        [  29, 8093, 8187,    6, 1452,  736,  439,   12,  159, 1422,    8,  736,
          439,   12,  159, 3857,    0, 6872, 1752,   35,  494,   15,   18, 1568,
         8187,    6, 1452,  736,  439,   12,  159, 1422,    0,   67,  116,  100,
          419, 3460,    8, 4197,    0, 1452,  288,  655,  439,   12,  159, 3857,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53, 1918,   10, 2554,  392,  759, 1086,    0,   29,    0,  100,   29,
          294, 1881,  440,    0,    7, 3516,  278,  540,    8, 6732,   62, 5725,
         2502,    6,    0,    8,  853,  174, 8227,   48,    0,   25,  150,    0,
          853,  174,  689,   11,   18,  703,    9,  906, 2937,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([58, 52, 44, 31, 35, 60, 50, 48], device='cuda:2') True tensor([[  67,  490,   24,   87,   17,    0,   19,   11,   48,  100,   25,   10,
          154,   17,   85,  419, 1800,  183,  120,   25,   11,   57, 2202,   84,
            0,    9,  155, 1992, 5896, 1247,    0,    7, 2583, 8748, 9169, 3413,
           54,   10,  155, 9879,    0,    7, 9879,   26, 3413,   54,   10,  155,
         2583, 8748, 9169,    8,   33,   26,  138,  155, 4941,   26, 3803,    8,
         8965,   48,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [5609,   10,    7, 1793,  183,  368, 5673,    0, 1276,    6,  499,   87,
         4986,   79,  261, 1269, 2536,   20,   79, 1970,    6,    0,  166,   26,
          509,  254,   21,   34,    9,   95,  424, 3400, 2197,   11,    6,  383,
            0,   67,   19,  499,  154,   17,  250,  225, 2396,   26, 4803, 5040,
            0,   38,  187,   66,   86,  226, 2373,    9,    7, 9091, 1313, 9094,
            3,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  56,  525,   11,    6, 4197,  246,   10,  267, 1848,    0,   38,  160,
            0, 1491,    0,    0,   33, 4925, 2823,    6,   55,  815, 6091, 1086,
            3,  267,    0, 1848,    0,  148,   63, 1102, 4907,    0, 1260,   69,
            0, 4340,  982,    0,    0,   38,  511,   17,   11,    6,   86,   77,
           46,  415, 1040,    0,   26, 1028,  126,    0,   71,   13,  264,    0,
         1375,    0, 2199, 1132,  820,    3,  225,    0,    0,  246,    0,    0,
           38, 2207, 4925,  164,    0, 2823,   55, 1581,    3,    3,  225,    0,
         8640,   48,    8,  246,    0,    0,   38,  187,  192,   11,   18,  135,
            0,  103,   17,   11,    6,    0, 1581,    3, 1995,  241,  221,  109,
            0, 1581,    3, 1793,    0, 1086,    0,    0,   67,    0, 3296,    0,
           21,   11,    6, 1581,    3,    3,   68,  194,   65,    2],
        [   8,    9,    7,  370,  207,  294,   12,  170,   63,  115, 5954, 4273,
            6,   17, 3412,  108, 1259, 1882,    0,  108,  203,    6,  777, 1991,
            0,  108, 3857,    0,   69,    7, 1080,    6,   17,   33,  659,  601,
          170, 3560, 4390,    0,   24,   73,  884, 1179, 5625,   54,    8,   39,
          128,   93,  195,   54,    7, 1289,   24, 2056,    0,   24, 8099,    0,
           24, 3836,    0,   24, 2153,    0,   73,  450,  170, 4324,   12,  183,
         1179,  250,  659,  205, 1226,   71,  108, 3227,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   71,   33,  822, 5108,    9,   13, 3708, 4774,    0,   24,   66,
         6960,   48,    9,  824, 2723,   45,  726, 1090,   17,   24,   73, 3814,
            7, 1729,   35, 1727,  430,  690, 3275,  110,  249,  132,   10, 1421,
          439,   12,   70,   26, 5914, 1417,   46,  248, 1086,  690, 3275,  110,
          249,    0,   24,   87,   21,   85, 7738,    0,  820,  828,    0,    8,
           24,   73,  499,  875,   17, 3151,  346,    0,  630,   12, 1823,    0,
          103,   25,   73,  875,    9,  143, 3708,    6,    0,   24,   73,  204,
          229,    7,  126, 2446,  276, 3028,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 875,   54,   94,  970,   10,  970,   71,  108, 3648,   26,   13,  207,
           12,  875,   54,  134,  970,   10,  970,   71,   94, 1004,  183,    0,
         1004,  672,    0, 4355,  931,  659,   66,  226,  133,  341,   10,  108,
          447,    0,   67,  148,    0,  100,  170,    0,  144, 1080,    6,    8,
         4989,    0, 2084,  335,   18, 1991,    6,    8, 3128, 2261,    9,  159,
          931,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([ 64,  65, 130,  82,  92,  63], device='cuda:3') tensor([1.0000, 1.0000, 0.8271, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
 > at.  tensor(157.1290, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[  67,   25,  296,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        [  29,   70,  321,  ...,    1,    1,    1],
        ...,
        [  29,   24,   73,  ...,    1,    1,    1],
        [ 229, 1118,   63,  ...,    1,    1,    1],
        [ 103,   25,   11,  ...,    1,    1,    1]], device='cuda:3') tensor([ 9,  9, 13, 11, 14, 10, 14, 16, 15, 10, 13, 13,  9, 18, 11, 17, 20, 14,
        17, 14, 13, 11,  7, 14,  7, 14, 13, 13, 16, 10, 12, 14, 13,  9, 11, 13,
        12, 14, 11, 12, 15, 10, 11, 11, 14,  9, 10, 11, 15, 13,  7, 15, 15, 13,
         8, 17, 13, 20,  7, 16, 14, 13,  7, 13], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 0.8105, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8555, 1.0000,
        1.0000, 0.8579, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000,
        1.0000, 0.8984, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8369,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8882,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:3', dtype=torch.float16)
 > at.  tensor(16.9683, device='cuda:3', grad_fn=<MulBackward0>)
True True tensor([[  67,   24,  150,   17,  545,   91,   12,  117,   26,  204, 4637,   54,
            0,    8,  188,   13,  341,  207,   12, 3101,  126,  138,   10,   87,
          282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   89, 1970,   34,   13, 3090,  756,    0,    8,   89,  291, 3769,
            0,    0,   34,   13, 1390,   37,  684,    8,    0,  113,   13, 1116,
         1040,   37,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  164,  450,   25,    0,   13,  546,   12,  248,  341,    0, 2352,
            0,    0,  248, 1284, 2352,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  391, 1551, 1065,    0,   17, 2182,  144, 3814,   48,   89,  637,
            8,  333,  473,  279,    9,   21, 3706,   55,  110,   10,   13, 1098,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 3195, 6482,   35, 6937,  227, 3134,  221,    0,   19,    0,   20,
            0,  108, 2805,    0,  132,   10,  108, 7009,    6,   11, 1492, 1485,
           12, 3402,   93,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  246,   53,  169,  780,   29,  835,   10, 1008,    7, 8365,    6,
           12,    7, 3311,    0, 7443,   17, 1573,  162, 3754,    8,  401,    7,
          370,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  289, 1228, 3374,    0,  281,   12,   25,  154,   80,  708,
            0,   21,   11,    6, 1301,    0, 2703,  901,  439,   12,  708,   87,
          722, 1228, 3374,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 291, 1105,   18, 3396, 1095,    0,    7, 4480,   79, 1019, 2071,    0,
         1095,    7,    0,    0,    0, 5640, 1375,   79, 1019, 2071,  755,    0,
          254,   94,  148,  162,    9,  509, 1862,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  116, 2213,   54,    8, 9171,    0,   38, 3127,   89,
         3397,   26,    3,   70,   11,    6, 1226,   71,  682,  616,  249,  407,
            6,    0,   38,  187,   11,  121,  144,  185,  453,  110,  128,    6,
           85,  682,  616,  249,  407,    6,    0,    2],
        [ 101, 5523,   17, 1381,  220,   51, 1405,  131,    7,   94,    0,    8,
           24,  583,  251,   94, 9475, 4637,  373,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  162, 1052, 1563,   94,    0,   67,   53,  162,  113,  172, 7176,
           20, 3979, 2120,    0,    8,   53,  162, 9231,   62,   12, 4391, 3834,
           12, 3716,   35, 7305, 2527,   15,  609,   96,    0, 5653,   10,    7,
          218,  415, 2633, 2120,    0,    2,    1,    1],
        [  85, 4615,    0, 9785,  369,   46,   17,  101,   11,   48,  226,  987,
         1181,  131,   39, 6651, 2266,  160,   22,    0,   13, 5289,  126,   10,
         2585,   20,  366,  565,    0,  276,   10, 9304,    7,  282,  126,   12,
          565,    0,    2,    1,    1,    1,    1,    1],
        [   0,    8,   53,  276,    0, 2982,   80, 1790, 8661,    0,    0,  276,
            0, 1445,   53,  162,    9, 3454,    0,  276, 1445,   53,  162,  830,
         1785,  810,    7, 5167,  608, 2020,   12,  159,  931,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  117, 4312,   22,    6,    0, 1025, 3043,   54,    0,   53,  415,
          158, 1515,   71,    7, 7224,    0,    8,    9,  117,  415,  158,  340,
         1955,   53, 3121,   13, 1905, 1762,   12,  941,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   85, 8195,   12, 2931, 2735, 1221,   46, 1016, 4618,
          866,  693,   46,  120,  248, 4618,  866,  693,  162,   86,  952,   10,
          545,  218,    0,   19,  169,  939, 8966,  834,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 131,    0, 5114,  235, 2399, 1006,    0,  889,   63,    0,    0,    9,
           13,    0,  841,    0,    0, 1449, 1778,  159, 5186,   10,    7, 1722,
         3609,   12,  422, 6222,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([27, 28, 19, 26, 29, 27, 29, 33, 44, 21, 42, 39, 35, 34, 34, 30],
       device='cuda:1') tensor([1.0000, 0.8887, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000, 0.8208],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(55.6638, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[  53, 2828,  987,  ...,    1,    1,    1],
        [6938,   25,   66,  ...,    1,    1,    1],
        [   8,   12,  538,  ...,    1,    1,    1],
        ...,
        [  29,   24,   66,  ...,    1,    1,    1],
        [  67,  985,  687,  ...,    1,    1,    1],
        [   0,    8,  432,  ...,    1,    1,    1]], device='cuda:1') tensor([14, 13, 27, 18, 21, 17, 19, 15, 18, 17, 24, 12, 15, 15, 33, 26, 20, 11,
        22, 14, 15, 22, 24, 13, 20, 16, 15, 17, 15,  9, 16, 21, 15, 13, 16, 26,
        19, 13, 24, 13], device='cuda:1') tensor([1.0000, 1.0000, 0.8550, 0.8975, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8848, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8677], device='cuda:1', dtype=torch.float16)
 > at.  tensor(26.3197, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[3929,  155,  874,  103,   25,  154,   25,  135,    0,   84,   63, 1230,
            0,   91,   26,  850, 6566,  131,    7, 2932,    8,    7,  218, 1498,
         5804,   10,  170,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    7,  467,    0,    7,  453, 6578,  148,   34,    7, 1951,   12,
           13, 1318,    8, 5555,   55,   24, 6868,    7, 1406,    0, 2082,    8,
          858, 4007,    6,   10,  367, 4404,    8,   51,   13,  461,   12,    7,
          546,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   21,   11,    6, 3849,    0,   21, 3560,    6,    0, 1259, 1599,
            0,    0,   21, 2293,    6, 1890,   13,  525,  287, 7195,    6,    8,
         1890, 3577,  901, 7195,    6,    0,  100,    0,   56, 2175, 1488,   35,
          868,    6,    0,  166,   26,  133,  341,  106,    0, 1120,  419,  218,
         2475,   12, 6293,    0,    2,    1,    1,    1,    1],
        [  21,  204, 1847,  214,   24,  169, 2807, 3301,   46,  214,  100,  150,
           48, 2184,    6,  156,    6,  109, 4377,   93, 2058,  424,    6,    6,
           46,    8,   73, 2575,  134,  199,   13, 4458,   18,  160, 1032, 1116,
          111,   45,   37,    0,  166,   25,   73,  774,  199, 1120,  419, 1862,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1384, 1224,  368,    6,   80,  423,  759, 9162,    6,   13,
          383,    0,  166,   26,   80, 1450,  439,   12,   77,    7, 1947,  619,
         5948,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1291,   22,  140,  416,   22,  110,  556, 1949,   11,    6,   13,  391,
           35,  511,   35,  290,   35, 6979,   35,  715,  234, 1927, 3156,   12,
         1954, 6935,    9,    7,   43,   59,  371, 1644,  827, 1151, 3144,    6,
            0,   85,   80,    3, 5494, 1360, 5997,    0,    8,   19,   11,  121,
          226, 1797,   54,   84,   55,  294,  215,    0,    2],
        [  94,  506,  204,  498, 1557,   54,  126, 8282,    6,    0,  934,    0,
            0,  109,    0,   53,  506,   86,   66,    0,   13, 4561,  583,   71,
         1230,   94,   10,    0,  456,   80,   13, 6711,  120,   53,  220,  116,
           87,   13,    0, 4454, 3836,    8,    0,  175,   21,  691,    0,   71,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   7,  853,   22,  335,   26,   17,   25,   73,  446, 2533, 5203,  336,
           21,    0,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,   11,  121,  116, 3498,   25,   63, 3302,   12,    7,  133,
          946,    8, 9743, 1081,   17, 5577,    6,    8,  896,  227, 2174,  232,
          445, 6442, 1052,   62,   48,  562,    6,    8, 1122, 1079,  292, 3751,
            6,   73, 9478, 2808,   55,  159,  447, 3965,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  274, 6269,    8, 6269,  199,    7, 1710,   12, 5554,    0,    8,
            9,   17,  207,   24,  175,  914, 4616,    8, 4616,   10,   33, 2570,
         2240,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  218, 2604,    6,   12,  813,    0,   79,   25,   11,   57,  211,
         6449,    0,    0,    0, 4837,   63, 2875,   80,  860, 4825,    6,    0,
          106, 3836, 4506,    6,    0,    0, 1817, 4506,    6,    0, 1817,  837,
         5002,    8,   29, 4404,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   77,   87,   33,   13,  277, 4838,    0,  166,    0,   26,
          347,   24,    0,   73,    0,   77,  274,  132,   85,    7,  370, 1303,
            0, 3253,    8,  150,    0,   33,    8,  113,   33,    8,  113,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 225, 1639,   85,    7, 1749,  747,  670,   79,   13, 2829, 1020,    9,
            7,  384,  221,   11,    6, 2100,    0,   29,  225,  278,   10,  150,
           77,    7,  807,   17,  278, 3172,   22,  126,   12, 2092,    0,   55,
         1976, 1043,    0,  148,  162, 4111,   10,   51, 7806,   48,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 116,  336,   17,  183,  206,   19,   34,  961,   17, 7374,    0,   19,
         1260, 1004,    7, 4895,   12, 5205,    8, 4776,   84,   34,  486,  546,
           24,  162, 5054,    0,    7,   82,    9, 7723,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   34, 3642,   84,    0,  116,   13,  110,   57, 1802,   35, 1933,
           35, 1178,    0,    8,   19,  934,   48,   13,  422,  378,    9,   13,
         2414, 2196,    0,  125,   33, 5353,  220,   86,  601, 1584,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45, 2464, 1778,    0,   68,  194,   65,  211,    0,   53,
          162,  172,  488,  184, 2148,    6,    0,    8,   53,  465,   11,   18,
          135,  261,   80,  998,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:4') tensor([29, 39, 53, 50, 30, 57, 50, 18, 47, 27, 42, 38, 48, 34, 36, 30],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8652, 1.0000, 1.0000, 1.0000, 0.8394, 1.0000, 1.0000,
        1.0000, 0.8887, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(67.1718, device='cuda:4', grad_fn=<MulBackward0>)
True True tensor([[3706,   55,    7,    0, 3210,   12, 2058,   35,  249,   59,  240, 1144,
            0,   53,   11,   57,  133,  946, 2787,    9, 2047,   12,    0,    0,
           73,    0,   24,    0,  694,  214,   10, 2231, 2736,  941,    0, 1645,
         2078, 2399,   70, 2892, 1075,    0,   73,   87,    8, 2293,  359, 4531,
          946, 6710,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  13,  143, 4089,  207,   10,  154,   80,   89, 6674,    6,  506,   51,
            9, 2047,   12, 1387,  687,    8, 4480,    0,  206,   85,  419, 1800,
          613,    9,  183,    0,   71,  419, 1127,  723,    0,   19,  217, 1102,
         1387,    8, 8219,  106,   17, 2216,    0,   77,   79,   13, 2443,   12,
           70,   19,  296,   10,   87,  230,  115,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   7, 7374,   17, 5290, 1247,    0,   79, 4546, 1247,    0,  220,  508,
         3667,   10,   13,  733, 6608,   12,  341, 1603, 4345,   46, 1259, 1247,
            0,  618,   59, 1247,    0, 3109,  140,   57,  411,  266,   26, 2856,
         1247,   46, 4574,   48,    7, 2303,   12,    7, 1827,    8,    7, 4715,
           12,    7, 1136,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [4157,    0, 7957, 6266, 1119, 1017,   21, 1962,    0,   85,  109, 3019,
          964, 4157,    0,    8,  180, 4587,   13, 1206,  567,   10, 1005,   21,
           84,    0, 4958,    7,   59,  627, 1202,  290, 1408,    0, 6712, 5933,
         6962,   26, 2770,   10, 5989,   85, 1362,   20,  586,  922, 4157,   71,
         3955,   45,  502, 8366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   39,    9, 1324,    9, 1954, 6935,    0,   19, 1346,
           80,   13, 1361,   12, 8188, 4039,  148,  144, 1828,  244, 1339, 1027,
          587,  195, 3485,    0,    8,   13, 8188, 1793,  148,  692,   10,  508,
         4460,   69,   17, 3485,    0,    8,  211,  218, 3280,  692,   10,  205,
            8,  601,  267,  508, 4460,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 103,    0,   84,   11,    6,    0,    0,    0,    0,   91,  759, 4039,
           46,    8,   19,  172,  135,   33,   46,  148,   63,    0, 3163,   10,
          229,    0, 1583,  583,    6,    0,  148,   63,    0,    0, 3163,   10,
           51,   69, 3836,    0,   19,  217, 2497, 1123,   17,   24,   73,    0,
          204,  468,    7,  538,   12, 1261,    0, 2318,    0,   55,    7, 4959,
           12, 1146,    0,    2,    1,    1,    1],
        [   8,   29,   19,  246,    0,   38,    6, 1049,    3,    0,   29,   19,
            0,  175,  518,    0,    8,  225, 1559,   69,    0,    8,   71,   13,
          277,  523,   12,    7,  170, 1522,    0,    0,    0,   13,  176,    0,
            0,  180,  225, 1518,  336,    0,    8,  225, 1143,    0,   80,  423,
         2360,    0,    8,    0,    0,  225, 1518,  270,  336,    0,    8,  225,
           11,    6,   77, 6087,    6,    0,    2],
        [  85,    7,  183,   19,   34, 1931,   54,    9,  117,  341, 1427,    0,
           19,  474,   19,   34, 3258,   54, 4029, 1352,    0,   67,   69, 1858,
         2007, 1498, 1261, 9096,  237, 1181,    0,    8,   19, 4600,   19,   11,
           48,  204,  226, 3258,   54,   13, 1127,  546,   55,  143,  254,  423,
          215,    0,    8,    7, 2838,   69,  264, 1736,   34,   21,    6, 2402,
          119, 8197,  271,    0,    2,    1,    1]], device='cuda:5') tensor([52, 57, 53, 54, 55, 64, 67, 65], device='cuda:5') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8584, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(111.5854, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[  21,   11,    6,    0,   86,   79,  103,   19,  144,    0,  419,    0,
          479,   70,   21,  169,  274,  100,    0,    0,   67,   84,   34,   13,
          841,   17,   24,  169,   66,   10,   87,  250,   71,  264, 9520,    0,
           12, 5149,    8,  250, 1057,   10,   87,   71,    0, 1588,  468,    8,
          250, 1057,   10,   87,   71, 1136, 1370,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  116,  226, 4174,    0,  166,  818,  251,  148,  162,
         7220,  144,   10,   66,   13,  207,   12, 8254,  839,   10, 1703,  251,
          248,    8,   13,  854,  759, 1086,    0,    8, 1065,    0,  143,   55,
            7, 6580, 8068,    0,    8,    7, 3742, 7856,  249,    0,    8,    7,
         4528,    0,    8, 3097,    7, 2248,  181, 3414,   20,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  67,    7,  630,   13, 2003,   96,    0, 1189,    0,   29,  138,  261,
           87,   56,  234, 1265,  241, 2142,    0, 2994,  122,  241, 2142,    0,
           13,   59,  380,   18, 3110,    8, 3098,    7, 1503,   12,  886,  412,
         3276,  287,    9, 4928, 1703,   55,   17, 7752,    9, 2446,   10,   17,
         2340,   10,    7, 1503,   12, 7121,  484,    0,  166, 2231,    6,   17,
         3833, 1548,    0,    2,    1,    1],
        [  21,   11,    6,  434, 3919, 8570,    0,    8,   21, 6430,    6,    0,
         2584,  218,  214,    0, 7753, 2736,  296, 1212,    8,  227,   93, 1069,
           96,    0, 2797,   54, 1868,  187, 3635, 1319, 3562, 7289,    8,  218,
         2808,   35, 5444, 2840,    6,   10, 3814, 3001, 3188, 1068,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,  258,  636,  132,    9, 5950,   20, 1006,    0,    8,    7, 3564,
          162,  172, 3633,   17,    7, 5317,  144,  226,   13, 2786,    0,  125,
           85,   17, 2110,    0,   19,  144,   13,  277,  523,   12, 2080,    9,
           91,   12,   89,  488,   10,   96,    0,    8,   19,  474,    0,   38,
          122,   57,  411,    0,  125,   19,   11,   45,  142,   10,    7, 8022,
            6,    3,   68,  194,   65,    2],
        [   8,  115,  339,  110,  873,  908, 2955,  187,  445,  128,    0,    8,
          289,   17, 4586,  931,   46,  166,  185,   12,    7, 4793,   24,  175,
          568,   69,   33, 4959,   46,  120,   25, 2554,    7,  282,   12, 2307,
            0,   13, 2980,   37,    0,   13, 3700,    0,   13, 1276,    0,   53,
           63,  521, 1015, 3127,   54, 7553,  111,  199,    7, 2340,    0,    2,
            1,    1,    1,    1,    1,    1],
        [   0,   19,  113,  213,   10, 2678,   33,   91,  132,   13,    0,    0,
          523,    0,  138,   73,    0,   53, 4477,    0,   19,   11,   45,    0,
          142,   10,  388,  134,    0,   69,   13,  824,  586,  457, 5445,    0,
            0,  166,    0,   26,    7,   38,  122,  241, 1515,   10, 4853, 1435,
           85, 8251, 1243,    3, 1189,    0,   29,  168,   21,   26,    0,    2,
            1,    1,    1,    1,    1,    1],
        [1220,    9, 3023,    0, 2058,  181,  541, 1775,  271, 4814,    6,   66,
          226,  619,   10, 1754, 7371,  955, 2028,    0,  100, 4896,    6,    0,
         2678,  777,  812,    6,    0, 3183,    0, 2011, 6504,    6,    8, 4222,
            0,   17,   66,  226, 3150,  111, 7225,   62,  199, 2116,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:5') tensor([57, 59, 64, 48, 66, 60, 60, 48], device='cuda:5') tensor([0.8740, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(98.4842, device='cuda:5', grad_fn=<MulBackward0>)
True True tensor([[ 347,  568,   21,  988,    0,  148, 1006,    6,    0,   26,   33,  116,
           13, 1678,   17, 9004,    6,    8, 6904,    6,  722,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13, 4746,   35,  803,   18,  249,   26,   13,  382,  783, 1522,  723,
            0,    8,   24, 5874,    8,  749,    6,  174,  589, 6010,  722,   55,
            7,  218, 1543,    0,    2,    1,    1,    1,    1,    1,    1],
        [  70,   33, 3565,  170,   26,    0,   33,  164,   15, 4482,  181, 2958,
            0,   21,   11,    6,  113, 2092,  365,    8,  883,  140,  365,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57, 1211,   17,   17,   11,    6,  204, 1031,  156,   62,
          199,  281,   94,   11,    6, 2702,   32,   54,   85,  185, 1648,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  103,   17,   11,    6,   86,  890,    0,   53,   11,  121, 2107,
           69,   10,  289,    0,  281,  490,   53, 4853,  747,  670,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1325,   53, 5415,   33, 1165,    0,   24,   11,  158,  492, 6290,  565,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  10, 1141,   17,  630,    0,   19,   11,   45,  142,   10,   66,   10,
          884,  486,   91,    0,  166,   26,    0,   70, 1148,  120, 6249, 8221,
            6,  415,  158, 1515,    0,    2,    1,    1,    1,    1,    1],
        [ 115,    0,  138,  323,   19,  367,   10,   33, 1760, 3140,   12,   13,
           48, 1184,  140, 3085,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 4268,   12,    7,  744, 6213,   55,   33, 1599,   34,   17,
           21,  220,   51,  619,   69, 8786,    6,    8,   91,   35, 1933,   35,
         1178,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [1008,   46,   25,   11,   57,    7, 3698,    0,    0,    0,  125,   25,
          618,  155, 1037,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,   13, 2198,  614, 1810,   34,    7, 4023, 2922, 2371,   56, 4654,
         2041,    9,    7, 1261,   12,    7, 1384, 1224,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  26,   21,  148, 1073,    7,  281,  839,  109,  148,   26,  281, 8064,
           10,  267, 3557,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   73,  876,  121,   10, 2697, 2360,    0,   10,  206, 7616,  568,
           86, 2705,  307, 1906,    0,    8,   26,  417, 1841, 1059,   62,   71,
         1894,  688,    6,    0,    2,    1,    1,    1,    1,    1,    1],
        [2018,  527,   93,   35, 2287,  234,  439,   12,    7,  179,   11,    6,
         1682,  931,    9,    0,  108,    0, 1884,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 4479, 1339,  783,  511,   37, 1405,   13,    0,  682,  247,   17,
          101,  434,    0,   38, 1879,    0, 2102,    3,  166,   26, 1223,    0,
         8044,   55, 3708,    6,    0,    2,    1,    1,    1,    1,    1],
        [  19,  321,   12,  100, 1060,  117, 1532,    8, 2346,   18, 1011,   71,
          565,  359,   13, 2555,  237,  866,    9,  321,   12,   13,  854, 4656,
            6,  737,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  24,   66,  255,  200,   22, 6339,   54, 2200, 2362,    0,    8,   24,
           66,  244,  168,  264, 8973,    6,   69,   13, 4481, 3489,   10, 1764,
          387,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [2805, 2219,   69,   70,   91, 1985,  150,    0,    7, 2805,   12,   17,
          107,   20,   15,    8,  291,  156, 3837,  457,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103,   17,   11,    6,   86, 4949,  890,    0,   25,   73, 5652,
           91,    8,   25,  175,   13, 2685, 8831,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 106,   17,  552,   13,    0,  325,   12, 5828,    6,    0,  109,  126,
            6, 2893,    6,    0,    0,   79,   19,  434,  134,    0,    0,  214,
           17,   19, 1385, 1313,    0,    0, 6263,   89, 6878,    0,    2],
        [  19,  154,  245,   21,   11,    6,  528,   55,  170,   10, 2613,   70,
           21,   26,   10,   51, 2639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3695,   12,  134,   63, 1387,   10,  206,    7, 4458,  362,    6,   63,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  744,   26,    9,    6,  234,   54,   17,   84,   26, 8902, 1666,
         2214,   10,  415,  500, 1783,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0, 6482,  215,  581,    0,  903,    0,  846,  315,   22,   18,
          380, 3089,   35, 3304,   62,  284, 3780,   11,    6,  874,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([23, 29, 25, 25, 24, 14, 30, 18, 28, 18, 22, 17, 29, 21, 30, 28, 28, 22,
        21, 35, 19, 14, 19, 24], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8584, 1.0000, 1.0000, 1.0000, 0.8706, 0.8208, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8242, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       dtype=torch.float16)
 > at.  tensor(39.6049, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:35:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
tensor([[  24,   66,   13, 7524, 4040,    9,  166,   24, 1393,   10,   91,  723,
           10, 8714,   39,  467,  982, 2884,   12, 1833,    0,   10,   51,   89,
         4023,  570,   59,    0,   89,  772, 1751,    0,    7,  772, 5370,    0,
           89, 2444,   62,  521, 1105,   48,  480,    0,   89, 2294, 1661,  221,
          369,    0,   89, 5930, 4888,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,   70, 1320,  335,  684, 4600,    0,    8,   25, 1510,   21,    9,
            7,  227,   93,  362,  715, 2378,    6,   12,  632,  237,    6,   15,
          593,    0,   34,   17,    7,  535,    0, 3139,    0,  227,  233, 2341,
         4267,    6,   12,  574, 2256,   12,    7,  564,  322, 1910,  162, 1028,
           10,   13, 1387,    0,    8,   17,   24,  162,   80,   10,  954,  199,
           91,   12,  251, 9200, 2760,    6,   12, 1261,  120,  768, 2317,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19, 7750,   25,   10, 1064,    0,   55,    7,  245,  183, 3983,    9,
            7,    0,  179,    0,  168,   69,    7, 1366, 2110,    0,   13,  475,
           35, 5203,    0, 2178, 2445,  266,    0, 6580, 1411,  271,    0,  629,
          110,    0,    8,   89, 1751,    0,  903,    0,    0,  728,  649,   57,
           93, 2734, 2003,    0,    0,  106,    0, 7002,   11,    6,  728,   18,
         3411,  300,    6,  369, 7312,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  17,   26,  120,   13,  150,   48,   26, 6788,   62,    9,   13, 7312,
           10,   87,  250,   86,    9,  191,   48,   62,  131, 1377,   46,  100,
         1520,    7, 2192,   12,   13, 1390,    8, 2985,   21,  199,    7, 2192,
           12,   13,   10,  424,  412,    0, 1995, 2083,    0,  192,   11,   18,
          175,  110, 1226,    0,   19,  100, 1390,    8,   10,  424,  412,   96,
            0,   67,   33,   26,  116,  775,   20, 1718,   93,    0,   68,  194,
           65,    7,  150,   48,    6,   63,  180, 2685,   62,    0,  180, 5847,
            0,    2],
        [   7, 2193, 1793, 2493, 7012, 1198,    6, 1361,    0,   13, 1361,   12,
           94,  148,    0,   69,   13, 1284,  383, 1663,    0, 3284,  199,   13,
         3508,  982,  964,    0,   85,  159, 3506,  160,  128,    6, 3696, 3836,
            8, 2187,    6,    9, 2828,   62, 7689, 1974,   22,   18,    0,  100,
           33,    0,    8,   53,  456,   80, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,   29,  115,    7, 2932, 1381,    0,  148,   55,   13,   87,  610,
          215,    0,  188,  923, 3150,  111, 6226,   62,    7, 2688,    0,   26,
          115,    9,    7, 3140,   12, 1057,   10, 4007, 1179,   10, 2156,  109,
         6406,  346, 1711, 4166,    0,  125,    7, 6105,   10,   13, 3147,  234,
         1827,   26,   29, 4126,   17,   53,   73,   11,   18, 1799,   71,   21,
          419,  218,  207,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1232, 5917,    6,  427,  362, 1011,  131,    7, 3022,   46,    8,
           19,   11,  121, 2138,  185,  798,  215, 2115,   54,  336,    7, 4959,
          401, 1221,   69, 8633, 1232,    6,    0,    8,   66, 3395,   62, 3585,
         1118,    9,  392,   10,  798, 1075,    9,   33, 4959,    0, 2386,   12,
         3585, 1118,   46,  752,   10,  661,   70,  513, 1226,   71,  108, 1232,
         5917,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:4') tensor([55, 73, 67, 86, 57, 65, 63], device='cuda:4') tensor([1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(131.5358, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[   8,   46, 2307, 1008,   33,  283, 7772,    0,    0,   38,  160,  119,
          480,    0, 6880,   55,  155, 5223,  197,   46, 1363, 1850,   37,    0,
            0,    9,  545,    0, 1227,    0,   33,   34,  138, 7803, 1808,    0,
            0,  192,   11,   18, 3995,    0,    0,   13, 1192,  552,  126,    0,
           21,  144,  211, 3236,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,    7, 4174, 2078,  487,    0,    7, 1762,   12, 2718, 2202, 6611,
         6671,    9,    7,  774,   12, 5428,   34,   79,  488,   79,    7, 1762,
           12, 2718, 2202,  850, 3909,  607, 4357,  407,    9,    7,  774,   12,
         1947,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   85,    7,  467,   12,   21,    0,   24, 8152,   48,   17, 2878,
           12,  708,   73, 1082,   10,  368, 2930,    8,    7, 1465,   69,  159,
          447,    0, 5333, 2515,  366,   12,  148,  109,  206,   53,  162,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24, 1405,   13,  846, 4399, 7016,   12,   13,   56, 2515,  292,
         2404,    0,    8,   25,   73, 3684,  375,  688,  359,  341, 8829,    6,
           17, 2231,  341, 7125,    6,    0,   29,   17,   73,  601,   25, 5415,
           70,   11,    6,    9,    7,  563,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   11,    6,  916,   26,   53,  323,   21,    9, 2008, 1491,    8,
         6343,    0,  206,   21,   11,    6,   38,  626,   57, 6505,  128,   10,
          508,  561,  804,    6,    3,   53,  144,   10,  508,   21,  113,    9,
            7,  832,    0,    6,    0,   10,  175, 1719,  292, 2460,    0,   29,
           19,  154,   84,  162,  391,  832,    0,    6,    0, 2116,    9,  132,
         6034,  264, 1736,  148,  162,  461,   12,    7, 3677,    0,    2],
        [   0,   33,   26,   13, 1523, 7376,    0,    0,   21,   11,    6,   39,
          985,  687,    0,    0,   21,   11,    6,    0,   13, 4912,   12,   77,
         1587,   12,  813,    0,   86,  116,   80, 5807,    8,    0, 9827,    8,
         6689,    8,   29,   69,    0,   67,   80,  896, 2604,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   21,  169,  492,   66, 7747,   10,  110,   10,  154,   17,  116,
          125,   19,  144, 1157,   13, 5462,    9,  166,   13, 2232,   34,   13,
          227,   37, 1010, 2650,   37,   17,  101,   34, 3524, 2015, 1197,   12,
           77, 4039,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  169, 1017, 8788,   55,    7, 1732,    6,    0,    8,  780,   10,
         2895,   71,  134,    9,   13,  207,   17,   34,   79,    6,  762,  366,
         1094, 3835, 1256,    0, 6510,   54,   13, 2866,  979,   12, 3802,    8,
         7376,    9,  166,   24,  220, 1082,   10,  283,  540,    8, 1766,   91,
          486,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([54, 39, 37, 44, 71, 48, 40, 51], device='cuda:7') tensor([0.8584, 1.0000, 1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(81.2945, device='cuda:7', grad_fn=<MulBackward0>)
True tensor([[ 24, 498,  71,  ...,   1,   1,   1],
        [ 33,  34, 245,  ...,   1,   1,   1],
        [ 87,  21, 238,  ...,   1,   1,   1],
        ...,
        [886,   0, 476,  ...,   1,   1,   1],
        [ 29,  19, 415,  ...,   1,   1,   1],
        [596,  73,  66,  ...,   1,   1,   1]], device='cuda:7') tensor([ 6, 10, 10,  7, 10,  7, 10, 11, 10, 10,  6, 11, 10, 14, 10, 12, 14, 18,
        14,  8,  8, 14, 13, 11, 11, 15, 12, 11, 10, 14, 16, 10, 13, 20,  8, 17,
         9,  9, 21, 10, 17, 10, 12, 12,  8,  8,  8, 12, 14, 11, 10, 11,  9, 10,
        11, 12,  7,  9,  8,  4, 10, 12, 10, 13,  9, 14, 11, 15, 11, 10, 12, 14,
         8, 11,  7, 12,  9, 13,  9, 14, 13,  9, 13, 12,  8, 10, 10, 13],
       device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8633, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8174, 1.0000, 1.0000, 0.8384, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8755, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8091, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000, 1.0000, 0.8579,
        0.8691, 1.0000, 1.0000, 0.8516, 1.0000, 1.0000, 0.8652, 0.8286, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 1.0000, 0.8564, 1.0000,
        0.8223, 1.0000, 1.0000, 0.8276, 1.0000, 1.0000, 0.8394],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(13.5802, device='cuda:7', grad_fn=<MulBackward0>)
True tensor(13.5354, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[   8,   21,   11,    6,   13,   24,  237, 1355,   54, 1909,    0,   19,
          154,    0,    9, 8041,   12, 4972,    0, 3982,  366,   12, 4972,    0,
            8,   19,  154,   33, 5510,   12, 1102,  639,    8,  998,    9,    7,
         1136, 5055,   26,   39, 1909,  206,    7,  832,    0,    6,    0,   73,
          172,  305,   13, 5247, 2883,    0,    8, 7419,   26,   91,  663,    0,
            2,    1,    1,    1,    1,    1],
        [  29,  168,   25,   66,   13,  567,    0,   25,   66,  250,   46,    8,
           84, 1631,  227, 8263,   12,   17,  993,    9, 3804,   46,  185, 8829,
         2385,   39, 4312,   22,    0,    8,    7, 4312,   22, 3060,   96,   10,
         3793,    0,    8,  288,  120,   13, 6737,  925, 1585,   17,  188,    7,
          230, 1051,  174, 1991,  568,    7, 5250, 1085,    0, 1239,    0,    2,
            1,    1,    1,    1,    1,    1],
        [  84,  188,  226,   39, 2244,   48, 2080,  199, 2627,  106, 1146,    0,
          106,    0, 5036,    0,   67, 5830,    0, 2288,    0,    8,   77,   12,
           13, 5827,   24,  144,   33,    0, 3024, 2244,    9,    7,  245, 1345,
           12,   33,    0,  464,    0,  347,    0,   19,  154,   84,    0,   63,
          391, 2826,    0,  248,  535,   35, 4935,    0,    0,    0, 1649,    8,
            7, 7678,    0,    2,    1,    1],
        [  19,  246,    0,   38,  187,   11,  158,   87,   21,  106,  384,  237,
          803,    3,   53,  246,    0,   38,  469,   57,   11,    6,  211,  207,
           25,   11,   57,  142,   10, 6785,   13,  759, 6052,   35, 3481,  176,
           12, 2811,  839, 2202,    9,  384,  237,  803,    3,   29,    9, 6769,
            0,   19, 5016, 1222,   13, 6478,  244,  699,  411,    8, 3122,   10,
          264, 1027,  119,  232,    0,    2],
        [ 101,  246,    0,   38, 3127,   19,  692,   10,   51,   13, 2182,  684,
            3,    8,  101,  246,    0,   38,  200,  969,   19,  278,   10,    7,
         8459,  464,   12,  670,    0,   89, 3540,  465,   11,   18,  305,   21,
         6064,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19, 1060,  267,   80,  267, 3013,    0,    8,  225,  246,    0,   38,
         1969,  135,    0,  635,   19,   11,  158,  618,    0,  635,   19,   11,
          158,   14,    0,   67,   19,  192,   11,   18,   66,   13, 3013,    3,
          225,   34, 1922,  200,   54,   51, 1795, 2073,   18,    0,  166, 6121,
          267, 7150,  244,    7,  215,   10, 1393,  133, 1665,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [ 476,   25,    0,   68,  386,    0,   65,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 101,  591,   17,   53,  162,   56, 8862,   10,  384, 1050,  140,   54,
            7, 7522,  469,   18, 1568,    0,   10, 9547,  945,   80,   70,  506,
           51,    0,    8,  101,  591, 1730,   17,   53,  465,   11,   18, 1799,
          238,   71, 6765, 1955,  109,  802, 6027,   69,  251, 6765, 1955,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:6') tensor([61, 60, 64, 66, 39, 59,  8, 49], device='cuda:6') tensor([1.0000, 1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 0.8110, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(99.6466, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[ 225, 1119,    0,   38, 1461,    0,   17,   11,    6, 1284,    0,   70,
           26,   21,    0,   26,   17, 7148,  650, 3727, 1994,    3,    8, 4069,
            0,   19,   73,   51,   13,  277,  593, 4187,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,   34,  133, 3315,    8,   19, 1385,  185, 4673,  214,    8,
          442,  185, 1968, 3051,    6,   17,   19,  213,   10, 1452,   71,   25,
          440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 294, 3940,    0,   68,  386,   65,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  251,  370,  215,    0,   89, 1970,    8, 3460, 1102,   14,   48,
           12, 1167,    0,    8,   19,   34,  914,  244,  221,  505, 1563,   80,
         6088,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  162,  461,   12,   33, 5673,    0,   70,  169,   25,  289,
            0,   63,   25,  850, 1203,  494,    0,    0,    0,  238,    0, 2859,
           46,  138,   87,   25,    0,  276,  135,    0,  125,   25,   11,   57,
            0,   86, 3531,   10,  456,   80,   21,    0,    2],
        [  29,   33,   26,   86,  116,   13,    0, 6429,    0,   80, 7291,    6,
            0,    0,   21,   11,    6,   13, 6429,   80, 3295,    9, 1146,   79,
          238,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  89, 2574,   26, 5205,    8, 4970,   63,    7,  473,  825,  120,    7,
         3022,  188, 2096,   10,   87,   21, 1574,    0,    8,   24, 2775,   11,
           18, 5768,   62,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 276,   13, 2900, 6815, 5707,   62,  392,  439,  143,    9, 7001,  254,
          225,  323,    9, 8979,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [8310,   54,   17,    0,    7, 2468,   12, 2573,    0, 1067,  525,  436,
            0,   17,   19,   34,  752,   10,  747, 2653,    0,  288,   51, 1217,
            6,    9,  248, 6874,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,    7,  207,    7,  227,   93,   45, 1604, 2334, 2895, 5070,  170,
           46,   24,   11,  121,  115,  278,   13, 1234,   10, 9375,  347,  117,
          227,   93,   45, 1604, 2334,   63, 7071,  341,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   53,  245,  487, 2980,   54, 2263, 1315,    0,   21,  220,  305,
           79,  294,   79, 1111, 6052,   12, 2533, 1390,   10,  229,   13, 1127,
         1116,  569,   12, 2263, 1315,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 1797,    6,    7,   39,   22, 1522, 3667,    8, 2322,   12, 7932,
         2673,   96,    8, 4157,    6,  142,  270,  490,    7,   69,    6,  307,
           12,    7,  473, 2217, 1137,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  961,  736,  439, 7692,  629, 2481, 2286,   62,  235,
            8, 5225, 5108,    8, 2288, 5225, 5108,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  388,    9, 3189,    0,    9,    7,    0, 1832, 2930,    0,   84,
            0, 7774,   62,   77, 1587,   12,  993,  106,    7, 1465,    0,   80,
         2192, 6631,  793,    0,    0,  281,   12,  166,    0,   19,  465,   11,
           18,  661,    0,    2,    1,    1,    1,    1,    1],
        [   8,   21,  875,    6,   25,  199,    7, 7771,   12,   70,   11,    6,
          226,  434, 1440,    0, 2243,   59,  827,  760,    0,  883,  424,    0,
         1579,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,    0,  154,  281,   12,  134,  169,   51, 8661,    8,  958,  756,
            0,    0,    0,    8,    0,   79, 5474,    0,  294,   12,  134,  169,
          914,   51, 7078,  706,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([34, 27,  7, 27, 45, 27, 29, 18, 30, 34, 31, 32, 21, 40, 28, 30],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8877, 0.8564, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 0.8223],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(54.0231, device='cuda:6', grad_fn=<MulBackward0>)
True True tensor([[1189,    0,    0,   29,  339,   11,    6,    0,  289,   21,   11,    6,
         1491,    0,    0,    8,  339,   11,    6,  289,   21,   11,    6,  384,
          372, 1761,   20,    0,    8,  115,    0,   25,   73,    0, 1948,    0,
          205, 1817,    8,  446, 1835,   13, 1484, 3762, 2293,   59,    0,    2,
            1],
        [   0,   67,   25,  135,    0,   19,  154,   25,   63, 4130,   71, 2084,
         1625,  128,    6,    0,   46,    7,   81, 2510, 3496,   18, 9875,    6,
            0,    8,   77,  117,  214,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  91,  383,    0,   19, 7262,   62,  126,   12,    7, 2657,    9, 3918,
         1118,  432,   13, 3040,   71,   13, 3532,   91,    0,    8,   19, 5429,
           62,  199,   13,  682,  181, 3435,    0,  206,   19, 1060,    7, 2859,
           57,    6,    6,   55,   13, 2705,    0,    2,    1,    1,    1,    1,
            1],
        [   8,  120,   19,  289,  923, 1738,    0,   21,  220,  499,   51,   29,
         4733,   17,  211,   91, 3485,   12,  282,  700, 6317,    6,  486,    0,
          166,   26,   13, 3462,  474,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  113,    7,  473,  203, 2470,  247,   55,    7, 3299,  267,   48,
           12, 2533,  382,  399,    0,    8,  113,    0,   12,  538,    0, 3547,
         6952,   55,  486,  733, 3609,   12,  218, 1369,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  94,   63,   29, 4493,   12, 6608,   17,   53,  780,    8, 2509, 1459,
            0,  276,   94,  148,  192,   11,   18,  213,   10,  109,   73,   11,
           18,    0,   10,  873, 1417,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 432,   77,    0,  305,   13,  274,   85,  117, 2792,    6,    0,   97,
          737, 1010, 1144,    8,  415, 2633, 2041,    0,  179,   82,    6,    0,
         6229, 1106,    0, 7050,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7,  240, 2120,   66, 2138, 4021,    0,  752,   10,    0,  661,  347,
           21,  188,    0,    0,   33,  133, 2035, 1236,  297,  293,  111, 3495,
           35,   18,  500,   62,  800,    0,    8,    0,   53,   11,  121,  367,
          132,   71,    0,    0,   13,  800,   12, 1254,    0, 7052,    6,    0,
            2],
        [ 125, 2392,   37,  109, 1065,    0,   24,   11,  158,   51, 7258,   62,
           71, 4012,   80,   33,    0,    8,   21,   11,    6,  509,  103,   24,
          154,  835,   80,   21,    0,  276,  103,   24,  213,   10,  154,  835,
           80, 2826,  347,   24,  451,  492,   87,   21,    0,    2,    1,    1,
            1],
        [   8,  168,   21,   26,   55,    7,  245,  183,  291,  121,  233,   62,
           85, 1366,   46,  108,  245, 4878, 1806,  469,  816,   24,  293,  457,
            0, 6712, 5831,    6, 1103, 3007,   54,   71,  282, 1117,   13,   24,
          293,  457, 7757,   54,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,   70,   24,  487,   71,   34,  172,   13,  264,  207,   12,
          826,   80, 1009,   39,    9, 3562,    0,  359,   13, 1039,  434,  229,
         1261,    0,  166,   24, 6532,    9, 5855,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   34, 1061,    0,  225,   34,  798,    0,  100,   13,  325,   12,
            7, 1323,    6,   24, 1618,    0,   53, 3994,   11,   18, 2603,   56,
         7038, 1568,   80,  159, 1666, 4388,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 391,  215,  581,   19,   34, 1074,    9,   13, 5200,    9,   13,   56,
         2561,   35,   45,  973, 2102,   54,  325,    0,    8,  440,   19,   11,
           45, 4379,   85, 1366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,  225, 3673,   62, 1406,  290,  125,  225,  692,   10,  229, 1123,
          225, 3673,   62,  250,  225, 1425,  267,  708,  169, 2034,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19, 1425,   17,   85,   17,  765,   17,   70,    0,   19,  451,
          508,    0,    0,   10,   89,   94,   26, 1370,    8,  958,    0,    0,
            8,   17,   11,    6,    0,   70,   19,  513,  432,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   70,   19,   11,   45, 2659,   55,   26,    7, 5708,   12,   13,
          264, 1329, 6813,   46,   19,   11,  158,  367,   10,   33,   13,  277,
         1065,   46,    8, 1645,    7, 4936,   12,   13,  264, 1455,  199,    7,
         2781, 1234,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:0') tensor([48, 31, 44, 31, 34, 31, 30, 49, 46, 42, 33, 32, 30, 24, 35, 40],
       device='cuda:0') tensor([0.8550, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8433, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(60.4968, device='cuda:0', grad_fn=<MulBackward0>)
True tensor([[ 125,    9, 2483,   35,  606, 5731,    0, 2932, 6930,    6, 5989,    8,
          447,  281,   12,    7, 2483,    6,   46, 4346,  737,    0,  742,  160,
          140,    0, 3213,   46,    8,   53, 5470,    7, 2791, 2008,    8, 3156,
          199, 2248,  713, 1777,    0,    2],
        [  13, 4495, 1435,    9,    7,  832,    0,    6,    0, 2721,   17,    0,
           12, 5990, 8459, 3665, 1118,    0,  248,   35, 8516,    6,   12,    7,
         5990,  889,  144,  708,    8,  288,   91,   35, 8516,   12,    7, 5990,
          596,  144,  708,    0,    2,    1],
        [ 168,   26,   13,   29,  300,  166,  188,  278,   77,    7, 8681,   12,
         7343,    0, 3280,    0, 1976,   25,  289,    0,   19, 3250,   21,    0,
           39,   19,  362, 1838,  235, 5384,    9,   25,    8,  180,   25, 3745,
           10,   21,    0,    2,    1,    1],
        [ 822, 2317,    9,  984,   11,    6, 6339,   17, 4877,  244, 1953,    6,
           10, 2386,   12, 1655,   12,  215, 4373,    7, 6484,   12, 7616,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  67,   70,  143,  220,  225,   66,  691,  103,  225, 1631, 1088,  144,
            7, 1885,  818,   12, 3802,    6, 2875,   10,  267,   10,  719,   13,
          841,   17,    7, 9413,   17,   94, 1095,  144,   10,   51, 1529,   62,
         3840, 4173,    0,    2,    1,    1],
        [  19, 1223, 2894,   55, 3101,  839,   69,    7, 2291,    0,   68,  194,
           65,    8,   19, 5429,   80,    7, 5986,    6,    0,    8,   19,   66,
          591,   13,  555,  214,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  66,   25, 1060, 9832,  347,    8,  138,   29,  294, 9696, 2419,    6,
           66, 6028,   48,  244,    7,  473,  640,  215,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [1223,   70,   21,  968,  110,   34,   46,   68,  194,   65,   68,  386,
           65,   46,    7, 1587,   12, 1075,   24, 5559,   63,  324, 1075,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([42, 41, 40, 27, 40, 30, 22, 25], device='cuda:1') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:1', dtype=torch.float16)
 > at.  tensor(70.0887, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[  21,  914,   26,    7, 1226,  279,   10,   87,   10, 4746,    8,  368,
            7, 3939,    8,  946, 2780,    0,   38,  511,    7, 1381,    0,   53,
         1346,  267,  546,    8,    7,  218,   94,    0,    8,   53,  246,    3,
         1768,    0,   25,   11,   57,  230,    0,   24,  451,  229,   13, 2240,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 2964,  169, 2154,  724, 2468, 7147,   69,    7, 2618,    8,  719,
          117, 1230,   35, 6937,   35, 7596, 1710,    6,  442,   12,   13, 1127,
         2468,    0,  736,  439,  203, 1255, 4711, 1755,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [7823,   94,   63,    0, 4820, 1256,   94,    0,    8, 4820, 1256,   94,
           46,    7,  143,    8,  143, 4820, 1256,   94,   84,   63,    0,    7,
          143,    8,  143,   24,   11,  158,    0,    0,   66,   13, 4820, 1256,
            0,  179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  84,   34, 9199,    0,  180,  552,  853, 3174, 4796,    0,  440,   24,
           66,  211, 3174, 4796,    0,    7, 1575,   35,  372,   59,  247,  119,
         1819, 2735,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 180,   19, 1095,  108, 2333,  158,  240, 1028,   10,  670,    9,   58,
          389,    6,    8, 2852,   59,   18, 1374, 2197,    6,    0,    8,  474,
            0,   21,   11,    6,   86,  230,   55,  110,   10, 1005,   33,  218,
          939,  668,  232,  834,   29,   19,  339,  134,  116,   46,   53,  144,
           10,   51, 1644,  411,    8, 2736,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17,  370, 4506,   17,   24,  116,  323,   71,    7,    0, 6144,  779,
          569,  164, 2676,   66,  475,   35, 2371,    0, 1683,  979,   54,    0,
            8,    0,    0,    7, 3180,  164,  289,    0,   38,  786,    0,  346,
            0,  859,    0,    0,  230,    0,   13,  176,    0,   95,  266,    0,
           17,   11,    6,    7,    0, 2636, 4057,   10, 1557,    0,   17, 1683,
          518,   10,  155, 3280,    3,    2],
        [  19,  513,  270,   10,    7, 1224,    8,  487,  665,  336,   10,  150,
          103,   19,  220,  446, 3302,  206,    0, 7490, 1390,   54, 7521,    6,
          144,  226,    0,    0, 2517,   48,    0,    8,   21, 1932,  126,   84,
          162,    0, 3695,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,  230,    0,   29,  339,  110,  508,    0,   25,   39,  663,   12,
         8681,    0,   12,   13, 1760,  321,    0,    8,   19,  213,   10, 7032,
           13, 1455,   17,   19,  154,   26,    0,    0,  133, 4501,    0,  166,
           26, 8164,   54,    0,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([50, 34, 40, 31, 56, 66, 41, 42], device='cuda:1') tensor([1.0000, 1.0000, 0.8799, 1.0000, 1.0000, 0.8735, 0.8794, 0.8555],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(74.7457, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[ 976, 1475,    0, 2184,  176,    0,   24,  154,   29,    0,   29,  115,
           17,   24, 2534,  117, 2522,    8,  591,  117, 3422,   17,  339,  170,
           87,  117,  214,    0,   24,  487,   10, 2252,   17,    0, 3423,    0,
          995,   17,   24,   73,   87,   71, 1780,    0,  995,   17,   24,   73,
           87,   71,   13, 1472,   12, 1780,    8,   13, 2705,   24,   73,  115,
           87,   71, 5414,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  499, 3008,  813,   69, 3634,    6,    0,   67,  115,   24,   73,
         3008,   13,  325,  143,  813,    0,  143,  254,  700,  490,    0, 2761,
           54,   21,   26, 4865,    0, 4585,   54,   21, 4865,    0, 4912,   21,
           26, 4865,    0,  979,   54,   21,   26, 4865,    0,    8,   70,   24,
           73,   87,   26,   24,   73,  203, 1792,   33,  813,   55,  368,    6,
           17,   24,  492,  276,  934,   48,  120,   24,  245, 2861,   62,    7,
          660,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21, 1518,  126,   17,   71, 1421, 1601, 2421,    6, 3195,   54,
            9,    7, 5199,    0,  432,  101,  976,  261,  442,  284, 3140, 2226,
           69,   33,   46,  101,  144,  284, 7986,    8,  101,  144,  284,   46,
          101,  258,   57,   33, 1266,   55,  908,  254,  248, 1551,    8,   34,
          529,   10, 7554, 4115,   13, 4058,    8,  278,  923, 2294,  244,    7,
          409,   17,    0, 5098,   46,  284, 5098,   46,   21,   11,    6,    7,
          245,  183,  101,   11,    6, 1831,  100,  101,   11,    6,  144,   39,
         1266,    9,  815, 1132,  215,    0,    2,    1,    1,    1],
        [ 108,  296,   55, 3051,    0,  108,  296,   55, 4029,  687,    0,  109,
          108,  296,   55, 3069,    8, 8464,    0,  109,  108,  296,   55,  540,
          687,    8,   55, 2848, 2050,   93,    0,    8,  103,   25,  154,   80,
            7,  277, 1920,  148, 2456,    6,   69,  155, 1067,  265,    8,  148,
           26,  415,  878,  111, 7307,   62,  168,    8,  133, 7759,    8, 6128,
            0,    8,   85,  185,  613,   77,   12,  170,  296,   10,  205,  126,
          199,    7,  179,   10, 3231,    8,   10, 3522,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 2983,   84,   11,    6,   13, 3024, 7927, 4719,    0,    8,   33,
          172,  132,    6,  307,  110,    0,  125,   13, 1027,  631,  407, 3565,
           25,  432,   13, 2767, 6204,    0,   84,   11,    6, 7008,   80,   39,
         1061,   35, 1315,  322,  183, 5848,   10,   46,   24,  321,  290, 4971,
            0,  498,    7, 4971,   93,  979,    0,   67,   70,  281,   94,  192,
           11,   18, 2252,   26,   17,   69, 2313,   21, 1847, 3497,   10, 1421,
         1113,  109,  143,   55,    7,    9, 1610,   45, 1032, 1809,  424, 1420,
          233,  373,   10,  276, 1772,   10,  575,  132,    0,    2],
        [5902,  589,    6,   69,  837, 1827,  162, 6386,    0,   38,  597,  269,
          340,  407,  323,   21,    0,  347,   73,   11,   18,   24,    3,   19,
         1850,   62,   39, 2792,   69, 5063,    8,  434,   21,   38,  290, 2078,
         1525, 6664,    0, 9413,    8, 9696, 2419,    3,   19, 8063,   48,   13,
          630,   10,    7,  640,    3, 6463,   12,    7, 2672,   85,    7,  183,
            0,   38,  412, 2366,   26,    7, 1802,  322,   12, 8338,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  203,  779,  675,  793,   26,  142,   10, 3128, 2129,   12,  248,
         3834,    0, 2129,   24,  164, 8896,  117, 1752,   35,    6,  335,   18,
         1617,  457,  183,   35, 2470,    6,   96,    9, 1764,   20,  484,  480,
         1081,   12,  108,  447, 2573,  131, 1520,  203, 7712,  237, 2332,    0,
          109,  994,  117, 3382,    6,   63,  142,   10,  175, 7154,   48,    9,
          291, 1076,   20,  484,  480, 1081,   86,   12,  108, 2573,   46, 1187,
          111,    0,  131,   82,    0, 1599,  109, 2145,  586,  271,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([66, 75, 91, 82, 94, 72, 84], device='cuda:5') tensor([1., 1., 1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
 > at.  tensor(150.4254, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[  29,    0,   25,  ...,    1,    1,    1],
        [   8,  180,    7,  ...,    1,    1,    1],
        [   7, 4821,   10,  ...,    1,    1,    1],
        ...,
        [ 540,   53, 3122,  ...,    1,    1,    1],
        [   8,   19,   11,  ...,    1,    1,    1],
        [ 635,   24,  296,  ...,    1,    1,    1]], device='cuda:5') tensor([13, 13, 20, 13, 14, 18, 18, 10, 16, 11, 16, 15, 17, 11, 19, 12,  6, 12,
         8, 17, 12, 21, 13, 12, 15, 18, 17, 15, 19, 24, 15, 14,  9, 18, 11, 16,
        15, 14, 12, 21, 14, 15, 15, 11, 21, 14, 20, 17, 23,  7, 17, 14, 12, 14,
        15, 13], device='cuda:5') tensor([1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8799, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 0.8286,
        1.0000, 0.8652, 1.0000, 1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8677, 1.0000, 0.8550, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8882, 1.0000, 1.0000, 1.0000, 0.8105, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 0.8521, 1.0000,
        1.0000, 1.0000], device='cuda:5', dtype=torch.float16)
 > at.  tensor(19.7456, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[ 250,   17,  169,   51,  528,    0,   67,  113,  250,   17,    0,  169,
          305, 5542,   12,    0,   77,   12,  117, 7099,    6,   17,   91,    0,
            0,    0,  144,    0,   38,  511,    9,    7, 1165,   12,    7, 1827,
         2260,    0, 4619,   13,  325,   12,   94,    0, 4619,    0,   94,  148,
          162, 2129, 9829,    6,  109, 4488,   93,    0,    0,    0,    8,  113,
           86, 1057,    0,    0,    0,    9,   89,    0,    0,  447, 1165,    0,
           13, 3557,   10,    0, 4110,   80, 3903,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,  278, 1421, 3675,  126,   12,   21,   55,   89,  670,  125,
           19, 1831,   17,   24, 1591, 2637, 3181,    6,    8,  811,   35,  426,
          356, 5149,    8, 1127, 1848, 7178,   48,   10,  367,   10,   13,  670,
          206,   84,   34,  874, 2286, 5575,   62, 8517,   17,  877,   20,  307,
           62,  134,  333,  383,    0,  125,   21, 1017,    6,   13,   10,  375,
            8,   39, 7574,   80,  138,   25,  598,   80,   94,  535,  490,   25,
          508,  134,    7, 4340,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  432,  423,  215,   12, 7989, 1435,    8,   56, 2687,  240, 1032,
         6606, 6480,    0,  131,    0, 1655,   12, 5709, 2245,    0,  333, 2767,
         2931, 3182,    9,    7,  179,  188, 8152,   48,   17,    0,    7, 6929,
            6,    0,    0, 5914,   69,    7, 1232,   63, 2426,   10, 2034,    8,
           17,    7,  979,   12, 2753, 4429,   26,  211,    0,  143, 1565,   93,
          254, 3563, 4401,    6,   12, 2753,  824,   48, 4577,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   79,   25,  150,    7, 1819,  205,    9, 6714,  111,  359,   84,
            0,    8,  180,    0,    0,    0,   68,  386,   65,    8,    7,  281,
         4673, 1809,  411,   26,   17,   89,  874,  204, 1917,   62,  499,  535,
          890,   10,   87,   17,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   34,   89,  765,   10,  749,  221,    9,    0,   10, 2750, 1222,
         2433,   55,   70,   63,  172,  288,   13,  874, 1256,   12,    7,  133,
         1219, 5222, 3770, 3254,    0,    8,   19,  144,  116, 5893,   13,  488,
            0, 1061,   35, 1315,  322, 1039,   55, 2829, 1020, 1747,  160,  741,
            0, 3150,  111,    0,    8,   19, 1425,   19,  220, 6785,   13, 3028,
         1329,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  269,  290, 1543,  106,   10, 1059,  188,  691,    7,  291,   18,
         1397,  156,  457,    0,  391, 1543,    6, 4437, 2361, 1268,    0,  895,
         1101,  269,  484,    0, 1912, 2172,    0, 5087, 2172,    8, 1339, 1373,
          699,   57,   77,   85,    7,  370,  183,   46, 6835,  203,  620,  293,
            6,   62, 4437, 1395, 4781,    9,  166,   24,  205,  126,    0, 2213,
          132, 1546,  121,   22, 1691,  269,  484,    0,  388,    9,    7, 4437,
            6,   17,  204,   66,    7, 4273,    6,    0, 3020,  126,    7,  269,
          290,    8,  180,  339,  134,  205,    0,    2],
        [   7,  218, 8314,   12, 1780, 1119,    0,   38, 1865,   89,  227, 2704,
            7,  179,   34, 1621,    3,  166,   26,   10,  289,   21,   11,    6,
         1301,   17,   19,   73,   11,   18,   87,  778,    0,   67,   19,   73,
         1123,  111,   87,  250,    0,   19,   73,   55,  122,  366,    0,   19,
           73,  570,    0,   19,   73,  575,  132,    0,   19,   73, 6295,    0,
           19,   73,   51,   13,  461,   12,   33, 2469,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:4') tensor([81, 78, 71, 42, 63, 92, 70], device='cuda:4') tensor([0.8003, 1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(142.2732, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[  29,  180,   24,  388,  117,  427,  412,  430,  158,    6,  540,    9,
           13, 1127, 1692,   10,  150,   70,   53,  169,   87,    0,    8, 3188,
           54,   69,    7, 3530,    0,   24,   66,  185,  427,  412,  430,  158,
            6,   69,    7,  859,   17,   63, 1944,  336,    8,   21,  100,    6,
           10, 2677,    7,  218, 1710,    6,    9,   21,    6, 1422,    0,    2,
            1,    1],
        [  25,   11,   57, 3487,   62,   71, 7863,   54,   21,  346,  461, 1010,
          111,   35,    6,   20,   15, 2397, 1408,    6,    0,    9,    8, 2584,
          119,  218, 5933, 4601,    6,    0,   85, 1629,   35, 1898,  684, 3049,
            6,    0, 1189,    0,    8,   77,   25,   66,   10, 5971,   25,   63,
          155,  248, 1893,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,  169,   51,  100,   13, 3019,   35,  262,  411,  176, 1064,  206,
           25,  150,    7,  688,   85,    7,  467,   12,    7, 7781,    0,   67,
           21,   11,    6,   13, 2701, 1723, 1064,    0,   68,  194,   65,    8,
           84,   11,    6,  211,  207,   12, 2432, 2881,   80,    7,  688,   85,
            7,  467,   12,    7, 7781,    0,    2,    1,    1,    1,    1,    1,
            1,    1],
        [  19,  144,  288,   33, 6629,   46,   89, 4795, 1583,    0,   89, 1192,
            8,   13, 3434, 2626, 6951,   46,   17,   19, 6414,   21,    9,    7,
         2422,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   0,  225,   15,  290,   19,   93,   15, 4835,    0,    0,  238,    0,
           21,   11,    6, 4810,   17,   25,  451,  884,   17,  125,   91,   12,
            7,  214,   17,   11,    6,    0,    0,  916,   80,  378, 2639,    0,
           26,   25,  204,  175,   13,  341, 5200, 5072,    0,  613,  120,   25,
         6337,    7,    0,  207, 6878,   62,   94,  229, 4388,    0,    2,    1,
            1,    1],
        [   8, 5348,  203,   45,   22, 1026,    0,    0,  148,   34,  172, 1968,
           80,   21,    0,    0,    0,  144,   91,  524,    0,    0,  101,  465,
           11,   18,    0,  100, 1339,  742, 2919,  373,    0,  125,  101,  474,
           21,  169,    9,    6,  300,   18,   94,   71, 7576,   11,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1141, 4433,  270,    9,    7,  832,    0,    6,    0,    0,    9,
         6848,  518,    7, 4139,    0,   38,  779,  180,   19,  205,  270,  637,
            8,   19,  456,   10,   94, 3006,    9, 2667,   35, 1966, 1265, 3916,
            6,   84,    0,    8,   53,   11,   48,  289,    3,   25,  135,    0,
          417, 2847,    0,   25,   73,   11,   18,  172, 1897,  518,    7, 4139,
            0,    2],
        [  70,   11,    6, 4767,   55, 1136,  958,   26,  113, 4767,   55, 2216,
         2314, 5149,  125,   21,   11,    6,  172,  999,   55, 1201,   10,   66,
          486,  574,  187,    0,  211,   91,  213,    6,   21,    0,    8,    0,
          204,    0, 7009,    6,  192,   11,   18,  213,   10,  205,  637,   71,
           13,  903,  763, 2129,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([60, 53, 55, 27, 59, 49, 62, 54], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8911, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(80.8863, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:35:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
tensor([[   9,   33, 1760, 1165,    0,    0,   24,  162,  802,   13, 2270,   45,
            0,  109,    0,    0,  339,  110,  116,  150,  103,    0,   19,   73,
          175,   33, 4481,    0,    0,   13, 9238,    0,  217, 1076, 5997,   56,
         8368,  922,    0,    0, 2806,  240, 2175,  249,    0,    2],
        [   7,  225,  158,   12,    7, 1052,  249,  781, 1158,   34,  561,   48,
          199, 1580, 6032,   85,   13, 2290,   17,   24,   11,   57, 2438,   54,
          131,    7,  467,   12,   33, 1910,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    9,  251, 1113,    0,   19, 1608,   11,   18,  283,   69,    7,
         6389, 6035,    0,   19, 1608,   11,   18, 1927,   13, 4446,  109, 2618,
           39, 5651,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  144,    7, 1666,  941,   35, 9748,  688, 9372,    6,   17,  842,
          785, 1601,   10,   82,   45,  132,    8,  180,   25,  162,  859,  665,
           13,  321,   12, 4594,  111, 1650,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,   77,   12,   13, 5827,    0,  168,   19,  217,    0, 6815,
          241,  829,  106,  747,  670,    9,  564,  957, 1132,    0,    8,   33,
         1148,    0,    8,   24, 2252,   17,   38,  290, 1408,  197,  513,  755,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 1043,   19,   11,   45,   29, 7105,   80,  467,   54, 3753, 3023,
         1382,   11,   18,  116,  125,   21,   11,    6,  999, 3023,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,  154,   80,   21,   79,   13, 1780,   35,   18, 1397,    0,
         1523, 1909,   12,  294, 2386,   12, 1655,   12, 2602,    0,  166,   26,
         2546,  829,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1831,   17,  103,   19,  220, 3665,  929, 7897,    0,   19,  220,
         4075,   17,    0,  432,   77,    0,   19, 1359,   11,   18,  172, 2702,
          111,   19,  158,    0,   21,   34,  185, 4416, 4113,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    9,   33, 1165,    0,   21,   11,    6,   39, 1663, 1039,  369,
           55, 2327, 1034,  240,   20,   69,  117, 4601,  825, 3275,   35, 3794,
         2885,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   56, 3168, 2445,   93,   11,    6,   13,  133,  528,  461,   12,
            7, 9655,    8,   24, 5150,  108, 1613,   10,  274,    9, 1820,   55,
         4455,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   69,    7,  218,    0,  874,    0,   24,   66, 1075,   17,
            0,   63,  244,  737,  891,   54,    0,   69,  837, 2694,    0, 5955,
           10,  159, 6768,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2072,  988,   26,  250,   17,  689,   11,   18,  100,   10, 2895,  133,
          261,    0, 3706,  359, 7036,    0,    8,   12,  538,   24,  169,  100,
           10, 1082,  143,   80,   21,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    7,  218, 2475,   12,   81,   18,  340, 8978,   26,    7,
          227,  424,    6,  560, 5824,  424, 1825, 1158,    0,    8,  117, 1816,
          985,  132,  227,  760,  233,    6,   55,   13, 1074,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1980, 1068,    0,   24,  296,   13,  574,  833,   12,  524,   35, 1105,
          505,   54,    0,   86,  116,  524,   35, 5001, 3635,  833,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   33,   34, 2861,   62,  131,   39,  786,  727,   46,  166,   26,
          100,    0, 1730,    9,    7,  467,    0,  101,  164,   86,   66,  995,
            0,  125,   21,  164,  417,  827,  603, 1906,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,   87,   25, 1005,    7,  909,  609,    6,  369,   12,    0,    7,
            0, 2018, 2510,  429,   12,    7, 1683,  724, 1025, 6433,   54,    7,
         8094,  429,    0,   12,    7,    0, 1478,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([46, 32, 28, 32, 38, 24, 28, 35, 28, 27, 29, 31, 35, 24, 34, 33],
       device='cuda:3') tensor([0.8242, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 1.0000, 0.8706],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(50.3897, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[   7, 3288,   63,  ...,    1,    1,    1],
        [  53,  113,  180,  ...,    1,    1,    1],
        [ 282,   26,   80,  ...,    1,    1,    1],
        ...,
        [4069,    0,   21,  ...,    1,    1,    1],
        [2869, 3540, 4712,  ...,    1,    1,    1],
        [  33,   26,    7,  ...,    1,    1,    1]], device='cuda:3') tensor([15, 15, 16, 13, 21, 10, 18, 12, 12, 14, 16,  9, 14, 19, 15,  8, 15, 16,
        15, 12, 10, 15, 19, 18,  8, 23, 16, 18, 13, 12, 11, 20, 19,  8, 15, 18,
        11, 23, 15, 13, 18, 15, 15, 11, 16, 13,  7, 14], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8711,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000], device='cuda:3', dtype=torch.float16)
 > at.  tensor(20.8375, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[ 125,    7,  688,  ...,    1,    1,    1],
        [1228,    0,  155,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        ...,
        [   7,  214,   24,  ...,    1,    1,    1],
        [   8,   21, 4333,  ...,    1,    1,    1],
        [   8,   29,    0,  ...,    1,    1,    1]], device='cuda:3') tensor([ 8, 10, 11, 15, 12,  9,  8, 12, 10, 10, 17, 16, 12, 10, 10, 11, 12, 18,
         6, 11, 12,  8, 10, 10, 10, 11, 11,  9, 10, 10, 12, 18, 16, 12, 14, 14,
         7,  9, 12,  9, 12, 11,  9, 10,  9, 10, 10,  7,  9, 11, 14,  9,  8,  8,
         6, 12, 10, 12,  8,  9, 11,  8, 11,  8, 12,  8,  7, 13, 11, 15, 17,  8,
        13, 10, 12, 13, 11,  9, 11,  6], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8872, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8379, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8101, 1.0000, 0.8486,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8057, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8257, 1.0000, 1.0000,
        1.0000, 0.8301, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(13.9769, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[   9,    7,  248,  215,  490,   89, 7102,    0, 4743,  439,   12,    7,
         8224, 1580, 2217, 3258,  116,  110,  543,   62,  755,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   33,  842,   77,   12,    7, 3677,    6,   17,  144,  700,  226,
         5823,   62,   69, 2667,  262, 2651,  480,    6,   17,  162, 1719,  292,
          121,   48,  244,   13,  798,   35, 1933, 2760,  131,    7,  664, 1151,
            0,    2,    1,    1,    1,    1],
        [   9,  409,    0,  276,    7, 4273,   93,  813,   17,  155, 1893,   63,
         9830,    0,  155, 4896,    6,   63, 9830,    0,   26, 1501, 3012,  982,
          125,   21,  220,  641,  995,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [2266,   45,    0,   39, 5486,  140, 1997,   26,   39, 5204,   29, 2583,
         9646, 2010,    0,   25,  465,   11,   18,  276,  135,   21,   34, 1254,
         1325,   25, 3128,   48,   21,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 473, 1822,    0,   19,   34,    9, 8393,   71,    7, 6099,   12,  958,
            0,  125,  103,   25,  192,   11,   18,  135,    0,   84,   11,    6,
           13,  452, 1534,   20,  371,  126,  174,   57, 1161,    9, 8393,   85,
            7,  765,    0,    2,    1,    1],
        [   9,  409,    0,    9,    7, 1909,  206,  225,  925,  106,    0,  143,
          254, 1737,  439,   12,    7, 1579,    6,  684,  589, 9447, 1682,  188,
         1220,   14,   48,   12,   33, 1599,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   66,  185, 2468,   80,    7,   29,   35, 8309, 5308, 5874, 8282,
            0,  166, 3336,  518,  131, 2659,    0,  138, 5308,   26,    7, 5874,
         8282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  25,   66,    7, 2375,   82,   45,  322,   12,  155, 1037, 2202,  230,
          558,   10,   25,    0,   67,   25,   63,  113, 1363,   10,  205, 1273,
          727,   54,  336,    7, 8464,  713, 1117,  155,  447, 1066,    0,    2,
            1,    1,    1,    1,    1,    1],
        [  25,  175,   10,  780,  134,  126,    0,  722,   71,  134,    0,  417,
         6127,  436,  134, 1325,    7, 5462,  901,   24,  293,    6,  126,    0,
          490,   25,   66,   10, 1557,  134,  270,    0,    8,   25,   11,  158,
          175, 4744,   55,   21,    0,    2],
        [   7,  747, 2653,   12, 7151,    0,   29, 3761,    0,   34,    7, 5605,
           12,    7, 8575,  853,  650,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  53,   63,  401,   21,  125,    0,   12,   91, 7857,  288,    0,   53,
           66,    0,   13,    0, 4960, 4469,   10, 1082,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   24,  172, 7827,    7, 2918,   10,  213,   10,   87,   21,    0,
          860,   17,    7,  296,   55, 7935,  445,   56, 1783,  870,   45,  925,
          132,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 639,  188,  367,   12, 1137,   17,   11,    6, 2156,   54,  170,   10,
          150,    7,  984,  106,  672,    8,  205, 2027,  199,    7, 1580,    6,
         5309,  111,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [3088,   46,   24,   11,   57,  133, 3926,   46,   17, 1455,   38, 1355,
          777,  232,    3,    7, 8750,   26,   86,  595,  121,   48,  126,   12,
           13, 1472,   12,  877,  221, 1547,    0,  126,   12,   13, 1886,  362,
           12, 3164,    0,    2,    1,    1],
        [   8,  180,    0,   13, 7685,    0,   13, 1838,   20,   26,   15,    0,
          323,   33, 1968, 1692,    0,  225, 2393, 1613,    9,   10, 2026, 1195,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,  120,   19,   34,   13, 2145,  586,   54, 1850,   35,   48, 1327,
           85,  415,  675,   45,  174,  407, 2811,    0,   19,   34, 2027,    9,
           13, 6528,   35,  240,   35,  737, 1410, 7424,   12,   89, 3557,    0,
            2,    1,    1,    1,    1,    1]], device='cuda:1') tensor([23, 38, 31, 31, 40, 32, 27, 36, 42, 19, 22, 27, 28, 40, 26, 37],
       device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8276, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(50.4923, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[ 115,   33,  415,    6, 3784, 2410,   35, 5444, 1466,  659,   46,    8,
           19,  154, 7674,   10,   46,  229,  170,  598,  133, 1986,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,  409,    0,   33,  479,   12,  203,    6,  234,   57,  140,  945,
            7,  733,  479,   26,   29,  528,   17,   19, 1808,   10,  154,   80,
           70, 1148,   71,  801,  639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,  154,   80,    7, 2695,   12, 6105,    9,    7,  270,   12,  155,
         1066,  125,  214,   73,   51, 1932,  336, 1552, 2117,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 126,  168,  440,    0,   84,   63,   13, 3631,   12,   94,   46,   13,
         2045,  556,  881, 1341,   48,   55,  663,   46,  148,  169,  100,   86,
          288,   10,  991,   13, 3400,    0,   67, 1108,   13, 3400,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 1043,    0,   19,  154,    0,   26,   17,    7, 2705,    6,  369,
          373,  703,   53,   11,  121, 5248,   22,   10,   51, 2705,    6,  369,
          373,    0, 6872,    7,  963,  291,   20,  362,  614,   93,   62,  598,
           21,   11,    6,  226, 1911, 1965,   18, 3840,  134,    0,    2,    1,
            1,    1,    1,    1,    1],
        [ 339,   11,    6,  368,    7, 1232,   79,    7,  772, 3976, 3180,   24,
            0,   66,    0,    0,    8,  661,   85,   70, 3151,   94,  169, 1703,
           55,   33,    0,   29,   53,    0,  175,    7, 8568,   12, 2573,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  79,   13, 1920,    0,   19, 6090,   13,  325,   12, 4528,    8,  144,
         1771,   12,   56,  706, 1251, 1320, 1993,  693,  100, 2483,  371, 1434,
           93,    8, 2481,  726, 2992,    8,    7,  672, 1810,    8,   13,  277,
          523,   12, 4289,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  53,   11,   57,  434,    7, 5875, 4460,  415, 3729,   18,    6,    0,
            8, 2245,   66, 2107,  270,    8, 1797,   62,  143,  813,   69,   77,
           12,  117,   94,  333,  555,  215,  700, 1313,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 103,   25,   11,   48, 2107,  199,    7, 3227,   12,    7,   94,    0,
          238,    0,   25,  169,   66, 1110,   13,  325,   12, 6146,    0,    8,
           25,  169,   66, 1110,   13,  325,   12, 1259, 4117,    0,    8,   25,
          169,   66, 1110,   13,  325,   12,  214,   17,  169,  305,   13,  535,
          183,   10, 4971,    0,    2],
        [ 245,    0, 3942,   53,   11,   57,   86, 6902,    9,    6,   15,    6,
          235,  366,   10,  218,   94,   11,    6, 4734,    0,   53,   63,    9,
            6,   15,    6,  235,  366,   10, 1964,    6,   17,  218,   94,   63,
            9,  838,   18,   57,    6,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6, 1223,  952,  100,   25, 2153,    0,    8,   29,    0,
           55,  663,    0,   24,   11,   57,  826,   29,  261,  117, 1113,   80,
         1291,   22,  140,  416,   22,  125,   12,    7, 2496,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   19,  591, 4822,  765,    6,   12, 6950,    9,    7, 7343,   17,
           89, 6792, 1290,  804,  607,   62,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  24,  220,  154,    0,   38, 1871,    0,  220,   24,    0,   87,   10,
          229,  749,  532,  713,  509,    3,   21,   11,    6,   85, 1686, 1254,
            7,  749,  532,    6,  169, 1529,   55,    7,    0,    0,  324,   12,
          749,  532,  713,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   86, 1525,  596,    0,   21,   11,    6,   55,  170,
            0,   55,  170,  596,    0,   55,  108,  708,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 276,    7,   73,  287,   96,  873, 6608, 3008,    6,  206, 7400, 2270,
           48,   48,  232,  106, 1318,   10, 1318,    0, 2823,   54,  995,  106,
          593,  322, 1203,  426,   10, 5966, 6926,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,  564,  895,  584,    0,   56,   62, 1251,  110,   59,  656,  505,
         1017,    7, 1797,   55,    7,  535,  608, 4480, 5147,   48,    9,   91,
         3350,   85,  640, 2602,    0,  815,    0,  895,  895,  887, 2360,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([24, 31, 23, 36, 47, 37, 41, 34, 53, 44, 35, 20, 41, 22, 34, 37],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8945, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8901, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(57.7949, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[  25,  116,  175,    7, 8570,    0,  347,    0,  125,    7, 1922,  181,
           85, 1922,  195, 2270, 3956,  188,   13,   56, 4030,  741,  266, 5126,
           80, 1665,   35, 5489, 3153,    6,  803, 1326,    0,   38, 5276,   26,
            7,  207,   21, 7674,   10,   51,    3,    8,  225, 3860,    6,   21,
           17,  207,  183,    8,  183,  554,    0,    8,  103,   25, 1336,  293,
           57,  237,   71,  267,    0,  225,  164,  289,    0,   38, 1969,  135,
           70,    0,   25,   11,   57, 1226,    0,   33,   26,    7,  772,  207,
           21, 7674,   10,   51,    9,   33, 5743,    3,    2],
        [  53,   66, 2314,   93,   35,  779,  569,   54, 1187,    6,  100,   38,
            6,  786,  373,   93,   45, 1604,  724,  197,  109,   38,  372,   59,
          247, 2583, 6874,    3,   19,   11,   45,   86,  142,   10,  205,  199,
            7, 3201,    6,   12,  117, 1288,  115,    0,   67,    7, 1890,  613,
           26,   33,    0,  103,  419,   12,  134, 1945,   62,   33, 4949,  111,
         3495,   35,   18,  500,   62, 2070,   12,    7, 8575, 1726,    0,  180,
           24,  451,  150,  264, 5554,  378, 1621,   85,    7, 1979,  176,  140,
         1585,   71,    7, 8575,  853,  650,    0,    2,    1],
        [  13, 2672,   34, 1621,    0,   39, 8547,   13,   48,  525,  365,   59,
          866,   34, 1223,    9,  879,  945,   94,   10, 2802,    7, 2672,    0,
            8,   84,   34,  211, 1757,    0,   38, 1871,   63,   24,  142,   10,
           87,    3,   38,  187,  192,   11,   18,  135,    3,    9,   13,  555,
         1113,    0, 1953,    6,   12, 1655,   12,   94,   84,   46, 7417, 5902,
          589,    6,  148,  162, 2659,    7, 3955,  119,  724,   12, 9657, 8428,
            6,    0,   38,   15, 4399,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  120,   19,  316,  122,    0,  359,   89,   86,   96,    0,   70,
           19, 4776,   19,  144, 3924,  346,   26,   17,    0,    7,    0, 5638,
          415,  829,    0,   69,  108, 1247,   26,  100,    7,    0, 5638,  415,
          829,    0,    0,   69,   13, 2035,  221,  387,    0,  848,    8,  848,
            0,    8,   94,  474,    7, 5638,  415,  829,   69,  108, 1247,    0,
            0,   34,  100,   13, 2103,  366,    0,  415,  829,   17, 3524,  442,
          108, 1247, 2183,   37,  109, 5167,   37,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   24,  316,  122,  108,  101,  356,    6,    8,  246,    0,   38,
          533,   11,   57,   86,    9,    7,  800, 1678,    0,   24,  213,   10,
          305,   91, 1269,   85,   13,  183,    8,  305,    7, 1269,  230,  359,
          670,    0, 2955,   10, 2900,    0,    8,  175,  134, 7070,   55,  509,
         1074,    0,   13,  747, 2070, 1329,    3,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   19,   11,   48,  100,   25,   10, 1683,   13, 3955,  930, 2735,
            9,   13,  207,   17,   91,  188,   22,   11,   18, 1620,   62,  490,
            0,  934,   13, 3955,  930, 2735,   17,  689,   11,   18,  641,  384,
         1105,  810,    7,  984,    0,  934, 3343, 4867,  170,   87,   33,  131,
           13,  140, 1236,   45,  300,  829,    8,  909,  609,  777,   18,  829,
            8,  987,  607,  445,   54, 2483, 1643,    6,  126,   12,  129,  128,
          160,  271,  862,  881,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 3699,  132,  378,  529,   10, 1157,   79, 1029,  187, 3684,   22,
         1337,    0,   79, 1029,  187, 6073,    0,   80, 1111, 1345, 1065,   46,
           29,   13, 2701,   12, 1498, 1345, 1065,   46,    8,  513,  106, 6871,
           19,   10, 6871, 1051,    0, 3699,  132,  401, 8899,  283,   85, 1137,
         1839,  120,   19, 3447,   62,   10,    7,  832,    0,    6,    0,    0,
            8,   66, 2277,   48,   10, 4432,   33, 2468,  244, 4401, 2205,   10,
         1387,   10,   13,   87,  610, 1234,    6,  115,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([93, 92, 79, 81, 56, 78, 82], device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 0.8413, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(135.2952, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[   8,   29,  103,   19,  162,  142,   10,  508,   25,    7,  467,   54,
           12,   33,  546,    0,   21,  169,  205,  250,  100,   33,    0,    8,
           17,   11,    6,   70, 5017, 2912,  110,   10, 4379,   10,   25,  168,
           85, 1366,   80,  546,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,    7, 1043,   55,   33,    0,    7, 1043,   17,    7, 5367,
         2735,  689,   11,   18,   66,  419, 4585, 2836, 6672,   26,  125,    7,
         4910,    6, 1446,  535,  581,   17, 1719,  293,  356,   26,  593,  832,
           18,  233,  235, 4356,   10,   56, 2917,  597,   93,   55, 4585, 2836,
         6672,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1037, 4343, 6976,   34,   69,    7, 1832,    0,    0,    8,   89,
         3780,  246,    0,    0,   38,  779,    0,  995,  132,   84, 2320,   10,
         4432,    3,    8,    0,  225,  321,   12, 1260,  346,    7, 2884,    0,
            8,    0,    0,    0,  225,  246,    0,   38,  174, 1069,    0,    0,
           94,  540,    3, 3111,    0,   24,  144,   13,  207,  199,    7, 2469,
            0,    2],
        [ 115,   19,  217,  961, 1819,    6,    0,  473,  464,    0,   19, 5893,
           13, 1819,  434,   38, 2347,  588,  929,  889,    3,   38, 2347,  588,
          929,  889,  197, 3447,    6,   10, 1261,    0,   67,  486,  461,   12,
          108, 4990,  589, 1261,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19,  474,   19,   11,   48,  456,   13,  277,  523,   80,  185,  116,
          488, 1288,   80,   33,    0,    8,  180,  175, 3570,  270,  126,  168,
           29,   24,   73,  456, 2895,  366,  111,   13,  277,  523,  143,    8,
          154,    8,  884, 1532,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   70,    0,   19,  323,   26,    0,   19,  842,   77,   33, 1221,
            8,   19,  442,   21,  199,    0,   13,   46,  238,    0, 1223,   21,
           11,    6,    0,   13, 2913,  682,  455, 1434,    0,   12,   33,   91,
         4700,    0,    0,    8,   21,    0,  595, 2334,   13,  316, 1076,  266,
          436,    0,   12,    0,  284, 4896, 4437,   69,    7,  270,    0,    2,
            1,    1],
        [   9,  117, 2822,    6,    0,   25,  150,   39, 5020, 4373,   62, 4358,
            0,    8, 6013, 1708,  117,  824,  569,    6,   12, 1593,   18,   63,
          117, 3275,    0,  203,  140,   18, 1409, 2130, 1318,    6,  166, 2456,
         1646, 5494, 4653,    7, 2059,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   33, 1119,   17,   21,   11,    6, 3756,    0,  131, 5057,    0,
           10, 2032, 1456,    7, 1503,   46,   19,    0,   20,    0,    0,    7,
         3140,   46,    8,    7,  765,  502,   12,   13, 8683,    0,  125,    7,
         1529,   12, 8634,   21,    0,  131, 5057,    0, 2317,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:5') tensor([42, 51, 62, 42, 42, 60, 43, 48], device='cuda:5') tensor([1.0000, 0.8086, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(84.4603, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[  25,  135,    0,   25,   11,   57, 2465,    0,   25,   66,   77,  117,
          214,  142,   55,   25,    0,   38,  511,   19,   11,   45,  100,    3,
          125,   19,   11,   45,  168,    8,   21,   11,    6, 3939,    0,    8,
           25,  135,    0,   19,  321,   12,  100, 2180,   18,   18,    6, 3884,
          176,    0,   38, 3794,    0,   17,   11,    6,    7, 9132,  608, 1043,
           19,   11,  121,  700, 1346,   55, 4432,   54,   10, 2240,  670,    0,
            2],
        [  29,  339,   11,    6,  498,  518,  131,  384, 2172,   54,  185, 2047,
            0,   70,   26, 7898,    0, 7898,   26,    7, 6408,   24, 1064,  120,
           24,  154,   17,  108, 2082, 2196,  220,   51,  509,  109, 8661,  103,
           24,  144,  691,  250,  341,    9,    7, 1406,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 103,   53,  169,    0, 1744,  116,   13,  277,  908,  183,    0,    0,
           69,  752,   10, 2509,  159,  207, 3298, 1573,    0,    8,   13,  277,
          523,  143,    0,    0,   69, 1880, 5917,   85,  637,    0,   53,  506,
            0,  508,  159, 3226,   13,  509, 2333,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,   66,   10,  229, 1123,   53,   11,   57, 3163,   10,   51,    7,
         3348,   12,   33,  453,  753,   12,  108,    6,    0,   13,  753,   17,
           26,  100,  211,  218,    0,   13,  753,   17,  217, 6167,   96,  110,
          333, 1127,  383,    0,   13,  753,   17,   11,    6, 2084, 1625, 1563,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [8084, 3531,  170,   10, 1103,  297,  362,    6,   20,  199,   13,  858,
           12,   70, 6204, 3071,  506,  274,  100,    9,   13, 5591,   35, 1218,
          375,  140, 1011,  179,  206,   94,   66, 2214,   10, 3901, 2465, 5259,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,  220,   51,  143, 9649,  254,   10,   51,   56,   15,  121, 3576,
           62,    9, 6462,    0,   10,   51,    7,  473,   12,  155,   94,   10,
         2056,  155, 1234,    0,   10,   66,  211,  207,   10, 2423,   69,    7,
         6266,   12,    7,   39,  430,  119, 1215,  109, 9237,    7, 4893,   12,
            7,  708,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    7, 5873, 5318,   48,   17,  244,    7, 1830,  215,   53,  162,
         2839,   54, 1723,    6,    0, 1061,  584,    3, 4039,   14,   48,  909,
         2965, 1049,  111,    0,   86,  106, 3611,    0,   67,  106,    7, 4699,
           17, 3611,   26,  999,   55,   25,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 5385,   71,    7, 2184,   59, 1775,  221,   20,    0,   71,
           33,   38, 3794,    3,    7, 4503,  465,   11,   18,   66,    7, 8511,
           12,  129, 2286, 1331,   54, 2184,   59, 1775,  221,   20, 4821,    0,
           38, 3794,  197,    9,   33,  841,   26,   13,  909, 4674,    0,    8,
           13,  909, 4674, 1847,   39, 2987,    0,  166,   26,   13,  211,  500,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:2') tensor([73, 46, 45, 50, 38, 52, 44, 62], device='cuda:2') tensor([1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(80.7225, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[  21,   11,    6,  ...,    1,    1,    1],
        [ 778,    9,    7,  ...,    1,    1,    1],
        [4701,   80,   17,  ...,    1,    1,    1],
        ...,
        [  29,   70,   26,  ...,    1,    1,    1],
        [  53,  192,   11,  ...,    1,    1,    1],
        [  21,   26,  593,  ...,    1,    1,    1]], device='cuda:2') tensor([11, 10, 11, 12, 10, 15,  7,  8,  9,  9, 12, 10, 14, 14, 16, 12,  9, 15,
        21,  9, 12,  8,  6, 10, 11,  4,  7, 15, 14, 10, 10, 10, 11, 10, 12, 12,
        10, 18, 10, 10, 10, 11,  5, 14, 13, 11,  9,  6, 12, 10, 11, 10, 11, 12,
        10, 10, 11, 24, 12, 12, 10, 11, 10, 13, 15, 12, 15,  9, 10, 10,  8,  8,
        10, 10,  9,  9, 10,  8, 14, 11, 10, 10, 10, 13,  7, 12, 10, 13, 13, 10,
        16, 15, 13,  9, 10,  9], device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8301, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8901, 1.0000, 0.8003, 1.0000, 1.0000, 0.8989, 1.0000, 1.0000, 0.8726,
        0.8867, 0.8872, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8013, 1.0000,
        1.0000, 0.8872, 1.0000, 0.8774, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8062,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8101, 1.0000, 1.0000, 1.0000,
        0.8711, 1.0000, 0.8911, 0.8647, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8564, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 0.8848, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8501, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(12.4439, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[  24,  175,  700,  ...,    1,    1,    1],
        [ 851,    0, 1598,  ..., 1262,    0,    2],
        [  29,  339,   11,  ...,    1,    1,    1],
        ...,
        [ 125,   21,  188,  ...,    1,    1,    1],
        [  70,  169,   25,  ...,    1,    1,    1],
        [  19,  116,  465,  ...,    1,    1,    1]], device='cuda:2') tensor([ 7, 16, 12,  7,  9,  8, 10,  7, 14, 15,  9,  9,  9, 11,  8,  6,  9,  7,
         7,  8,  9, 13,  7, 10,  9,  8,  7,  9,  7, 11, 11, 11,  8, 15, 10,  9,
        11,  8,  9,  8, 15,  7,  8, 10,  8, 12,  9, 10,  9,  6,  8, 11, 14, 10,
         8, 12, 10,  8, 14, 14, 12,  9, 10,  8,  8, 11,  6, 12, 10, 13,  8,  8,
        10, 10, 10, 12,  8,  9,  7,  8,  8,  9, 11, 11, 12,  9,  9,  7, 10,  7,
        10, 10,  8, 11,  8,  8, 10,  8,  9,  8, 11,  8,  8,  7, 12, 10, 13,  7,
        14,  9, 15,  9,  8,  8, 10, 10, 10,  7,  8, 10], device='cuda:2') tensor([[  91,   12,    7, 3373,   19, 3395,   62,    0,   10,   45,    8, 6819,
           15, 6760,  918,    6,    0,  162, 1828,   13, 6230,  120,    0,   79,
          963,    8, 3150,  264, 1736,  373,    0,  159,  245, 1269,   34, 8465,
           71,  346, 9414,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  25,   13,   59, 2315,   20,    0,    0,    7,  415, 1124,  160,  271,
            9,    7, 1361,    0,    8,  131,  401,   17,    0,    0,   25,  175,
            7,  370, 5204,    0,  929,    7, 8820, 1623,  693,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 3757,  457, 1006, 1529,   26,    7, 3187, 9225,   12, 8829,  368,
         5465, 2840,    9,   13, 2042,    0,    8,   21,  113,  583,    6,   55,
            7,   56, 9826,   12, 2840, 4166, 1241, 7202, 1006,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  276,   71,   33, 4673, 2394,    0,   24, 6182,   10, 3121,    0,
            8,   24,  323,    0,  746,   12,    0,  467,    6,  132, 7461, 3486,
            0, 1056,   10,    3,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,   24,   66,   10, 1031,  235,    7,   56, 2687,    0,  125,   24,
          296,   10,  175,    7, 4233,  128,  800,   12, 4709,    6,  359,   10,
          175,    7, 2931, 2808,    0,    8,  131, 1031,  235,   54,    7,   56,
         2687,    0,   24,   11,   57, 2983, 2762, 4581, 1588,    0,    2,    1,
            1,    1,    1,    1,    1],
        [1377, 2293,    6,   13,  245,  903, 5575,    0,  166, 1064,  180,  203,
         2329,   96,    0, 1405,   35,  160,  689,   11,   18,  641,  291,  627,
          232,  457,    0,   21,  818, 7013,    9, 6390,   12, 1064,    0,   38,
         1824,    0,   29,   70,   11,    6,   69,    7,  245,  903, 5575,   12,
            7, 2645, 1066,    0,    2],
        [1723, 6068,   26,    7, 3139, 2719,    0,  903,  693,   18,  561,    9,
         1491,    0,    8, 1155, 1754,    6,   84,    0, 1155, 1754,    6,   84,
          125,   21,  689,   11,   18, 3833,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   13,  546,   12,    7, 1644, 1327, 1632,  783,    0,
          166,  818,   38,   22, 2592,  846, 2845,    3,   29,    9,  117, 1666,
         6814,    6,    0,  125,  288, 6814,    6,   66,   13, 1644, 1327, 1632,
          783,    0, 1273,  303,   18,   35, 3794, 5558,    0,    2,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:7') tensor([41, 35, 35, 29, 47, 53, 32, 46], device='cuda:7') tensor([1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(71.7325, device='cuda:7', grad_fn=<MulBackward0>)
True tensor([[  33,   26,  204,   13, 3869,   17, 4974,    6,   85,    7, 2937, 1408,
         7174,   85, 6579, 2519,  670,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   33, 6735,   12, 1396, 2233,  106,  378,  116,   13,
         1466,   37,  199, 1692,   37,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9,  116, 1992,  215,    0,   24, 4311, 1469,  132,   10,    7, 4389,
           12, 6204,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,    9,  409,    0,   24,   66, 2134,   55, 7691,   80,    7, 9270,
           15,   18,  768,   12,   29,  237, 5997,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  33,   26,    7,   51, 4999,  741, 3647,  941, 2394,    9, 4517,    0,
          166,   26,   91,   12,    7, 1849,  608, 3632,    9,    7,  179,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  67,  735,   24, 1566,  371, 1993, 2482,   10, 3858,    0,    0,   71,
          267,  245,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,    9,    7, 1165,   12,    7, 1291,  821,    0,   84,  204,   66,
          226,   91,  109,  248,   17,   66,  226, 1110,    9,    7, 2533,    0,
            2,    1,    1,    1,    1,    1,    1],
        [   7,  279,   80,  283,    9, 2018,   22,  713,   26,   33,    0,   53,
          192,   11,   18,  403,    6,   96,    6,   80,  251, 7806,    6,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2884,   12, 7470,    6,  568,  450,   25,  250,    0,    8,
         1275,   21, 3565,   25,  250,   17,   11,    6, 1226,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 101,    0, 1918,   10,    0,    0,  468,  284,  427,    6, 5294,    6,
           46,   13,  264, 5931,   12,  427,    6, 5294,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   0,    0,    8,   19, 1917,   62, 1117,   12,   17,  563, 7373, 7443,
           89, 4764,    0, 4157,  169,  498,   10, 3424,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   33,   26,   13,  277, 1819,   19,  442,   80,    7,  961,   12,
            7,  417,  727,   96, 1436,  165,   20, 4879,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  25,   63, 4265,   54,   10,  575,   94,   17,   25,  661,  134,    0,
           17,   25, 2990,   71,  134,    0,   17,   25,   11,   57,  461,   12,
            7,  370, 1361,   79,  134,    0,    2],
        [  19,  213,   10, 3499,    9,   89, 1259, 5258,    7, 1890, 1455,   12,
            7, 8022,    6,   46, 4469,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  25,   73, 1157,  629,    7, 3744,    9,  251, 1532,    0,    8,    7,
         1141,  188, 1155,   10,   87,   71, 4289,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,    0, 2138,    7,  245,  785, 1601, 7396, 7833,  111,    0,  752,
           10, 2866,   89, 1259, 1882,  346,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,   29,   24,  487,  826,   80,    0,  238,    0,  138,   87,   24,
          229,  251,  248, 4082,    6,    8, 1393,  134,  199,   91,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6,  347,   43,  588,    6,   26,  115,    9,    7, 4370,
           53,   63,    9,    8,   17,   11,    6,  347,   56,  525,   26,    9,
            7, 4370,   53,   63,    9,    0,    2],
        [ 903,    0,  853,   48,   22,  293,  552,   10,    7,  452, 1539, 1315,
           93,    8,  225, 1945,   62,   17,  225, 3367,  132,    9,  832,  156,
         3056,   20,    0,    2,    1,    1,    1],
        [  33,  987,  607,  445,   26,  736,  439,  442,  132,   12,    7,  225,
          158,    6,   12, 2481,    6, 5110,  266, 3529,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9,  409,    0,  108, 6180,   46,  131,   38, 3392,  197,   19,  641,
           24, 6814,    6,   46,   91,   12,  108,  488, 6180,    6,   34,    7,
          561,   22,  455,    0,    2,    1,    1],
        [  19,  220,  116,  450,   84,   34,   13, 3917,  694,  850,   21,   46,
            9,  333, 3489,    0,  333, 5082,    0,  333, 1375,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   26,  914, 3529,    0, 3706,   17,  117,   66,  226, 3572,   62,
            8, 9304,   62,  850, 4213,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [7894, 2071,  627, 5668,    6, 2027,    9,  108, 1402,  162, 2568,    9,
          564,  895,  895,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:7') tensor([19, 19, 16, 21, 25, 16, 25, 25, 23, 23, 22, 22, 31, 19, 21, 20, 24, 31,
        28, 22, 29, 23, 19, 17], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8286, 1.0000, 1.0000, 1.0000,
        0.8311, 0.8413, 1.0000, 1.0000, 1.0000, 1.0000, 0.8843, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(37.4304, device='cuda:7', grad_fn=<MulBackward0>)
True tensor([[  84,   11,    6,  288,  391,  214,   25,  296,   10,  135,    0,  138,
          261,   94,  229, 1117,    7, 1484,    0,  138,  261,   94,  229, 3494,
          994,    9,   13, 2414, 1201,    8,  138,  261,   24,  229,    9, 2737,
           10,  150, 1179,   24,   73, 3757,   21,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1313, 6249, 8221,    6,   63, 2015, 1197, 2185, 2658,    6,   12,    7,
         1479,    0, 1823,   48,   35, 6665, 3479,    6,    0,   53,   63, 4402,
          442,  132,   12, 2072,  988,    0,    8,   17,   11,    6,   70,   25,
          150,    9,   33, 5674, 1410, 9787,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  487, 1692,   54,    0,  961, 3555,    0,  952,   10, 3698,    6,
            9,    7, 1726,    0, 1520,  159, 6382,    0,    8,  203, 2172,   54,
            0,    8,   19, 3803,   69,  203, 2172,   54,    8,  203, 2172,   54,
           55, 1303,    6,    8, 1822,  856,    6,   55,  244,  785,  215,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,  217,   26,   13, 3280,   12, 6936, 3023,  148, 1505, 7105,
          111, 2091,    9,   33, 6120,   80,  392,  215,  581,  120,   13, 2745,
         1060,  110,   13,  630,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 125,   21,   34,  288,  131,   86,  378,  529,   10, 4075,   17,  225,
           34, 1226,    0,   17, 6229,  220,  508,   13, 1838,   20,    7, 7343,
          225, 1918,   10,  135,   17,  225,   34,  230,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,  288, 3699,  125,    7, 2781,  775, 1026,  307,  373,  144,
           10,  205,  106,  316,   59, 3333,   10, 2902,   20, 3594,    0,  166,
           26,   13,  248,   35,  715,  234, 2365, 3046,    0,   10, 4342,    7,
         3432,   17,  859,   69,    7, 1998,  322,    0,  125,    7,  558, 3432,
         1359,   11,   18,  336,   55,   13,  535,  183,    0,    2],
        [  24,  591,   13, 8319,   22,  148, 1202,    6,   13, 1201,   17,   11,
            6, 2875,   10, 2156,    7,   25,  322,   10,  722, 1192, 3374,   69,
         8549,   48, 3374,  521,  779, 1212,    8, 8549,   48, 4528,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9, 1760,    0,   38,  469,  227, 2174,   37,   85, 1290,   45,  904,
          197,  166,   34, 1466,   62,   79,  171,  458,   37,   11,    6, 3495,
          119, 4546,  777,   20,  430,    0,  284,  772,  283,   46,   94,  169,
          367,  939,  181,  292,   45,  834,   77,  244,    7,  179,   10,  150,
           21,   46,   34,  204,   13,   55,  821,   93,    0,    2]],
       device='cuda:6') tensor([45, 44, 49, 30, 34, 58, 37, 58], device='cuda:6') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:6', dtype=torch.float16)
 > at.  tensor(73.1226, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[   7,  370,  521, 6906,    0,    7,  370,   29,   59, 2405,    0,    7,
          370, 1790,   12,  378,   51,   18, 3304,   62,    8,   86, 5804,   54,
           10, 2307,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  154,    7, 1043,   33, 1429,   29,  238,   26,  125, 1308, 3006,
         1997,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  24,  296,   10,  446, 1081,   12, 9695, 3834,   12, 4619,   17,   63,
          143, 4414,    0,    8,  192,   11,   18,  735, 6430, 2423,   54,  518,
          108, 3916,  199,  108, 5627,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8, 5493,    0,  101,    0,  246,  101,   34,  142,   10,  175, 4532,
           12,    7,    0, 2902,    6, 4775, 1890,   46,   68,  194,   65,   46,
            0,    8,    7,    0, 2073,   45, 4775, 1890,    0,    0,  593,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 115,    0,    7, 3966, 1472,   12,   33,    0,    7,  305,   35,  176,
         2175, 2187,  106,   13,  740, 3238, 4774, 1109, 5412,   26,   33,    0,
           70,   24,  979,    0,   24, 1082,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   0,   67, 1223,    0, 1155,  261,   26,  142,   69, 1325,   25,  498,
            0,   10,  175,    0, 6509,  411,  266, 5408,    0,    0,    8,  131,
           17, 2110,    0,    0,   25,   11,   57,   86,  665,  453,    0,   25,
           11,   57,    0,   86, 1790,  453,    0,   25,   11,   57,   86, 1057,
           17,  261, 2314,    0,    2],
        [   7,  744,  279, 4673,   80,    7, 1579,  371, 1898,  424,  371,   26,
            0, 2027,  199,  801, 1137,   46, 1992,   10, 1536,  215,  801,   46,
          117, 1816, 2924,   11,   18, 2532,  886,  371,  322, 1921,    0,   53,
           11,   57, 2532,  110, 1488,   35,  424,  371,  322, 1921,    0,    2,
            1,    1,    1,    1,    1],
        [   7,  218,  461,   12,    7,  546,   63, 2720, 2261,   17,    7, 2932,
         1381, 1427,   69,   77, 1881, 6101,   69,    7, 2932, 1465,    0, 2134,
           79,   13,  567,   12, 1244,   35,  607,    6,  140,  763, 1386,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:6') tensor([28, 16, 31, 37, 32, 53, 48, 37], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 0.8457, 1.0000, 0.8691, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(70.4159, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[   8,   24,   77,  135,   17,  103,   25,   66,   91, 6270,    0,   25,
           73, 2026,   91, 8971,  131, 4619,    7,  218,  248,  214,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  154,   17,   84,   26, 7033,   13,  264,  321,   12, 1478,   12,
         2895, 6053,   17,   11,    6, 2439,   10, 4504, 1817,  230,  115,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 473,  464,    0,   85,   33,  183,    0,   84,  162,  423, 6504,    6,
            0, 2697, 6214, 1118, 4691, 5056,  709,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 294,   94,  154,    0,   38, 1367,    0,   21,   11,    6, 2930,    0,
            8,   21,   11,    6,  691,  131, 3927,    8,  993,  100,   17,    3,
          238,    0,   53, 9736,   21,    0,   67,    7, 3573,    6,   63,   77,
          691,  131,  874,    0,    2],
        [   7,  218,  207,   26,   17,  155,  572,   26,   86,  288, 1520,    9,
         6445,   19,  362,  300,    6,   96,    0,   25,   11,   57,  113, 1557,
           54,  126,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 1511, 3316,    0,  109,    7, 1511,   12,    7, 1511,   26,  138,
           19,  100,   10, 4576, 1160,   45,  174,   57,    0,   10,  375, 1650,
           46,   53, 1985,  450,  117,  214, 9335,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  245,   12,   77,    0,    7,  245, 2110,   17,   24,   11,  121,
         1110,   12,    7, 1465,   34,   17,   21,   34,  142,   10, 3249, 2930,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,  162, 7037,    6, 4127,   62,   85, 1102,  467,    6,   12,  545,
          595,    0,   79,  103,   24,  162, 6959,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,  101,  492,  469,  982, 6953,  518,   91,   12,    7,  281, 2694,
          366, 6105,    6,  419,  753,  188,  700, 1110,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 101,  278,   13, 8429,    0,    8,  101,   34,  172,    0,  172, 4594,
            0,   29,  101,  172, 1608,   11,   18,  450,  110,   70,   10,   87,
          558,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   19,  154,   21,   11,    6,    9,    6,  221,   20,   10,  703,
           17,   24,  164,  700,  229,   13, 1504,   17,   26, 9519,  111,  110,
          338,   18, 1327,  371,  816,    0,   21,   11,    6,   39, 3756, 2484,
            0,    2,    1,    1,    1],
        [   7,  744, 3743,  106,    7, 1219,   26,   13, 1153,   12, 2060,  636,
            8,  284,  248, 3636,    6,  893,  126,   12,  159,  595,    9,  159,
          447, 1927, 1408,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,    7, 5003,    0,   19, 2020,   89, 1613,   10, 3522,    7, 6462,
            6,    9,  159,  447,  931,  359, 6762,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  17,  941,   26, 5823,   62, 1585, 7990, 1726, 3744,  346,  199,    7,
         6447, 2353,    6,    0, 2173,   54,    9, 2749, 1284,   13,  234,  240,
          290,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  552,  132,   71, 3900,  800, 2703,  432,   19,  204,  465,   11,
           18, 1757,   55,   13, 2322,    0,    8,  513,   80, 1016, 2360,    8,
         6356,   62,   13,  846, 1331,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   0,    8,  138,  261, 1747, 2611,  220,   53,  875,   10, 3848,  103,
           53, 5248,   10, 4996,    0,    0,   69,    7, 2517,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([24, 25, 21, 41, 28, 33, 26, 22, 22, 27, 38, 29, 21, 27, 31, 23],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(46.3429, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[ 29,   0,  19,  ...,   1,   1,   1],
        [ 29, 138,  63,  ...,   1,   1,   1],
        [ 91, 279,  19,  ...,   1,   1,   1],
        ...,
        [  8,  53,  77,  ...,   1,   1,   1],
        [ 68, 194,  65,  ...,   1,   1,   1],
        [ 33,  26,  13,  ...,   1,   1,   1]], device='cuda:4') tensor([19,  8, 26, 17, 18, 20, 15, 22, 22, 27, 20, 16, 36, 27, 19, 21, 26, 18,
        19, 23, 21, 15, 32, 20, 21, 22, 16, 28, 21, 22, 18, 20],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8721,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8657, 1.0000], device='cuda:4',
       dtype=torch.float16)
 > at.  tensor(32.1468, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[  29,   70,   10,   87,    0,  238,    0,   39, 1141,  659,  367,  106,
         1560,  121, 3165,    8, 2185,  502,    6, 1574,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    0, 1296,   10,    0,   87,   33,    0,   21,   11,    6,   13,
          325,   12, 8512,    8, 3236,   17,    0, 1202,    6,    9,   13,    0,
          133,  747,  203,  779,  675,  793,    0,    2,    1],
        [  19,  246,    0,   38,  242, 1865,   20,   25,  162, 2188,    0,   21,
           34,  116,  155, 3181,    8, 3806,    0,  109, 3181,    8, 3181, 3328,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [ 440,    0,   21,   26, 1970,    6,    0,   86, 1276,    6,    0,  148,
         2508,    7,  281,  283,   35, 5203, 3382,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,   13,  324,  279,    0,  125,    9, 1296,   10,
          229,  995,   13, 1874,    0,   25,   66,   10, 2484,   80,   21,  245,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6,   13,  341,  561,   10,   51,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 4460,   12,   13,  384,  372,   93,   62, 8093, 8187,   26,   86,
          142,   10, 6264, 4032, 5099,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 6532,    7, 5408,  338, 1344,   71,   39, 6291, 5108,   12,  655,
            3, 1086,    9, 1565, 3979,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   11,    6,  276,  143, 1248,   26,   21,  164, 3162,  126,   13,
         1850,  709, 6649,   69,    7, 4057,    0,  125,   21,  135,    6,    7,
          207,   12,    7,   21,  510,    0,    2,    1,    1],
        [   8,   79,    7, 9448,  480,    6,   12,  117, 3522, 1118,    0,   24,
           66,  159,  211,  424,   48,  266, 2011,  415,  234,    6,   54,  359,
          108,  447, 2200, 2374,    0,    2,    1,    1,    1],
        [ 238,    0,  274,   85,  185,  801, 1081,    0,  185, 1081,   17,   24,
           11,   48, 1220, 6356,   62,    0, 1696,  140,  292, 4635,    0, 1696,
          140,  292, 4635,  172, 2244,    6, 4232,    0,    2],
        [  24,  144, 3181,    6,  148,  169,  367,    9,    0, 3636,  188,   79,
          322,  424,    0, 6108,    6,  132, 6686,    9,  415,  656,  292, 4117,
            6,  333, 2223,    0,    2,    1,    1,    1,    1],
        [ 115,   17,   19,   11,  121,  691,   17,   46,   19,   11,  158, 1945,
           21,    9,   13,  765,   46,   19, 1412,  450,   25,   17,   19,  217,
           39,   13, 2553,    0,    2,    1,    1,    1,    1],
        [  67,    7, 7552,   17,    7,  942,  188,  442,   10,    7, 1136, 1909,
           26, 4050,    0,    8, 1120,   39, 1670, 1478,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   67,  218,  825,   53,   11,   57,    0,   86,    0,    8,    0,
           25,   66,  288,  427,  174,  541,  233, 1611,   55,    0,  341, 5204,
            6,    0,    2,    1,    1,    1,    1,    1,    1],
        [5385,   13, 3495,  998,    0,  860,   79, 7286,  109, 3717,    0, 2475,
         6414,    6,   21,    6, 4401,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  703,   17,    7, 1880, 5917,    6,   63,   13, 1412,   55, 1777,
           10, 6433,   21,    6, 2247,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  279,   73,  388, 4743,    3, 4997,    6,  690, 3350, 3298,   39,
         5414,    6, 3516,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    0,   53,  246,   10,    7, 5410,    6,    0,   38, 5034,    0,
          108,  415,   59,  338, 4482,    6,    0,   63,   39,    0,    0, 5441,
         1303,  424,   57,    0,    2,    1,    1,    1,    1],
        [   7,  288,    0,  279,   17,   53,   73,   11,   18,    0, 4585,   26,
            7,    0, 4129, 3585, 2908, 6649, 1241,   17, 1472,   12, 1719,  293,
          356,    0,    2,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  162,  142,    9,    7,  595,  270,   10,    7, 8145,    0,
            0,   24,  162,  826,    0,  347,   34,   33,    0,   29, 1894,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   86, 5424,   62,    0,   17,   11,    6,   13, 1931,
            8,   21,   11,    6, 1897,   69,    7,  876, 6106,   22,  128,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   53,  144,   39, 1248, 3473,    0,  166,   34,   53,  465,   11,
           18,   66,   10,  229,  419,  839,  106,   21,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  229, 6730,    6,    8, 1560,  300,  901, 8018,    6,   12, 6027,
            0,   24, 3065, 3285, 6443,    0,   24,  135,   17,   24,   63,  230,
            0,    8,   53,   63, 1226,    0,    2,    1,    1]],
       device='cuda:7') tensor([22, 32, 26, 21, 26, 10, 19, 19, 31, 30, 33, 29, 29, 22, 27, 20, 19, 17,
        29, 27, 25, 25, 22, 31], device='cuda:7') tensor([1.0000, 0.8369, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8042, 1.0000, 1.0000, 1.0000,
        0.8306, 0.8911, 0.8936, 1.0000, 1.0000, 1.0000], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(40.3585, device='cuda:7', grad_fn=<MulBackward0>)
True True tensor([[ 166,   26,  347, 2290,  233,  824,   59,  338,  650,  583,    6, 1017,
          187,    0,   38,  469, 3920,  290,   20, 2992,   12,    7,  858,    3,
           21, 3565,  170,   80,  159, 1406,    0,   67, 3412,  369,   12,   13,
         3413, 3565,  170,   21,   11,    6, 1254,   55,  170,   10,   66,   13,
          535,  858,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [3761,    0,  103,   33,   26,  230,    0,   24, 1412,  203,  500, 1547,
            0,    7, 1880,    8, 1519, 7153,    6,    0,    8,  509,   87,    0,
           21,   71,   13,  102,  794,  378,    0,    9,    0, 1206,    0,    0,
          100,    9, 4748,   85,  969,    6, 3706,  929,    7, 8869,    6,    0,
          109,    7,  811,  140, 7710,   12,  596,    8,  848,  236, 1486,   18,
            6,    0,    2,    1,    1,    1,    1],
        [  67,   19, 1932,   10,   89,   13,  500,   18,  120,   19,  278,   17,
         3493,    8,  246,    0,   38, 1969,  135,    0,   19,  154,   33,  116,
          818,   17,   55,    7,  245,  183,    9,   89,  282,    0,  378, 1714,
          188, 4744,  172,  238,    3,   29,   19,  465,   11,   18,   66, 2214,
           10,   17,  813,  125,    7,  837, 5002,  336,  110,  465,   11,   18,
           66, 2214,   10,   17,  813,    0,    2],
        [ 115,    0, 5873,  296,   10,  229,  333, 3916,   10, 8711,   17,   39,
         7681,   17,  188,  226, 3498,   10,   51,   51,   22, 1987,  266, 1010,
           56, 1628,   13, 5895, 3677,   26, 8509,   10,    7,  461,  266,  763,
          480,    6,   12,    7, 3677, 1042,    7, 3677,  188,  226, 3768,   48,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  29,   55, 4212,    0,    7, 1650, 9787,   26,  923, 1052,  491,   18,
            0, 1120,  690, 2470, 3080,    0,    8,  419, 1289,   17,   19,   79,
          779,  609,  436,   71, 9787, 5953,    7,  370,  207,    0,  860,   79,
           38,    6,  500,    6,  307,  197,   46,   13,  133,  851,  266,   93,
         1455,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   8,  144,  860,   46,   21,   34,   39, 2576,   35,  781,   15,   54,
         1064,   10, 1510,  134,  456,   80,    7,  179,   17,   26, 1094,   10,
          367,  359,  639,    8,  948,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   48,    0,    0,  116,  570,   10,   51,  388,    9, 1416,
           12,  185,   12,    7,  281, 1248,  954, 1118,    0,    8, 6264, 1118,
            0,    9,    0, 1491,    0,    0,   38,  511,   13, 3087, 1065,    0,
         1366, 1583,   48,  110,  132,    8, 1742,  110,   33,   13, 1820,    0,
           19,   11,   45,    0,  168,    0,   29,    0,   89, 3013,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [3379, 5486,  412,   45, 1115, 1449,    6,  340,    0, 3379,   63, 5053,
           62,    9,    7, 2059,    9,   91,  561,   55,  294,  422, 2042,    6,
            0,   67,  103,   24, 3474,  108, 3064,  106,    7, 2591, 2023,   10,
            7,  830,  200,  236,    6,    0, 3379,  873,  133, 4889,  577, 1611,
            0, 1944,    8, 2288,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([52, 63, 67, 50, 51, 31, 60, 54], device='cuda:0') tensor([1.0000, 0.8687, 1.0000, 1.0000, 1.0000, 1.0000, 0.8716, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(105.2890, device='cuda:0', grad_fn=<MulBackward0>)
True tensor([[3111,    7, 1732,  465,   11,   18, 2320,  923,   29,   51,   22,  236,
           22, 3903,    0,    8,  120,  225,    9,    6,  365,   62,   17,   19,
          150,  156, 2519, 2303,    0,   19,  316,  111, 1661,  297,   62,    0,
            8,  166, 4075,   48,   10,   51, 4113,  800,  248,    0,    2],
        [   8,   29,  103, 2439,   71,   13,  555, 2960,   12,  170,    9,   33,
          964,    0,   24,   73, 7647,   10,  826,   80, 7053,   54,    9,   33,
          341,  688,    0,  180,   24,   73,   16,  287,   17, 1417, 2041,   19,
           34, 6037, 1069,   10, 3928,    0,    2,    1,    1,    1,    1],
        [  89, 3965,  440,   26,   10, 1914,  126, 1004, 7806,    6,    8, 7750,
          694,  826,  199,   33,  488, 2469,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101, 1559, 5990, 1323,    6,    0,  889,    8,  596,    0,  199,    7,
         2260,    0,    8,  101, 2700,  134, 3611, 1256, 2469,    6,   10,   66,
         1025,  101,   32,   96,  134,  132,   10,   13, 1116,  111, 8068,   29,
          101,   73,  150,  134, 4015, 3611,   62,    0,    2,    1,    1],
        [  53, 6960,   48,   17,    7,  207,   94, 5973,  185, 1760, 1361,   12,
         1573,    0,   34, 6027, 1490,    9, 1218, 6357,    0,    0,    0,   71,
            7,  207,   53,    9,    6,  365,   62,   69,  378, 5973, 1574,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115, 2283,    0,   79,   21, 2546,  436,    6,  270,    0,  138, 2117,
          155, 5897, 9023,    6,    0, 1239,  115,    0, 2283,   21,   79,   21,
         2546,  436,    6,  270,  554,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  432,   13, 2767, 9028,   12,    0,  155, 3612, 6385,  567,    0,
            7, 2116, 1149, 3320,   71,   13, 7069,  874,  266,    0, 1034,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101,  591, 2869,  133,  528, 4506,    6,    0,    8,   29,   24, 6631,
         1011,   33,    8,   24, 2721,   17,   33,   26,   13,  207,   17,  172,
         1429,   55, 3101, 3001,   35, 1966, 1265, 4506,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33, 1321,  133,  341,  254,  419, 3839,   69,  984,   17,   19,   11,
          121,  700, 1110,    0,    8,   19,   11,  121, 1110,   13,  325,   12,
         3839,   69,  984,    0,  703,  110,    0,   68,  194,   65,  274,   85,
           33, 3189,    9,    7, 1403,    0,    2,    1,    1,    1,    1],
        [  21,   34,    7,  403,    6, 3211,   71, 3039,    0,   13,  140, 1355,
         3471,  187,   62,  131,    7, 4401,   12, 1519, 3473, 2551,   54,  336,
            7,  179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,    0,  125,    7,  572,   26, 5802,   54, 2015,
         1428,   12,  214,   17,   63,    0,  415,   59, 8186,    9,  277,    0,
          765,    6,   12,    0,  183,    8,   17, 7010,    0,   10,   91,  486,
            9,  277,  765,    6,   12, 2786,  366,  183,    0,    2,    1],
        [  67,  244,    7,  501,  895, 1047,    6,    8,   11,    0, 1536,    6,
            0,    0,    7, 2008, 6248,    0, 1381, 1446,   53,  692,   10, 6896,
         4174, 1115,    0,    0,    8,   29,    0,   70,   53,  323,   34,    0,
           53,  487,   10, 2750,  596,  199,    7, 9824,    0,    2,    1],
        [   0,   13,  555,    0,    0, 2960, 2602,    0,  755,    0,    9,  383,
          741,    0, 1851, 1353,    0,    0,  109, 6893,    8, 1106,  233,  174,
          234, 1106, 2836,    0,   53,  144, 3695,   12,   70,   24,    0, 2807,
           10,   51,    7, 8968,   55, 2786,    0,    2,    1,    1,    1],
        [  19,   34,  133, 6011,  120,   19,  487,   10,   87,   33,   10,  150,
           17,    0,    9,  409,    0,  276,    7,   94,  148, 9235,   48,    7,
          368,   12, 1752, 2856, 1054, 5786,  131,    7, 3973,  144,   22,   11,
           18,  204,  691,   17,    0,    2,    1,    1,    1,    1,    1],
        [  21,   26,  108,  688,    8,   86,  108, 2072,  687,   17,  664, 2836,
           15,    6,  170,    0,   38, 3328,  583,   10, 2332,  440,   26,   17,
           24,  296,   10,  175,  244,  108, 1728,   12,  138, 2583, 9646, 1894,
           24,   73,   51,    9,  545,  218,   11,    6,  931,    0,    2],
        [  91,   12,    7,  214,   17,  956,  432,   17, 1064,   26,   17,   19,
          474,   80,    7,  218,  889,  148,  162,    9, 2536,  140,   37,  922,
         1585, 1763,   12,  110,    0,    8,  138,  261,   19,  692,   10, 1452,
           33,   71,  134,    0,    2,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([47, 43, 20, 45, 37, 31, 25, 35, 43, 28, 46, 46, 44, 42, 47, 41],
       device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8789, 1.0000, 0.8970, 1.0000, 1.0000,
        1.0000, 0.8662, 0.8765, 0.8071, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(64.6848, device='cuda:0', grad_fn=<MulBackward0>)
True tensor([[  24, 1621,   86,  288,   39, 2987,    0,    0,   67,   85,    7, 1740,
           12,   33, 2987,   24,    0, 1621,   13, 4601,    0,    0,  672,    0,
            0,    8,   33,  672,   34,   80,    7,    0, 1064,    0,   12,   13,
         4328,    0,    7, 1064,   12, 7376,    8,   12,  540,  687,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,    7, 8088,   54,  279,   26,   17, 4623, 2050,  373, 2523,   66,
          226, 5037, 4031,   17,   63, 2414,   10,  108, 1715,    0,    8,   53,
           11,  121,  591,   17,   13,  800,   12,  134,    0,  120,   53,   11,
           57,   80,    7, 1137,   12,  108, 1715,    0, 5368,   15,  131,   13,
          409,  240,   12,   79,  261,   79,  423,    0,  689,   11,   18,  473,
           55,  133,  535,    0,    2],
        [ 545,  188,  822,    0, 2683,  964,    6,    0,  206,    7, 8869,    6,
            0,  596,    0, 1585,   71,  963, 2352,    8, 4478,    0,  185,   79,
          963,   79, 1646,  215,  801,    0,   63, 6324,   10, 3284,   18, 1617,
            7, 7009,    6,    0, 9695,  134,   10, 1713,  143,  896,    8, 8939,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,    7, 5407,   12,  961,   13, 2688, 3860,   59,   55,   13, 6049,
         3367,  199,   70, 1505, 2134,   79,    7, 1465,   12,  214,    0,  166,
           26, 2318,   39, 2735,  115,   71, 4960,   19,  362, 1838, 1428,   55,
          958, 1006,    0,  941, 7824,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 185, 1075,   46,  305, 6124,    0,   39, 4174,  768, 6503,   71,   13,
         2127,   86,   17,  341,  106, 5200,  699,  241,  352,   11,    6,    0,
          131,    7,  207,   46,   91,  383,  473, 9472,    0,  278, 1759,  820,
          439,   12,   77,   21,    6,  941,  106, 8307, 2791,    0, 2248,  111,
         2626,    8, 2678,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,   63,  133,  555,  214,    0,  133,  555,  214,   17,   25,   73,
          172,   87,   17,  164,  468,    7,  207,   17,   25,   73, 2644,  117,
         1587,   12, 4390,    8, 1064,   70,   19,  169,  583, 3849,   13, 1395,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   39,  276,  143,  528,  774,   12, 9063,  271,   55,  565,    0,
           17,   56,  111,   22, 4014, 3570,  650,  492,  220, 3890,    0,   34,
           13,  570,   12,   46, 3524,   46, 7683,    0,    8, 1790,  126,   70,
         2275,  372,   59, 1563, 2028,   12,  282,   73, 2231,   79,   13,  926,
         2653,   10,    7, 3462,  687,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 148,  220,   66,  934,   48,   17,   13, 1127, 1369,   12, 2951,   57,
          200,   35, 3794, 6814, 1074,    9,    7, 6356,    6,   12,    7, 5495,
          179,  169, 7831,  199, 2955,  634,   18, 3772, 5869,   12, 2232, 2399,
            8, 2434,    7,  133, 5495,    6,   53, 1412,   66,  903,   20,  556,
           62,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:1') tensor([48, 65, 50, 43, 53, 38, 55, 51], device='cuda:1') tensor([0.8169, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(92.1265, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[ 148,   63,   77,   12,    7,  218,  596,   69,  117,  255,  945, 2696,
            6,    0,   19,  591, 6087,   93,  122,  551,  237,  820, 1132,  895,
          828,    0,  225,  246,  225,   34,   13,   38, 2470,   22, 2088,  148,
           26, 1767,    8,  126,  606,   54,    3,  225, 2884,   62,  267, 1329,
           79,   38,  153, 1040,   37,    3,  225,  246,  225,   26,   38,    6,
          233,  111,    0, 2292,    8, 1751,  111,    3,  225,  100,    6,   10,
          229,   94, 4265,   38,  128,  959,    3,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   85,    7, 1565,   12, 5604,   54, 1222,   13,  277,  523,    0,
          120,   19,  274,   85,   70,   11,    6,  142,   69,   71, 1578,  639,
          117, 1113,    0,   24,   63,   86, 3983, 3019,  359,   71,   33, 3046,
            0,    8,  120,   19,  274,   85,   70,   26, 1592,   10,  108, 8281,
            8,  108, 4621,    0,   89, 1127, 7034,   26,   17,   24,   13,  160,
           11,   18, 1110, 1155, 1094,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  17,   26,  347,   25,    0,    9,  155,  294, 4159,    8, 9034,    0,
           63,   13, 3764, 1986,  365,    0,   13, 1986,  365,   17,   11,    6,
         4690,  111,  341,    9,  155, 2395,  572,    9, 3201,  254,    7,  572,
           12,   39, 2216,  736,  215,  581,    0, 4126,  111,  341,    9,    7,
         3201,    6,  106,    7,  572,   12,    7, 2313, 2216,    3,  215,  581,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,   19,  213,   25,   10,  934,   17,   33, 2088, 1603,   26,
         2952,    0,    8,   21,   11,    6, 6487,    0,    8,   21,   11,    6,
         4469, 1584,    0,    8,   21,   11,    6, 9101,    0,    8,   21,   11,
            6,  985,  687,    0,    8,   21,   11,    6,    9,  191,    6,  429,
            0,    8,   21,   11,    6, 9059,    0,    8,   21,   11,    6, 2549,
            0,    8,   21,   26,    9,   18, 2738,  366,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3524,  109,  218,   46,  635,   91, 9737,   62,  486,    0,   24,   11,
          158,  492,  135, 1456,  347,   46,   67, 1042,   53,  278,   13, 9745,
          336,  134,    0,   53,  162,   77,    9,    7,  370, 9745,    0,  115,
           77,    7, 4488,   35,  140,   57,  922,  876, 5521,   12, 4618,    0,
           77,    7,  453,  687, 1621,  131, 8218,    0, 1917,    6, 4775,   62,
         1117,    7, 9745,    8,   24,   11,  121,  278,   13, 1629, 3438,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   26,  824,  174,  779,  527,    0,    0,   91,   12,    7, 1881,
           17,   33, 8223,  140,  338,  525,  128, 6688,    0,  447,   62,    0,
            0,    8,   39,  916,  279,   80,  824,  174,  779,  527,    0,    0,
           26,    7,  655,   35,  737, 2109,  447,   37,   12,   33, 1850,   62,
            0,   13, 1329,   13,   48,  352,   18,    0,    8,   33, 1329,   13,
           48,  352,   18, 4223,   62,   91,   12,    7, 7577,    0, 2348,  106,
            7, 3141, 3928,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,   66,    0,   13,  277, 1964,   69,   89,  664, 6729,    0,
            8,    0,    0,   21, 1119,    0,   38,  242, 1027, 1792,   19,   11,
           45,    0,    7, 3806, 1251,    0,   17,   11,    6,  347,    3, 4701,
            0,  211,    0,  143, 7052,    0,    0,   38,  187,   11,   45,    0,
            0,    0,    7, 3806, 1251,    0,    0,   25,   11,   57,   86,    0,
         2456,    3,    0,    0,    0,    8,   84,   11,    6,    0,  825,    0,
           55,  663,    0,    0,    0,  103,   89,   29,   22,   11,    6, 1431,
         1516,    7, 2657,  985,    0,    7, 2918,    6,   66,   10,  135,   25,
          192,   11,    0,   18, 1396, 1004,   33, 1375,    0,    2],
        [  70,   33, 1221, 3556,  170,   46,   86,  116,   89, 2260,    0,   67,
          108, 6142,    0,    8, 6579, 2519,  670,  143, 6902,   46,   26,   17,
           24,   66, 3764, 2791,    9,   13, 1027,  631,  407,   55, 3001, 5605,
            0,   17,  108, 1740,    0,  166,  188, 1108,   62, 1645,  143, 1167,
         5152,    9,   13, 2931,  207,  254,  419,  218,    0,  492,  442,   91,
           12,   21,    6,  447,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([ 80,  67,  62,  70,  73,  77, 106,  66], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8936, 0.8052, 1.0000],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(115.9916, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[ 115,    0,  101,   34,   13, 6791,  903, 3902,   18,  365,    0,    0,
            0,  101,   34,   13, 6791,  903, 3902,   18,  365,    8,  101, 5523,
           84,   26,   13,  785,   35, 1625, 1710,    0,  166,  188,   39,  811,
         4674,    0,   13, 8092, 2332,    0,   13, 1747,  389, 7988,    0,   13,
         2322,   54, 2332,    8,   13,   58, 5594,   20,  445,    0,  166,   26,
            7,  291,  371,  121,  810,  109,    7,  203,  779,  675,  793,   12,
            7,  546,    0,    2,    1,    1,    1,    1,    1,    1],
        [  25,  659,   86,  154,   12, 3139,  424,  233,   79,  378,  198, 1558,
          300, 1032,    0,   67,   55,  110,   21,   34,  198, 1558,  300, 1032,
            0,  125,   19, 4776,    0,  116,   79,   19,   34, 1106,   57,  119,
          810,   71,   33,  524,   46,   19,   34,  740,    9, 1319,   35, 7469,
           22, 1146,    0, 4402,    0,   85,    7,  183,   46,   19, 4776,   17,
          333, 1319,   35, 7469,   22, 2593,  958,  283,   37,   17,   19,   34,
          740,   71,  144,   13, 3139,  424,  233, 3078,    0,    2],
        [ 115,   19,   11,   45,  142,   10, 2797, 2117, 2869,  143, 3676,    6,
           17,  164,   51, 4501,    9,   56, 9338,  155, 1370,    8, 3557,    0,
          109,  103,   25,   11,   57, 4313,    0,  138,   25,  506, 8863,  155,
          447, 4313,    8,  415,  500, 1783,   54,   12,  963, 2245,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2901,  245,    0,  180, 2455,    0,   24, 2859,   62,    0,  452,   48,
           35, 2917,  429, 1511,  244,    7, 3549,    0,  180, 2496,    6,  244,
            7, 1465,    0,  321,   12, 2285,    0,    8,  180,    7, 3901, 1583,
         7747,    0, 2901,    0, 2455,    0,   56, 7263,    0, 1228,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  172,  916,  120,   25,  780,    0,    8,    0, 1531,
            0,   13,  207,   10, 2872,   13, 2872,  371,  340,   54, 3182, 1405,
          518, 2288, 1412, 4117,    6,    0,   68,  194,   65,    0,  339,  110,
          450,    0,   25,   17,   84,   11,    6,   86,  593,  294,    0,   94,
            0, 2091,    9, 2516,   54,    9,   17,    0,    0,   86,  276,    7,
            0,  427, 6034, 1167, 4445,    0,  148,   24,   11,   48, 5443,   80,
            3,  584,  759, 1086,   55,   85,   17, 2110,    0,    2],
        [  89, 4856,    0,  255,  121,    0,    8,   19, 1570,  244,   10,    7,
         1396,    6,   12,    7, 1930, 3449,  458, 4910,   10, 6682,   17, 3486,
           71,   29,  294,  218,   94,    0,    8,   19, 1608,   11,   18,  601,
           67,  154,  138, 1019,   24,  552,  336, 1979,  122, 1630,  230,    6,
            8, 1094,  138, 1019,   24, 1918,   10,  205,  336, 2787,   12, 6535,
          369,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,  220, 1393,  134,  132,  109, 1393,  134,  346,    0,   94, 4600,
          134,    0,   67,    9,    7, 5970,    6,    0,    7,  941, 3741, 7602,
            0,    8, 4597,  373,  487,  826,   80,  138,   10, 2554,  941,    0,
           29,   70,  956,    0,    7,   59,  626,    6, 3668, 5125, 1446,   10,
         2699,   13,  264, 1396,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,   73,  154,   12, 2932, 5743,    6, 1645,   79, 1291,   22,  241,
          505,    0,  746,   12,   39,  985,   35, 4797,  279,    0,  230,    0,
           68,  194,   65,  206, 1288,  106,   91,  723,   73,   51,  415,  777,
           62,    8, 3411,  652,  922, 1004,    7, 1711,  567,    0,   17,   84,
           73,   51, 1986, 1181, 3479,    6,   12, 2932,  896,    0, 3188,   54,
           69,    7, 2353,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([76, 82, 48, 48, 82, 63, 54, 65], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8784, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(113.3477, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[  10,  388,   21,  133, 1948,    0,    7, 6730,   29, 1019,  188,  226,
           17,  103,   25,  508,   94,  890, 2273, 6053,    0,  103,   25,  508,
          134,  890, 5259,    0, 3226,  164,    9,   20,  879,   18, 3042, 2679,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,  391,  214, 3803, 1028,  132,   55,  333,  524,    0, 6664,    0,
         5691, 4140,    8,    7,   94,  148, 1202,  134,   46,    7, 3348,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,  120,   25,  175,   10,  227, 6085, 6106,    0,  204,    0,   21,
           73,   51,  250,   12,   39,   39,  816,  297,  424,  505,    0,  125,
           84,   63,    3,   94, 1074,   84,  148,   63,   86, 5607, 1719, 3475,
         1811,   25,   79,   25,   11,   57, 1028,  199, 3594,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  13,  325,   12,   94,   71,  452,  265,  192,   11,   18, 1570,    0,
           67,   89, 1848,  465,   11,   18,  703,    9,   38, 3760,   11,   18,
            3,   89, 1970,   11,    6,   81,  587,   34,    0,   38, 1969,   73,
           87,   21,    0, 1586,   25,   73,   73,    3,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   17,   26,  347,   19,  213,   10,  456,   10,   25,   80, 1729,
          290,  846, 2956,    0,  125,   19,  703,   24,   63,   13,  453, 8098,
            9, 8969,   54,   13, 2805,   55, 2394,  929, 4067, 3399,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,   11,  158,  229, 1123,   21,   11,    6, 3008,   48, 8289,    0,
           38, 9920,  159, 8460,    6,   66, 2107,  132,  248,   35,    0,  391,
           35,    0,  717,  181, 1178,    8,   53, 1531,   21,  126,    0,  125,
           21,   11,    6,    7,  245, 8370,   48, 2212,   53,   11,  121,  144,
            9,  159,  282,    0,    2,    1],
        [ 103,   25,  229,  143,    0,  254,    0,    0,  655,    3, 6479,    6,
           13,  464,    0,    9,    7,  832,    0,    6,    0,    0,    0,  419,
         2263, 1020, 2244,   25,   11,   57,  142,   10, 1064,  164,   66,  288,
            0,   13, 2768,    0,    0,    0, 2768, 4455,   69,  155,  244, 1021,
          238,   35,  242,   54,    0,    2],
        [9026,  131,   33,   89,  781,  266, 1466,    0,   89, 1751,    8,   19,
          144,   33, 3310,  479,    0,  339,   11,    6, 2033,  108,  743,   85,
           13,  341,  824,    6, 1090,    9,   13,  341, 1503,  545, 1303,   12,
          883,  424, 5089,    8, 1452,  251, 1352,   69,   13, 4813,    0,    2,
            1,    1,    1,    1,    1,    1]], device='cuda:3') tensor([38, 25, 47, 45, 37, 53, 54, 48], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8237, 1.0000],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(74.0402, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[ 103,  185,   12,   70, 5070,    6,   94,    9,  108, 4621,   10,  229,
           77,   12,    7, 4388,   24,  229,  162, 3474,   62,   10, 4621,    9,
          166,   94,   66,  593,  555, 4387,    6,    0,   86,  288,  169,  251,
           94,   11,    6,  931,   51, 3480,   48,    0,   67,  108,    6,  169,
           51, 3480,   48,  113,    0,  166,   26,   70, 5767,    6,  583,   13,
           38, 1803,   20,  412,   35,  389,  927,  586,   54,  954,    3, 4232,
         1665,  365,  338, 1337,  793,  164,  229, 1459,  509,  518,   46,   86,
          116, 1714,   94,   46,  125,   12,  138,   77,   33, 9733, 2573, 1764,
          652, 1317,    6,  170,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,   73, 8794,   20,  111,  450,   25,   17, 1267,  411,  458,  128,
            0,  728,  158,   35,  287,    0,  775,   20,  727,   12, 9779,    0,
         1492,  156,    0, 3997, 4999,    0, 1747, 2142,    0, 5183, 1353, 1027,
            0, 1487, 3174,  388,  901,    8,   10,  424,  412, 1406,   20,   87,
           86, 3715,  359,   13, 6069,   17,   11,    6,  415,  233,   62,  132,
          850,  155, 1729,  502,   96,   17,   11,    6, 3463,   10,  367,  126,
           39,  109,  597, 2658,    9,  155, 1922,  119,    8,  851, 3304, 2946,
            7, 1956,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,    7,  753,    9,    7,  179,   71,    7, 6100,  837, 2694,    0,
            7,  800,   91,  753,   69,  837, 2694,   26,  264,  742,   20,  128,
          511,    0,   68,  386,   65,  238,  691,    0,  492,  226,    0, 1412,
          205,    0,   68,  194,   65,    7,  753,   71,    7, 1686,  837, 2694,
            0,   19,   11,   45, 4701,   10,  289,    0,   26, 2346,   48,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7,  841,    7,  841,   12, 1057, 4311,  106, 2027, 1117, 1835,  126,
          199,    7,  203,  121,  237,  271,    0,   10,   66, 1565,   62, 1835,
           55,  250,   17, 4639,   10, 1109, 1102, 1117,   25,    8, 1019, 2816,
           25,    0,    8,   17,  434,   25,  270,    9,    7,  467,   10,    7,
          288, 2397,   25,  220, 2679,    0, 3045,   79,   25,  323,    0,    9,
          155,  883,  122,    6,   12,  570,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  24,  205,    9,    7, 7373,    6,   71,    7,  269,  290,   46,   24,
         2677,  159, 9060, 3183,   46,   21,   11,    6,  976, 1248,    0,   21,
          598,    6, 1968,    0,    8,  180,    0,  509, 1094,    0,   24,   11,
          121,  278,  108,  447, 3479,   12,  269,  290, 4656,    6,  737,  373,
            0,  108,  447, 3222, 2083, 1019, 1367,    0, 1339,  783, 2734,  741,
            0,  148,   73,  305,   13,  488,  269,  290,    8,    9,   91, 5563,
            0,  388,   21,  199,   39,   56,   15,  121, 3576,   20,   12,  563,
            0,   29,   17,   24,   73,  204,  283,   71,    7,  269,  290,    8,
         1082,    7, 4814,    6,   21, 1847,   10,   86,    9, 3196,   57,   33,
         1390,  148,  492,  150,    6,   13,  853,  569, 1020,    9,    7,  985,
         1580,    0,    2]], device='cuda:3') tensor([102,  88,  61,  67, 123], device='cuda:3') tensor([1., 1., 1., 1., 1.], device='cuda:3', dtype=torch.float16)
 > at.  tensor(179.6561, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([1.0000, 1.0000, 0.8237, 1.0000, 1.0000, 0.8369, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(78.3327, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[   0,   19,  144,    7,    0, 2212,   10, 2115,  336,    0,    7, 1291,
           45,  603,  603,    0,  427, 2984,  430,    9, 2008, 1146,   71,    0,
           39, 3698,    9, 1031, 1967,  541, 3379,    0,    2,    1,    1,    1,
            1,    1],
        [   8,   12,  538,    0, 1042,   25,   66,  264, 1893,    0,  276,    7,
          801, 6878,    6,    0,  276,  155,  637,  873,  250,  341,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,    7,    0, 1043,  159,  813,  188,  419, 2070,   26,    0,    0,
          238,    0,  125,  211,   91,  994,  188, 2214,   10,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   0,    0,    8, 3108,  144,  246,    9,   39, 3836,   17,   24,  451,
            0,  172,  875,    0,  250,  264,    0,   10, 1366,    0,  250,   17,
           24, 2775,   11,   18, 3498, 2307,    0,    2,    1,    1,    1,    1,
            1,    1],
        [  69,   33, 1472,   12, 1780,   26,    7, 1261,    7,  207,   19, 1385,
           21,  106,   89, 1067,  971,  455, 1431,    8, 1037,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  108,  630,  172,   34,    0,   87,  108, 1752,  352, 4041,    6,
         7420,  138,   24,  154,    8,  598,   80, 1469,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 169,  378,   13, 1396, 4988,   48,  106, 7801,   55,   13,  555, 4299,
          131,  378, 4744,  131,   10,  861,  229,   13, 1878,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  101, 2770,   33,  567,    0,   67,   21,  172,  465,   11,   18,
          283,    0,  125,  284, 7393,  162, 5785, 4777,   54,  545,  218,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19,   11,  121, 1621,   13, 2695,  434,   38, 2366,   10, 1303,  197,
            8,   19,  703,   21,   11,    6,  142,   10,  468,    7,  207,   25,
          274,   85,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2993,   51, 7989,    0,   38,  511,  180,    0,   12,  538,    0,    7,
          558,  279,   19, 1510,   26,    3,   19,   11,  121,  442,   21,   10,
            7, 1219,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  103,   25,   11,   57,    0,    9,    7,  417, 1236,  556,  240,
          589,    0,    0,    0, 1052, 8374,  371,  626,    0,   25,   11,   57,
          142,   10, 2034,   13, 1335, 1273,  303,   18,  434,   13, 2973,   93,
            0,    2],
        [  29,   12,  538, 1976,   25,  192,   11,   18,  172,   87, 1835,  689,
           11,   18,  172,  175,  691, 8289,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  79,   13, 5425,   59,    0,   19,   34, 8784,   54,    0, 5184,    0,
            8,   19,  692,   10,   51,   39, 2420,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,  276,  115,    0,  432, 1450,  215,    0,   53,   11,   57,   86,
          132,   84,    0,   53,   66,   86,  203,  699, 2633, 1181, 1501,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 103,   25,  204,  274,   85,   13,  453, 1201,    0,   25,   11,  158,
         4712,  735,  150,   77,   12,  117,  391,  214, 1028,  199,  722,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 719, 6382, 8193,    6,   17,  811,  265, 1782,    0,   94,   10,    7,
         4620,    6,    0,   12,  159, 2332,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,    7, 5410,   26,  172,    0,  101,  954,    6,  199,    7, 3140,
           12,  378,   39, 9005, 1198,   12,   77,   12,  117, 6710,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   34,  391, 1113,    0,    0,  717,    0,  110,  128,    6,    0,
            8,   85, 5826,   69,  227, 2124, 2366,    0,    0,    0,   24, 5623,
            7,  501,  887,  895,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  665,   85, 1228, 6529,    6, 1850,   62,  131, 5543,    6,    0,
           13,  179,   12, 5407,  985,    6,  132,   55,  565,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24,  144,  521,  870,   45,    6,    9,   77,    7,  203,  181,   59,
         1159,  866,    6,    9,    7, 6870,    6,    8,    7, 3061,    0,  125,
         8939,   97, 1203,  551,    6, 7955,    0,    2,    1,    1,    1,    1,
            1,    1],
        [  29,  154,   80,    0,   17,  765,    0,  270,    9, 8085,  584,    0,
            0,  554,  206,   13,  664,   37, 1327, 1563,    0, 3848, 1505,   13,
         1366, 1251, 3848,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  281,   12,    7, 1583,    6,   84,   63,   86,  744,   35,
         2514,   85,   77,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0,  108,  288,  818,   12, 3802,  169,   51,  359,    7,
          801,   35,  181, 3488,  369,   62, 1850, 2100,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,    0,    7,  207,  117, 2032, 2261,    0,    0,    0,  162,  691,
            0,   84,   11,    6,  226,   13, 1017,   12, 4795,    6,    0,    8,
           33,    0,   26,  206,   25,  175,   10,  150,    0,    2,    1,    1,
            1,    1]], device='cuda:5') tensor([33, 24, 24, 32, 23, 22, 23, 25, 30, 28, 38, 20, 21, 25, 25, 21, 24, 30,
        23, 32, 29, 17, 22, 34], device='cuda:5') tensor([0.8315, 1.0000, 0.8818, 0.8154, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8682, 1.0000, 1.0000, 1.0000, 1.0000, 0.8813, 1.0000, 0.8257,
        1.0000, 1.0000, 0.8022, 1.0000, 1.0000, 0.8340], device='cuda:5',
       dtype=torch.float16)
 > at.  tensor(39.4051, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[ 125,   70,   25,  296,   26,    7, 3189,    0,   86,    7, 7619,    0,
           68,  194,   65,   68,  386,   65,   29,  347,  192,   11,   18,   25,
          203,   22,   18,    7, 7619,    0,  109,    0,  276,  509,    0,  203,
           22,   18,  126,  155,  447, 7619,   10,  218,   94,    8,  229,  185,
          839,  106,   21,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,  103,   25,   87,  296,   10,  456,   80,  250,    0,   25,   73,
         1503,   21,    9,   13,  207,   17, 2700,   25,  211, 2693,  340,  181,
         1625,  369,    0,  860,   79,    0,   38,  187,  172,  213,   10, 1202,
           33, 1943,  411,  715,   22,    0,   29,   19,  296,   10, 2365,  785,
          825,   13, 1822,    8, 6015,   89,   79,    6,  103,   19,  192,   11,
           18,    0, 1189,    3,    2,    1,    1,    1,    1,    1,    1],
        [ 432,  392,  215,    9,  749, 3333,  399,    0,  106, 1553,  886,  371,
          322, 1921,  109,  106, 1553, 2792,    6,   10, 4100, 2353,  128, 3753,
            6,    0,   24,   11,  121, 1110,   17,   94,  213,   10, 1202,   55,
           13,  509,  858,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  103,   24,  487,  665,  199,   17, 2753,  813,    0,   24,   11,
           48,  150,   13,  800,   12,  170, 1522, 7777,    6,  126,   84,   46,
           12,  538,    0,   13,  325,   12,  422, 2753,  813,    0,   67,  113,
         3343,  237,    8, 8430,  813,    0, 4402,  106,  214,   17,   63, 1501,
         3919,  982, 1241,  155,  211,    6,   20,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,   77,    7, 2826,   25,  150, 2884,   62,  168,    0,   24,  154,
           84,   11,    6,   13,  453, 2212,   55, 6904, 1740,    6,   10, 6757,
            9,   33, 9648,    0, 2695, 1522,  111, 4112,   93,    8, 3066, 7806,
           12, 7016, 3001, 5605,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   53, 5164,  248,   12,  134,  132,    0,    8,   53,   66,  248,
          205,   59,  247, 1032, 1830,   35, 1859,   35,  820, 1047, 1103,  849,
            6,  693,   12, 7097,  214,   10,  134,    0,    8,   24,  289,    0,
           38,  200,  176, 1328,   91,  169,   25,  100,   10,  508,  132,    3,
           38,  187,   66,   10,  508,   91,  132,    3,   38,   93,   96,    0,
           24,  296,   91,   79, 2808,   12,    7, 2092, 1039,    0,    2],
        [  77,   17,   19,  217, 5290,    6,  106,  120,   19,  278, 3298,   13,
         2365,    9, 1051,   15,  760,    0,  461,   12,    7,  321,  762, 1486,
            6, 1411,   17, 6609, 4712,    3, 8867,  708,  106, 1122,  878, 2627,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  294,    0, 2076,   15,    6,    0,    0, 2638,   18,  810,    0,
         9344, 2544, 4513,    0,  159,  205,   35,  412, 5492,   10, 1661,   15,
            6,  436,   26,  521, 2391,   54, 1335, 1336,  480, 1611,    0,    0,
           12,  682,  649,  539,   20,    0,    9,    7,  774,   12, 5668,  187,
         2084, 1371,  241,  140,  609,  679,    6,    0,    0,  109,  941, 4058,
            6,    8, 3489,    6,    0,    2,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([53, 65, 41, 57, 42, 71, 38, 66], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8560],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(83.8551, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 09:36:46 | INFO | train_inner | epoch 001:    104 / 1474 loss=20.057, trans_loss=5.874, nll_loss=4.682, w2v_ctc_loss=22.343, task_loss=1.701, contrastive_loss=3.266, total=4220.1, n_correct=124.07, ppl=25.68, accuracy=2.94, wps=18358.2, ups=1.46, wpb=12591.5, bsz=473.8, num_updates=100, lr=4.098e-06, gnorm=2.849, clip=0, loss_scale=8, train_wall=76, gb_free=18.6, wall=138
2023-08-17 09:36:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-17 09:37:53 | INFO | train_inner | epoch 001:    205 / 1474 loss=16.562, trans_loss=5.861, nll_loss=4.693, w2v_ctc_loss=17.031, task_loss=1.629, contrastive_loss=3.237, total=4115.86, n_correct=114.28, ppl=25.86, accuracy=2.777, wps=18473.7, ups=1.5, wpb=12291.2, bsz=459, num_updates=200, lr=8.096e-06, gnorm=7.306, clip=18, loss_scale=4, train_wall=66, gb_free=19.3, wall=204
2023-08-17 09:38:58 | INFO | train_inner | epoch 001:    305 / 1474 loss=9.91, trans_loss=5.822, nll_loss=4.68, w2v_ctc_loss=6.894, task_loss=1.576, contrastive_loss=3.176, total=4080.91, n_correct=112.79, ppl=25.63, accuracy=2.764, wps=18594.8, ups=1.53, wpb=12190.4, bsz=439.4, num_updates=300, lr=1.2094e-05, gnorm=2.334, clip=1, loss_scale=4, train_wall=65, gb_free=18.5, wall=270
2023-08-17 09:40:04 | INFO | train_inner | epoch 001:    405 / 1474 loss=9.379, trans_loss=5.742, nll_loss=4.609, w2v_ctc_loss=6.119, task_loss=1.364, contrastive_loss=3.209, total=4176.41, n_correct=102.61, ppl=24.4, accuracy=2.457, wps=18930.2, ups=1.52, wpb=12470, bsz=461.3, num_updates=400, lr=1.6092e-05, gnorm=1.539, clip=0, loss_scale=4, train_wall=65, gb_free=19.4, wall=336
2023-08-17 09:40:06 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-17 09:41:12 | INFO | train_inner | epoch 001:    506 / 1474 loss=9.218, trans_loss=5.73, nll_loss=4.614, w2v_ctc_loss=5.829, task_loss=1.221, contrastive_loss=3.3, total=4185.2, n_correct=94.14, ppl=24.49, accuracy=2.249, wps=18456.8, ups=1.48, wpb=12505.7, bsz=487.4, num_updates=500, lr=2.009e-05, gnorm=1.288, clip=0, loss_scale=2, train_wall=67, gb_free=19.1, wall=403
2023-08-17 09:42:18 | INFO | train_inner | epoch 001:    606 / 1474 loss=9.119, trans_loss=5.79, nll_loss=4.691, w2v_ctc_loss=5.669, task_loss=1.19, contrastive_loss=3.255, total=4137.35, n_correct=87.02, ppl=25.84, accuracy=2.103, wps=18768.6, ups=1.52, wpb=12337.2, bsz=474.5, num_updates=600, lr=2.4088e-05, gnorm=1.163, clip=1, loss_scale=2, train_wall=65, gb_free=18.7, wall=469
2023-08-17 09:42:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-17 09:43:24 | INFO | train_inner | epoch 001:    707 / 1474 loss=9.063, trans_loss=5.823, nll_loss=4.738, w2v_ctc_loss=5.617, task_loss=1.256, contrastive_loss=3.136, total=4144.52, n_correct=81.05, ppl=26.69, accuracy=1.956, wps=18789.7, ups=1.52, wpb=12376.1, bsz=454, num_updates=700, lr=2.8086e-05, gnorm=1.168, clip=0, loss_scale=1, train_wall=65, gb_free=19.3, wall=535
2023-08-17 09:44:29 | INFO | train_inner | epoch 001:    807 / 1474 loss=9.004, trans_loss=5.935, nll_loss=4.876, w2v_ctc_loss=5.448, task_loss=1.215, contrastive_loss=3.156, total=4132.5, n_correct=68.59, ppl=29.37, accuracy=1.66, wps=18817.7, ups=1.53, wpb=12330.1, bsz=464, num_updates=800, lr=3.2084e-05, gnorm=1.33, clip=0, loss_scale=1, train_wall=65, gb_free=18.8, wall=600
2023-08-17 09:45:36 | INFO | train_inner | epoch 001:    907 / 1474 loss=8.929, trans_loss=6.084, nll_loss=5.061, w2v_ctc_loss=5.259, task_loss=1.243, contrastive_loss=3.059, total=4165.04, n_correct=42.74, ppl=33.38, accuracy=1.026, wps=18647.3, ups=1.5, wpb=12438.6, bsz=458.3, num_updates=900, lr=3.6082e-05, gnorm=1.314, clip=1, loss_scale=1, train_wall=66, gb_free=18.7, wall=667
2023-08-17 09:46:42 | INFO | train_inner | epoch 001:   1007 / 1474 loss=8.748, trans_loss=6.135, nll_loss=5.12, w2v_ctc_loss=4.969, task_loss=1.243, contrastive_loss=3.05, total=4135.88, n_correct=34.59, ppl=34.77, accuracy=0.836, wps=18831.3, ups=1.52, wpb=12354.5, bsz=457.9, num_updates=1000, lr=4.008e-05, gnorm=1.489, clip=0, loss_scale=1, train_wall=65, gb_free=19.3, wall=733
2023-08-17 09:47:47 | INFO | train_inner | epoch 001:   1107 / 1474 loss=8.522, trans_loss=6.171, nll_loss=5.158, w2v_ctc_loss=4.706, task_loss=1.267, contrastive_loss=2.957, total=4152.66, n_correct=36.13, ppl=35.71, accuracy=0.87, wps=19042.8, ups=1.54, wpb=12385.6, bsz=454, num_updates=1100, lr=4.4078e-05, gnorm=1.759, clip=0, loss_scale=1, train_wall=64, gb_free=18.5, wall=798
2023-08-17 09:48:52 | INFO | train_inner | epoch 001:   1207 / 1474 loss=8.32, trans_loss=6.179, nll_loss=5.171, w2v_ctc_loss=4.516, task_loss=1.335, contrastive_loss=2.836, total=4122.37, n_correct=41.56, ppl=36.03, accuracy=1.008, wps=18883.1, ups=1.53, wpb=12314, bsz=436.2, num_updates=1200, lr=4.8076e-05, gnorm=2.164, clip=0, loss_scale=1, train_wall=65, gb_free=18.6, wall=863
2023-08-17 09:49:58 | INFO | train_inner | epoch 001:   1307 / 1474 loss=8.164, trans_loss=6.166, nll_loss=5.154, w2v_ctc_loss=4.33, task_loss=1.239, contrastive_loss=2.793, total=4071.58, n_correct=49.91, ppl=35.6, accuracy=1.226, wps=18257.4, ups=1.5, wpb=12154.5, bsz=447.9, num_updates=1300, lr=5.2074e-05, gnorm=2.312, clip=0, loss_scale=1, train_wall=66, gb_free=18.5, wall=930
2023-08-17 09:51:04 | INFO | train_inner | epoch 001:   1407 / 1474 loss=7.997, trans_loss=6.149, nll_loss=5.136, w2v_ctc_loss=4.168, task_loss=1.273, contrastive_loss=2.853, total=4117.88, n_correct=53.02, ppl=35.16, accuracy=1.288, wps=18732, ups=1.52, wpb=12305.9, bsz=449.1, num_updates=1400, lr=5.6072e-05, gnorm=2.367, clip=0, loss_scale=1, train_wall=65, gb_free=18.8, wall=995
2023-08-17 09:51:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([[  29,    0,   25,   11,  121,  278,   33, 2196,    0,    9, 7194,    0,
           24,  144,   39, 1980,  852,   22,  820, 4747,   17, 3793,   62,  106,
         4435,  199, 2048,    9,   13,  555, 8347, 3885,   71,   39, 1719,  293,
           15,   18, 7347, 1882,   12, 1992,  439,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   73,  150,    7, 8639,   12, 1146,    9,    7,  245,  555,
         1598,    6,  168,    0, 2247,   34, 3600,    0,   94,  162,  204,  893,
         1714,   37,  254,  159, 1848,    0,    8, 1275,  276, 1714,   37,  254,
          159, 9703,  144,  226,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  125,   85,    0,    7,  467,   12,    7,    0,  383,    0,   70,
           24,   66,   10,  575,    0,   26,   17, 2394,   71,    0, 8307,  941,
           26,  324,   55,    7,   94,    0,   55, 1729,  290,   56, 1775,  221,
            6,    0,   17,   63,    0, 3754,  440,    8, 2603,   55,  251,  148,
            0,    0, 2775,   11,   18,  226, 2188,    0,    2,    1,    1],
        [  29,    0,  117,   63, 5168,    6,  620,  307,    6,    0, 1579, 1688,
         3635,    6,    0, 5588,    6,  162,  378,  144,   77,   80,   33,  283,
            0, 1819,    6,  162,  378, 4598, 2871,  490,   94,  144,  204,  144,
            7,  274,   85,    7,  283,    0, 5911,    6,  162,  893, 3006,   46,
           77, 3868,   12,  214,   46,  453,  906, 1386,    6,    0,    2],
        [ 294,    9,   89, 2042,   46,  125,   12,  238,   35,  160,  191,  793,
           62, 5370,   54,    8, 1244,   35,   20,  426,  510, 1370,   46,  162,
          837, 1181,   10,  703,   17,   24,  162, 1986,  277, 5589,  181,  372,
          636,    6,   46,   68,  194,   65,  148,  162,  142,   10,  205,  126,
            8, 2554,    7,  179,    0,    2,    1,    1,    1,    1,    1],
        [ 168,   63,    7, 1219,  785,  214,   17,   94,   71, 1850,   35,  587,
          241, 5188, 2247,  289,    0,   38, 3328, 5114, 1611,   66, 1781,    3,
           38,  187,   11,   45,   86, 4493,   10,   87,   70, 1073,  110, 1767,
            3,   38,  187,  598, 4616,   10,   89, 1431,    8, 1037,    3,   38,
          187,  661, 1222,  509,    0,    2,    1,    1,    1,    1,    1],
        [  21,  442,  110, 2743,  103,   84,  220,   51,   13,  509,  207,   46,
           13,  207,   10,  452,  551, 1236,   45,  121,   22,   18, 1723,    8,
         1094, 3067,    7, 4102,   12,  282,   17,  506, 8291,  111, 2302, 2518,
           12, 2116, 6683,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,    7, 1377,   12,    7, 1009,   26,   17,   21, 1321,   85,
          665,   46,   19,  641,   17,   11,    6,   21,    6, 7202, 8069,    0,
         1102,   21,    6, 1810,    8,   21,    6, 9337,  521,  430,  235,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:1') tensor([45, 42, 57, 59, 54, 54, 41, 37], device='cuda:1') tensor([1.0000, 1.0000, 0.8008, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(95.3019, device='cuda:1', grad_fn=<MulBackward0>)
tensor([[  67,    0,   21,  188,   10,    0,   51, 7118,    0,   29,   19,   11,
          158,  456,   80, 4312, 1098, 1994, 7289,    0,   68,  194,   65,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1057,  453, 4997,    6,   26,   86,  890,    0,    8, 1094,   24,   11,
          121,  226, 8160,    9, 3023,   71, 4997,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   26,  324,   10,   51, 1751,  111,    0,    8,   21,   11,    6,
          324,   10, 1082,  120,   86,   10,   51,    0,   67, 3695,   12,   17,
          818,   24,   66,   10,   51, 4493,    0,    2,    1],
        [  21,   11,    6,   13, 8308,    0,    8,   25,  154,    0, 8308,    0,
         1768,    0,   67,   33,   34,   86,  116,  419, 8308,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7,  245, 1289,   17,   19, 4415,  162,    9, 6062,    0, 1088,
            0,  140,    0,    0,   69,    7,  423,  322,   39, 2227, 1305, 1020,
           12,  984,  383,    0,    2,    1,    1,    1,    1],
        [   7,   94,  148, 5523,    9,    7, 1106, 2836, 3460,    6,   11, 2484,
         1639,   71,  134,   71, 2011,    8, 7889,    8, 3918, 1118,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 682,    0,   19,  641,    0,   70,  169,   21,  305,   10,    0,  690,
            6,  241, 2427,  108, 1507,    0,   10,  346, 7378,   21,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 440,   19,  213,   10,  521,  616,    6,    6,  250,   10,   25,    0,
           67,  245,   12,   77,   19,   11,   45,  142,   10,  884,   25,   13,
         1323,   12, 1532,    0,    2,    1,    1,    1,    1],
        [  55, 4212,    0,   89,  277, 3460,    0,  101,   11,    6,  133, 7069,
          111,   13,  387, 2459,    0,  101,   11,    6, 1752,  352, 4041,    0,
          101,   73,   11,   18,  456,   85,   77,    0,    2],
        [   8,  120,   25, 2215,  251, 2636,  277, 4059,    9,  155,  874,    0,
          108, 1329,   26,   86,   10,  289,    0,   38, 5034,   85,  267,    0,
          225,   11,    6, 2636,    0,    2,    1,    1,    1],
        [  21,   11,    6,   86,  116,   17,   25,   11,  158,  492, 3121,  155,
         1832,  307,  109,  155, 1890,    6, 3903,    0,  109,  155, 1269,  120,
           25,   11,   57,   85,  838, 2739,  713,    0,    2],
        [  39,   19,  371, 4676,   34, 3489,    0,    8,  284,  595, 1764, 1949,
           62,  199,   94,   69,    7,  926,   12,    7, 2291,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  185,  841,    0,   17,   11,    6,   70,  956,    0, 1456,    0,
           10,    7, 1017,   12,   94,   24,  162,  665,   85,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1482, 2700,   25,   13,    0,   51,   18,    0,   25,  164,    0,  274,
           85,   13, 9096, 3158,    0,    8,   25,  164,    0,  150,    7,  858,
            0,    7,  858,  164,   51, 6461,    0,    2,    1],
        [   8,   19,  278,  172,  324,   85, 3044,   54,  138,  294,  955, 4925,
            6,   25,  169,  296,   55,    7,   94,  148,  162,  142,   10,   14,
            9,  117, 3539,    6,    0,    2,    1,    1,    1],
        [ 238,    0,   21,   34, 1968,    0,  853, 8637,    6,   89, 1066,    0,
           33,   26,   70,   24,  278,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,    0, 1005, 2432,   89, 1629,   20,  606,   10,    0,  270,  518,
            0,    8,  339,  110, 3890,   70,   19,  499,   66,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,   91,  854,   12,    7,   94,    0,   19,  388,    0,    9,   13,
          572,   17,   11,    6,    0,  321,   12,    7, 3505,    0,    0, 7518,
           48, 6129,  572,    0,    2,    1,    1,    1,    1],
        [  63,  155,  451,  373, 3163,    0,    8,   53,  162,    0,   53,  162,
         7070,    0,  211, 5060,  859,  291,   18,  234, 2871,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 244, 3455,   62,    0,  244,  335,   62,    0,  244, 4014,   20,    0,
         3572,   62,  126,    0,    7, 3349, 4609,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  409,  552,  126,   12,   60,    0, 1323,   12,  215,  581,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,   73,   21,   51,   17,   86, 1891,    7,  179, 6461,  111, 2700,
          170,   13, 6166, 5542,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   34, 2291, 6292,    0,  985,   35,  290,  551, 5588,    6,    0,
          743,   54,    8, 1809,  484,  945,    8,   10,  484,  945,    0, 2994,
         1344,    8,  853,   22, 1105,   57,    6,    0,    2],
        [  67,   33,   26,  113,    0,  103,   19,  305,   13,  488, 1396,  270,
          106,   33,    0,   33,   26,  113,   13,  133, 1392, 3046,   55,  110,
            0,    2,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([24, 22, 32, 23, 29, 24, 24, 29, 33, 30, 33, 23, 23, 32, 30, 19, 23, 29,
        23, 21, 13, 18, 33, 26], device='cuda:3') tensor([0.8950, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8906, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8774, 1.0000, 1.0000, 0.8340, 0.8726,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
 > at.  tensor(40.8748, device='cuda:3', grad_fn=<MulBackward0>)
tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8130, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8696, 1.0000, 1.0000, 0.8672, 0.8652,
        1.0000, 1.0000, 1.0000, 0.8687, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8955, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8750, 1.0000, 0.8003, 1.0000, 1.0000, 0.8154,
        1.0000, 1.0000, 1.0000, 0.8145, 0.8315, 0.8403, 1.0000, 1.0000, 0.8857,
        0.8267, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8125, 1.0000,
        1.0000, 1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8857, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 0.8867, 1.0000, 1.0000, 1.0000,
        0.8481, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
 > at.  tensor(10.0523, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[  17,   11,    6,  ...,    1,    1,    1],
        [  86,   85,   77,  ...,    1,    1,    1],
        [ 120, 8121,  728,  ...,    1,    1,    1],
        ...,
        [  53,   11,   57,  ...,    1,    1,    1],
        [   8,   86,  288,  ...,    1,    1,    1],
        [  24,   63,   86,  ...,    1,    1,    1]], device='cuda:2') tensor([12, 18, 15, 13, 14, 14, 16, 14, 15, 17, 11, 11, 14, 16, 14, 13, 13, 13,
        13, 10, 18, 12, 12, 15, 13, 18, 16, 15,  9, 16,  9, 17, 12, 15, 13, 11,
        13, 15, 11,  9, 17, 20, 14,  9, 11, 15, 11, 15, 13,  9, 13, 12, 11, 10,
        21, 15, 10,  7,  9, 19, 14, 16, 15, 11], device='cuda:2') tensor([0.8955, 1.0000, 0.8086, 1.0000, 0.8545, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8662, 0.8711, 0.8091, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8262, 0.8496, 1.0000, 1.0000,
        1.0000, 0.8403, 1.0000, 1.0000, 1.0000, 1.0000, 0.8154, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8760, 0.8003, 1.0000, 1.0000, 1.0000, 0.8896, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:2', dtype=torch.float16)
 > at.  tensor(16.9263, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[ 115,  103,   19,  ...,    1,    1,    1],
        [  55,  663,    0,  ...,    1,    1,    1],
        [  55,    7,  473,  ...,    1,    1,    1],
        ...,
        [  33,   26,   13,  ...,    1,    1,    1],
        [  29, 4232,  818,  ...,    1,    1,    1],
        [ 251,   63, 3744,  ...,    1,    1,    1]], device='cuda:2') tensor([29, 22, 29, 26, 20, 21, 33, 26, 32, 27, 36, 30, 40, 35, 23, 34, 24, 45,
        38, 14, 20, 28, 16, 25], device='cuda:2') tensor([1.0000, 0.8721, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8198, 0.8774, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8545,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(43.5382, device='cuda:2', grad_fn=<MulBackward0>)
tensor([[  29,  250,   55,   25,   10,  154,   80,    0,   79,   24, 5332,  117,
          264, 5130,    6,    8, 3901, 5259,    0,   79,   24,  722,   71,  117,
         3684, 1941,  264,   10,   93,    6,    0,  138,  261,   63,   24, 1420,
         1811,  518,  521,  121, 2227, 1068,   55, 7127,    8, 3069,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   34,   56, 3856, 7489, 1298, 1490,    0,   67,   19,  154,    7,
          143,  528,  279,   26,   21,   34, 3315,  125,   19,  278,   10,  305,
           69,  486, 2883,    0, 2603, 1800,   17,   21, 2947,   62,    9,   13,
         4098,    0,    8,   17,   26,   79, 2326, 2420,    0,   68,  194,   65,
           24,   11,   57,  142,   10, 4938,  132,    7,  733,  279,    0,  192,
           11,   18, 4110,    0,   68,  194,   65,   19,  217, 2541, 1079, 1344,
            0,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1],
        [  24, 2516,  236,  436,    0,   24, 2508,    0,   10,  291,  699,  352,
            7,   94,  172, 5555,   55, 2872,   54, 3382,   46,   55, 7644,   54,
         2518,  106, 4518,  336,    7,  179,    0,  113, 2134,   79, 1503, 1436,
          959,   54,    0,    8,   55, 4777,   54,    7, 1422,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   69,    7,  859,   63,    7, 6848, 3927,    0,  166,  180,    0,
            9,    7, 6412,    0, 5070,  251, 2216, 3675,   10,   51, 7893,  922,
            8, 5493,  109, 8520,  133,    0,  133,  555, 4400,  187, 2404,    6,
            0,   10,   51, 2185,  959, 1011,  540,   69, 2696,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   69, 1219,   12,   77,  518,  117, 3002,    0,   53, 1005, 2762,
            0,   68,  194,   65,   24,  192,   11,   18,  135,   70,   11,    6,
          142,   69,  854,    7,  183,    0,   29,   21,  388,    6,  170,    9,
           13,  133, 1953,  241, 1032, 3140,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   34,   13, 3008,    9,    0,   89, 3594,   17,  144,   13,   38,
         1341,  200,    6,  197, 1729,  502,   20,    9,    0,  159, 3508,    0,
            0,    0,    8,   89, 3181, 1412,   66,  244,  620,  918,  110,  952,
            0,   10, 1183,   80,  138, 7721,   19,  474,   33, 1729,  502,   20,
            0,   34,    0,  125,   13, 1323, 1113,  490, 4468, 1949,   20,   15,
            0,  225, 2036, 2592,   89, 8659,   54, 1066,    0,  131, 2385,  110,
           33,   38, 1341,  200,    6,  197,    0, 1729,  502,   20,    0,    2],
        [   0,  101,  968,  110,   17,    0,  778,   46,    7, 1809, 1305,    0,
            0,    7,   29,   57, 1911,  292,  411,    6,    0,    0,    0,    7,
         1487,   22,  335,    0,    0, 6898,    0,   77,   12,    7,    0, 2673,
           18,  292,  160, 2719,  160,  128,    0, 1540,   59, 4911,    0,    0,
            8, 2788,  407,  140, 6509,    6,   46,  162,  378, 6121,  131,  185,
         8219, 2294, 6146,   17,   19,  220,   86, 1008,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,   11,    6, 1241,   17, 3297,   17,   24,  164,   77, 1665,
          340,  699,  352,    7, 5407,   12,  378,   70,   24,   63,    0,   13,
         5030, 6033, 1369,    0, 5030, 4837,   12,   56,   15,    6,  234,   54,
           17,   77,   94,    6,    8,   77, 5291,    6,  446,   13,  207,   10,
         9780,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([48, 77, 47, 47, 44, 84, 70, 51], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8970, 0.8232, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(98.4842, device='cuda:6', grad_fn=<MulBackward0>)
tensor([[   9,   56,  525,   11,    6,    0,  179,    0,    7,  415, 1040,    0,
         4925,    6,  144,   13, 6149, 9417,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2129,   21,  689,   11,   18, 1620,   85,   77,    0,  109,   21,   11,
            6,  250,  994,    0,   13, 1192, 1810,  109,  185,  255,   45,   22,
         1974,  416,  279,    0,   67,    9,  419, 1165,   21,   11,    6,   86,
          461,   12,  948,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   80,   39, 1759,   35, 6937,   35, 7596, 6069,   12,
         5933,    0,   21,   11,    6,  278,   13, 2960,  109,   29, 3189,    6,
           69, 1219,    0,   69,   17,  926,   26,    7, 6342,    0,    8,  168,
           26,  185, 2260, 2852,  174,   54,    0,    8,   21,   11,    6, 3364,
           10,   33, 7373,   12, 3411,  221,   20,    0,    2],
        [ 554,    0,   85,   80,  423,    3,  215,   79,   13, 5030,   35,  181,
         2482, 2361,  170,    0,    8,   24,  288,  859, 1146,   80, 1992,    3,
          215,  581,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,  323,  101,   86,  135,   46,   21,   11,    6,   70,   24,   11,
          121,   77,   55,  606, 2490,   46,   17,   24,   11,   57,   86,    7,
          245, 1649,   10,  991,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 339,  110, 7032,   25,   10,   89, 4149,  264, 1039,    0,   38,  712,
          287, 1090,    9, 3164,    3,  166, 1505,   13, 3213,   15, 3634,  281,
         2523,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 432, 1655,   12,  203,    6,  856,    6,    9,  203,  372,   93,    0,
            7, 1269,   34,  591,    0,    8,   24, 6853,   62,    7,  203,  500,
          369,   12,    7, 1037,  359, 2481,  174, 1434, 1395,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 109,  305, 6908,    0,  440,    0,  149,    6,   96, 1744,   39, 2313,
           12, 2199, 1601,  690, 3474,  665,   55, 2519, 6629,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  71,   33,    9, 1066,    0,   24, 2534,   13, 1501,  264, 6129,  567,
           10, 1766,    7, 3708,    9,  419, 3440,   12,  672,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  274,   85,    7, 1912, 1375,    0,    7, 2384,   71, 1962,
         4266,   12, 5894,    0,   25,   73,  150,    9,  251, 2384,    0,  204,
            0,    7,  800,   12, 4224, 3883,    6,   26, 2585,   57,  484,   54,
           13,  325,   79, 3538,  909, 2460, 1068, 1143,  132,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,  185,   94,    9, 7419,  552, 1585,    8,   53,  246,    0,
           38,  160, 2402, 5448,    0,    7,  134,   20,   12,    7, 7419,  422,
         1611, 9797,   26,  913, 6811,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1239,    0,   19,  213,   10,  575,   25,  321,   12,   13, 7306, 6143,
            0,   67,    9,   21,   26,   89, 7143,   71,   70,   11,    6,  142,
           10, 1085,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  79,   25,   73,  934,    0,   19, 2138,   13,  325,   12,  183,   85,
         8145, 3069,    0,   68,  194,   65,   89, 3780, 2220,   11,   18, 2744,
            6,    6,  110,   55,    7,  473,  248, 1345,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 1321,  453,    0,    8,   25,  169,  205,  199,   13,  384, 1838,
          436, 3084,    8,   77,   12,   13, 5827,  150,   17,   69,    7,  225,
         1591,    0,    8,   21,   11,    6, 1968,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 7602,  110,   17,   21,  169,   66,  226, 3643, 3296,    0,   55,
          728,    6,  335,   10,  367,  270,  554,    8,  554, 2219,   69,    7,
          341,  183, 5331,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 276, 1445,   24,   11,  121,  367,   10,   33,  106,  341, 3773,    0,
           24,   77,   66,   10, 1529,   69,  108, 2410,   10,  468,    7,  207,
           17, 1459,  154,    6,   80,  896,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([20, 41, 57, 28, 30, 27, 35, 23, 23, 47, 31, 28, 34, 33, 30, 32],
       device='cuda:5') tensor([0.8584, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(59.7782, device='cuda:5', grad_fn=<MulBackward0>)
tensor([[ 339,   11,    6,    0, 3522,    0,    0,    7,  179,   77,  336,  170,
           71,   33,  264, 7260,    0,    8,    0,  661,    7, 1192,  179,  106,
           13, 4149,  264, 3064,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   13,  277,  523,  143,  567,  411, 1181,    0,   89,
         5199,   26, 6224,  660, 1740,    6,    9, 2406,    6,   12,   77,  214,
            0,   29,   19,  135,   13,  277,  523,   80, 2567,   13, 8192, 1422,
            0,    2,    1,    1,    1,    1,    1],
        [   8,  103,   21,  144, 3548,   48,    0,    7,  858,   12, 6895,  169,
          914, 1220,   51,  168,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 225,  246,    0,   38, 3127,   21,   11,    6,   86, 3718,    0,  125,
           84,   26,  288,   91,  282,    0,    8,   84,  451,  288,   51,   91,
         1723,    3,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1732, 1107,    0,   77,    7,  126,  603,  234,   54,   12, 3429,  369,
            8, 6408,   17,  552,  106,  108,  753,   34,  250,  172,   17,  164,
         5258,    0,  700, 1917,   71,  110,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  434,   56, 1949, 3287,    9, 8393,    0,  206,   19,
         5747,   21,    0,   21,   11,    6,  434,   81, 1027,  372,  168,   69,
            7, 3156, 4710,    0, 1031,  287,    9, 5712,    0,   29,  606,  994,
         6228,    0,    2,    1,    1,    1,    1],
        [  29,  103,   24,  274,  336,  554,    0,   84,   63,   80,  717,   94,
          499, 3642,    0,    8,  204,   19,   11,   45,   86,  142,   10,  388,
           25,   69,    7, 4057,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1250,   12,   13, 8690,    0,  116,   33,  225,  265,  560,   48,   11,
            6, 7304,    0,    8,  101, 1119,   46,  101,   11,    6,    9,    6,
          300, 1011,   46,   38,  727,   19,   13, 2918,   17,   25,  169,  367,
           10,  110,   71, 3020,    6,    3,    2],
        [  24,  113, 5332,   62,  131, 3158,  959, 6037,  856,  502,    7, 2100,
           12, 9380, 8428,    6,   55,    7,  942,   12,  264, 1736,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   12,   77,    7,   94,    6,   17,   19,   11,  121,  700,
          226,   71,    0,    7,  281, 3586,   63,    7, 1097, 2415,   12,    7,
           43,   59,  371, 1644,  827, 1151,  384, 1954,  455, 1943,  455,    9,
         6869,  415,  237, 4599,  407,    0,    2],
        [  19,   34, 4022,  215,  801,   85,    7,  183,    0,    8,   19, 1425,
           17,   17, 3463,   24,  144,   13, 3286, 1565,   12, 1057,   13, 1269,
           71,   13, 4460,  384,  616,  140,   18,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  19,    0,  278,   10,   87,   70,  294,  596,   66,   46,    7,    0,
          728, 3174,   69,    7,   51, 3174,    0,    8,    7,    0,   38,  174,
          195,  195,  195,  195,    3,   89, 3780,    0,  552,   71,  110,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  29,    0,    7,  245, 1017,   12,    0, 2296, 3403,    6,    0,   53,
          467,  132,    0,    9,    7,    0, 3020,   81,   69,  284,    0,   19,
         1218,    0,   67,    7, 1590,   12,    0,  134,   63,  172,   70,   11,
            6,  528,  168,    0,    2,    1,    1],
        [  67,  120,   24, 2982,   10,   94,    0,   21, 2947,   62,   17,   13,
         2079,   22,  356, 2958,  144,  956,    9,  117, 6786,    6,   77, 1004,
            7, 1384, 1224,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1436,   59,  760,    8, 1546,  607,  322, 1873,   17,  145,   37,  144,
          226,   79, 1226,   80, 7103,  378, 4733,   79,  101,  144,  226,   80,
         1848, 7993,   21,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    0,    7, 1793,  958, 1006,  567,    0,    0,  188,  143,
            0,  254,   21,    6, 3718, 1452,   12,   56, 1251,    6, 2470,   22,
          140,  793,   46,   10, 4223,   21,    6,    0,  862,  233,  297,  833,
            0,    0,   10,   51, 1123,    0,    2]], device='cuda:7') tensor([30, 38, 18, 27, 32, 39, 30, 43, 24, 43, 33, 37, 41, 29, 29, 43],
       device='cuda:7') tensor([0.8560, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8857, 0.8438, 1.0000, 1.0000, 0.8413],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(52.9922, device='cuda:7', grad_fn=<MulBackward0>)
tensor([[  39, 5410,  148,   11,    6, 5184,  134,   71,   13, 2705,    8,   13,
         1780,  169,  914,  305, 1345,    0,  109,   21,  169,  305,  276,   13,
          464,   10, 3152,   77,    7, 7035,    6,    0,   77,   12,    7, 1362,
           20,  586, 1428,    0,   25,   73,  288,  719,  250,  100,   33,  359,
           39, 4516,    0,    2,    1,    1,    1,    1],
        [ 391, 6730,    6,   63,    0,  442,  131,    7, 7832, 1880,    0, 8789,
           12,  108,  183,    0,  860,   13,  567,   26,    0, 5317, 1490,   56,
         2687,  494,    0,    0,    0, 1880,  111, 6083,    0,    8, 2645,  111,
            0,   19,  237, 1875, 3027, 3147,    0,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [5832,   17,    7, 5347,  119,  572,  492,  934,   48,   21,    6, 3764,
         2070,    9, 1221,    0,  635, 4586, 1183,  994,  106,   70,   21,   26,
           53,   11,   57,   86,  923, 1123,   19,   66,    0, 5352,  249,   54,
            0,   17,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [1953, 3619,  215,  581,    0,    7, 2048,    8,  931,   18, 1994,    8,
         3380,    6,  162,   86,  276,   91, 1953,  322,   12,   91,  439,    8,
         2220,   11,   18,  276,   66,  226, 7548,   69,  860,   13, 5851,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,  875,    7,  392,    0, 1074,    8, 1244,   35,   45, 2147,
           54,   51,   22, 1150,    6,    9,    7, 7996,  964,    0,    0,    0,
         3570,    0,    0,   34,  116,   51, 1763,  110,    0,    8,   34,  100,
            0,   38, 2045,   45,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  86,   10, 6903,    0,   84,   11,    6,   13, 4505, 5656,  142,   69,
            0,   29,  276,  103,   25,   87, 5032,  250,    0,    7, 5168, 6142,
            8,  832,  140,   51,   59,  636, 2876,   63,    9,   33, 2285, 4505,
         5656,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24,  323,  446,    0,    0,   17, 4764, 1674,   18, 6051, 1101,
            0, 4198,    6, 6478, 5933,    6,    0,    0,    0,    0,   13, 1118,
           15,  266,    8, 3222,  292,   45,  996,   46,   13, 1118,   15,  266,
            0,   85,  747,  890, 4266,   10,   66, 1167,   35,  140,  904,   54,
         4608,    0,    2,    1,    1,    1,    1,    1],
        [  29,    7,  942, 1757,  706,    6,    0,   53,  175,  540,    8,   53,
         1531,   53,   11,   57,  142,   10,  468,    7, 1187, 2008, 3612,   10,
          229,   21, 2015,  250,  994,    0,   29,   53,  468,   21,   10, 2008,
         1633, 7861,    0,  100,   33,   26,  142,   10, 2828,   70,   11,    6,
          172,  142, 1226,    9,    7,  942,    0,    2]], device='cuda:4') tensor([52, 45, 40, 37, 42, 39, 51, 56], device='cuda:4') tensor([1.0000, 0.8242, 1.0000, 1.0000, 0.8584, 1.0000, 0.8413, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(74.6564, device='cuda:4', grad_fn=<MulBackward0>)
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 7 terminated with the following error:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 330, in train
    valid_losses, should_stop = validate_and_save(
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 421, in validate_and_save
    valid_losses = validate(cfg, trainer, task, epoch_itr, valid_subsets)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 505, in validate
    trainer.valid_step(sample)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 1127, in valid_step
    _loss, sample_size, logging_output = self.task.valid_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 806, in valid_step
    loss, sample_size, logging_output = self._per_task_pair_valid_loss(per_task, model, criterion, sample)
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 683, in _per_task_pair_valid_loss
    loss, sample_size, logging_output = criterion(model, sample[per_task], per_task)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge.py", line 453, in forward
    if update_num < 100 and update_num % 20 == 0:
TypeError: '<' not supported between instances of 'NoneType' and 'int'

/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 32 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-08-17 13:57:30 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14825
2023-08-17 13:57:30 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:14825
2023-08-17 13:57:30 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:14825
2023-08-17 13:57:30 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:14825
2023-08-17 13:57:31 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:14825
2023-08-17 13:57:31 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:14825
2023-08-17 13:57:31 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:14825
2023-08-17 13:57:31 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14825
2023-08-17 13:57:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-17 13:57:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-17 13:57:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-17 13:57:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-17 13:57:31 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 13:57:32 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-17 13:57:32 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 13:57:32 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 13:57:32 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-17 13:57:32 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 13:57:32 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-17 13:57:32 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-17 13:57:32 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 13:57:32 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-17 13:57:36 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14825', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-17 13:57:36 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-17 13:57:36 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-17 13:57:36 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-17 13:57:36 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-17 13:57:36 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 13:57:40 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-17 13:57:40 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-17 13:57:40 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-17 13:57:43 | INFO | root | load pretrained hubert
2023-08-17 13:57:45 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 13:57:46 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 13:57:49 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 13:57:49 | INFO | root | share the sematic adapter and textual encoder
2023-08-17 13:57:49 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-17 13:57:49 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-17 13:57:49 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-17 13:57:49 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-17 13:57:49 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-17 13:57:49 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-17 13:57:49 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 13:57:49 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 13:57:49 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 13:57:49 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 13:57:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-17 13:57:55 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-17 13:57:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-17 13:57:56 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 13:57:56 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 13:57:56 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 13:57:56 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 13:57:56 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 13:57:56 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 13:57:56 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 13:57:56 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 13:57:56 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 13:57:56 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 13:57:56 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-17 13:57:56 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-17 13:57:56 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 13:57:56 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 13:57:56 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-17 13:57:56 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 13:57:56 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 13:57:56 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 13:57:58 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 13:57:59 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 13:58:48 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-17 13:58:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 13:58:48 | INFO | fairseq.trainer | begin training epoch 1
2023-08-17 13:58:48 | INFO | fairseq_cli.train | Start iterating over samples
True tensor([[   7,  422,  874,    0,    0,    9, 6788,  369,   12, 3648,    0,   26,
            7,  874,    9, 2761,   12,   13,  572,    0,    7,  572,   26,    0,
            0,    9, 2761,    0,   12,   13,  874,    0,    8,  722,   26,    7,
         7465,  131,  166,  251,    0,  248,   63, 3249,   62,    9,    7,  772,
          207,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [1179,   25,  135,   21,  109,   86,    0, 1335, 2348,   12, 1613,   71,
          747, 2693,   11,    6,    8, 1335, 2348,   12,   13,    0,  265,    0,
         6466,    6,  148,  205,   10,    7,  281,  909,  119,  236, 1563, 8562,
            9,  108,  753, 1772,    9,  909,   35, 3080,  109,  909,   35, 9376,
           54,    8, 4429,    0,    8,   53,  467,  132, 2762,  159, 2767,    6,
            0,    2],
        [  67,   21,  465,   11,   18,  468,    7, 2932, 1880,  567,    0,    8,
          113,    7, 2932, 3612, 1381,  832,   18,  233, 1181,   33, 3612, 1181,
         3860,   59, 1710,   10, 5111,   15,   21,    6,  768,   10, 5574,    7,
         1749, 1381,    8,    7,  341,  409, 1955,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  29,   56, 1285,    0,  347,  192,   11,   18,   25,  450,  117, 6010,
           70,   21,   11,    6,  100,  378, 7000,   71,   33, 4716,  434,  427,
          821,  407,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  409,    0, 2753,  427, 1105,  810, 3565,   17,   21,  659,   51,
         1953,    6,   12, 1655,   12,  215,  801,    0,  166,  818,   17,   33,
         1167,  659,   66,  245,   13, 2003,   15,  106,    7, 1247,   12,   13,
          258, 1591,   17, 2963, 1585, 1763,    7, 1644,  511,  762, 1054,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,   13,  563,   93, 2484, 9473,   13, 4866,   10, 2266,
          389,  552, 1252,   11,    6,   38, 5001,  411,  293,    3,   24,   73,
          205,   10,   91,   12,    7,  822,  608,   56, 9694, 1075,    0, 3109,
         3902,    0,    8,  150,  113,   13, 1523, 3609,   12, 2718, 7719,    0,
          106,  747,    9, 1665,   10, 1962,    9, 1912,    0,    2,    1,    1,
            1,    1],
        [  33, 2780,   34, 6291,  111, 2770,   55,    7, 2932, 1232,    0,   67,
          115,   21,   11,    6, 2823,   54,  100, 3139,  140, 2704,    6,    9,
            7,  832,    0,    6,    0,    8, 2627,    0,  206, 2406,    6,   63,
         4213,   48,   10, 3067, 3316, 1006,   85, 3646, 1729,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 108, 2476,   26,   10, 8711,   17,   53, 1516,   71,   13,  509,  841,
           12,  138,   10,  229,  214,  254,  120,   53, 6565,    0,    8,    7,
         2027, 6936,  475, 2041,   17,   25,   73, 1531,  214,  126,  131, 1974,
          416,   54,  336,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([51, 62, 45, 28, 50, 58, 47, 41], device='cuda:0') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(86.9355, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 13:59:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
True tensor([[   9,   13, 1880, 4745,    0,   25,  498,   71,  211, 2837,    8,  211,
         4774,   10, 1393,   17,  199, 2332,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,  131, 3696,    7, 4221,    0,   24,   73,  446,  126,  261,
          143,   80,  155, 1037,  254,   25,  914,  135,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   11,    6,    7, 6099,   12,  958,    9,    7, 6869, 8521, 2353,
           12, 5205,    0,    9,  421,  234,   48,  365,  221,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,  162,   86, 2841,    9, 1979,    0,  290,    0,   69,    7, 5225,
           38, 2862,  781,  918,   93,  197, 1017,    0, 2503,   18,  650,  568,
           86, 2115,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [3398, 7936,    8,  284, 1543, 1948, 1985,   87,   21, 2373,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   84,   11,    6,  288,   91,  227,   15,    6, 2158,    0,    0,
          207,   10,    0,  205,  106,   84,    0,    0,   25,  175,  717,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,   10,  101,  128,    0,    0,   10, 1008,    0,   10,  289,    0,
           38,  533, 2056,   13, 1738, 1234,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 206,   11,    6,    7, 2808,    0,   38,  533, 1412, 2215,  159, 2360,
           10,    7, 2182,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 473,  111,    0,   33,   26,   13, 2322,   35, 5444, 2595,  856,  293,
            0,  125,   19,   66,   29,  294, 1516,    6,    9,   89, 1995,  918,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [7494,  185, 1118,  307,   34,   39, 1830,   35, 1933,   35, 1178, 2988,
          120,  101,   34, 1920,   22, 1371,   62,  106, 3022, 1146,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   33,   26,  116,  133,  946,   46,  183,   26,   69,    7, 3089,
           35, 7988,  340,    8,    7, 2509,   26,   69,    7, 1995,   35, 7988,
          340,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,    0,  169,  154, 1827, 1794,    0,  162, 4491,    0,  166,  818,
           53,  451,  601,   25,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   0,    8,   10,   87,   17,    0,   25, 1275,    0,   66,    0,   10,
         4988,   13, 1472,   12, 2066,  416, 1374,  572,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 1220,   25,  659,   51, 1211,   12, 1835,    0,   38,  187,
           11,   45,   86,    9,   18, 2687,  241,   54,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,   11,    6,    7,  613,   12,  117, 3742, 3228,    6,   17,   25,
           11,  121,  591,    0,   38,  511,  101,  246,    3,  238,    0,   19,
           11,  121,  211,  479,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [  33,  188, 8166,   19,  362, 1838, 1428,   55,   70,    7, 1479,   26,
          142,   10,   87,    9,    7,  858,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  17,   11,    6, 5653,   10, 1719,   59, 5292, 3147,  111, 1830,  439,
           55, 2677,    8,  248,   10,  391,  439,   55, 5031,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,  180, 2568,  347,   19,  305,    0,    0,   29,  261, 7003,  106,
         4990,  589,  596,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7, 2403,   26,   17,   84,   11,    6,   13,  785,   35, 9058, 3900,
            0,   86,    7, 1953,  322,   35, 3149,   35,  290,   35, 8856, 3900,
          100,   10, 4482, 2147,    0,   67,   13,  785,   35, 9058, 3900,    0,
            2],
        [   8,    7, 1714,  608,  423,  439,    0,   53,  305,   80,  248,  439,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 244, 2869, 4021,    0, 2008, 6248, 1405,   91,   12,    7,  281, 3861,
          338, 3232,  128, 4621,   24,  135,   80,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  19,   11,   45,  142,   10,  575,   25,  138, 7218,  204, 2895,    6,
           71,  108, 5967,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 110,  153,  240, 1547,    6, 3067,   62, 6873,    0,    8, 1645, 9745,
            6,    0,    0,    0,  593,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  67,  103,   25,  192,   11,   18,   66, 6759,   35,    6, 2174,  297,
           62, 3867,    0,  180,    7,  630,   12, 1703, 6230,   26,   13,  341,
           91, 5020,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:6') tensor([20, 22, 23, 28, 12, 25, 20, 17, 26, 24, 27, 18, 22, 22, 30, 20, 23, 17,
        37, 14, 21, 18, 19, 28], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8721, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8916, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000, 0.8887,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000], device='cuda:6',
       dtype=torch.float16)
 > at.  tensor(34.7195, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[1250,    0,   53,  ...,    1,    1,    1],
        [ 832, 1337,   22,  ...,    1,    1,    1],
        [  67,   25,  113,  ...,    1,    1,    1],
        ...,
        [  24,  278,    0,  ...,    1,    1,    1],
        [  24,  321,   12,  ...,    1,    1,    1],
        [  19,   11,   45,  ...,    1,    1,    1]], device='cuda:6') tensor([11, 11,  9, 13, 12, 12, 12, 12, 16,  9, 10, 11, 10, 11, 11, 12, 13, 11,
        11, 10,  9, 13, 17, 15,  7,  9, 11, 12, 17,  6,  7,  9, 10,  8, 10,  8,
        10, 11, 14, 13, 11,  9,  8, 19, 12,  9,  9,  6,  8, 16,  8, 12, 11, 13,
        10, 12, 10, 13, 10, 15,  7, 12, 11, 12, 13,  9, 12,  8, 13, 20,  8,  8,
         8,  7,  9, 11, 12, 11, 11, 12], device='cuda:6') tensor([0.8091, 1.0000, 1.0000, 0.8789, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000,
        0.8555, 1.0000, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8218, 0.8652, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 0.8984, 1.0000,
        0.8286, 1.0000, 1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8838, 0.8677, 1.0000, 1.0000, 1.0000, 0.8369,
        0.8652, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  True tensor([[  55,  663,    0,    9,  264,  109,  232,  221,    6,   24,   66,  682,
         3196,   22, 2932,  896,    0,  206,   53, 3860,   75,  241,  221,   77,
          236,  866,    8, 6387,    8,   29,  234,  452, 6029, 7401,    0,    2,
            1,    1,    1],
        [ 115,   24,   11,  121,  492, 1110,   13, 7123,  100,   33,   12,   13,
         1470, 3189,    0,   67, 1470, 3189,    6,   73,   51, 1346,    0,  276,
          103,   53,   11,   57,   86, 1110,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [ 168,   26,  138,  488,   12,   13,  775,  411,   37,   34,  859,    0,
           21,    0,   34, 1523,    0,    0,   10,    0,    0,  508,   25,   13,
          841,   12,    7,    0, 1823,    0,    0,    0,   84,   25,  205,    0,
            2,    1,    1],
        [ 115,    7, 1885, 5055,    0,   12,  538,    0,   26,  206,   13,  325,
           12,    7, 2394, 4793,    8, 2791,  513,  199,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  29,   21,  818,   17,   51,   20, 7931,  373,   63,  133,  324,   85,
          203, 4030,  140,   54, 2429,   35, 2611,    6,    0,    8,   29,   53,
           11,  121,  226,  529,   10, 3258,  251, 4513,   96,    0,    2,    1,
            1,    1,    1],
        [  24,   66,    7, 2216,  768,   10,  205,    0,   38, 5276,   26,   70,
           19,  213,   10,   87,    3,    8,    9,   13,  613,  982, 1479,    0,
           17,    0,   10,  110,    0,   26,   13, 1968,  279,    0,    2,    1,
            1,    1,    1],
        [  25,   73, 6265,  281,   12,   17,   13, 1674,    6,    0,  125,  103,
           25,   11,   57, 4298,  336, 3594,    0,    8,    7,  595, 3336, 1233,
         2233, 4457,  111,    0,   25,   11,   57,  492,  142,   10, 1713,   17,
          595,    0,    2],
        [  29,    0,   85,   33,  613,    0,   19,   87,  213,   10,  450,   25,
           17,  185,   12,    7, 2767, 4131,   24,   66,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 101, 1119,    0,   38,   18, 2704,  518,  155,  227,  715,   96,    3,
          125,   33,   26, 1320,  111, 2059,    0,   77, 2808,   10,    7, 9510,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  131,   13,  316,  287,    0, 7492,  136,  760,  322,   93,
            8,  697, 1262, 2178,  713,    0,    8,   53,   11,   57, 2091,    9,
         2567, 1507,    0,  116, 5000,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   0,    9,  264, 1884,  166,    0,   63,  142,   10,   51, 1405,    0,
           33,  169,   86,   51, 3118, 1623,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  67,   21,  465,   11,   18,  229,    0,   13, 1878,   10,  565,    0,
          103,    0,  284,  708,  162,  142,   10,   51,    0,   13, 2988,  109,
            0,   13, 2088,    0,   53,  162,    0,  142,   10,  205,   10,  670,
            0,    2,    1],
        [  67,   24,  499,  213,   10,   51, 1123,   17,   33,   26,    7,  281,
         4089,    8, 2426, 3180,   17,   24,   73,   51, 2985,  199, 2406,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,  768,   62,  131, 1962,   35,  699,  119, 3050,    6,  411,
          639,    0, 7176,   20,  111,  752,   10, 1914, 4907,   10,  875,  134,
          199,    7, 1232,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   26,    7, 5714,   17, 7076,    9,    7, 3227,   12,  423,  759,
         2270, 1098,   18,  500,    6,  148,  618,   69, 1102,  926,    6,   12,
            7, 8783,    8, 6907, 4895,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  213,   25,   10,  150,  138,   33, 7223,   26, 5802,   62,    0,
            8,   21,   11,    6,  142,   10, 2546,  436,   29,   25,  150,   17,
           21,   11,    6, 1117,  126,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([36, 32, 37, 22, 35, 35, 39, 22, 26, 31, 20, 38, 26, 29, 31, 31],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8418, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8721, 0.8208, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(48.4796, device='cuda:4', grad_fn=<MulBackward0>)
True True tensor([[  67,    7,  143,   24, 1260,   85,   21,    0,    7,  143,   24, 1873,
           17, 1057,  453, 3540,   34,    7,  133, 1890,  279,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,    0,   24,  552,  132,   71,   13, 4445,    0,    8,    0,  204,
           70,  956,   34,   91,    0, 2767, 2923,   37,    9,  264, 1736,   46,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 211,    0,    0,   19,  641,    0,    0,   19,  154,   46,    0,  682,
            0,   67,   26,   17,  155, 4234,    0,  846,  174,    0,    0,  211,
            0,    0,  682,    0,   70,   34,   17,   91,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,   89, 9264,    0,   86,  860,   13,  488,   91,    0,   19,
          213,   10,  289,  116,   13, 1455,   80, 2263,  556, 5610,   54,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  142,   10, 4431,   35,  140, 4076,   69,  832, 1486,
          335,    0,    8,   24,   73,  150,  832, 1486,  335, 2546,  829,   69,
           21,    6,  926, 1585,   71,   21,    6, 4241,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [   9, 1296,   10, 2572,   25,   33, 1678,    0,   19,   11,   45,  142,
           10,  296,  185, 5825,    6,   10,  367,  132,   69,    6, 5072,  172,
         2117,    0,    8,   24,   11,   57,  142,   10,   87,   13,  277, 1860,
           35,  399,  102,  287,    0,    2],
        [  24,  465,   11,   18,   87,   33,  131,  204, 2905, 3141,    0,  100,
           25,   87,    9,   13, 1192,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17, 1073,   13,    0, 1878,  629, 1891,    7,  595,    0,    9, 1416,
           12,   25,    8,    0, 4958,   54,    7, 4292,    0,  109,  893,  199,
           39, 4292,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 115,   26,   33,  250,   17,   24,  451,   51,  101,   62, 1256,   12,
            0,   12,  538,    0,   21,   11,    6,  528,   17,   24,  135,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,   12,   33,   10,  289,   17,    7, 4719,  629,   70,   24,   73,
         7039,    8,   70,   24,   73, 2032,   26,  288,  142,   10, 4038,   22,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33,   26,  106,   38,  469,  291,   18,  187,   62, 1224,   12, 1491,
            3,    9,  564,  868, 1047,    0,   84,  162, 1107,  820, 5149,  690,
         7653,   20,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  60,  116,  188,   13,  264, 3742, 6211,   17,  368,    6, 1019,  908,
          768,  254,  108, 6211,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 5346,   12, 1009,   17, 1241,   13, 1127,  567,   26,    0,  554,
            0, 2816,  419, 1661,   57,  969,    6,  369,  109,  419, 1620,   54,
         5974,  440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 214,   63,  341,  125,  115,   19,  135,   17, 4751,   26,    7, 1037,
         2829,   17, 1459,  188,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 103,   25, 1260,   85, 2264,  241,  140, 1782, 5152, 1160,  158,   25,
          162, 2639,    0,   25, 2220, 1631,  830,  150,  347,   53, 5953,   48,
         6387,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,    0,  692,   10,   66,   13,  740,  672,   71,    0,    7,  230,
            0,  688,    0,   71,    7,  230, 4157,    0,   71,    7,  230, 1404,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([23, 25, 34, 25, 35, 42, 19, 28, 26, 26, 28, 19, 28, 18, 27, 26],
       device='cuda:0') tensor([1.0000, 0.8755, 0.8188, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8677],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(46.5689, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 13:59:02 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
True tensor([[  67,  108, 8830,   48,  589,    0, 7557,    6,    0,  166,   63,    7,
         7557,    6,   17,  601,  170,   10, 6108,    8, 2544,    8,   51,    0,
            0,   13,  562,   18,    0,    8, 9063,   62,    8,   29, 4404,    8,
           29,   69,    0,   53,    0,   63,  261,  143, 7678,   62,  131, 1912,
          688,    0,    2],
        [  24,  116,  144, 6985, 2421,    6, 4502,   80,  854,   39, 3350,  581,
           77, 1004,    7, 6869, 9271,    0, 3381,   70,   11,    6,  956,    0,
            8,   24,   11,  121, 2138,   13,  325,   12,  183,    9,    7, 1726,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 185, 2193, 6248,   22,    6,    9, 1777,  150,  156,   79,   93,  675,
           45,    9, 5222, 1290, 1373,    6,    6,  693,    0,   67,  294,   73,
           51, 5853,  131,    7, 2932, 3288,    0,    8,  203, 1203, 1015,  922,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 106, 1244,   35, 1966,  673,   54, 2409,   10, 2465, 3602,    6,   10,
           33, 2178, 2445,  266, 1192,   69,   89,  906,    0, 3927,   63, 4015,
         5869,   12, 2434,  108,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  154,   21,   26, 2497, 1226,   55, 1221,   10, 1772,    9,    7,
          245,  561,  929,   13, 2226, 1757,   55,   70,  169, 1085,   10,    7,
          461,  266,  763,  480,    6, 1042,    7, 3677,  188, 3699,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   7, 4100, 1649, 1085,  143, 1149,    0,   67,  914,   69,    7, 1296,
           12,   13, 4400,   15,   22,  996,    0,  333,  555, 7691,  109,  333,
          555, 3619,  215,    0,   67,   21,   11,    6,  499,  250,   10,   51,
         4837,   12,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7745,  659,   51,    0,   67,   21,  689,   11,   18, 4432,
           10,  452, 1539,  128,    6,    0,   17,   24,   66, 5020, 6083, 5475,
            6,   46,    7, 1320,   59,  816,  300, 1079, 1643, 1794,  230,   85,
            7, 1219,  859,   35, 2514, 4209,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   9,  409,    0,    7,   29,   70,   26,    0,    9,    7,  572,    0,
         2317,    9, 7418, 3782,    0,    7,  214,   17,  229,  155,  572,  283,
            0,    7,  214,   17,  229,  155, 3236,  283,    9,  155,  572,    0,
           63, 8220,  111,  415,   59, 8186,   71, 2317,    9, 2011, 3043,    0,
            2,    1,    1]], device='cuda:7') tensor([51, 38, 38, 31, 36, 40, 44, 49], device='cuda:7') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(69.2369, device='cuda:7', grad_fn=<MulBackward0>)
True tensor([[ 131, 1183,  148,   26, 1948,  664,   62,  132,   71,    7, 4617,  292,
         2109, 1775, 4040,    6,   12,   33, 6536,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   19,   11,   48,  100,   10,  227, 2174,  232,  445,  159,  283,
           71,   13,  277, 7074, 4112,    0,    8,    7, 4112,   26,   33,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1425,   19,  144,   10, 1336,  235,    0,   19,  442,   17, 3486,
            8,   79, 2392,   79,   19,  442,   21,    0,   39, 2285, 1790,   12,
         2922,  552,  244,  110,    0,    2,    1,    1,    1,    1],
        [ 877,  290, 2689, 2345, 1212,  188, 2277,   48,   10, 2699,  143,    8,
          143, 5593,   10,   33,    0, 5413, 3301, 5690,    6,  199,  459,  510,
           96,   17,  719, 2070,    0,    2,    1,    1,    1,    1],
        [  29,   24, 6103,  154,   12, 4781,   79,  378, 3816,  214,   17,   24,
          991,   55,  133, 3926, 3965,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  12,  538,    0,  108, 4734,  192,   11,   18,  116,  468, 1004,  183,
            0,   53,  113,  468,  106,  561,   10,  561,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53,   66, 4595, 1122,   59,  699,   35,    6,  819,  424, 1942,   96,
            8, 4515,    6,   17,   63,   86, 3412,   62,  131,  883,   48,  293,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103, 2263,   96,  498,    0,   10, 3424,    0,  180,    7,  723,
           73, 5415,    0,    0,    7,  524,    8, 3536,    7, 2517,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 427,  412,  430,  158,   13,  188,   13, 1905,  321,   12, 6873, 1117,
           17,    0,  120, 8491,   48,    0,    7,  427,  412,  430,  158, 3336,
           10, 1051, 1672,  153,  336,    0,  116, 7743,    0,    2],
        [  29,  417,  140, 2362,    6,   93,   26, 3423,   13, 1396, 2233,  199,
           39, 3993, 1874,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,  188,  226,    7, 3841,    6,   20,   12,  333, 1670, 2127, 9746,
            6,    0, 3448, 7414,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  225, 4576,    6,   10,  110,    7, 1790,  225, 1831,   79,   13,
          785,   35, 1933,   35, 1178, 3642,    9,   17, 6460, 4057,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  116,  323,   13, 4454, 2761,   69, 5335,   80,    7, 5169,    8,
         1095,  248, 8099,    6,  558,   10,  545,  218,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 4639,   17,   24,  220,   51, 1155,  143,  254, 3857,    8, 1064,
            0,   67,  138,   10, 3918,    6,   20,  126,    7,   91,  106,    7,
          218,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2551,   12,  324,    8, 6790,  214,   26, 2720,   48,   10,
         7572,    8,  211,  234, 1410,  837, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  101,   11,    6,    0,   13, 4961, 1531,    0,    0,   79, 3708,
           18, 3304,    0,  169,   66,  246,    0,   79,  238,   79,   13, 1880,
         1531,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   10, 4637,  373,    0, 2192,   26,  116,  486, 6101,  567, 4111,
           10,   51, 4637,   62,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 8900,   62,  250,   19,  172,  144,   13, 2184,   22,  311,   69,
            0,   26,   17,    7, 1956,  204,  213,    6,   10,  283,   55,  159,
          110,  128,    0,    2,    1,    1,    1,    1,    1,    1],
        [   8,   17,   11,    6,  593, 1706,   12,   13,  183, 2760,   55,   77,
            7,  341, 1565,  409, 1215,   17,   24,  135,   10,  468,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1777,  188, 1220, 6380,   48,   17, 2439,  558,  464,    0,   53,   11,
           57, 5332,   54,   13, 4369,  200, 1515, 2902,    8, 3585,  567,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   63,  423,   12,  117, 3357,    0,    8,   53,   63, 4673,    9,
          159, 4268,    8,  159, 4972,    0,    8,   53,   63, 2762,   94,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45,  976, 8372,   17,   24,   66,  535, 1313, 5260,    7,
          613,  206, 4387,    6, 3480,  108,   24, 1591, 2637,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13,  717,   35, 1315,  322,  801, 5725,    6,  221, 1339, 5061,  292,
            6,    6, 3222, 1026,   14,   48,   71,   33,    9,   21,    6, 9442,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  69,    7,    0,  939, 2836,  834,   26,    0,    7,   56,  111,    6,
         3620,  266,  207,   17,  368,    6, 2829, 7337, 7393,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([20, 25, 30, 30, 20, 22, 26, 24, 34, 17, 18, 24, 22, 27, 21, 27, 18, 28,
        24, 25, 25, 23, 26, 23], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8550, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8848, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8975], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(40.5196, device='cuda:7', grad_fn=<MulBackward0>)
True True tensor([[  67,   24,  150,   17,  545,   91,   12,  117,   26,  204, 4637,   54,
            0,    8,  188,   13,  341,  207,   12, 3101,  126,  138,   10,   87,
          282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   89, 1970,   34,   13, 3090,  756,    0,    8,   89,  291, 3769,
            0,    0,   34,   13, 1390,   37,  684,    8,    0,  113,   13, 1116,
         1040,   37,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  164,  450,   25,    0,   13,  546,   12,  248,  341,    0, 2352,
            0,    0,  248, 1284, 2352,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  391, 1551, 1065,    0,   17, 2182,  144, 3814,   48,   89,  637,
            8,  333,  473,  279,    9,   21, 3706,   55,  110,   10,   13, 1098,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  24, 3195, 6482,   35, 6937,  227, 3134,  221,    0,   19,    0,   20,
            0,  108, 2805,    0,  132,   10,  108, 7009,    6,   11, 1492, 1485,
           12, 3402,   93,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  246,   53,  169,  780,   29,  835,   10, 1008,    7, 8365,    6,
           12,    7, 3311,    0, 7443,   17, 1573,  162, 3754,    8,  401,    7,
          370,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  289, 1228, 3374,    0,  281,   12,   25,  154,   80,  708,
            0,   21,   11,    6, 1301,    0, 2703,  901,  439,   12,  708,   87,
          722, 1228, 3374,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 291, 1105,   18, 3396, 1095,    0,    7, 4480,   79, 1019, 2071,    0,
         1095,    7,    0,    0,    0, 5640, 1375,   79, 1019, 2071,  755,    0,
          254,   94,  148,  162,    9,  509, 1862,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  116, 2213,   54,    8, 9171,    0,   38, 3127,   89,
         3397,   26,    3,   70,   11,    6, 1226,   71,  682,  616,  249,  407,
            6,    0,   38,  187,   11,  121,  144,  185,  453,  110,  128,    6,
           85,  682,  616,  249,  407,    6,    0,    2],
        [ 101, 5523,   17, 1381,  220,   51, 1405,  131,    7,   94,    0,    8,
           24,  583,  251,   94, 9475, 4637,  373,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  53,  162, 1052, 1563,   94,    0,   67,   53,  162,  113,  172, 7176,
           20, 3979, 2120,    0,    8,   53,  162, 9231,   62,   12, 4391, 3834,
           12, 3716,   35, 7305, 2527,   15,  609,   96,    0, 5653,   10,    7,
          218,  415, 2633, 2120,    0,    2,    1,    1],
        [  85, 4615,    0, 9785,  369,   46,   17,  101,   11,   48,  226,  987,
         1181,  131,   39, 6651, 2266,  160,   22,    0,   13, 5289,  126,   10,
         2585,   20,  366,  565,    0,  276,   10, 9304,    7,  282,  126,   12,
          565,    0,    2,    1,    1,    1,    1,    1],
        [   0,    8,   53,  276,    0, 2982,   80, 1790, 8661,    0,    0,  276,
            0, 1445,   53,  162,    9, 3454,    0,  276, 1445,   53,  162,  830,
         1785,  810,    7, 5167,  608, 2020,   12,  159,  931,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  117, 4312,   22,    6,    0, 1025, 3043,   54,    0,   53,  415,
          158, 1515,   71,    7, 7224,    0,    8,    9,  117,  415,  158,  340,
         1955,   53, 3121,   13, 1905, 1762,   12,  941,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   85, 8195,   12, 2931, 2735, 1221,   46, 1016, 4618,
          866,  693,   46,  120,  248, 4618,  866,  693,  162,   86,  952,   10,
          545,  218,    0,   19,  169,  939, 8966,  834,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 131,    0, 5114,  235, 2399, 1006,    0,  889,   63,    0,    0,    9,
           13,    0,  841,    0,    0, 1449, 1778,  159, 5186,   10,    7, 1722,
         3609,   12,  422, 6222,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([27, 28, 19, 26, 29, 27, 29, 33, 44, 21, 42, 39, 35, 34, 34, 30],
       device='cuda:1') tensor([1.0000, 0.8887, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000, 0.8208],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(55.6638, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[  53, 2828,  987,  ...,    1,    1,    1],
        [6938,   25,   66,  ...,    1,    1,    1],
        [   8,   12,  538,  ...,    1,    1,    1],
        ...,
        [  29,   24,   66,  ...,    1,    1,    1],
        [  67,  985,  687,  ...,    1,    1,    1],
        [   0,    8,  432,  ...,    1,    1,    1]], device='cuda:1') tensor([14, 13, 27, 18, 21, 17, 19, 15, 18, 17, 24, 12, 15, 15, 33, 26, 20, 11,
        22, 14, 15, 22, 24, 13, 20, 16, 15, 17, 15,  9, 16, 21, 15, 13, 16, 26,
        19, 13, 24, 13], device='cuda:1') tensor([1.0000, 1.0000, 0.8550, 0.8975, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8838, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8848, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8677], device='cuda:1', dtype=torch.float16)
 > at.  tensor(26.3197, device='cuda:1', grad_fn=<MulBackward0>)
True True tensor([[ 800,   91,    0,  103,  886,  424, 3848,   11,    6,  853,  200,  237,
           26,  593, 3401,  120, 3213,  187, 2044,    6, 1570,    6,  199,    7,
          964,    0,  568,   17,  641,   21,   11,    6,  735,  226,  593, 3401,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24, 8396, 1469,  106,    0,  108, 2914,   37,   93,    0,  131, 2532,
          336, 3494,    0,    0,   67, 1223,   24,   77,   66,   33, 1738,    0,
         2914,   37,   93,   17,   24,   63,  746,    0,   12,    0, 5472, 1117,
          108, 3183,    6,    8,    0, 1459,  994,   26,  126,   84,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2937,  982,  218,  943,    6,    9,  108, 6249,  451,   66,  774,   62,
         3928,    0,    8, 1800,  282,   13, 2333,   10,  175,  850, 1408, 1072,
            6,    0,  109, 3119,  294, 2518,   12,  215, 3928,  254,  956,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,  125,   29,  294,   12,    7, 2787,    9, 1370, 2924,   11,   18,
          116,    9,  670,    0,   53,   11,   57,    9, 1037,    8, 1237,    0,
           70,   25,  113,  296,    0, 6957,    0,   26,  143,   69,    7,  230,
          874,  926,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 115,    0,  103,   25, 1816,   66,   13, 5590,   20,  521,   20,  874,
           93,    0,  115,   11,    6,   13,  324,  183,   10,  388,   21,    9,
          155,  874,    8,  598,   21,    0, 2603,   69,    7,  467,    0,   17,
           11,    6,  916,    0,   25,   11,  158,  446,  126,  347,   21,   11,
            6,  434,   13,  862,  365,  232, 1218,   20, 5590,   20,    0,    2,
            1,    1],
        [ 248,  215,  581,   85, 1366,    0,   19,  154,   46,   19,   11,  121,
          367,   10,   33, 7034,   46,   19,  154,   19,  659,   66,  226, 5187,
          106,   13, 3643,  384, 7710,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,   11,    6,    0,    0,    0,    0,    9,    0,   89, 6525,  230,
          115,    0,   38,  187,  144,   33, 1850, 2536,   48,    0, 1850,   62,
           69,    0,    7, 1850,    6,   20, 2286,  307,    0,    0, 4813,  248,
          215,  581,   69, 2154, 1374,   18,  881,   11,    6,  383,    0,   21,
           34,    7,  133, 2354,    0,    7,  473, 2829,    9,    7,  535, 8311,
            0,    2],
        [  29,  115,    0,  103,   24,  274,  270,   85,    7, 3253,    0,   85,
          108, 8196,    6,   17,  162, 5690,   54, 1004,    7, 3253,    0,  117,
         8196,    6,   63,  100,    7,  384,  174, 2003,   12,  108, 2626,  567,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:2') tensor([38, 48, 39, 40, 60, 31, 62, 38], device='cuda:2') tensor([1.0000, 0.8271, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(75.0274, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[ 225,    0,  509,  ...,    1,    1,    1],
        [ 101,  876,  140,  ...,    1,    1,    1],
        [1979,  265,    0,  ...,    1,    1,    1],
        ...,
        [ 109,   63,   24,  ...,    1,    1,    1],
        [ 117,   63, 4700,  ...,    1,    1,    1],
        [   8,   33,  321,  ...,    1,    1,    1]], device='cuda:2') tensor([14, 12, 21, 10, 15, 16, 11, 12, 18, 11, 13, 17, 11, 18,  9, 13, 12, 24,
        19, 14, 20, 21, 20, 19, 17, 18, 19, 24, 10, 15, 10, 13, 18, 14, 10, 15,
        17, 17, 19, 22, 21, 17, 19, 18, 20, 14, 12, 12], device='cuda:2') tensor([1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740, 0.8652, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8677, 1.0000, 1.0000, 1.0000, 0.8979, 1.0000, 0.8882, 1.0000, 0.8799,
        1.0000, 1.0000, 1.0000], device='cuda:2', dtype=torch.float16)
 > at.  tensor(22.4525, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[   7,  245,   91,   26,    0,   25,  305,   17,  524, 4474,  199,   21,
            6,  523,    6,    8, 3675,   29,   17,   25,   73, 5382, 7394,  251,
          523,    6,    8, 3675,    0,    8,  180,   12,  538,   25,   87,    7,
          744,  461,    0,   25,  388,   77,   12,  117,  523,    6,    8, 3675,
          270,  540,  554,   10,  367,   10,  155, 7034,    0,    2,    1,    1],
        [  24,  169,  492,   87,  860,   13, 3310,  279,    0,    0,   67,   46,
            9,  419,    0, 1165,    0,   24,    0,  591,  103,   24,  116, 1223,
         2396,    0,  134,    0,   77,  346,    8, 1296,   62,    0,  134,    0,
            0,   17,  281,    0,   94,  169,  204, 2990,   70,    7, 1296,   54,
          451,   51,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1141,   26, 1111, 6504,    6,    0,    8, 1008,    0,  423,
           12,  117, 3049, 4515,    6,  162, 1028, 1004,    7,   13, 1966,  407,
          816,  333, 1127, 1303,    0,   29,   70,  162,  117, 1816,  401,   71,
           77,    7,  839,   53,  162,  961,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [7616,    8,  415,  584,   26,   91, 4401,    0,    0,    0,   68,  386,
           65,   67,    9,  108, 5605,  336,    7,  179,    0,   24,   66,   77,
         1587,   12,  218, 4401,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,  278,   13, 1335, 1793, 7470,   97, 1411,   37, 2091,  890,
           10, 1557,   13, 1543,   12, 9829,    6,   10, 3022, 1146,   71,  170,
           10, 3522,    7, 4749, 5242,   11,    6, 1051, 2256,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   21,   11,    6,  321,   12,  100,    0,   25, 1057,   13, 1329,
            9, 7419,    0,    8,  180,   25,  175, 6899, 1314,    0,   10,    0,
          574,    0, 9784,    0,    0,    0,    8,  155,  886,  153,   26,   86,
         1767,   80,   33,  125,   25,   11,  121,  278,   10, 1703, 1404, 1610,
           57,    0,    0,  125,   25,   11,   57,    0, 2107, 1978,    0,    2],
        [  29, 8093, 8187,    6, 1452,  736,  439,   12,  159, 1422,    8,  736,
          439,   12,  159, 3857,    0, 6872, 1752,   35,  494,   15,   18, 1568,
         8187,    6, 1452,  736,  439,   12,  159, 1422,    0,   67,  116,  100,
          419, 3460,    8, 4197,    0, 1452,  288,  655,  439,   12,  159, 3857,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  53, 1918,   10, 2554,  392,  759, 1086,    0,   29,    0,  100,   29,
          294, 1881,  440,    0,    7, 3516,  278,  540,    8, 6732,   62, 5725,
         2502,    6,    0,    8,  853,  174, 8227,   48,    0,   25,  150,    0,
          853,  174,  689,   11,   18,  703,    9,  906, 2937,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([58, 52, 44, 31, 35, 60, 50, 48], device='cuda:2') tensor([[3929,  155,  874,  103,   25,  154,   25,  135,    0,   84,   63, 1230,
            0,   91,   26,  850, 6566,  131,    7, 2932,    8,    7,  218, 1498,
         5804,   10,  170,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,    7,  467,    0,    7,  453, 6578,  148,   34,    7, 1951,   12,
           13, 1318,    8, 5555,   55,   24, 6868,    7, 1406,    0, 2082,    8,
          858, 4007,    6,   10,  367, 4404,    8,   51,   13,  461,   12,    7,
          546,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   21,   11,    6, 3849,    0,   21, 3560,    6,    0, 1259, 1599,
            0,    0,   21, 2293,    6, 1890,   13,  525,  287, 7195,    6,    8,
         1890, 3577,  901, 7195,    6,    0,  100,    0,   56, 2175, 1488,   35,
          868,    6,    0,  166,   26,  133,  341,  106,    0, 1120,  419,  218,
         2475,   12, 6293,    0,    2,    1,    1,    1,    1],
        [  21,  204, 1847,  214,   24,  169, 2807, 3301,   46,  214,  100,  150,
           48, 2184,    6,  156,    6,  109, 4377,   93, 2058,  424,    6,    6,
           46,    8,   73, 2575,  134,  199,   13, 4458,   18,  160, 1032, 1116,
          111,   45,   37,    0,  166,   25,   73,  774,  199, 1120,  419, 1862,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 1384, 1224,  368,    6,   80,  423,  759, 9162,    6,   13,
          383,    0,  166,   26,   80, 1450,  439,   12,   77,    7, 1947,  619,
         5948,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1291,   22,  140,  416,   22,  110,  556, 1949,   11,    6,   13,  391,
           35,  511,   35,  290,   35, 6979,   35,  715,  234, 1927, 3156,   12,
         1954, 6935,    9,    7,   43,   59,  371, 1644,  827, 1151, 3144,    6,
            0,   85,   80,    3, 5494, 1360, 5997,    0,    8,   19,   11,  121,
          226, 1797,   54,   84,   55,  294,  215,    0,    2],
        [  94,  506,  204,  498, 1557,   54,  126, 8282,    6,    0,  934,    0,
            0,  109,    0,   53,  506,   86,   66,    0,   13, 4561,  583,   71,
         1230,   94,   10,    0,  456,   80,   13, 6711,  120,   53,  220,  116,
           87,   13,    0, 4454, 3836,    8,    0,  175,   21,  691,    0,   71,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   7,  853,   22,  335,   26,   17,   25,   73,  446, 2533, 5203,  336,
           21,    0,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,   11,  121,  116, 3498,   25,   63, 3302,   12,    7,  133,
          946,    8, 9743, 1081,   17, 5577,    6,    8,  896,  227, 2174,  232,
          445, 6442, 1052,   62,   48,  562,    6,    8, 1122, 1079,  292, 3751,
            6,   73, 9478, 2808,   55,  159,  447, 3965,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  274, 6269,    8, 6269,  199,    7, 1710,   12, 5554,    0,    8,
            9,   17,  207,   24,  175,  914, 4616,    8, 4616,   10,   33, 2570,
         2240,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  218, 2604,    6,   12,  813,    0,   79,   25,   11,   57,  211,
         6449,    0,    0,    0, 4837,   63, 2875,   80,  860, 4825,    6,    0,
          106, 3836, 4506,    6,    0,    0, 1817, 4506,    6,    0, 1817,  837,
         5002,    8,   29, 4404,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,   77,   87,   33,   13,  277, 4838,    0,  166,    0,   26,
          347,   24,    0,   73,    0,   77,  274,  132,   85,    7,  370, 1303,
            0, 3253,    8,  150,    0,   33,    8,  113,   33,    8,  113,   33,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 225, 1639,   85,    7, 1749,  747,  670,   79,   13, 2829, 1020,    9,
            7,  384,  221,   11,    6, 2100,    0,   29,  225,  278,   10,  150,
           77,    7,  807,   17,  278, 3172,   22,  126,   12, 2092,    0,   55,
         1976, 1043,    0,  148,  162, 4111,   10,   51, 7806,   48,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 116,  336,   17,  183,  206,   19,   34,  961,   17, 7374,    0,   19,
         1260, 1004,    7, 4895,   12, 5205,    8, 4776,   84,   34,  486,  546,
           24,  162, 5054,    0,    7,   82,    9, 7723,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   34, 3642,   84,    0,  116,   13,  110,   57, 1802,   35, 1933,
           35, 1178,    0,    8,   19,  934,   48,   13,  422,  378,    9,   13,
         2414, 2196,    0,  125,   33, 5353,  220,   86,  601, 1584,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,   11,   45, 2464, 1778,    0,   68,  194,   65,  211,    0,   53,
          162,  172,  488,  184, 2148,    6,    0,    8,   53,  465,   11,   18,
          135,  261,   80,  998,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:4') tensor([29, 39, 53, 50, 30, 57, 50, 18, 47, 27, 42, 38, 48, 34, 36, 30],
       device='cuda:4') tensor([1.0000, 1.0000, 0.8652, 1.0000, 1.0000, 1.0000, 0.8394, 1.0000, 1.0000,
        1.0000, 0.8887, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(67.1718, device='cuda:4', grad_fn=<MulBackward0>)
True True tensor([[  67,  490,   24,   87,   17,    0,   19,   11,   48,  100,   25,   10,
          154,   17,   85,  419, 1800,  183,  120,   25,   11,   57, 2202,   84,
            0,    9,  155, 1992, 5896, 1247,    0,    7, 2583, 8748, 9169, 3413,
           54,   10,  155, 9879,    0,    7, 9879,   26, 3413,   54,   10,  155,
         2583, 8748, 9169,    8,   33,   26,  138,  155, 4941,   26, 3803,    8,
         8965,   48,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [5609,   10,    7, 1793,  183,  368, 5673,    0, 1276,    6,  499,   87,
         4986,   79,  261, 1269, 2536,   20,   79, 1970,    6,    0,  166,   26,
          509,  254,   21,   34,    9,   95,  424, 3400, 2197,   11,    6,  383,
            0,   67,   19,  499,  154,   17,  250,  225, 2396,   26, 4803, 5040,
            0,   38,  187,   66,   86,  226, 2373,    9,    7, 9091, 1313, 9094,
            3,   68,  194,   65,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  56,  525,   11,    6, 4197,  246,   10,  267, 1848,    0,   38,  160,
            0, 1491,    0,    0,   33, 4925, 2823,    6,   55,  815, 6091, 1086,
            3,  267,    0, 1848,    0,  148,   63, 1102, 4907,    0, 1260,   69,
            0, 4340,  982,    0,    0,   38,  511,   17,   11,    6,   86,   77,
           46,  415, 1040,    0,   26, 1028,  126,    0,   71,   13,  264,    0,
         1375,    0, 2199, 1132,  820,    3,  225,    0,    0,  246,    0,    0,
           38, 2207, 4925,  164,    0, 2823,   55, 1581,    3,    3,  225,    0,
         8640,   48,    8,  246,    0,    0,   38,  187,  192,   11,   18,  135,
            0,  103,   17,   11,    6,    0, 1581,    3, 1995,  241,  221,  109,
            0, 1581,    3, 1793,    0, 1086,    0,    0,   67,    0, 3296,    0,
           21,   11,    6, 1581,    3,    3,   68,  194,   65,    2],
        [   8,    9,    7,  370,  207,  294,   12,  170,   63,  115, 5954, 4273,
            6,   17, 3412,  108, 1259, 1882,    0,  108,  203,    6,  777, 1991,
            0,  108, 3857,    0,   69,    7, 1080,    6,   17,   33,  659,  601,
          170, 3560, 4390,    0,   24,   73,  884, 1179, 5625,   54,    8,   39,
          128,   93,  195,   54,    7, 1289,   24, 2056,    0,   24, 8099,    0,
           24, 3836,    0,   24, 2153,    0,   73,  450,  170, 4324,   12,  183,
         1179,  250,  659,  205, 1226,   71,  108, 3227,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   71,   33,  822, 5108,    9,   13, 3708, 4774,    0,   24,   66,
         6960,   48,    9,  824, 2723,   45,  726, 1090,   17,   24,   73, 3814,
            7, 1729,   35, 1727,  430,  690, 3275,  110,  249,  132,   10, 1421,
          439,   12,   70,   26, 5914, 1417,   46,  248, 1086,  690, 3275,  110,
          249,    0,   24,   87,   21,   85, 7738,    0,  820,  828,    0,    8,
           24,   73,  499,  875,   17, 3151,  346,    0,  630,   12, 1823,    0,
          103,   25,   73,  875,    9,  143, 3708,    6,    0,   24,   73,  204,
          229,    7,  126, 2446,  276, 3028,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 875,   54,   94,  970,   10,  970,   71,  108, 3648,   26,   13,  207,
           12,  875,   54,  134,  970,   10,  970,   71,   94, 1004,  183,    0,
         1004,  672,    0, 4355,  931,  659,   66,  226,  133,  341,   10,  108,
          447,    0,   67,  148,    0,  100,  170,    0,  144, 1080,    6,    8,
         4989,    0, 2084,  335,   18, 1991,    6,    8, 3128, 2261,    9,  159,
          931,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([ 64,  65, 130,  82,  92,  63], device='cuda:3') tensor([1.0000, 1.0000, 0.8271, 1.0000, 1.0000, 1.0000], device='cuda:3',
       dtype=torch.float16)
 > at.  tensor(157.1290, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[  67,   25,  296,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        [  29,   70,  321,  ...,    1,    1,    1],
        ...,
        [  29,   24,   73,  ...,    1,    1,    1],
        [ 229, 1118,   63,  ...,    1,    1,    1],
        [ 103,   25,   11,  ...,    1,    1,    1]], device='cuda:3') tensor([ 9,  9, 13, 11, 14, 10, 14, 16, 15, 10, 13, 13,  9, 18, 11, 17, 20, 14,
        17, 14, 13, 11,  7, 14,  7, 14, 13, 13, 16, 10, 12, 14, 13,  9, 11, 13,
        12, 14, 11, 12, 15, 10, 11, 11, 14,  9, 10, 11, 15, 13,  7, 15, 15, 13,
         8, 17, 13, 20,  7, 16, 14, 13,  7, 13], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 0.8105, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8555, 1.0000,
        1.0000, 0.8579, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000,
        1.0000, 0.8984, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8369,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8882,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8677, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:3', dtype=torch.float16)
 > at.  tensor(16.9683, device='cuda:3', grad_fn=<MulBackward0>)
True True tensor([[3706,   55,    7,    0, 3210,   12, 2058,   35,  249,   59,  240, 1144,
            0,   53,   11,   57,  133,  946, 2787,    9, 2047,   12,    0,    0,
           73,    0,   24,    0,  694,  214,   10, 2231, 2736,  941,    0, 1645,
         2078, 2399,   70, 2892, 1075,    0,   73,   87,    8, 2293,  359, 4531,
          946, 6710,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  13,  143, 4089,  207,   10,  154,   80,   89, 6674,    6,  506,   51,
            9, 2047,   12, 1387,  687,    8, 4480,    0,  206,   85,  419, 1800,
          613,    9,  183,    0,   71,  419, 1127,  723,    0,   19,  217, 1102,
         1387,    8, 8219,  106,   17, 2216,    0,   77,   79,   13, 2443,   12,
           70,   19,  296,   10,   87,  230,  115,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   7, 7374,   17, 5290, 1247,    0,   79, 4546, 1247,    0,  220,  508,
         3667,   10,   13,  733, 6608,   12,  341, 1603, 4345,   46, 1259, 1247,
            0,  618,   59, 1247,    0, 3109,  140,   57,  411,  266,   26, 2856,
         1247,   46, 4574,   48,    7, 2303,   12,    7, 1827,    8,    7, 4715,
           12,    7, 1136,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [4157,    0, 7957, 6266, 1119, 1017,   21, 1962,    0,   85,  109, 3019,
          964, 4157,    0,    8,  180, 4587,   13, 1206,  567,   10, 1005,   21,
           84,    0, 4958,    7,   59,  627, 1202,  290, 1408,    0, 6712, 5933,
         6962,   26, 2770,   10, 5989,   85, 1362,   20,  586,  922, 4157,   71,
         3955,   45,  502, 8366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 120,   19,   34,   39,    9, 1324,    9, 1954, 6935,    0,   19, 1346,
           80,   13, 1361,   12, 8188, 4039,  148,  144, 1828,  244, 1339, 1027,
          587,  195, 3485,    0,    8,   13, 8188, 1793,  148,  692,   10,  508,
         4460,   69,   17, 3485,    0,    8,  211,  218, 3280,  692,   10,  205,
            8,  601,  267,  508, 4460,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 103,    0,   84,   11,    6,    0,    0,    0,    0,   91,  759, 4039,
           46,    8,   19,  172,  135,   33,   46,  148,   63,    0, 3163,   10,
          229,    0, 1583,  583,    6,    0,  148,   63,    0,    0, 3163,   10,
           51,   69, 3836,    0,   19,  217, 2497, 1123,   17,   24,   73,    0,
          204,  468,    7,  538,   12, 1261,    0, 2318,    0,   55,    7, 4959,
           12, 1146,    0,    2,    1,    1,    1],
        [   8,   29,   19,  246,    0,   38,    6, 1049,    3,    0,   29,   19,
            0,  175,  518,    0,    8,  225, 1559,   69,    0,    8,   71,   13,
          277,  523,   12,    7,  170, 1522,    0,    0,    0,   13,  176,    0,
            0,  180,  225, 1518,  336,    0,    8,  225, 1143,    0,   80,  423,
         2360,    0,    8,    0,    0,  225, 1518,  270,  336,    0,    8,  225,
           11,    6,   77, 6087,    6,    0,    2],
        [  85,    7,  183,   19,   34, 1931,   54,    9,  117,  341, 1427,    0,
           19,  474,   19,   34, 3258,   54, 4029, 1352,    0,   67,   69, 1858,
         2007, 1498, 1261, 9096,  237, 1181,    0,    8,   19, 4600,   19,   11,
           48,  204,  226, 3258,   54,   13, 1127,  546,   55,  143,  254,  423,
          215,    0,    8,    7, 2838,   69,  264, 1736,   34,   21,    6, 2402,
          119, 8197,  271,    0,    2,    1,    1]], device='cuda:5') tensor([52, 57, 53, 54, 55, 64, 67, 65], device='cuda:5') tensor([0.8887, 1.0000, 1.0000, 1.0000, 1.0000, 0.8208, 0.8584, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(111.5854, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[  21,   11,    6,    0,   86,   79,  103,   19,  144,    0,  419,    0,
          479,   70,   21,  169,  274,  100,    0,    0,   67,   84,   34,   13,
          841,   17,   24,  169,   66,   10,   87,  250,   71,  264, 9520,    0,
           12, 5149,    8,  250, 1057,   10,   87,   71,    0, 1588,  468,    8,
          250, 1057,   10,   87,   71, 1136, 1370,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  21,   11,    6,  116,  226, 4174,    0,  166,  818,  251,  148,  162,
         7220,  144,   10,   66,   13,  207,   12, 8254,  839,   10, 1703,  251,
          248,    8,   13,  854,  759, 1086,    0,    8, 1065,    0,  143,   55,
            7, 6580, 8068,    0,    8,    7, 3742, 7856,  249,    0,    8,    7,
         4528,    0,    8, 3097,    7, 2248,  181, 3414,   20,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [  67,    7,  630,   13, 2003,   96,    0, 1189,    0,   29,  138,  261,
           87,   56,  234, 1265,  241, 2142,    0, 2994,  122,  241, 2142,    0,
           13,   59,  380,   18, 3110,    8, 3098,    7, 1503,   12,  886,  412,
         3276,  287,    9, 4928, 1703,   55,   17, 7752,    9, 2446,   10,   17,
         2340,   10,    7, 1503,   12, 7121,  484,    0,  166, 2231,    6,   17,
         3833, 1548,    0,    2,    1,    1],
        [  21,   11,    6,  434, 3919, 8570,    0,    8,   21, 6430,    6,    0,
         2584,  218,  214,    0, 7753, 2736,  296, 1212,    8,  227,   93, 1069,
           96,    0, 2797,   54, 1868,  187, 3635, 1319, 3562, 7289,    8,  218,
         2808,   35, 5444, 2840,    6,   10, 3814, 3001, 3188, 1068,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,  258,  636,  132,    9, 5950,   20, 1006,    0,    8,    7, 3564,
          162,  172, 3633,   17,    7, 5317,  144,  226,   13, 2786,    0,  125,
           85,   17, 2110,    0,   19,  144,   13,  277,  523,   12, 2080,    9,
           91,   12,   89,  488,   10,   96,    0,    8,   19,  474,    0,   38,
          122,   57,  411,    0,  125,   19,   11,   45,  142,   10,    7, 8022,
            6,    3,   68,  194,   65,    2],
        [   8,  115,  339,  110,  873,  908, 2955,  187,  445,  128,    0,    8,
          289,   17, 4586,  931,   46,  166,  185,   12,    7, 4793,   24,  175,
          568,   69,   33, 4959,   46,  120,   25, 2554,    7,  282,   12, 2307,
            0,   13, 2980,   37,    0,   13, 3700,    0,   13, 1276,    0,   53,
           63,  521, 1015, 3127,   54, 7553,  111,  199,    7, 2340,    0,    2,
            1,    1,    1,    1,    1,    1],
        [   0,   19,  113,  213,   10, 2678,   33,   91,  132,   13,    0,    0,
          523,    0,  138,   73,    0,   53, 4477,    0,   19,   11,   45,    0,
          142,   10,  388,  134,    0,   69,   13,  824,  586,  457, 5445,    0,
            0,  166,    0,   26,    7,   38,  122,  241, 1515,   10, 4853, 1435,
           85, 8251, 1243,    3, 1189,    0,   29,  168,   21,   26,    0,    2,
            1,    1,    1,    1,    1,    1],
        [1220,    9, 3023,    0, 2058,  181,  541, 1775,  271, 4814,    6,   66,
          226,  619,   10, 1754, 7371,  955, 2028,    0,  100, 4896,    6,    0,
         2678,  777,  812,    6,    0, 3183,    0, 2011, 6504,    6,    8, 4222,
            0,   17,   66,  226, 3150,  111, 7225,   62,  199, 2116,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:5') tensor([57, 59, 64, 48, 66, 60, 60, 48], device='cuda:5') tensor([0.8740, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8369, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(98.4842, device='cuda:5', grad_fn=<MulBackward0>)
True True tensor([[ 347,  568,   21,  988,    0,  148, 1006,    6,    0,   26,   33,  116,
           13, 1678,   17, 9004,    6,    8, 6904,    6,  722,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13, 4746,   35,  803,   18,  249,   26,   13,  382,  783, 1522,  723,
            0,    8,   24, 5874,    8,  749,    6,  174,  589, 6010,  722,   55,
            7,  218, 1543,    0,    2,    1,    1,    1,    1,    1,    1],
        [  70,   33, 3565,  170,   26,    0,   33,  164,   15, 4482,  181, 2958,
            0,   21,   11,    6,  113, 2092,  365,    8,  883,  140,  365,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57, 1211,   17,   17,   11,    6,  204, 1031,  156,   62,
          199,  281,   94,   11,    6, 2702,   32,   54,   85,  185, 1648,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  103,   17,   11,    6,   86,  890,    0,   53,   11,  121, 2107,
           69,   10,  289,    0,  281,  490,   53, 4853,  747,  670,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1325,   53, 5415,   33, 1165,    0,   24,   11,  158,  492, 6290,  565,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  10, 1141,   17,  630,    0,   19,   11,   45,  142,   10,   66,   10,
          884,  486,   91,    0,  166,   26,    0,   70, 1148,  120, 6249, 8221,
            6,  415,  158, 1515,    0,    2,    1,    1,    1,    1,    1],
        [ 115,    0,  138,  323,   19,  367,   10,   33, 1760, 3140,   12,   13,
           48, 1184,  140, 3085,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    7, 4268,   12,    7,  744, 6213,   55,   33, 1599,   34,   17,
           21,  220,   51,  619,   69, 8786,    6,    8,   91,   35, 1933,   35,
         1178,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [1008,   46,   25,   11,   57,    7, 3698,    0,    0,    0,  125,   25,
          618,  155, 1037,  282,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,   13, 2198,  614, 1810,   34,    7, 4023, 2922, 2371,   56, 4654,
         2041,    9,    7, 1261,   12,    7, 1384, 1224,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  26,   21,  148, 1073,    7,  281,  839,  109,  148,   26,  281, 8064,
           10,  267, 3557,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   73,  876,  121,   10, 2697, 2360,    0,   10,  206, 7616,  568,
           86, 2705,  307, 1906,    0,    8,   26,  417, 1841, 1059,   62,   71,
         1894,  688,    6,    0,    2,    1,    1,    1,    1,    1,    1],
        [2018,  527,   93,   35, 2287,  234,  439,   12,    7,  179,   11,    6,
         1682,  931,    9,    0,  108,    0, 1884,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 4479, 1339,  783,  511,   37, 1405,   13,    0,  682,  247,   17,
          101,  434,    0,   38, 1879,    0, 2102,    3,  166,   26, 1223,    0,
         8044,   55, 3708,    6,    0,    2,    1,    1,    1,    1,    1],
        [  19,  321,   12,  100, 1060,  117, 1532,    8, 2346,   18, 1011,   71,
          565,  359,   13, 2555,  237,  866,    9,  321,   12,   13,  854, 4656,
            6,  737,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  24,   66,  255,  200,   22, 6339,   54, 2200, 2362,    0,    8,   24,
           66,  244,  168,  264, 8973,    6,   69,   13, 4481, 3489,   10, 1764,
          387,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [2805, 2219,   69,   70,   91, 1985,  150,    0,    7, 2805,   12,   17,
          107,   20,   15,    8,  291,  156, 3837,  457,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,  103,   17,   11,    6,   86, 4949,  890,    0,   25,   73, 5652,
           91,    8,   25,  175,   13, 2685, 8831,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 106,   17,  552,   13,    0,  325,   12, 5828,    6,    0,  109,  126,
            6, 2893,    6,    0,    0,   79,   19,  434,  134,    0,    0,  214,
           17,   19, 1385, 1313,    0,    0, 6263,   89, 6878,    0,    2],
        [  19,  154,  245,   21,   11,    6,  528,   55,  170,   10, 2613,   70,
           21,   26,   10,   51, 2639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [3695,   12,  134,   63, 1387,   10,  206,    7, 4458,  362,    6,   63,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  744,   26,    9,    6,  234,   54,   17,   84,   26, 8902, 1666,
         2214,   10,  415,  500, 1783,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0, 6482,  215,  581,    0,  903,    0,  846,  315,   22,   18,
          380, 3089,   35, 3304,   62,  284, 3780,   11,    6,  874,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([23, 29, 25, 25, 24, 14, 30, 18, 28, 18, 22, 17, 29, 21, 30, 28, 28, 22,
        21, 35, 19, 14, 19, 24], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8584, 1.0000, 1.0000, 1.0000, 0.8706, 0.8208, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8242, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:0',
       dtype=torch.float16)
 > at.  tensor(39.6049, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 13:59:03 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
tensor([[  24,   66,   13, 7524, 4040,    9,  166,   24, 1393,   10,   91,  723,
           10, 8714,   39,  467,  982, 2884,   12, 1833,    0,   10,   51,   89,
         4023,  570,   59,    0,   89,  772, 1751,    0,    7,  772, 5370,    0,
           89, 2444,   62,  521, 1105,   48,  480,    0,   89, 2294, 1661,  221,
          369,    0,   89, 5930, 4888,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  67,   70, 1320,  335,  684, 4600,    0,    8,   25, 1510,   21,    9,
            7,  227,   93,  362,  715, 2378,    6,   12,  632,  237,    6,   15,
          593,    0,   34,   17,    7,  535,    0, 3139,    0,  227,  233, 2341,
         4267,    6,   12,  574, 2256,   12,    7,  564,  322, 1910,  162, 1028,
           10,   13, 1387,    0,    8,   17,   24,  162,   80,   10,  954,  199,
           91,   12,  251, 9200, 2760,    6,   12, 1261,  120,  768, 2317,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19, 7750,   25,   10, 1064,    0,   55,    7,  245,  183, 3983,    9,
            7,    0,  179,    0,  168,   69,    7, 1366, 2110,    0,   13,  475,
           35, 5203,    0, 2178, 2445,  266,    0, 6580, 1411,  271,    0,  629,
          110,    0,    8,   89, 1751,    0,  903,    0,    0,  728,  649,   57,
           93, 2734, 2003,    0,    0,  106,    0, 7002,   11,    6,  728,   18,
         3411,  300,    6,  369, 7312,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  17,   26,  120,   13,  150,   48,   26, 6788,   62,    9,   13, 7312,
           10,   87,  250,   86,    9,  191,   48,   62,  131, 1377,   46,  100,
         1520,    7, 2192,   12,   13, 1390,    8, 2985,   21,  199,    7, 2192,
           12,   13,   10,  424,  412,    0, 1995, 2083,    0,  192,   11,   18,
          175,  110, 1226,    0,   19,  100, 1390,    8,   10,  424,  412,   96,
            0,   67,   33,   26,  116,  775,   20, 1718,   93,    0,   68,  194,
           65,    7,  150,   48,    6,   63,  180, 2685,   62,    0,  180, 5847,
            0,    2],
        [   7, 2193, 1793, 2493, 7012, 1198,    6, 1361,    0,   13, 1361,   12,
           94,  148,    0,   69,   13, 1284,  383, 1663,    0, 3284,  199,   13,
         3508,  982,  964,    0,   85,  159, 3506,  160,  128,    6, 3696, 3836,
            8, 2187,    6,    9, 2828,   62, 7689, 1974,   22,   18,    0,  100,
           33,    0,    8,   53,  456,   80, 5002,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   8,   29,  115,    7, 2932, 1381,    0,  148,   55,   13,   87,  610,
          215,    0,  188,  923, 3150,  111, 6226,   62,    7, 2688,    0,   26,
          115,    9,    7, 3140,   12, 1057,   10, 4007, 1179,   10, 2156,  109,
         6406,  346, 1711, 4166,    0,  125,    7, 6105,   10,   13, 3147,  234,
         1827,   26,   29, 4126,   17,   53,   73,   11,   18, 1799,   71,   21,
          419,  218,  207,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1232, 5917,    6,  427,  362, 1011,  131,    7, 3022,   46,    8,
           19,   11,  121, 2138,  185,  798,  215, 2115,   54,  336,    7, 4959,
          401, 1221,   69, 8633, 1232,    6,    0,    8,   66, 3395,   62, 3585,
         1118,    9,  392,   10,  798, 1075,    9,   33, 4959,    0, 2386,   12,
         3585, 1118,   46,  752,   10,  661,   70,  513, 1226,   71,  108, 1232,
         5917,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:4') tensor([55, 73, 67, 86, 57, 65, 63], device='cuda:4') tensor([1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(131.5358, device='cuda:4', grad_fn=<MulBackward0>)
True tensor(13.5354, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[   8,   21,   11,    6,   13,   24,  237, 1355,   54, 1909,    0,   19,
          154,    0,    9, 8041,   12, 4972,    0, 3982,  366,   12, 4972,    0,
            8,   19,  154,   33, 5510,   12, 1102,  639,    8,  998,    9,    7,
         1136, 5055,   26,   39, 1909,  206,    7,  832,    0,    6,    0,   73,
          172,  305,   13, 5247, 2883,    0,    8, 7419,   26,   91,  663,    0,
            2,    1,    1,    1,    1,    1],
        [  29,  168,   25,   66,   13,  567,    0,   25,   66,  250,   46,    8,
           84, 1631,  227, 8263,   12,   17,  993,    9, 3804,   46,  185, 8829,
         2385,   39, 4312,   22,    0,    8,    7, 4312,   22, 3060,   96,   10,
         3793,    0,    8,  288,  120,   13, 6737,  925, 1585,   17,  188,    7,
          230, 1051,  174, 1991,  568,    7, 5250, 1085,    0, 1239,    0,    2,
            1,    1,    1,    1,    1,    1],
        [  84,  188,  226,   39, 2244,   48, 2080,  199, 2627,  106, 1146,    0,
          106,    0, 5036,    0,   67, 5830,    0, 2288,    0,    8,   77,   12,
           13, 5827,   24,  144,   33,    0, 3024, 2244,    9,    7,  245, 1345,
           12,   33,    0,  464,    0,  347,    0,   19,  154,   84,    0,   63,
          391, 2826,    0,  248,  535,   35, 4935,    0,    0,    0, 1649,    8,
            7, 7678,    0,    2,    1,    1],
        [  19,  246,    0,   38,  187,   11,  158,   87,   21,  106,  384,  237,
          803,    3,   53,  246,    0,   38,  469,   57,   11,    6,  211,  207,
           25,   11,   57,  142,   10, 6785,   13,  759, 6052,   35, 3481,  176,
           12, 2811,  839, 2202,    9,  384,  237,  803,    3,   29,    9, 6769,
            0,   19, 5016, 1222,   13, 6478,  244,  699,  411,    8, 3122,   10,
          264, 1027,  119,  232,    0,    2],
        [ 101,  246,    0,   38, 3127,   19,  692,   10,   51,   13, 2182,  684,
            3,    8,  101,  246,    0,   38,  200,  969,   19,  278,   10,    7,
         8459,  464,   12,  670,    0,   89, 3540,  465,   11,   18,  305,   21,
         6064,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19, 1060,  267,   80,  267, 3013,    0,    8,  225,  246,    0,   38,
         1969,  135,    0,  635,   19,   11,  158,  618,    0,  635,   19,   11,
          158,   14,    0,   67,   19,  192,   11,   18,   66,   13, 3013,    3,
          225,   34, 1922,  200,   54,   51, 1795, 2073,   18,    0,  166, 6121,
          267, 7150,  244,    7,  215,   10, 1393,  133, 1665,    0,    2,    1,
            1,    1,    1,    1,    1,    1],
        [ 476,   25,    0,   68,  386,    0,   65,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 101,  591,   17,   53,  162,   56, 8862,   10,  384, 1050,  140,   54,
            7, 7522,  469,   18, 1568,    0,   10, 9547,  945,   80,   70,  506,
           51,    0,    8,  101,  591, 1730,   17,   53,  465,   11,   18, 1799,
          238,   71, 6765, 1955,  109,  802, 6027,   69,  251, 6765, 1955,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:6') tensor([61, 60, 64, 66, 39, 59,  8, 49], device='cuda:6') tensor([1.0000, 1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 0.8110, 1.0000],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(99.6466, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[ 225, 1119,    0,   38, 1461,    0,   17,   11,    6, 1284,    0,   70,
           26,   21,    0,   26,   17, 7148,  650, 3727, 1994,    3,    8, 4069,
            0,   19,   73,   51,   13,  277,  593, 4187,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,   34,  133, 3315,    8,   19, 1385,  185, 4673,  214,    8,
          442,  185, 1968, 3051,    6,   17,   19,  213,   10, 1452,   71,   25,
          440,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 294, 3940,    0,   68,  386,   65,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  55,  251,  370,  215,    0,   89, 1970,    8, 3460, 1102,   14,   48,
           12, 1167,    0,    8,   19,   34,  914,  244,  221,  505, 1563,   80,
         6088,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   25,  162,  461,   12,   33, 5673,    0,   70,  169,   25,  289,
            0,   63,   25,  850, 1203,  494,    0,    0,    0,  238,    0, 2859,
           46,  138,   87,   25,    0,  276,  135,    0,  125,   25,   11,   57,
            0,   86, 3531,   10,  456,   80,   21,    0,    2],
        [  29,   33,   26,   86,  116,   13,    0, 6429,    0,   80, 7291,    6,
            0,    0,   21,   11,    6,   13, 6429,   80, 3295,    9, 1146,   79,
          238,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  89, 2574,   26, 5205,    8, 4970,   63,    7,  473,  825,  120,    7,
         3022,  188, 2096,   10,   87,   21, 1574,    0,    8,   24, 2775,   11,
           18, 5768,   62,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 276,   13, 2900, 6815, 5707,   62,  392,  439,  143,    9, 7001,  254,
          225,  323,    9, 8979,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [8310,   54,   17,    0,    7, 2468,   12, 2573,    0, 1067,  525,  436,
            0,   17,   19,   34,  752,   10,  747, 2653,    0,  288,   51, 1217,
            6,    9,  248, 6874,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,    7,  207,    7,  227,   93,   45, 1604, 2334, 2895, 5070,  170,
           46,   24,   11,  121,  115,  278,   13, 1234,   10, 9375,  347,  117,
          227,   93,   45, 1604, 2334,   63, 7071,  341,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   53,  245,  487, 2980,   54, 2263, 1315,    0,   21,  220,  305,
           79,  294,   79, 1111, 6052,   12, 2533, 1390,   10,  229,   13, 1127,
         1116,  569,   12, 2263, 1315,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21, 1797,    6,    7,   39,   22, 1522, 3667,    8, 2322,   12, 7932,
         2673,   96,    8, 4157,    6,  142,  270,  490,    7,   69,    6,  307,
           12,    7,  473, 2217, 1137,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  25,   11,   57,  961,  736,  439, 7692,  629, 2481, 2286,   62,  235,
            8, 5225, 5108,    8, 2288, 5225, 5108,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  388,    9, 3189,    0,    9,    7,    0, 1832, 2930,    0,   84,
            0, 7774,   62,   77, 1587,   12,  993,  106,    7, 1465,    0,   80,
         2192, 6631,  793,    0,    0,  281,   12,  166,    0,   19,  465,   11,
           18,  661,    0,    2,    1,    1,    1,    1,    1],
        [   8,   21,  875,    6,   25,  199,    7, 7771,   12,   70,   11,    6,
          226,  434, 1440,    0, 2243,   59,  827,  760,    0,  883,  424,    0,
         1579,  287,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,    0,  154,  281,   12,  134,  169,   51, 8661,    8,  958,  756,
            0,    0,    0,    8,    0,   79, 5474,    0,  294,   12,  134,  169,
          914,   51, 7078,  706,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([34, 27,  7, 27, 45, 27, 29, 18, 30, 34, 31, 32, 21, 40, 28, 30],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8877, 0.8564, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 0.8223],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(54.0231, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[ 125,    9, 2483,   35,  606, 5731,    0, 2932, 6930,    6, 5989,    8,
          447,  281,   12,    7, 2483,    6,   46, 4346,  737,    0,  742,  160,
          140,    0, 3213,   46,    8,   53, 5470,    7, 2791, 2008,    8, 3156,
          199, 2248,  713, 1777,    0,    2],
        [  13, 4495, 1435,    9,    7,  832,    0,    6,    0, 2721,   17,    0,
           12, 5990, 8459, 3665, 1118,    0,  248,   35, 8516,    6,   12,    7,
         5990,  889,  144,  708,    8,  288,   91,   35, 8516,   12,    7, 5990,
          596,  144,  708,    0,    2,    1],
        [ 168,   26,   13,   29,  300,  166,  188,  278,   77,    7, 8681,   12,
         7343,    0, 3280,    0, 1976,   25,  289,    0,   19, 3250,   21,    0,
           39,   19,  362, 1838,  235, 5384,    9,   25,    8,  180,   25, 3745,
           10,   21,    0,    2,    1,    1],
        [ 822, 2317,    9,  984,   11,    6, 6339,   17, 4877,  244, 1953,    6,
           10, 2386,   12, 1655,   12,  215, 4373,    7, 6484,   12, 7616,   69,
          984,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  67,   70,  143,  220,  225,   66,  691,  103,  225, 1631, 1088,  144,
            7, 1885,  818,   12, 3802,    6, 2875,   10,  267,   10,  719,   13,
          841,   17,    7, 9413,   17,   94, 1095,  144,   10,   51, 1529,   62,
         3840, 4173,    0,    2,    1,    1],
        [  19, 1223, 2894,   55, 3101,  839,   69,    7, 2291,    0,   68,  194,
           65,    8,   19, 5429,   80,    7, 5986,    6,    0,    8,   19,   66,
          591,   13,  555,  214,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  66,   25, 1060, 9832,  347,    8,  138,   29,  294, 9696, 2419,    6,
           66, 6028,   48,  244,    7,  473,  640,  215,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [1223,   70,   21,  968,  110,   34,   46,   68,  194,   65,   68,  386,
           65,   46,    7, 1587,   12, 1075,   24, 5559,   63,  324, 1075,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([42, 41, 40, 27, 40, 30, 22, 25], device='cuda:1') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:1', dtype=torch.float16)
 > at.  tensor(70.0887, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[  21,  914,   26,    7, 1226,  279,   10,   87,   10, 4746,    8,  368,
            7, 3939,    8,  946, 2780,    0,   38,  511,    7, 1381,    0,   53,
         1346,  267,  546,    8,    7,  218,   94,    0,    8,   53,  246,    3,
         1768,    0,   25,   11,   57,  230,    0,   24,  451,  229,   13, 2240,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   7, 2964,  169, 2154,  724, 2468, 7147,   69,    7, 2618,    8,  719,
          117, 1230,   35, 6937,   35, 7596, 1710,    6,  442,   12,   13, 1127,
         2468,    0,  736,  439,  203, 1255, 4711, 1755,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [7823,   94,   63,    0, 4820, 1256,   94,    0,    8, 4820, 1256,   94,
           46,    7,  143,    8,  143, 4820, 1256,   94,   84,   63,    0,    7,
          143,    8,  143,   24,   11,  158,    0,    0,   66,   13, 4820, 1256,
            0,  179,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  84,   34, 9199,    0,  180,  552,  853, 3174, 4796,    0,  440,   24,
           66,  211, 3174, 4796,    0,    7, 1575,   35,  372,   59,  247,  119,
         1819, 2735,    9,    7,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 180,   19, 1095,  108, 2333,  158,  240, 1028,   10,  670,    9,   58,
          389,    6,    8, 2852,   59,   18, 1374, 2197,    6,    0,    8,  474,
            0,   21,   11,    6,   86,  230,   55,  110,   10, 1005,   33,  218,
          939,  668,  232,  834,   29,   19,  339,  134,  116,   46,   53,  144,
           10,   51, 1644,  411,    8, 2736,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  17,  370, 4506,   17,   24,  116,  323,   71,    7,    0, 6144,  779,
          569,  164, 2676,   66,  475,   35, 2371,    0, 1683,  979,   54,    0,
            8,    0,    0,    7, 3180,  164,  289,    0,   38,  786,    0,  346,
            0,  859,    0,    0,  230,    0,   13,  176,    0,   95,  266,    0,
           17,   11,    6,    7,    0, 2636, 4057,   10, 1557,    0,   17, 1683,
          518,   10,  155, 3280,    3,    2],
        [  19,  513,  270,   10,    7, 1224,    8,  487,  665,  336,   10,  150,
          103,   19,  220,  446, 3302,  206,    0, 7490, 1390,   54, 7521,    6,
          144,  226,    0,    0, 2517,   48,    0,    8,   21, 1932,  126,   84,
          162,    0, 3695,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  77,  230,    0,   29,  339,  110,  508,    0,   25,   39,  663,   12,
         8681,    0,   12,   13, 1760,  321,    0,    8,   19,  213,   10, 7032,
           13, 1455,   17,   19,  154,   26,    0,    0,  133, 4501,    0,  166,
           26, 8164,   54,    0,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([50, 34, 40, 31, 56, 66, 41, 42], device='cuda:1') tensor([1.0000, 1.0000, 0.8799, 1.0000, 1.0000, 0.8735, 0.8794, 0.8555],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(74.7457, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[   8,   46, 2307, 1008,   33,  283, 7772,    0,    0,   38,  160,  119,
          480,    0, 6880,   55,  155, 5223,  197,   46, 1363, 1850,   37,    0,
            0,    9,  545,    0, 1227,    0,   33,   34,  138, 7803, 1808,    0,
            0,  192,   11,   18, 3995,    0,    0,   13, 1192,  552,  126,    0,
           21,  144,  211, 3236,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,    7, 4174, 2078,  487,    0,    7, 1762,   12, 2718, 2202, 6611,
         6671,    9,    7,  774,   12, 5428,   34,   79,  488,   79,    7, 1762,
           12, 2718, 2202,  850, 3909,  607, 4357,  407,    9,    7,  774,   12,
         1947,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   85,    7,  467,   12,   21,    0,   24, 8152,   48,   17, 2878,
           12,  708,   73, 1082,   10,  368, 2930,    8,    7, 1465,   69,  159,
          447,    0, 5333, 2515,  366,   12,  148,  109,  206,   53,  162,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24, 1405,   13,  846, 4399, 7016,   12,   13,   56, 2515,  292,
         2404,    0,    8,   25,   73, 3684,  375,  688,  359,  341, 8829,    6,
           17, 2231,  341, 7125,    6,    0,   29,   17,   73,  601,   25, 5415,
           70,   11,    6,    9,    7,  563,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   11,    6,  916,   26,   53,  323,   21,    9, 2008, 1491,    8,
         6343,    0,  206,   21,   11,    6,   38,  626,   57, 6505,  128,   10,
          508,  561,  804,    6,    3,   53,  144,   10,  508,   21,  113,    9,
            7,  832,    0,    6,    0,   10,  175, 1719,  292, 2460,    0,   29,
           19,  154,   84,  162,  391,  832,    0,    6,    0, 2116,    9,  132,
         6034,  264, 1736,  148,  162,  461,   12,    7, 3677,    0,    2],
        [   0,   33,   26,   13, 1523, 7376,    0,    0,   21,   11,    6,   39,
          985,  687,    0,    0,   21,   11,    6,    0,   13, 4912,   12,   77,
         1587,   12,  813,    0,   86,  116,   80, 5807,    8,    0, 9827,    8,
         6689,    8,   29,   69,    0,   67,   80,  896, 2604,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   21,  169,  492,   66, 7747,   10,  110,   10,  154,   17,  116,
          125,   19,  144, 1157,   13, 5462,    9,  166,   13, 2232,   34,   13,
          227,   37, 1010, 2650,   37,   17,  101,   34, 3524, 2015, 1197,   12,
           77, 4039,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  169, 1017, 8788,   55,    7, 1732,    6,    0,    8,  780,   10,
         2895,   71,  134,    9,   13,  207,   17,   34,   79,    6,  762,  366,
         1094, 3835, 1256,    0, 6510,   54,   13, 2866,  979,   12, 3802,    8,
         7376,    9,  166,   24,  220, 1082,   10,  283,  540,    8, 1766,   91,
          486,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([54, 39, 37, 44, 71, 48, 40, 51], device='cuda:7') tensor([0.8584, 1.0000, 1.0000, 1.0000, 1.0000, 0.8721, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(81.2945, device='cuda:7', grad_fn=<MulBackward0>)
True tensor([[ 24, 498,  71,  ...,   1,   1,   1],
        [ 33,  34, 245,  ...,   1,   1,   1],
        [ 87,  21, 238,  ...,   1,   1,   1],
        ...,
        [886,   0, 476,  ...,   1,   1,   1],
        [ 29,  19, 415,  ...,   1,   1,   1],
        [596,  73,  66,  ...,   1,   1,   1]], device='cuda:7') tensor([ 6, 10, 10,  7, 10,  7, 10, 11, 10, 10,  6, 11, 10, 14, 10, 12, 14, 18,
        14,  8,  8, 14, 13, 11, 11, 15, 12, 11, 10, 14, 16, 10, 13, 20,  8, 17,
         9,  9, 21, 10, 17, 10, 12, 12,  8,  8,  8, 12, 14, 11, 10, 11,  9, 10,
        11, 12,  7,  9,  8,  4, 10, 12, 10, 13,  9, 14, 11, 15, 11, 10, 12, 14,
         8, 11,  7, 12,  9, 13,  9, 14, 13,  9, 13, 12,  8, 10, 10, 13],
       device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8633, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8174, 1.0000, 1.0000, 0.8384, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8755, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8091, 0.8799, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8105, 1.0000, 1.0000, 1.0000, 1.0000, 0.8579,
        0.8691, 1.0000, 1.0000, 0.8516, 1.0000, 1.0000, 0.8652, 0.8286, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 1.0000, 0.8564, 1.0000,
        0.8223, 1.0000, 1.0000, 0.8276, 1.0000, 1.0000, 0.8394],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(13.5802, device='cuda:7', grad_fn=<MulBackward0>)
True True tensor([[1189,    0,    0,   29,  339,   11,    6,    0,  289,   21,   11,    6,
         1491,    0,    0,    8,  339,   11,    6,  289,   21,   11,    6,  384,
          372, 1761,   20,    0,    8,  115,    0,   25,   73,    0, 1948,    0,
          205, 1817,    8,  446, 1835,   13, 1484, 3762, 2293,   59,    0,    2,
            1],
        [   0,   67,   25,  135,    0,   19,  154,   25,   63, 4130,   71, 2084,
         1625,  128,    6,    0,   46,    7,   81, 2510, 3496,   18, 9875,    6,
            0,    8,   77,  117,  214,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  91,  383,    0,   19, 7262,   62,  126,   12,    7, 2657,    9, 3918,
         1118,  432,   13, 3040,   71,   13, 3532,   91,    0,    8,   19, 5429,
           62,  199,   13,  682,  181, 3435,    0,  206,   19, 1060,    7, 2859,
           57,    6,    6,   55,   13, 2705,    0,    2,    1,    1,    1,    1,
            1],
        [   8,  120,   19,  289,  923, 1738,    0,   21,  220,  499,   51,   29,
         4733,   17,  211,   91, 3485,   12,  282,  700, 6317,    6,  486,    0,
          166,   26,   13, 3462,  474,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,  113,    7,  473,  203, 2470,  247,   55,    7, 3299,  267,   48,
           12, 2533,  382,  399,    0,    8,  113,    0,   12,  538,    0, 3547,
         6952,   55,  486,  733, 3609,   12,  218, 1369,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  94,   63,   29, 4493,   12, 6608,   17,   53,  780,    8, 2509, 1459,
            0,  276,   94,  148,  192,   11,   18,  213,   10,  109,   73,   11,
           18,    0,   10,  873, 1417,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 432,   77,    0,  305,   13,  274,   85,  117, 2792,    6,    0,   97,
          737, 1010, 1144,    8,  415, 2633, 2041,    0,  179,   82,    6,    0,
         6229, 1106,    0, 7050,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   7,  240, 2120,   66, 2138, 4021,    0,  752,   10,    0,  661,  347,
           21,  188,    0,    0,   33,  133, 2035, 1236,  297,  293,  111, 3495,
           35,   18,  500,   62,  800,    0,    8,    0,   53,   11,  121,  367,
          132,   71,    0,    0,   13,  800,   12, 1254,    0, 7052,    6,    0,
            2],
        [ 125, 2392,   37,  109, 1065,    0,   24,   11,  158,   51, 7258,   62,
           71, 4012,   80,   33,    0,    8,   21,   11,    6,  509,  103,   24,
          154,  835,   80,   21,    0,  276,  103,   24,  213,   10,  154,  835,
           80, 2826,  347,   24,  451,  492,   87,   21,    0,    2,    1,    1,
            1],
        [   8,  168,   21,   26,   55,    7,  245,  183,  291,  121,  233,   62,
           85, 1366,   46,  108,  245, 4878, 1806,  469,  816,   24,  293,  457,
            0, 6712, 5831,    6, 1103, 3007,   54,   71,  282, 1117,   13,   24,
          293,  457, 7757,   54,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   29,   70,   24,  487,   71,   34,  172,   13,  264,  207,   12,
          826,   80, 1009,   39,    9, 3562,    0,  359,   13, 1039,  434,  229,
         1261,    0,  166,   24, 6532,    9, 5855,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 101,   34, 1061,    0,  225,   34,  798,    0,  100,   13,  325,   12,
            7, 1323,    6,   24, 1618,    0,   53, 3994,   11,   18, 2603,   56,
         7038, 1568,   80,  159, 1666, 4388,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 391,  215,  581,   19,   34, 1074,    9,   13, 5200,    9,   13,   56,
         2561,   35,   45,  973, 2102,   54,  325,    0,    8,  440,   19,   11,
           45, 4379,   85, 1366,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,  225, 3673,   62, 1406,  290,  125,  225,  692,   10,  229, 1123,
          225, 3673,   62,  250,  225, 1425,  267,  708,  169, 2034,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   19, 1425,   17,   85,   17,  765,   17,   70,    0,   19,  451,
          508,    0,    0,   10,   89,   94,   26, 1370,    8,  958,    0,    0,
            8,   17,   11,    6,    0,   70,   19,  513,  432,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,   70,   19,   11,   45, 2659,   55,   26,    7, 5708,   12,   13,
          264, 1329, 6813,   46,   19,   11,  158,  367,   10,   33,   13,  277,
         1065,   46,    8, 1645,    7, 4936,   12,   13,  264, 1455,  199,    7,
         2781, 1234,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:0') tensor([48, 31, 44, 31, 34, 31, 30, 49, 46, 42, 33, 32, 30, 24, 35, 40],
       device='cuda:0') tensor([0.8550, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8003, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8433, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(60.4968, device='cuda:0', grad_fn=<MulBackward0>)
True tensor([[ 976, 1475,    0, 2184,  176,    0,   24,  154,   29,    0,   29,  115,
           17,   24, 2534,  117, 2522,    8,  591,  117, 3422,   17,  339,  170,
           87,  117,  214,    0,   24,  487,   10, 2252,   17,    0, 3423,    0,
          995,   17,   24,   73,   87,   71, 1780,    0,  995,   17,   24,   73,
           87,   71,   13, 1472,   12, 1780,    8,   13, 2705,   24,   73,  115,
           87,   71, 5414,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  499, 3008,  813,   69, 3634,    6,    0,   67,  115,   24,   73,
         3008,   13,  325,  143,  813,    0,  143,  254,  700,  490,    0, 2761,
           54,   21,   26, 4865,    0, 4585,   54,   21, 4865,    0, 4912,   21,
           26, 4865,    0,  979,   54,   21,   26, 4865,    0,    8,   70,   24,
           73,   87,   26,   24,   73,  203, 1792,   33,  813,   55,  368,    6,
           17,   24,  492,  276,  934,   48,  120,   24,  245, 2861,   62,    7,
          660,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21, 1518,  126,   17,   71, 1421, 1601, 2421,    6, 3195,   54,
            9,    7, 5199,    0,  432,  101,  976,  261,  442,  284, 3140, 2226,
           69,   33,   46,  101,  144,  284, 7986,    8,  101,  144,  284,   46,
          101,  258,   57,   33, 1266,   55,  908,  254,  248, 1551,    8,   34,
          529,   10, 7554, 4115,   13, 4058,    8,  278,  923, 2294,  244,    7,
          409,   17,    0, 5098,   46,  284, 5098,   46,   21,   11,    6,    7,
          245,  183,  101,   11,    6, 1831,  100,  101,   11,    6,  144,   39,
         1266,    9,  815, 1132,  215,    0,    2,    1,    1,    1],
        [ 108,  296,   55, 3051,    0,  108,  296,   55, 4029,  687,    0,  109,
          108,  296,   55, 3069,    8, 8464,    0,  109,  108,  296,   55,  540,
          687,    8,   55, 2848, 2050,   93,    0,    8,  103,   25,  154,   80,
            7,  277, 1920,  148, 2456,    6,   69,  155, 1067,  265,    8,  148,
           26,  415,  878,  111, 7307,   62,  168,    8,  133, 7759,    8, 6128,
            0,    8,   85,  185,  613,   77,   12,  170,  296,   10,  205,  126,
          199,    7,  179,   10, 3231,    8,   10, 3522,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29, 2983,   84,   11,    6,   13, 3024, 7927, 4719,    0,    8,   33,
          172,  132,    6,  307,  110,    0,  125,   13, 1027,  631,  407, 3565,
           25,  432,   13, 2767, 6204,    0,   84,   11,    6, 7008,   80,   39,
         1061,   35, 1315,  322,  183, 5848,   10,   46,   24,  321,  290, 4971,
            0,  498,    7, 4971,   93,  979,    0,   67,   70,  281,   94,  192,
           11,   18, 2252,   26,   17,   69, 2313,   21, 1847, 3497,   10, 1421,
         1113,  109,  143,   55,    7,    9, 1610,   45, 1032, 1809,  424, 1420,
          233,  373,   10,  276, 1772,   10,  575,  132,    0,    2],
        [5902,  589,    6,   69,  837, 1827,  162, 6386,    0,   38,  597,  269,
          340,  407,  323,   21,    0,  347,   73,   11,   18,   24,    3,   19,
         1850,   62,   39, 2792,   69, 5063,    8,  434,   21,   38,  290, 2078,
         1525, 6664,    0, 9413,    8, 9696, 2419,    3,   19, 8063,   48,   13,
          630,   10,    7,  640,    3, 6463,   12,    7, 2672,   85,    7,  183,
            0,   38,  412, 2366,   26,    7, 1802,  322,   12, 8338,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  203,  779,  675,  793,   26,  142,   10, 3128, 2129,   12,  248,
         3834,    0, 2129,   24,  164, 8896,  117, 1752,   35,    6,  335,   18,
         1617,  457,  183,   35, 2470,    6,   96,    9, 1764,   20,  484,  480,
         1081,   12,  108,  447, 2573,  131, 1520,  203, 7712,  237, 2332,    0,
          109,  994,  117, 3382,    6,   63,  142,   10,  175, 7154,   48,    9,
          291, 1076,   20,  484,  480, 1081,   86,   12,  108, 2573,   46, 1187,
          111,    0,  131,   82,    0, 1599,  109, 2145,  586,  271,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([66, 75, 91, 82, 94, 72, 84], device='cuda:5') tensor([1., 1., 1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
 > at.  tensor(150.4254, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[  29,    0,   25,  ...,    1,    1,    1],
        [   8,  180,    7,  ...,    1,    1,    1],
        [   7, 4821,   10,  ...,    1,    1,    1],
        ...,
        [ 540,   53, 3122,  ...,    1,    1,    1],
        [   8,   19,   11,  ...,    1,    1,    1],
        [ 635,   24,  296,  ...,    1,    1,    1]], device='cuda:5') tensor([13, 13, 20, 13, 14, 18, 18, 10, 16, 11, 16, 15, 17, 11, 19, 12,  6, 12,
         8, 17, 12, 21, 13, 12, 15, 18, 17, 15, 19, 24, 15, 14,  9, 18, 11, 16,
        15, 14, 12, 21, 14, 15, 15, 11, 21, 14, 20, 17, 23,  7, 17, 14, 12, 14,
        15, 13], device='cuda:5') tensor([1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 0.8799, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8735, 1.0000, 1.0000, 0.8789, 1.0000, 1.0000, 0.8286,
        1.0000, 0.8652, 1.0000, 1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8677, 1.0000, 0.8550, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8882, 1.0000, 1.0000, 1.0000, 0.8105, 0.8003, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 0.8521, 1.0000,
        1.0000, 1.0000], device='cuda:5', dtype=torch.float16)
 > at.  tensor(19.7456, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[ 250,   17,  169,   51,  528,    0,   67,  113,  250,   17,    0,  169,
          305, 5542,   12,    0,   77,   12,  117, 7099,    6,   17,   91,    0,
            0,    0,  144,    0,   38,  511,    9,    7, 1165,   12,    7, 1827,
         2260,    0, 4619,   13,  325,   12,   94,    0, 4619,    0,   94,  148,
          162, 2129, 9829,    6,  109, 4488,   93,    0,    0,    0,    8,  113,
           86, 1057,    0,    0,    0,    9,   89,    0,    0,  447, 1165,    0,
           13, 3557,   10,    0, 4110,   80, 3903,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,  278, 1421, 3675,  126,   12,   21,   55,   89,  670,  125,
           19, 1831,   17,   24, 1591, 2637, 3181,    6,    8,  811,   35,  426,
          356, 5149,    8, 1127, 1848, 7178,   48,   10,  367,   10,   13,  670,
          206,   84,   34,  874, 2286, 5575,   62, 8517,   17,  877,   20,  307,
           62,  134,  333,  383,    0,  125,   21, 1017,    6,   13,   10,  375,
            8,   39, 7574,   80,  138,   25,  598,   80,   94,  535,  490,   25,
          508,  134,    7, 4340,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  432,  423,  215,   12, 7989, 1435,    8,   56, 2687,  240, 1032,
         6606, 6480,    0,  131,    0, 1655,   12, 5709, 2245,    0,  333, 2767,
         2931, 3182,    9,    7,  179,  188, 8152,   48,   17,    0,    7, 6929,
            6,    0,    0, 5914,   69,    7, 1232,   63, 2426,   10, 2034,    8,
           17,    7,  979,   12, 2753, 4429,   26,  211,    0,  143, 1565,   93,
          254, 3563, 4401,    6,   12, 2753,  824,   48, 4577,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   79,   25,  150,    7, 1819,  205,    9, 6714,  111,  359,   84,
            0,    8,  180,    0,    0,    0,   68,  386,   65,    8,    7,  281,
         4673, 1809,  411,   26,   17,   89,  874,  204, 1917,   62,  499,  535,
          890,   10,   87,   17,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   34,   89,  765,   10,  749,  221,    9,    0,   10, 2750, 1222,
         2433,   55,   70,   63,  172,  288,   13,  874, 1256,   12,    7,  133,
         1219, 5222, 3770, 3254,    0,    8,   19,  144,  116, 5893,   13,  488,
            0, 1061,   35, 1315,  322, 1039,   55, 2829, 1020, 1747,  160,  741,
            0, 3150,  111,    0,    8,   19, 1425,   19,  220, 6785,   13, 3028,
         1329,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  269,  290, 1543,  106,   10, 1059,  188,  691,    7,  291,   18,
         1397,  156,  457,    0,  391, 1543,    6, 4437, 2361, 1268,    0,  895,
         1101,  269,  484,    0, 1912, 2172,    0, 5087, 2172,    8, 1339, 1373,
          699,   57,   77,   85,    7,  370,  183,   46, 6835,  203,  620,  293,
            6,   62, 4437, 1395, 4781,    9,  166,   24,  205,  126,    0, 2213,
          132, 1546,  121,   22, 1691,  269,  484,    0,  388,    9,    7, 4437,
            6,   17,  204,   66,    7, 4273,    6,    0, 3020,  126,    7,  269,
          290,    8,  180,  339,  134,  205,    0,    2],
        [   7,  218, 8314,   12, 1780, 1119,    0,   38, 1865,   89,  227, 2704,
            7,  179,   34, 1621,    3,  166,   26,   10,  289,   21,   11,    6,
         1301,   17,   19,   73,   11,   18,   87,  778,    0,   67,   19,   73,
         1123,  111,   87,  250,    0,   19,   73,   55,  122,  366,    0,   19,
           73,  570,    0,   19,   73,  575,  132,    0,   19,   73, 6295,    0,
           19,   73,   51,   13,  461,   12,   33, 2469,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]], device='cuda:4') tensor([81, 78, 71, 42, 63, 92, 70], device='cuda:4') tensor([0.8003, 1.0000, 0.8882, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(142.2732, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[  29,  180,   24,  388,  117,  427,  412,  430,  158,    6,  540,    9,
           13, 1127, 1692,   10,  150,   70,   53,  169,   87,    0,    8, 3188,
           54,   69,    7, 3530,    0,   24,   66,  185,  427,  412,  430,  158,
            6,   69,    7,  859,   17,   63, 1944,  336,    8,   21,  100,    6,
           10, 2677,    7,  218, 1710,    6,    9,   21,    6, 1422,    0,    2,
            1,    1],
        [  25,   11,   57, 3487,   62,   71, 7863,   54,   21,  346,  461, 1010,
          111,   35,    6,   20,   15, 2397, 1408,    6,    0,    9,    8, 2584,
          119,  218, 5933, 4601,    6,    0,   85, 1629,   35, 1898,  684, 3049,
            6,    0, 1189,    0,    8,   77,   25,   66,   10, 5971,   25,   63,
          155,  248, 1893,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  21,  169,   51,  100,   13, 3019,   35,  262,  411,  176, 1064,  206,
           25,  150,    7,  688,   85,    7,  467,   12,    7, 7781,    0,   67,
           21,   11,    6,   13, 2701, 1723, 1064,    0,   68,  194,   65,    8,
           84,   11,    6,  211,  207,   12, 2432, 2881,   80,    7,  688,   85,
            7,  467,   12,    7, 7781,    0,    2,    1,    1,    1,    1,    1,
            1,    1],
        [  19,  144,  288,   33, 6629,   46,   89, 4795, 1583,    0,   89, 1192,
            8,   13, 3434, 2626, 6951,   46,   17,   19, 6414,   21,    9,    7,
         2422,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   0,  225,   15,  290,   19,   93,   15, 4835,    0,    0,  238,    0,
           21,   11,    6, 4810,   17,   25,  451,  884,   17,  125,   91,   12,
            7,  214,   17,   11,    6,    0,    0,  916,   80,  378, 2639,    0,
           26,   25,  204,  175,   13,  341, 5200, 5072,    0,  613,  120,   25,
         6337,    7,    0,  207, 6878,   62,   94,  229, 4388,    0,    2,    1,
            1,    1],
        [   8, 5348,  203,   45,   22, 1026,    0,    0,  148,   34,  172, 1968,
           80,   21,    0,    0,    0,  144,   91,  524,    0,    0,  101,  465,
           11,   18,    0,  100, 1339,  742, 2919,  373,    0,  125,  101,  474,
           21,  169,    9,    6,  300,   18,   94,   71, 7576,   11,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   7, 1141, 4433,  270,    9,    7,  832,    0,    6,    0,    0,    9,
         6848,  518,    7, 4139,    0,   38,  779,  180,   19,  205,  270,  637,
            8,   19,  456,   10,   94, 3006,    9, 2667,   35, 1966, 1265, 3916,
            6,   84,    0,    8,   53,   11,   48,  289,    3,   25,  135,    0,
          417, 2847,    0,   25,   73,   11,   18,  172, 1897,  518,    7, 4139,
            0,    2],
        [  70,   11,    6, 4767,   55, 1136,  958,   26,  113, 4767,   55, 2216,
         2314, 5149,  125,   21,   11,    6,  172,  999,   55, 1201,   10,   66,
          486,  574,  187,    0,  211,   91,  213,    6,   21,    0,    8,    0,
          204,    0, 7009,    6,  192,   11,   18,  213,   10,  205,  637,   71,
           13,  903,  763, 2129,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:0') tensor([60, 53, 55, 27, 59, 49, 62, 54], device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8652, 0.8911, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(80.8909, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 13:59:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-17 13:59:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
tensor([[   9,   33, 1760, 1165,    0,    0,   24,  162,  802,   13, 2270,   45,
            0,  109,    0,    0,  339,  110,  116,  150,  103,    0,   19,   73,
          175,   33, 4481,    0,    0,   13, 9238,    0,  217, 1076, 5997,   56,
         8368,  922,    0,    0, 2806,  240, 2175,  249,    0,    2],
        [   7,  225,  158,   12,    7, 1052,  249,  781, 1158,   34,  561,   48,
          199, 1580, 6032,   85,   13, 2290,   17,   24,   11,   57, 2438,   54,
          131,    7,  467,   12,   33, 1910,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    9,  251, 1113,    0,   19, 1608,   11,   18,  283,   69,    7,
         6389, 6035,    0,   19, 1608,   11,   18, 1927,   13, 4446,  109, 2618,
           39, 5651,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  144,    7, 1666,  941,   35, 9748,  688, 9372,    6,   17,  842,
          785, 1601,   10,   82,   45,  132,    8,  180,   25,  162,  859,  665,
           13,  321,   12, 4594,  111, 1650,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,   77,   12,   13, 5827,    0,  168,   19,  217,    0, 6815,
          241,  829,  106,  747,  670,    9,  564,  957, 1132,    0,    8,   33,
         1148,    0,    8,   24, 2252,   17,   38,  290, 1408,  197,  513,  755,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7, 1043,   19,   11,   45,   29, 7105,   80,  467,   54, 3753, 3023,
         1382,   11,   18,  116,  125,   21,   11,    6,  999, 3023,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    0,  154,   80,   21,   79,   13, 1780,   35,   18, 1397,    0,
         1523, 1909,   12,  294, 2386,   12, 1655,   12, 2602,    0,  166,   26,
         2546,  829,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1831,   17,  103,   19,  220, 3665,  929, 7897,    0,   19,  220,
         4075,   17,    0,  432,   77,    0,   19, 1359,   11,   18,  172, 2702,
          111,   19,  158,    0,   21,   34,  185, 4416, 4113,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    9,   33, 1165,    0,   21,   11,    6,   39, 1663, 1039,  369,
           55, 2327, 1034,  240,   20,   69,  117, 4601,  825, 3275,   35, 3794,
         2885,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   56, 3168, 2445,   93,   11,    6,   13,  133,  528,  461,   12,
            7, 9655,    8,   24, 5150,  108, 1613,   10,  274,    9, 1820,   55,
         4455,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   69,    7,  218,    0,  874,    0,   24,   66, 1075,   17,
            0,   63,  244,  737,  891,   54,    0,   69,  837, 2694,    0, 5955,
           10,  159, 6768,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2072,  988,   26,  250,   17,  689,   11,   18,  100,   10, 2895,  133,
          261,    0, 3706,  359, 7036,    0,    8,   12,  538,   24,  169,  100,
           10, 1082,  143,   80,   21,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,    7,  218, 2475,   12,   81,   18,  340, 8978,   26,    7,
          227,  424,    6,  560, 5824,  424, 1825, 1158,    0,    8,  117, 1816,
          985,  132,  227,  760,  233,    6,   55,   13, 1074,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1980, 1068,    0,   24,  296,   13,  574,  833,   12,  524,   35, 1105,
          505,   54,    0,   86,  116,  524,   35, 5001, 3635,  833,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   33,   34, 2861,   62,  131,   39,  786,  727,   46,  166,   26,
          100,    0, 1730,    9,    7,  467,    0,  101,  164,   86,   66,  995,
            0,  125,   21,  164,  417,  827,  603, 1906,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 138,   87,   25, 1005,    7,  909,  609,    6,  369,   12,    0,    7,
            0, 2018, 2510,  429,   12,    7, 1683,  724, 1025, 6433,   54,    7,
         8094,  429,    0,   12,    7,    0, 1478,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([46, 32, 28, 32, 38, 24, 28, 35, 28, 27, 29, 31, 35, 24, 34, 33],
       device='cuda:3') tensor([0.8242, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8530, 1.0000, 1.0000, 1.0000, 1.0000, 0.8706],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(50.3897, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[   7, 3288,   63,  ...,    1,    1,    1],
        [  53,  113,  180,  ...,    1,    1,    1],
        [ 282,   26,   80,  ...,    1,    1,    1],
        ...,
        [4069,    0,   21,  ...,    1,    1,    1],
        [2869, 3540, 4712,  ...,    1,    1,    1],
        [  33,   26,    7,  ...,    1,    1,    1]], device='cuda:3') tensor([15, 15, 16, 13, 21, 10, 18, 12, 12, 14, 16,  9, 14, 19, 15,  8, 15, 16,
        15, 12, 10, 15, 19, 18,  8, 23, 16, 18, 13, 12, 11, 20, 19,  8, 15, 18,
        11, 23, 15, 13, 18, 15, 15, 11, 16, 13,  7, 14], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8711,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8320,
        1.0000, 1.0000, 1.0000], device='cuda:3', dtype=torch.float16)
 > at.  tensor(20.8375, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[ 125,    7,  688,  ...,    1,    1,    1],
        [1228,    0,  155,  ...,    1,    1,    1],
        [  84,   11,    6,  ...,    1,    1,    1],
        ...,
        [   7,  214,   24,  ...,    1,    1,    1],
        [   8,   21, 4333,  ...,    1,    1,    1],
        [   8,   29,    0,  ...,    1,    1,    1]], device='cuda:3') tensor([ 8, 10, 11, 15, 12,  9,  8, 12, 10, 10, 17, 16, 12, 10, 10, 11, 12, 18,
         6, 11, 12,  8, 10, 10, 10, 11, 11,  9, 10, 10, 12, 18, 16, 12, 14, 14,
         7,  9, 12,  9, 12, 11,  9, 10,  9, 10, 10,  7,  9, 11, 14,  9,  8,  8,
         6, 12, 10, 12,  8,  9, 11,  8, 11,  8, 12,  8,  7, 13, 11, 15, 17,  8,
        13, 10, 12, 13, 11,  9, 11,  6], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8872, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8379, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8101, 1.0000, 0.8486,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8057, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8257, 1.0000, 1.0000,
        1.0000, 0.8301, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(13.9770, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[ 115,   33,  415,    6, 3784, 2410,   35, 5444, 1466,  659,   46,    8,
           19,  154, 7674,   10,   46,  229,  170,  598,  133, 1986,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,  409,    0,   33,  479,   12,  203,    6,  234,   57,  140,  945,
            7,  733,  479,   26,   29,  528,   17,   19, 1808,   10,  154,   80,
           70, 1148,   71,  801,  639,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,  154,   80,    7, 2695,   12, 6105,    9,    7,  270,   12,  155,
         1066,  125,  214,   73,   51, 1932,  336, 1552, 2117,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 126,  168,  440,    0,   84,   63,   13, 3631,   12,   94,   46,   13,
         2045,  556,  881, 1341,   48,   55,  663,   46,  148,  169,  100,   86,
          288,   10,  991,   13, 3400,    0,   67, 1108,   13, 3400,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 1043,    0,   19,  154,    0,   26,   17,    7, 2705,    6,  369,
          373,  703,   53,   11,  121, 5248,   22,   10,   51, 2705,    6,  369,
          373,    0, 6872,    7,  963,  291,   20,  362,  614,   93,   62,  598,
           21,   11,    6,  226, 1911, 1965,   18, 3840,  134,    0,    2,    1,
            1,    1,    1,    1,    1],
        [ 339,   11,    6,  368,    7, 1232,   79,    7,  772, 3976, 3180,   24,
            0,   66,    0,    0,    8,  661,   85,   70, 3151,   94,  169, 1703,
           55,   33,    0,   29,   53,    0,  175,    7, 8568,   12, 2573,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  79,   13, 1920,    0,   19, 6090,   13,  325,   12, 4528,    8,  144,
         1771,   12,   56,  706, 1251, 1320, 1993,  693,  100, 2483,  371, 1434,
           93,    8, 2481,  726, 2992,    8,    7,  672, 1810,    8,   13,  277,
          523,   12, 4289,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  53,   11,   57,  434,    7, 5875, 4460,  415, 3729,   18,    6,    0,
            8, 2245,   66, 2107,  270,    8, 1797,   62,  143,  813,   69,   77,
           12,  117,   94,  333,  555,  215,  700, 1313,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 103,   25,   11,   48, 2107,  199,    7, 3227,   12,    7,   94,    0,
          238,    0,   25,  169,   66, 1110,   13,  325,   12, 6146,    0,    8,
           25,  169,   66, 1110,   13,  325,   12, 1259, 4117,    0,    8,   25,
          169,   66, 1110,   13,  325,   12,  214,   17,  169,  305,   13,  535,
          183,   10, 4971,    0,    2],
        [ 245,    0, 3942,   53,   11,   57,   86, 6902,    9,    6,   15,    6,
          235,  366,   10,  218,   94,   11,    6, 4734,    0,   53,   63,    9,
            6,   15,    6,  235,  366,   10, 1964,    6,   17,  218,   94,   63,
            9,  838,   18,   57,    6,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6, 1223,  952,  100,   25, 2153,    0,    8,   29,    0,
           55,  663,    0,   24,   11,   57,  826,   29,  261,  117, 1113,   80,
         1291,   22,  140,  416,   22,  125,   12,    7, 2496,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  67,   19,  591, 4822,  765,    6,   12, 6950,    9,    7, 7343,   17,
           89, 6792, 1290,  804,  607,   62,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  24,  220,  154,    0,   38, 1871,    0,  220,   24,    0,   87,   10,
          229,  749,  532,  713,  509,    3,   21,   11,    6,   85, 1686, 1254,
            7,  749,  532,    6,  169, 1529,   55,    7,    0,    0,  324,   12,
          749,  532,  713,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   86, 1525,  596,    0,   21,   11,    6,   55,  170,
            0,   55,  170,  596,    0,   55,  108,  708,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 276,    7,   73,  287,   96,  873, 6608, 3008,    6,  206, 7400, 2270,
           48,   48,  232,  106, 1318,   10, 1318,    0, 2823,   54,  995,  106,
          593,  322, 1203,  426,   10, 5966, 6926,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9,  564,  895,  584,    0,   56,   62, 1251,  110,   59,  656,  505,
         1017,    7, 1797,   55,    7,  535,  608, 4480, 5147,   48,    9,   91,
         3350,   85,  640, 2602,    0,  815,    0,  895,  895,  887, 2360,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([24, 31, 23, 36, 47, 37, 41, 34, 53, 44, 35, 20, 41, 22, 34, 37],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8945, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8901, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(57.7951, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[  25,  116,  175,    7, 8570,    0,  347,    0,  125,    7, 1922,  181,
           85, 1922,  195, 2270, 3956,  188,   13,   56, 4030,  741,  266, 5126,
           80, 1665,   35, 5489, 3153,    6,  803, 1326,    0,   38, 5276,   26,
            7,  207,   21, 7674,   10,   51,    3,    8,  225, 3860,    6,   21,
           17,  207,  183,    8,  183,  554,    0,    8,  103,   25, 1336,  293,
           57,  237,   71,  267,    0,  225,  164,  289,    0,   38, 1969,  135,
           70,    0,   25,   11,   57, 1226,    0,   33,   26,    7,  772,  207,
           21, 7674,   10,   51,    9,   33, 5743,    3,    2],
        [  53,   66, 2314,   93,   35,  779,  569,   54, 1187,    6,  100,   38,
            6,  786,  373,   93,   45, 1604,  724,  197,  109,   38,  372,   59,
          247, 2583, 6874,    3,   19,   11,   45,   86,  142,   10,  205,  199,
            7, 3201,    6,   12,  117, 1288,  115,    0,   67,    7, 1890,  613,
           26,   33,    0,  103,  419,   12,  134, 1945,   62,   33, 4949,  111,
         3495,   35,   18,  500,   62, 2070,   12,    7, 8575, 1726,    0,  180,
           24,  451,  150,  264, 5554,  378, 1621,   85,    7, 1979,  176,  140,
         1585,   71,    7, 8575,  853,  650,    0,    2,    1],
        [  13, 2672,   34, 1621,    0,   39, 8547,   13,   48,  525,  365,   59,
          866,   34, 1223,    9,  879,  945,   94,   10, 2802,    7, 2672,    0,
            8,   84,   34,  211, 1757,    0,   38, 1871,   63,   24,  142,   10,
           87,    3,   38,  187,  192,   11,   18,  135,    3,    9,   13,  555,
         1113,    0, 1953,    6,   12, 1655,   12,   94,   84,   46, 7417, 5902,
          589,    6,  148,  162, 2659,    7, 3955,  119,  724,   12, 9657, 8428,
            6,    0,   38,   15, 4399,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  120,   19,  316,  122,    0,  359,   89,   86,   96,    0,   70,
           19, 4776,   19,  144, 3924,  346,   26,   17,    0,    7,    0, 5638,
          415,  829,    0,   69,  108, 1247,   26,  100,    7,    0, 5638,  415,
          829,    0,    0,   69,   13, 2035,  221,  387,    0,  848,    8,  848,
            0,    8,   94,  474,    7, 5638,  415,  829,   69,  108, 1247,    0,
            0,   34,  100,   13, 2103,  366,    0,  415,  829,   17, 3524,  442,
          108, 1247, 2183,   37,  109, 5167,   37,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   24,  316,  122,  108,  101,  356,    6,    8,  246,    0,   38,
          533,   11,   57,   86,    9,    7,  800, 1678,    0,   24,  213,   10,
          305,   91, 1269,   85,   13,  183,    8,  305,    7, 1269,  230,  359,
          670,    0, 2955,   10, 2900,    0,    8,  175,  134, 7070,   55,  509,
         1074,    0,   13,  747, 2070, 1329,    3,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   19,   11,   48,  100,   25,   10, 1683,   13, 3955,  930, 2735,
            9,   13,  207,   17,   91,  188,   22,   11,   18, 1620,   62,  490,
            0,  934,   13, 3955,  930, 2735,   17,  689,   11,   18,  641,  384,
         1105,  810,    7,  984,    0,  934, 3343, 4867,  170,   87,   33,  131,
           13,  140, 1236,   45,  300,  829,    8,  909,  609,  777,   18,  829,
            8,  987,  607,  445,   54, 2483, 1643,    6,  126,   12,  129,  128,
          160,  271,  862,  881,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 3699,  132,  378,  529,   10, 1157,   79, 1029,  187, 3684,   22,
         1337,    0,   79, 1029,  187, 6073,    0,   80, 1111, 1345, 1065,   46,
           29,   13, 2701,   12, 1498, 1345, 1065,   46,    8,  513,  106, 6871,
           19,   10, 6871, 1051,    0, 3699,  132,  401, 8899,  283,   85, 1137,
         1839,  120,   19, 3447,   62,   10,    7,  832,    0,    6,    0,    0,
            8,   66, 2277,   48,   10, 4432,   33, 2468,  244, 4401, 2205,   10,
         1387,   10,   13,   87,  610, 1234,    6,  115,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([93, 92, 79, 81, 56, 78, 82], device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 0.8413, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
 > at.  tensor(135.2890, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[   9,    7,  248,  215,  490,   89, 7102,    0, 4743,  439,   12,    7,
         8224, 1580, 2217, 3258,  116,  110,  543,   62,  755,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   33,  842,   77,   12,    7, 3677,    6,   17,  144,  700,  226,
         5823,   62,   69, 2667,  262, 2651,  480,    6,   17,  162, 1719,  292,
          121,   48,  244,   13,  798,   35, 1933, 2760,  131,    7,  664, 1151,
            0,    2,    1,    1,    1,    1],
        [   9,  409,    0,  276,    7, 4273,   93,  813,   17,  155, 1893,   63,
         9830,    0,  155, 4896,    6,   63, 9830,    0,   26, 1501, 3012,  982,
          125,   21,  220,  641,  995,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [2266,   45,    0,   39, 5486,  140, 1997,   26,   39, 5204,   29, 2583,
         9646, 2010,    0,   25,  465,   11,   18,  276,  135,   21,   34, 1254,
         1325,   25, 3128,   48,   21,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 473, 1822,    0,   19,   34,    9, 8393,   71,    7, 6099,   12,  958,
            0,  125,  103,   25,  192,   11,   18,  135,    0,   84,   11,    6,
           13,  452, 1534,   20,  371,  126,  174,   57, 1161,    9, 8393,   85,
            7,  765,    0,    2,    1,    1],
        [   9,  409,    0,    9,    7, 1909,  206,  225,  925,  106,    0,  143,
          254, 1737,  439,   12,    7, 1579,    6,  684,  589, 9447, 1682,  188,
         1220,   14,   48,   12,   33, 1599,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  19,   66,  185, 2468,   80,    7,   29,   35, 8309, 5308, 5874, 8282,
            0,  166, 3336,  518,  131, 2659,    0,  138, 5308,   26,    7, 5874,
         8282,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  25,   66,    7, 2375,   82,   45,  322,   12,  155, 1037, 2202,  230,
          558,   10,   25,    0,   67,   25,   63,  113, 1363,   10,  205, 1273,
          727,   54,  336,    7, 8464,  713, 1117,  155,  447, 1066,    0,    2,
            1,    1,    1,    1,    1,    1],
        [  25,  175,   10,  780,  134,  126,    0,  722,   71,  134,    0,  417,
         6127,  436,  134, 1325,    7, 5462,  901,   24,  293,    6,  126,    0,
          490,   25,   66,   10, 1557,  134,  270,    0,    8,   25,   11,  158,
          175, 4744,   55,   21,    0,    2],
        [   7,  747, 2653,   12, 7151,    0,   29, 3761,    0,   34,    7, 5605,
           12,    7, 8575,  853,  650,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  53,   63,  401,   21,  125,    0,   12,   91, 7857,  288,    0,   53,
           66,    0,   13,    0, 4960, 4469,   10, 1082,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,   24,  172, 7827,    7, 2918,   10,  213,   10,   87,   21,    0,
          860,   17,    7,  296,   55, 7935,  445,   56, 1783,  870,   45,  925,
          132,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 639,  188,  367,   12, 1137,   17,   11,    6, 2156,   54,  170,   10,
          150,    7,  984,  106,  672,    8,  205, 2027,  199,    7, 1580,    6,
         5309,  111,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [3088,   46,   24,   11,   57,  133, 3926,   46,   17, 1455,   38, 1355,
          777,  232,    3,    7, 8750,   26,   86,  595,  121,   48,  126,   12,
           13, 1472,   12,  877,  221, 1547,    0,  126,   12,   13, 1886,  362,
           12, 3164,    0,    2,    1,    1],
        [   8,  180,    0,   13, 7685,    0,   13, 1838,   20,   26,   15,    0,
          323,   33, 1968, 1692,    0,  225, 2393, 1613,    9,   10, 2026, 1195,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  29,  120,   19,   34,   13, 2145,  586,   54, 1850,   35,   48, 1327,
           85,  415,  675,   45,  174,  407, 2811,    0,   19,   34, 2027,    9,
           13, 6528,   35,  240,   35,  737, 1410, 7424,   12,   89, 3557,    0,
            2,    1,    1,    1,    1,    1]], device='cuda:1') tensor([23, 38, 31, 31, 40, 32, 27, 36, 42, 19, 22, 27, 28, 40, 26, 37],
       device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8276, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
 > at.  tensor(50.4937, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[ 251, 1051,  174,  ...,    1,    1,    1],
        [   7,   10, 3464,  ...,    1,    1,    1],
        [  67,   19,   11,  ...,    1,    1,    1],
        ...,
        [  29,  117,  126,  ...,    1,    1,    1],
        [  17,   11,    6,  ...,    1,    1,    1],
        [  17,   11,    6,  ...,    1,    1,    1]], device='cuda:1') tensor([19, 30, 24, 19, 25, 25, 30, 15, 23, 12, 21, 26, 24, 25, 19, 40, 11, 24,
        23, 21, 21, 12, 24, 11, 19, 14, 17, 28, 20, 26, 22, 21],
       device='cuda:1') tensor([1.0000, 1.0000, 0.8955, 1.0000, 0.8892, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8232, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8896,
        1.0000, 1.0000, 0.8794, 1.0000, 1.0000, 1.0000, 1.0000, 0.8418, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:1',
       dtype=torch.float16)
 > at.  tensor(32.7353, device='cuda:1', grad_fn=<MulBackward0>)
True tensor([[  91,   12,    7, 3373,   19, 3395,   62,    0,   10,   45,    8, 6819,
           15, 6760,  918,    6,    0,  162, 1828,   13, 6230,  120,    0,   79,
          963,    8, 3150,  264, 1736,  373,    0,  159,  245, 1269,   34, 8465,
           71,  346, 9414,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  25,   13,   59, 2315,   20,    0,    0,    7,  415, 1124,  160,  271,
            9,    7, 1361,    0,    8,  131,  401,   17,    0,    0,   25,  175,
            7,  370, 5204,    0,  929,    7, 8820, 1623,  693,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 3757,  457, 1006, 1529,   26,    7, 3187, 9225,   12, 8829,  368,
         5465, 2840,    9,   13, 2042,    0,    8,   21,  113,  583,    6,   55,
            7,   56, 9826,   12, 2840, 4166, 1241, 7202, 1006,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  276,   71,   33, 4673, 2394,    0,   24, 6182,   10, 3121,    0,
            8,   24,  323,    0,  746,   12,    0,  467,    6,  132, 7461, 3486,
            0, 1056,   10,    3,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,   24,   66,   10, 1031,  235,    7,   56, 2687,    0,  125,   24,
          296,   10,  175,    7, 4233,  128,  800,   12, 4709,    6,  359,   10,
          175,    7, 2931, 2808,    0,    8,  131, 1031,  235,   54,    7,   56,
         2687,    0,   24,   11,   57, 2983, 2762, 4581, 1588,    0,    2,    1,
            1,    1,    1,    1,    1],
        [1377, 2293,    6,   13,  245,  903, 5575,    0,  166, 1064,  180,  203,
         2329,   96,    0, 1405,   35,  160,  689,   11,   18,  641,  291,  627,
          232,  457,    0,   21,  818, 7013,    9, 6390,   12, 1064,    0,   38,
         1824,    0,   29,   70,   11,    6,   69,    7,  245,  903, 5575,   12,
            7, 2645, 1066,    0,    2],
        [1723, 6068,   26,    7, 3139, 2719,    0,  903,  693,   18,  561,    9,
         1491,    0,    8, 1155, 1754,    6,   84,    0, 1155, 1754,    6,   84,
          125,   21,  689,   11,   18, 3833,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   13,  546,   12,    7, 1644, 1327, 1632,  783,    0,
          166,  818,   38,   22, 2592,  846, 2845,    3,   29,    9,  117, 1666,
         6814,    6,    0,  125,  288, 6814,    6,   66,   13, 1644, 1327, 1632,
          783,    0, 1273,  303,   18,   35, 3794, 5558,    0,    2,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:7') tensor([41, 35, 35, 29, 47, 53, 32, 46], device='cuda:7') tensor([1.0000, 0.8691, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
 > at.  tensor(71.7301, device='cuda:7', grad_fn=<MulBackward0>)
True tensor([[1042,   24,  468,  251,  248,  214,   46,  103,   25,  289,    7, 2593,
            6,   63, 1714,    8,   53,  296, 3039, 8570,    0,   25,   66,    7,
         1670,  595, 1795,   12,  324, 7226,    6, 1944, 3298,    7, 4959,    0,
           71,   70,    0, 3023,    6,   55,    7, 1714,    0,  896,  203, 3870,
           55,  251,  148,   63, 8422,    0,    8, 2922, 7931,  373,   55,  251,
          148,   63, 6499, 4186,   82,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   0, 2266,  140,    0,    0,    0,    0,    0,   13, 6014,   12,   56,
          386,    0,    0,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   8,   21, 1847,   13,  670, 3516, 6467,  148,   26,  142,   10, 1436,
         1993,   93,   55,   25,    8,  289,    0,   38,  156, 3837,    0,    7,
         8260,   26,  752,   10,   19,  362, 1782,   33,    0,   67,   25,   66,
            7, 3473,   10,   87, 5422,    3,    8,   21, 1847,   39, 4414, 5370,
         1740,  148,   26,   86,  288,   84,    0, 3243,   13, 7771,  333,  383,
            0,   67,  148,   26,  461,   12,  108, 8755,    0,  961, 4012,   55,
          159,  807,    0,  108,  807,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33, 1760,  203,  356,   46,   21,   11,    6,   39, 4174, 1228,   46,
           67,   21,   34, 3498,    9, 1927,   35,  160, 6292,    6,    9,  564,
          957,  820,   46,    9,    7,  384,   18,  292,  235, 1909,    0,    9,
          409,   46,    8,   21,  909,  140,   62,   62,   13, 1591, 1314, 1988,
          311,  140, 1994,   11,    6,   38, 9103,    3,   68,  194,   65,   29,
           19,  100,   10,  154,   84,   34,  250,  142,   69,   84,   12,    7,
          264, 2042,   12,  229, 1118, 1028,  126,   12,   33,    0, 5493,   38,
         9103,    3,    2,    1,    1,    1],
        [   0,   19, 1704,   11,   18,    0,  205,  199, 3201,    6,   80,    0,
            0,    0,   70, 2912,   10,    0,   13, 3486,   19,  442,    0,   67,
          339,   11,    6,    0,  116,    0,  289,   21, 3006, 8939,    0,    0,
         9621,    6,    0,    0,  218, 8829,    6,    8,   13, 1365,    0,    0,
           68,  194,   65,   19, 1223, 1446,   17,   21,   34,   19,    0,    0,
           86,    7, 2421,  109,    7, 2493,    0,  109,    0,  995,   17, 5725,
         1663, 1222,    0,   17,   34,    7,    0,  288,    0, 4802,    9, 9480,
         3584, 2539,  269,   54,    0,    2],
        [  19,   34, 1060,  131, 1106,  233,  650,  973, 1670,    0,   13, 3434,
         1067,  525,  436, 1484,    0,  166,   26,    7, 3299, 3434, 1067,  525,
          436, 1484,    9,    7,  179,   46,   53, 1060,  110,   10,  694,   13,
         3585,  575, 5667,  322,   55, 7996,   85,    7, 1670, 9302, 8517, 3718,
            9,  264, 1736,    0,    9, 3697,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:7') tensor([67, 17, 79, 87, 90, 56], device='cuda:7') tensor([1.0000, 0.8477, 1.0000, 1.0000, 0.8018, 1.0000], device='cuda:7',
       dtype=torch.float16)
 > at.  tensor(152.3491, device='cuda:7', grad_fn=<MulBackward0>)
True tensor([[  84,   11,    6,  288,  391,  214,   25,  296,   10,  135,    0,  138,
          261,   94,  229, 1117,    7, 1484,    0,  138,  261,   94,  229, 3494,
          994,    9,   13, 2414, 1201,    8,  138,  261,   24,  229,    9, 2737,
           10,  150, 1179,   24,   73, 3757,   21,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1313, 6249, 8221,    6,   63, 2015, 1197, 2185, 2658,    6,   12,    7,
         1479,    0, 1823,   48,   35, 6665, 3479,    6,    0,   53,   63, 4402,
          442,  132,   12, 2072,  988,    0,    8,   17,   11,    6,   70,   25,
          150,    9,   33, 5674, 1410, 9787,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  487, 1692,   54,    0,  961, 3555,    0,  952,   10, 3698,    6,
            9,    7, 1726,    0, 1520,  159, 6382,    0,    8,  203, 2172,   54,
            0,    8,   19, 3803,   69,  203, 2172,   54,    8,  203, 2172,   54,
           55, 1303,    6,    8, 1822,  856,    6,   55,  244,  785,  215,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   19,  217,   26,   13, 3280,   12, 6936, 3023,  148, 1505, 7105,
          111, 2091,    9,   33, 6120,   80,  392,  215,  581,  120,   13, 2745,
         1060,  110,   13,  630,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 125,   21,   34,  288,  131,   86,  378,  529,   10, 4075,   17,  225,
           34, 1226,    0,   17, 6229,  220,  508,   13, 1838,   20,    7, 7343,
          225, 1918,   10,  135,   17,  225,   34,  230,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,  288, 3699,  125,    7, 2781,  775, 1026,  307,  373,  144,
           10,  205,  106,  316,   59, 3333,   10, 2902,   20, 3594,    0,  166,
           26,   13,  248,   35,  715,  234, 2365, 3046,    0,   10, 4342,    7,
         3432,   17,  859,   69,    7, 1998,  322,    0,  125,    7,  558, 3432,
         1359,   11,   18,  336,   55,   13,  535,  183,    0,    2],
        [  24,  591,   13, 8319,   22,  148, 1202,    6,   13, 1201,   17,   11,
            6, 2875,   10, 2156,    7,   25,  322,   10,  722, 1192, 3374,   69,
         8549,   48, 3374,  521,  779, 1212,    8, 8549,   48, 4528,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9, 1760,    0,   38,  469,  227, 2174,   37,   85, 1290,   45,  904,
          197,  166,   34, 1466,   62,   79,  171,  458,   37,   11,    6, 3495,
          119, 4546,  777,   20,  430,    0,  284,  772,  283,   46,   94,  169,
          367,  939,  181,  292,   45,  834,   77,  244,    7,  179,   10,  150,
           21,   46,   34,  204,   13,   55,  821,   93,    0,    2]],
       device='cuda:6') tensor([45, 44, 49, 30, 34, 58, 37, 58], device='cuda:6') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:6', dtype=torch.float16)
 > at.  tensor(73.1196, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[   8,  143,  254,   13, 4430, 1065,    0,  120,   19,  456,   10, 2299,
          606, 5996,    6,    0, 1179,    9, 2591,   15,  741,    0,  264, 2266,
          373, 2383,  109,    7, 2100,   12,    7, 1950, 1318,    0,    8,   24,
          456,   80,    9,  122,  338,   48,    0,   53,   77,  289,   17,   53,
           11,   57,  752,   10, 7369,  267, 6266,    8,  267, 5289,    8,  172,
          991,   69,    7,  291, 1256, 1105,  158,   62,  283,   12,  267,  282,
           11,    6, 4343,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  67,   24,  192,   11,   18,  276, 2036,  160,  156,  120,   24, 1510,
         1482,  289,   33,    0,  125,   24,   11,  121,    0, 1346,    0,   17,
          321,   12,    0,  993,    0,   55,    0,   29,  535,    8, 3524,   24,
           11,  121, 1501, 6936, 1181,    8, 3250,   62,    0, 4328,  111,    0,
            0,   33, 5126,   17, 4564,    8, 5187,   63, 3524,    9,  560,   15,
           18,  111,    0, 3249,   62,    8,   17, 2420,  724,    0,    9,    7,
            0,  467,    0,  164,    0,  735, 5017, 2141,   10,   13, 5295, 1410,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 131, 6763,    7, 1880, 4948,  366,  687,   12,    7, 1503,    0, 2603,
            9,  108, 8977, 1490, 8365,   62, 4621,    9, 1146,    0, 4793, 2527,
            6,   10,   13,  140, 2109,  241,  436, 8977, 1953,    6, 1955,   79,
          333, 1127, 8977, 1361,  115, 1772,    6, 8569,   10, 3284,    7, 1503,
            9, 1296,   10,  175, 2214,   10,    7, 5222, 4793, 2180,   20,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   33,   34,   13, 6954,    9,    7,  264, 1736,  825,  206,
            7, 8189,   34,  133, 1665,    0,    8,    7,  264, 1736, 1503,   55,
           15,    6,  266,    6, 5670,  552,    9,    8,   53,  204,  323,  185,
          948,   10, 4223,    7, 1665,   56, 1251,   20,   71,   17,  591,    9,
           13, 1943,  484, 2846,  679, 1922, 3832, 6412,  346,    7, 2291,    0,
           68,  194,   65,   29,   25,   73, 6212,  240,  155, 8189,   10, 5953,
         3610,   25,  213,  131, 2685,   54,   51,   20,   35, 6523,  111, 5464,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0, 2275,   84,    0,   19,   11,   45,  188,  221,    0,    0,   19,
           11,   45,    0,    0,   39, 2420,    0,    8, 3465,    0,    0,  120,
           19,  450,   94,   19,   11,   45,   39, 2420,    0,    0,    0,   53,
          116,  274,   85,  110,    8,  289,    0,   38,  870,    0,   25,    0,
         3765,    3,  109,   38, 1871,  321,   12, 7465,   87,   25,  283,    9,
            3,  238,  281,   12,   89,  283,   17,   19,  283,   71,   26,  172,
           13,  277,  523,   80, 4401, 3403,    6,   12,  740, 1552,    0,    0,
          254,  204,   13,    0, 3926, 7806,  109,   13, 3926, 4814,    0,    2],
        [4290, 6142,    6,    8, 1370,  128,  567,    0,   71,   13,  555, 6850,
            6,    0,   63,    9, 6063,  457,   12, 7220, 1613,    9,    7, 1336,
          480,  429,    8, 3316, 1918,   10, 1005,   33, 2462, 4718,  142,    0,
           29, 1881,   63, 3101, 5442, 1081,   10, 7992,   33,    0,   67,    9,
            7,  467,   21,  568,   86,  318,    6,  416,  121,    7, 1381,   12,
            7, 4937,   55, 2567,   33, 1370,  128, 1710,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    0,  120,    0,   17,  689,   11,   18,    0,  283,    0,  120,
           21,    0, 1518,  126,   17,    0,    0,   94,  148, 8523,    0,   71,
          170,   66,   77,    0,    7,  370,  409,    6,   24,   87,    8,   63,
          204,  976, 2465,    0,  180,   24,  954,   69,   10,   13, 1575, 6730,
            0,   53,  135,    7, 2403,    0,    8,   53,   63, 9700,  111, 9478,
           54,   21,   55,  159,  447, 3360, 1184, 1374,   18, 3965,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([77, 86, 61, 87, 96, 70, 73], device='cuda:6') tensor([1.0000, 0.8491, 1.0000, 1.0000, 0.8389, 1.0000, 0.8726],
       device='cuda:6', dtype=torch.float16)
 > at.  tensor(134.7330, device='cuda:6', grad_fn=<MulBackward0>)
True tensor([[  53,   73,  618,   10,   51, 1421,  215,  109,  143,    0, 3942,   24,
          154,  281,   12,  134,  914,  192,   11,   18,  229,   21,   10, 1421,
            9,    7, 2533,    0, 5349,    0,   56, 9172,    0,    7, 3726,  188,
          267,  245, 2610,  120,  225,   11,    6, 1498,  109, 1230,    0,   84,
         5575,   37,    0,  225,  188,   91, 2610,  288,  333,  785,  109, 1111,
          215,    0,   13,  535, 2760,   12, 5671, 3188,   15, 1255,  120,    7,
         1269,   26,  149,    6,   54,    0, 2544,   54,   71,    7, 1276,   85,
         1303,    0,    8, 4532,   54,   69,  267,  270,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,   11,   45, 1120, 1123,  281,   12,   25,   66,  474,    0,
           38, 1461,    0,  367,   69,    0,   73,   11,   18,   25,   87,  250,
          143, 3021,  254, 4502,   54,   85,  742, 4599,  693,    3,   19,   11,
           48,  100,   25,   10,  388,   33,  321,   12, 7754,   35, 1899,   37,
          156, 5250,    9,    7, 3317,   12,   70,   25,  169,   66,  474,  103,
           25,  144,  591,  155, 2088, 2841, 1930,  870, 1537,  109,  155, 2988,
         3696, 6264,    6,  812, 2637,    0,  230,    0,  281, 1848,  169,  446,
           17,  453,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  13, 5573,   12,   94,    9,  108,  753,  440,  148,   63, 1880,  111,
         4414,  703,   17,   53,   63, 7827,   48,  131,  570,   67,    7,  218,
          926,   26, 7827,   48,  131,  130,   20,    0,  154,   80,   21,    0,
          154,   80,   21,    0,  281,   94,   63, 3045,  336, 1211,    0,   38,
         1969,  135,    0,   89,   19,  262, 2992,   26, 2219,   69, 2827,   51,
          375, 1184,  237, 1068,    0,   19,  213,   10,  601,   94,    0,   67,
            7,  218, 1816,    0,   53,   11,   57, 6651,    8,  126,   10,  175,
          110,    3,   25,   73,   11,   18, 2694,   79,   13, 1504,  120,   25,
           66,   33,  321,   12,   79,   93,   45, 1604,  724,    0,   21,   11,
            6, 3756,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  180,   21,  164,   66,   70, 5767,    6,  583, 8153, 4608,   69,
           13, 1749, 2340,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   19,   11,   45,  142,   10, 2156,   25,   10, 1797,    7, 8574,
            8,   19,   11,   45,  142,   10,  508,   25,    7, 1298,    0,    8,
          103,   25,  700, 3265,   10, 2823,   21,    0,   25, 1412, 1964,   39,
         2990,  445,   17, 1119,    7,  839,  164,  367,  270,    8, 1766,    7,
          670,    0,   38,  511,   19, 1797,   62,  876,  195,  195,   93,    0,
            8,  101,   14,   48,   13,  464, 1065,    0,   67,   86,  490, 2432,
           13, 5996, 3335, 5829,  699,   93,   56,  901,  706,   70,   24,  162,
          401,    0,    8,  101, 2721,  132,    8,  246,    3,  876,  195,  195,
           93,  952,   80,   25,   77,  244,    7,  753,    0,   81,    0,    8,
           19,  213,   10,  601,   25,    0,   38,  511,  180,   13, 1300, 3335,
         1106, 1806,  741, 2961,  128,  340, 2721,  132,    0,    2]],
       device='cuda:5') tensor([ 94,  88, 112,  17, 130], device='cuda:5') tensor([1., 1., 1., 1., 1.], device='cuda:5', dtype=torch.float16)
 > at.  tensor(176.0940, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([1.0000, 0.8086, 1.0000, 1.0000, 1.0000, 0.8706, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(84.4603, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[  25,  135,    0,   25,   11,   57, 2465,    0,   25,   66,   77,  117,
          214,  142,   55,   25,    0,   38,  511,   19,   11,   45,  100,    3,
          125,   19,   11,   45,  168,    8,   21,   11,    6, 3939,    0,    8,
           25,  135,    0,   19,  321,   12,  100, 2180,   18,   18,    6, 3884,
          176,    0,   38, 3794,    0,   17,   11,    6,    7, 9132,  608, 1043,
           19,   11,  121,  700, 1346,   55, 4432,   54,   10, 2240,  670,    0,
            2],
        [  29,  339,   11,    6,  498,  518,  131,  384, 2172,   54,  185, 2047,
            0,   70,   26, 7898,    0, 7898,   26,    7, 6408,   24, 1064,  120,
           24,  154,   17,  108, 2082, 2196,  220,   51,  509,  109, 8661,  103,
           24,  144,  691,  250,  341,    9,    7, 1406,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 103,   53,  169,    0, 1744,  116,   13,  277,  908,  183,    0,    0,
           69,  752,   10, 2509,  159,  207, 3298, 1573,    0,    8,   13,  277,
          523,  143,    0,    0,   69, 1880, 5917,   85,  637,    0,   53,  506,
            0,  508,  159, 3226,   13,  509, 2333,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  24,   66,   10,  229, 1123,   53,   11,   57, 3163,   10,   51,    7,
         3348,   12,   33,  453,  753,   12,  108,    6,    0,   13,  753,   17,
           26,  100,  211,  218,    0,   13,  753,   17,  217, 6167,   96,  110,
          333, 1127,  383,    0,   13,  753,   17,   11,    6, 2084, 1625, 1563,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [8084, 3531,  170,   10, 1103,  297,  362,    6,   20,  199,   13,  858,
           12,   70, 6204, 3071,  506,  274,  100,    9,   13, 5591,   35, 1218,
          375,  140, 1011,  179,  206,   94,   66, 2214,   10, 3901, 2465, 5259,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  70,  220,   51,  143, 9649,  254,   10,   51,   56,   15,  121, 3576,
           62,    9, 6462,    0,   10,   51,    7,  473,   12,  155,   94,   10,
         2056,  155, 1234,    0,   10,   66,  211,  207,   10, 2423,   69,    7,
         6266,   12,    7,   39,  430,  119, 1215,  109, 9237,    7, 4893,   12,
            7,  708,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    7, 5873, 5318,   48,   17,  244,    7, 1830,  215,   53,  162,
         2839,   54, 1723,    6,    0, 1061,  584,    3, 4039,   14,   48,  909,
         2965, 1049,  111,    0,   86,  106, 3611,    0,   67,  106,    7, 4699,
           17, 3611,   26,  999,   55,   25,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 115,    0, 5385,   71,    7, 2184,   59, 1775,  221,   20,    0,   71,
           33,   38, 3794,    3,    7, 4503,  465,   11,   18,   66,    7, 8511,
           12,  129, 2286, 1331,   54, 2184,   59, 1775,  221,   20, 4821,    0,
           38, 3794,  197,    9,   33,  841,   26,   13,  909, 4674,    0,    8,
           13,  909, 4674, 1847,   39, 2987,    0,  166,   26,   13,  211,  500,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:2') tensor([73, 46, 45, 50, 38, 52, 44, 62], device='cuda:2') tensor([1.0000, 1.0000, 0.8320, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
 > at.  tensor(80.7225, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[  21,   11,    6,  ...,    1,    1,    1],
        [ 778,    9,    7,  ...,    1,    1,    1],
        [4701,   80,   17,  ...,    1,    1,    1],
        ...,
        [  29,   70,   26,  ...,    1,    1,    1],
        [  53,  192,   11,  ...,    1,    1,    1],
        [  21,   26,  593,  ...,    1,    1,    1]], device='cuda:2') tensor([11, 10, 11, 12, 10, 15,  7,  8,  9,  9, 12, 10, 14, 14, 16, 12,  9, 15,
        21,  9, 12,  8,  6, 10, 11,  4,  7, 15, 14, 10, 10, 10, 11, 10, 12, 12,
        10, 18, 10, 10, 10, 11,  5, 14, 13, 11,  9,  6, 12, 10, 11, 10, 11, 12,
        10, 10, 11, 24, 12, 12, 10, 11, 10, 13, 15, 12, 15,  9, 10, 10,  8,  8,
        10, 10,  9,  9, 10,  8, 14, 11, 10, 10, 10, 13,  7, 12, 10, 13, 13, 10,
        16, 15, 13,  9, 10,  9], device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8301, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8901, 1.0000, 0.8003, 1.0000, 1.0000, 0.8989, 1.0000, 1.0000, 0.8726,
        0.8867, 0.8872, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8013, 1.0000,
        1.0000, 0.8872, 1.0000, 0.8774, 1.0000, 1.0000, 1.0000, 1.0000, 0.8740,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8062,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8101, 1.0000, 1.0000, 1.0000,
        0.8711, 1.0000, 0.8911, 0.8647, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8564, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8584, 0.8848, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8501, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(12.4439, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[  77,  159, 6084,  ...,    1,    1,    1],
        [   8,   21,   11,  ...,    1,    1,    1],
        [  13,  122,    0,  ...,    1,    1,    1],
        ...,
        [  56, 6906,   26,  ...,    1,    1,    1],
        [ 284, 1187,   26,  ...,    1,    1,    1],
        [ 115,   33,   26,  ...,    1,    1,    1]], device='cuda:2') tensor([10, 11,  8,  9,  7, 11,  9, 18, 11, 13, 13,  8, 13,  7, 11, 14,  7, 10,
        10,  7,  7, 15,  6,  9,  9, 10,  7,  9, 11, 11,  9,  7, 11, 11,  9, 11,
        11,  8,  6, 14, 10,  8,  9,  8,  8,  8, 10, 10,  9,  9,  9, 13, 11,  9,
        12,  8, 12,  8, 18,  9, 10,  8,  9, 13,  7, 11, 11,  8, 11,  9, 11,  8,
        14, 13,  8,  9, 10, 12,  9,  8,  9, 10, 11, 10, 14, 10, 12, 13, 11, 10,
        10, 16,  9, 11,  6,  9,  8,  8, 12,  9,  9,  9, 11, 10],
       device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8647, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8340,
        0.8486, 1.0000, 1.0000, 0.8433, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8726, 1.0000, 0.8213, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8506, 1.0000, 1.0000, 1.0000, 0.8096,
        1.0000, 1.0000, 0.8198, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8970, 1.0000, 0.8149, 1.0000, 1.0000, 1.0000, 0.8765, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8579, 0.8594, 1.0000,
        0.8560, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8042, 1.0000, 1.0000, 1.0000, 1.0000, 0.8022, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8076, 0.8984, 1.0000, 1.0000,
        0.8604, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(11.0929, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[6984,    6,   66,  968,  170,   55,   13,  535,  183,   17,    7, 5933,
           12,   17, 2365, 1321, 5461,   67,  172,   21,   11,    6, 4402, 6465,
          672,   71, 2481,    6, 5110,  266, 5554,   56,  878, 4635,  336,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 120,   25,   11,   57,  401, 3144, 7082,    0,   25,  192,   11,   18,
          305,   13, 3280,  436,    9, 3144, 7082,    0,   25,  274,   55, 1482,
          148,  135,    6,    7,   56,  249, 3056,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  21,   34,  384,  827,  119,  829,   10,  150,    7,  946, 1052,  111,
         4796, 5445,   71,   13,  883,  233,  336,   21,    0,  206,    7, 3373,
           12,    7, 5244,    6,  144,  859,   86,   96,   10,  134,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 225, 8227,   48,    0,  225, 6295,   62,    0,  225, 4415,  132,  120,
         3326,  225, 1831,  225,   34,  378,  384, 1727,  121,   48,   12, 2214,
            0,   12, 1070,    0,   17,  746,   12,  279,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 185,  215,  581,    0,   19,   34, 3006,   79,   13, 1560,  609, 3685,
          866,    9,    0,  185,  133, 5167,  456,    6,    0,  629,    0,    7,
         3348,   12, 5071,    0,    8,    7, 3348,   12, 1922,  311,   22, 4128,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 108, 4597,  373,   66,  619,  159, 1436,  221,    6,   55, 1037,  811,
         2461,    6,   96,    0, 4504,   22,  609,   96,    0, 2115,    8,   55,
         2516,   54,  270,  199, 2288,  159, 6183,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 963,    0, 1665,   35, 6905,   48,   71,  284, 2185,   20,   20,  121,
            6, 3195,   62,  132,    0,  664, 2120,   12, 3763,    6,  100, 3577,
         3195,    6,   12,  415, 2374,    0,  101, 1260,  100, 3674, 2383,   20,
            7, 7615,  240,   81,  367,   10,  282,    0,    2,    1,    1,    1,
            1],
        [  84,   11,    6,  486, 8752,   12,    7,  452, 1539,  242,  158,  502,
           46,   17,   11,    6,  143,  254,  854,    7, 4168,    9,    7,  572,
           46,  554,    0, 1429,  133, 2414,  111,   10,  422, 6627,  774,  271,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8, 6293, 6650,    9, 2008,   20,  484,   18, 5036,    8, 1777,    9,
         1760,   26,   70, 1927,    6,    7, 3151,    6,   12,  452, 1539,  128,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  55,  663,    0,    7, 6769, 1362,   20, 2553,  128, 1051, 2553,   93,
           12,    7, 3003,  608,  160,  589, 1333,  424,    6, 1985,   51, 4600,
          929,   13,  656, 3837, 2482, 1395,    7, 1361,   11,    6,  837,  283,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  29,    9,   33,  207,    0,    0,    0, 1976,   24, 2770,    0,   24,
          162,  204,  529,   10,    0, 6754,   21, 1004,   77,  798,    3,    0,
         3061,  106,  383,   91,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   9, 1015,   22,    6, 3358, 2426, 1422,    6,  100,   33, 3380,  292,
         1150,   45, 1568, 2685,   46,   25,   73,   11,   18,  368,  846,  181,
            0,   21,  659, 4489,   39,  191,  760,  227, 4811,    6,    0,   67,
           25,   73,  368,  688,   46,   25,  150, 8263,   12,  688,   84,    0,
            2],
        [  67, 3097,    0,  211,  988,  138,  324,   25,   63,   85, 4101, 1395,
          132,    7, 5694,    0, 1797,    6,    0,    8,   77,    0,   17,    0,
          993,    0,   25,    0, 1988,   70,    0,    7, 2372,  290, 1434, 2120,
            0,    0,  583,   13,  862, 1026, 1832,    0,    2,    1,    1,    1,
            1],
        [   8,  120,   25,  175, 1387,    0,    7, 7035,   25,   11,   57, 2202,
            0,    0,    9,  384,   18, 4117,    6,    8, 1244,   35, 1966,  366,
            6,   25,    0,  230,   10,  155, 2657,  426,  265,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  55,  663,    0,  110,  484, 1212,   34,  717,  759,   12,    7, 1723,
            6,  270,   79, 2523,   79, 5611,    8,  115,   26,  850, 1016,    3,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   8,   84,   63, 2869,  218, 3885,    9,  166, 1749,    0, 6012,  292,
          959,    6, 3382,  203,  779,  675,  793,  188,  442,   13, 8166, 1878,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:4') tensor([37, 33, 36, 34, 38, 33, 45, 38, 27, 38, 30, 49, 45, 35, 26, 26],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8916, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8486, 1.0000, 0.8218, 0.8955, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
 > at.  tensor(60.5744, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[ 486,  723,    0,  ...,    1,    1,    1],
        [1732,    0,  567,  ...,    1,    1,    1],
        [ 488, 1075,    0,  ...,    1,    1,    1],
        ...,
        [  17,   34,  159,  ...,    1,    1,    1],
        [  29,   84,   11,  ...,    1,    1,    1],
        [   8,  333, 2291,  ...,    1,    1,    1]], device='cuda:4') tensor([16, 12, 18,  9,  9, 13, 11, 11,  9, 13, 14, 19, 16, 18, 12, 11, 22, 19,
        15, 16, 18, 21, 11, 14,  9, 15, 19, 17, 16,  7,  9, 16, 11, 14, 14, 20,
         8, 16, 11, 13, 13, 13, 18,  9, 17, 12,  8, 17, 18, 11, 15, 20, 11, 15,
        19, 13], device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8545, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8916, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8271,
        1.0000, 0.8423, 1.0000, 0.8335, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8950, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8135, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8960, 1.0000, 0.8335, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000], device='cuda:4', dtype=torch.float16)
 > at.  tensor(18.0450, device='cuda:4', grad_fn=<MulBackward0>)
True tensor([[1008,   70,   19,  ...,    1,    1,    1],
        [ 115,   19,   11,  ...,    1,    1,    1],
        [ 476,   25,  133,  ...,    1,    1,    1],
        ...,
        [  24,   11,   57,  ...,    1,    1,    1],
        [   0, 1573,  289,  ...,    1,    1,    1],
        [   8,   29,   19,  ...,    1,    1,    1]], device='cuda:4') tensor([ 9, 16, 11, 12, 13, 13,  7, 16, 13, 12, 10, 10, 27, 13, 14, 11,  9, 12,
        20, 11, 18, 13, 10, 12, 11, 11,  8, 11,  8,  7, 11, 11,  8, 10,  7, 14,
        10, 15,  7, 10, 13,  7, 15, 10, 15, 11,  9, 12,  8, 12, 14, 19, 12, 20,
        13, 11, 11, 52, 10, 13,  8, 11, 13, 12, 14, 12, 11, 15, 19, 14,  9, 11,
         8, 10, 11, 10, 11, 16, 19,  9], device='cuda:4') True tensor([[ 115,    0,   77,  244,    7,  179,    0,   84,   26,   91, 1455,   17,
           94,  148,   66, 8428,    6,  735,  450,  110,    0,   53,  598, 3754,
            0,    8,   53, 1149,  164,  450,  110, 1352,   12, 4495, 4513,   96,
           46,   12,   13, 5370,  148,   14,   48,    0,    8,   13, 1751,   17,
          513,  593, 2392,    0,    8,  999, 1569,   85,    7, 3280,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8, 1004,   13,  733, 3609,   12, 1726,    6,   46,  106, 3236,    0,
         4623, 2050,   93,    0, 1745,  948,    6,    0, 4690,    0, 2384,   12,
          749,  340, 1049,    0,    8, 1507,  100, 2744,  153,   35,    6, 4201,
           54,    0,    0,   29,   69,    8,   29,    0,    0, 4404,   46,   25,
          446,    0,   94,  148,  213,   10,   87,  214,  125,   53,  570,    0,
           21,    0,   67,   53,  213,   10,   87,  117,  214,   10,  133,  747,
         3057,    6,    0,    2,    1,    1,    1,    1,    1,    1],
        [  29,    0,   55,  185,   12,   25,    0,    7, 5054, 6985,  439,   26,
           39,  479,   55,   25,   10,  388,  199, 2332,    0,    8,   19, 1080,
           17,    0,   55,   77,   12,   25,    0,    0,   25,  164,  150,   21,
           79,   39,  479,    0, 2539, 2551,   54,    9, 1296,    0,   10,  601,
         4491,   51,    0,  143, 4089,    0,   10,    0,    0,  601,  596,  719,
         3557,    6,    0,   17,   29,  293,    0,    0,    8,   10,  601,    0,
         1387,    7, 5086,    0, 4719,   85,    7, 1219,    0,    2],
        [  71,    7, 4057, 2653,   69,    0,  143, 3537,  340, 5188,    0, 6214,
         1508,  860,   79,    7,    0, 5019,    6,    8, 3943, 5834,    6,    0,
            0,    0,   21,   26,    0,    0, 1829,   10, 3995,   17,  218, 6214,
          282, 1833,  108,  601,    0,  593,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   7,  207,   19, 7777,   21,  164, 1085,   26,  802, 6102, 8452,    6,
            8, 2583, 8452,    6,    0,   29,   24,  205,  106, 1107,  957,   10,
         1107,  895,  109, 1107,  828,    0,    8,   91,   17,   26,   86,  267,
          235,  457,  125,  148,  169,  213,   10, 2423,   69,   10,  159,  708,
            7, 3920,  290,  266, 8863,  445,   56, 8368,   96,   17,   53,  278,
         1450,  215, 3928,  106,  159, 1848,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 120,   24,  368,    7, 1455,   38, 3232, 1547,  140,   18,  197,  109,
           38, 4596,   37,    3,   70,   24, 3465,  641,   26,   13, 4405,    0,
         1183,  148, 1559, 4744,    0,    8,   24, 2527,   10, 5899,   17,   21,
           11,    6,  251, 4405,    6,  148,   63,  142,   10,   51,    7, 1649,
           10,  601,  170, 2026,    7,  172,  488,    0,  567,  266,  694, 4131,
           17,   24,  970,  100, 2127,  468,    0, 3135, 2041,    8,  837, 8226,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9, 7034,    0,   19,   11,  121,  367,   10,  703,   17,    7, 7470,
            6,   55,    7,  558, 3109,  469, 1921,   63,   77,  336,  170,    0,
          116, 4111,   55, 2805, 1020,   94,   71,    7, 5168, 2410,    0, 2541,
          607,    6,  609, 1076,  160, 1020, 4159,    0,    8, 7488, 4469,   10,
         3065,  687,  134,   10,  229,  159, 4989,   13, 1874,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([60, 76, 82, 44, 68, 74, 59], device='cuda:0') tensor([1.0000, 0.8975, 0.8291, 0.8066, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(133.3136, device='cuda:0', grad_fn=<MulBackward0>)
True tensor([[  19,   34,  851,  ...,    1,    1,    1],
        [  29,  103,   25,  ...,    1,    1,    1],
        [1065,    0,   19,  ...,    1,    1,    1],
        ...,
        [   8,    9,    7,  ...,    1,    1,    1],
        [  53,  132,    6,  ...,   18,    0,    2],
        [   0,   19,   11,  ...,    1,    1,    1]], device='cuda:0') tensor([22, 22, 22, 27, 21, 17, 24, 32, 13, 24, 25, 22, 24, 23, 26, 18, 30, 25,
        19, 22, 17, 21, 28, 24, 21, 26, 11, 25, 20, 22, 33, 22],
       device='cuda:0') tensor([1.0000, 1.0000, 0.8608, 1.0000, 1.0000, 1.0000, 1.0000, 0.8662, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8022, 1.0000, 0.8618, 1.0000, 0.8726,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8516], device='cuda:0',
       dtype=torch.float16)
 > at.  tensor(33.2188, device='cuda:0', grad_fn=<MulBackward0>)
True tensor([[   7,  409,  240,  281, 5555,   55,   17, 2182,   26, 5387,   58,    6,
          429,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [1269,  726,   59,  322,   26,    0,    8,  735,  164,   51,    0,   39,
         4126, 7074, 2792,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [2769,  649,  187, 3603,   37,    0,   19,  116,   66,   10,  175,  359,
            7, 6291, 1113,  206,   19,  192,   11,   18,  175, 9001,   48,    0,
            2,    1,    1],
        [   8, 1120,    0,   19,  169, 1120,  583,   21,  378,  946,   35,  525,
           48,   62,    0,   67,    9,    7,  772,  841,   12,    7, 1455,    0,
            2,    1,    1],
        [  67,  593,  294, 3002, 3560, 7253,   62,  566,  195,  195, 7309,    6,
          106,   97,  927, 2329,   54,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21,  451,   66,  211, 6850,    6,   55,  822, 6183,    0, 7031,   12,
         5388,  445,  109, 4456,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21,   11,    6,    7,  264,  741,   11,    6, 4271,    0,  103,   25,
          213,    0,   12, 8683, 4141,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 117,   63,    0,    0, 2465,    0, 1816,   46,  889,    8,  596,   46,
         4623,  760,  387,    6,    0,  100,   19,  246,    0, 5553,    0,    2,
            1,    1,    1],
        [ 169,   25,  100, 2744,   45, 2464,  930,   35,  233,    0,  148,   11,
            6, 3561,   80,  248,  759,   94,    0,  169,   25,  100,  565, 2107,
            0,    2,    1],
        [3943,    6,   63,    0,   25,  135,    0,    7,  637,   10,   80,   13,
         6897,   12,   77, 6214, 1369,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21, 1073,  110, 2749,    0, 6667,   10,  618,   85,   33,    0,  765,
            9, 1261,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  84,   11,    6,  113,   13, 6383, 1440, 1344,    6,  148, 1206,    6,
           13, 4457, 6389, 1232,    8, 4457, 6383,    0,    2,    1,    1,    1,
            1,    1,    1],
        [  67,   25,  150, 8747,    6, 6123,    0, 7168,  221,    0,    0,   86,
           13,  488, 1878, 2219,   69,    0,  148,  155, 4563,    6,   63,    0,
            2,    1,    1],
        [  85,    7,  467,   12,    7,  276,   54,    0,   19, 5853,    7,  473,
         8313, 2365,  270,   10,    7,  218,  926,   12, 4517,    0,    2,    1,
            1,    1,    1],
        [1771,   12, 2808,  106,  987,  607,  445, 4764,    6,    8,    7, 3727,
           15, 4345,    0, 4834,   26,  287, 1825,   96,    8,   29,   69,    0,
            2,    1,    1],
        [   8,  432,   13,  555, 1345,  513,  131,    0,   19, 1873,   17,  101,
           34,  341,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  55,   91,   12,   89, 2262,    0,   19, 1260,   85, 9548,  660,    9,
         1185,  828, 4621,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [1645, 1440,   26, 6501,  170,    0, 6501,  108, 3537,  429,    0,  109,
          108, 5384,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [7520,   97,   45,  236, 1991, 2734, 3967,  266, 2834, 1144,   63, 3756,
           10, 1799,   71,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  67,  120,   19, 1095,   33,    0,   19,   34,  172, 8288,    0,  125,
           21, 4639,  282, 3794,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21, 1847,   33,   56, 7101,  248, 1113,   10, 1137,   79,    0,  261,
            0,   79,    0,    7, 1417,    0,   56, 7101, 1137,    6,    9,   91,
          383,    0,    2],
        [  68,  194,   65,  168,  367,    7,  488,  211,   35,  679,    6,   12,
         5714,  694,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  21, 1143,   79, 1019,  270,  199,    7, 1406,   79, 1061, 1101,    0,
          125,   25,  492,  135,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 413,    8,   24, 7774,   77,  108, 1298,  413,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [7714,    0,   19,   34,  172, 4876,   62,   10,  446,  126,   17, 7002,
          188,   33,  524, 1639,  126,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19, 3008,  134,   79, 1289,    0,    8,    0, 1065,   69,    0,  203,
         1015,   20,  121,  134,   79, 2348,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  246,   24,   73, 1706,   15,    7, 8311,    0,   24,   73, 2551,
           21,  126,   10, 3258,    7,  415, 1231,    0,    2,    1,    1,    1,
            1,    1,    1],
        [ 115,    0,   69,    7,  218,  926,   12,    7, 2705, 2374,  300,  290,
         4433,  682,  804, 1052,  300,  626,    0,   13, 2544,   93, 1390,   54,
         2822,    0,    2],
        [ 138,   87,   24,  204, 3560,    7, 4690, 5573,   12, 2833,  106,  700,
         4877, 1069,    9,    7,  245,  561,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [1149,    0,    0,   24,  144,   10, 3070,  214,    0,    0,  126,   79,
            0,   13, 2173,  125,   12, 1160,   45,   54,  109, 8929,    6,  109,
         1976,    0,    2],
        [   7, 2636,  570,  282,   46, 3891, 1181,  131,    7, 2636, 8373,  372,
          430,    0,    7, 2636, 9303, 5369,    0,    2,    1,    1,    1,    1,
            1,    1,    1],
        [   8,    7, 7650,   96,    0,    7,   86,   96,    0,   79,   25,  135,
            0,   63,  116, 1051,  174, 1991,    6,    0,    2,    1,    1,    1,
            1,    1,    1]], device='cuda:2') tensor([15, 17, 25, 25, 19, 19, 19, 24, 26, 19, 16, 21, 25, 23, 25, 16, 17, 16,
        17, 18, 27, 16, 18,  9, 19, 20, 21, 27, 20, 27, 20, 21],
       device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8623, 1.0000,
        1.0000, 0.8677, 1.0000, 0.8491, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8203, 1.0000, 1.0000, 1.0000, 1.0000, 0.8862, 1.0000,
        1.0000, 1.0000, 0.8228, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
 > at.  tensor(31.5026, device='cuda:2', grad_fn=<MulBackward0>)
True tensor([[   8,   25,   73,  150,  168,    0,  101,   11,    6, 2439,   10,  175,
          143,  941,  199,    7, 1416,  128, 1436,  242,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 101,   11,    6,   80,  717,  109,  785,  825,   79,  743,   71,   33,
          946,  277, 3899, 1618, 1775,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 800,  248,    0,   13, 2585, 1533,   54, 1682,  818, 4028,  430, 4618,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1988,  562, 1425,   21,    0,  101,  619, 6031,  706,   10,  229,   77,
            7, 1122,  878,    6,  598, 1968,    8,   13,  724,  221,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 238,    0,   79,   71,  995,    9, 3804,    0, 1155,  172, 1073,  841,
         3706,    9,    7,  688,   12, 2569,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   33,  993,   26, 1944,  133, 2117,    0,    8,  131,    7,  207,
            0,   17,  279,   73, 3499,   80, 8255, 6052,   12, 4172,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,    7, 2248,  630,    0,   38, 2637,    0,   24, 2373,    3,   19,
          154,   21,  164,   86,  367,  106,  832, 2287,    0,    0,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,   11,    6,   46,   25,  135,    0,  440,   19,   11,   48,  100,
           10,  289,   21,   11,    6,   13, 3647, 1072, 6049, 1201,    0,   67,
            0,    9,  409,    0,   21,   11,    6, 3600,    0,    2],
        [ 948,  188,  291, 1090,    6,  793, 3042,  442,  108,  931, 5368,   37,
            8, 1978,    8,  958,  756,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24,  735,  164,  175,   70,   24, 2885,    0,   55,    0,    8,
          461,   12,   70,   24,    0, 2885,   55,   26,    9,  108,    0,    0,
         6501, 4401,    6,    0,    2,    1,    1,    1,    1,    1],
        [  21,   11,    6,   13,  546,   17,  931, 2027,    9,   89, 1259,    0,
            8,   19,   11,   45, 5592,   10, 1452,   33,   71,   25,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   29,    0,  168,   25,    0,   77,   63,    0, 1267,  381,    6,
         1331,  111,    0,    0, 7077,   48,   94,    0,  281,   12,   25,  274,
         2465,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24,  203,   35,  160, 7261,   62,   77,   12,    7, 7755,    6,
            0,  388,  134,  132,   69,   13, 6381,    0,    8,   56, 3728,  134,
          132,   84, 2584,    7, 6316,   37, 5714,    6,    0,    2],
        [ 108, 3555,   66, 2386,   12, 1655,   12, 6759, 2179,   96, 2595,  140,
          300,  829, 2386,   12, 8971,    6,  545,    0,   69, 3752,  183, 6753,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [1559,  132,    0, 1570,    6,   10,    7, 4209,    0,   21,   11,    6,
         1336,  366, 1069,    0,    8,   21, 4433,  346,    8,   21,   11,    6,
            9, 4514,  838,   18,   57,    6,    6,    0,    2,    1],
        [  79,   24,  289,    9, 3060,   22,  494,  556,    8,   10, 1373,  606,
            0,  148,   26,  148,    8,   70,   26,   70,    0, 1189,    0,   24,
          213,   10,  780,   10,  203, 4434,  140, 5366,    0,    2],
        [   8,   79,   53, 5604,   62,  111, 7928,   21,  346,    8,  498,  554,
            0, 1609,   26, 1592,   79,   13,  746,   12,  131,   35, 7614,   12,
          722,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [   8,   24, 1873,  230,  755,   17,    7, 1192,  144,   13,  488,  637,
         4910, 5542,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,  120, 9300,   26, 7014,    9,    7, 3611, 3071,    0,   21,   26,
           56, 7617,  829,   25,   10,  150,  156, 1766,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 281,   12,    7,  183,    0,    0,    0,  103,   53,   66,   39,  811,
          235, 5492,    0,   53,  451,  305,   21,    0,   38, 1967,  586, 1563,
          111,    0,    0,   25,   11,   57, 3021,    0,    0,    2],
        [ 391,    0, 2168, 1065,    0,   19, 5692,    0,    0,   13, 3768, 1017,
           12, 5184,    6, 1211,   24,   11,   48, 1704,    7, 5423,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8, 2394,  116, 1223,  142,   51, 1118,   37,  156,    0,   55,   13,
         4431, 1531, 2247,    9,   48,  783,    0, 1223,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  29,   24,  246,    0,   38, 5034,    0,  596,  274,  100,   53,   11,
           57,  976,  324,   80, 2985,  321,   12,    7, 9057,  755,    0, 8864,
          111, 2985,   21,  755,    0,    2,    1,    1,    1,    1],
        [ 120,   19, 1505, 2091,    9,  117, 1532,   80,  392,  215,  581,    0,
         2245,  474,   53, 1425,   70,  341, 3257,  162,  442,   12,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([22, 19, 14, 24, 20, 24, 25, 34, 19, 29, 24, 27, 34, 27, 33, 34, 27, 16,
        22, 34, 24, 22, 30, 24], device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8462, 1.0000, 1.0000,
        0.8428, 1.0000, 0.8008, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.8452, 0.8281, 1.0000, 1.0000, 1.0000], device='cuda:5',
       dtype=torch.float16)
 > at.  tensor(39.0982, device='cuda:5', grad_fn=<MulBackward0>)
True tensor([[   8,  180,   19,  278,  172, 8930,    0,  125,   19,  474,   21,   34,
         6756, 1829,   85,   17, 1137,   10,  116,  508,   94,    7, 7998,    6,
           12,    7, 5086,    9,  166,   53,  162,   13,  649,  551,   45,   54,
            0,    2,    1,    1,    1,    1,    1,    1],
        [  68, 4077,   65, 3173,    0,   29,  108, 1678,   26,   80,  116,  160,
         2582,   20,  571,    0,  125,  101,   11,    6,  226, 3049,   54,    0,
            8,    7, 2987,   26,   10, 3560,  565,  106,  893, 5853,  131,    7,
         1067,  265,   48,   46,    2,    1,    1,    1],
        [  17,   11,    6,    7,  800,   55,  170,   10, 4530,    0,   38,  511,
           24,  116, 6083, 1319, 2921,    6, 8338,  392,  322,    0,    8,  108,
         3966, 6407,   93,   34, 1056,    0, 1047,  852,  820, 1228,    6,  106,
         1185,  828,  341, 1075,    0,    2,    1,    1],
        [   8,   85,    7,  370,  183,    0,   19,  474,   80,   33, 3024, 1361,
           12,   94,   19, 1425,    0, 5538,    6,    0, 7621,    6,    0, 5577,
            6,    0, 4853, 1613,    0,   79,    6,  365,  480, 4479,    6,    0,
           25, 1187,   21,    0,    2,    1,    1,    1],
        [ 103,   24,  162,   10, 5831,    8,  203,  607,   57,  140,   18,   77,
          264, 2394,  336, 2555,  235,    0,   24,  220,  204, 6785,   17, 1682,
         2244,    0,   24,  474,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  67,    7,   91, 4272,   17,  772, 4490,   96,  347, 4272,    6,  603,
           18,  945,   26,  143, 6790,  440,  254,  700,   26,   33,   91,    0,
            7, 2973,   45,  300,  335, 4272,    0,  230,    0,   21, 3834,   69,
           13, 1715, 1941,  383,    0,    2,    1,    1],
        [ 115,   84,   34,   13,  133,  916, 5100,  131, 1747, 2142, 3684,   59,
         4037,   17,  278,   13,  325,   12, 2303,   80,   13, 3087,  581,    0,
            8,   33,   26, 1223,    7, 6484,   12, 3142,   69,    7, 2688,   10,
           77,  117, 4531,  341, 4813,    6,    0,    2],
        [  24,   11,  121,  278, 2869,  341, 1369,   12, 6534, 1028, 1585,    0,
            8,   19,   11,   45, 7443,   17, 2392,   24,   11,  158,   51,  529,
           10,  175,  185, 2872,   54,  540,   29,   24,   73,  175,  270,    8,
          498,  665,   85, 5894,    0,    2,    1,    1]], device='cuda:3') tensor([38, 41, 42, 41, 30, 42, 44, 42], device='cuda:3') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:3', dtype=torch.float16)
 > at.  tensor(69.8426, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[   8,   21,   11,    6,    0,   13,    0,    0,  976,  488, 1009,    0,
          138,    0,  488,    0,    0,  238,    0,   19,    0,   73,  508,   25,
            7, 2348,   46, 1802,    3, 3275, 5494,   46,   67,   17,  689,   11,
           18,  172,  450,   25,  133,    0,  261,    0,    2],
        [  19,   11,   48,  492, 1110,   21,  619,   79,   13, 3102,   71,   33,
          851, 1243,   54,    0,    8,   19,   34,  838,  587, 4476,   18,   10,
          154,   17,   38, 3784,   11,   62,  197,  169,  175,  199,    7, 4868,
           69,   89, 2283,    0,    2,    1,    1,    1,    1],
        [ 120,    7, 1868, 4674, 4657, 1446,   10, 6406,  346,  461,   12,    7,
          942, 1740,    0,   24, 1644,  606,   18,  187,  922, 3993, 7018,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 248,  215,  581,    0,   19,   34, 6036,   79,   39, 2420,   10, 6757,
            9,   39, 7996,  415,   45,  458,   45,  240,  829,  736,  215,   12,
         3967,  266,  998,    9, 2627,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,   21,  188,    7,   21,  234,  187, 2422, 1584,   46,   80, 1268,
            0,  868, 1101, 1369,   12, 3529,    0,   29, 1019, 2134,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  17, 3098,    0,   13, 2239, 6050,  369,   34,  850, 1408,    9,  108,
         1726,    6,    0,    9,  108, 8633,  567,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,  818,  250, 7678,   54,   13,  488, 2294, 3071,    0, 1149,  923,
           13, 3462, 2294, 3071,    0,   67,   21,   11,    6,  461,   12,   70,
           24,   87,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,  103,   25,   11,   57,   13, 8159,    0,   51,    7, 8159,  148,
           26,  735,  142,   69,   80,    0, 3039,    8,    7, 2645,  403, 3034,
          271,   10,   51,   13,    0,   82,  338,  240,    0,   55,    7,    0,
         1714,    0,    2,    1,    1,    1,    1,    1,    1],
        [  17,   11,    6,   13,  279,    0, 2991,   21,    0,   67,    7, 1590,
           12,  170,  135,   17,    7, 1465,   73,   51,   13,  172, 8121,  561,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   24,  229,    7, 4389,    6,   12,  251, 3380,  128,    6, 1456,
          230,    0,  103,   24, 1206,  159, 1862,    0,   24,   73, 1206,  876,
          649,   59, 1625,  369,    0,    8,  115,   24,   66,   13,  453, 7123,
            0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  29,  339,   11,    6,  289,   17,   24,  283,  835,    0,    8,   24,
          204,  192,   11,   18,  116, 5183,    7,  862, 2704,    6,    0,   67,
           24, 1396,  835,   69,    7,  862, 2704,    6,    8,  172, 3814, 6927,
            8, 3097, 3814, 5748, 1955,    0,    2,    1,    1],
        [  21, 1518,  126,   25,   73, 2365,    7,  572,   12,  860,   39, 2216,
           46,   33,   26,   13,  822, 1682,   12,  860, 3396,   46, 2365, 7573,
         5950,   20,  111,   55,   80,  640, 1551,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  84,   63, 1230,  341, 3058,    0, 1498,  341, 1167, 4345,    0,   67,
            7,  475,  630,   26,    0,  138,  238,   87,  117,  283,    9, 2894,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 115,    0,   71,  117,  370,  572, 6710,    0,   24,   11,  121, 2107,
          106, 1609,   10, 3548,   10, 2318, 6355, 1469,   71,  117, 6201,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33, 1599,   26,  434, 5827, 4488, 9414,    0,   68,  194,   65,   21,
           11,    6,   39, 1248, 5764,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  21,  659,   51,   17,   84,   34,   13,  453,   82,   10,   51, 9003,
          629, 3022,    8, 3156,    0,    8,   21,   34, 5763,   69,    7,   13,
         1674,    6,   12, 3226, 1525, 7531, 1144,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([45, 41, 26, 31, 24, 21, 28, 39, 26, 38, 43, 33, 26, 26, 19, 33],
       device='cuda:3') tensor([0.8086, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8340, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
 > at.  tensor(53.6255, device='cuda:3', grad_fn=<MulBackward0>)
True tensor([[ 670,   34,  453,    0,    8,  635,  142,   10,  670, 4327,  110,  175,
           10,   33, 1827, 2260,  561,    0,   19,   11,   45,   86, 1123,    0,
           68,  194,   65,  476,   25,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,  169, 4512,  132,    7, 7373,    6,    8,   56, 3430, 1004, 8425,
           51,  727,    6,   79,  103,   19,  513,  270,    9,  183,    8, 1505,
           13, 1269,  554,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 108, 1661,   57,  969,    6,  369,    0,    0,    0,   12,    7,  207,
            7, 1479, 1429,   26,  172,   46,    0,  188,  116, 4624,   62, 2447,
          297,   20,  586, 3042,    9, 4495,  215,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  21,   11,    6,   77,  103,    0,   29,   70,  103,   24,  162, 1074,
            9, 7755, 1318,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8,   24,   66,   13,  325,   12, 6627,    8, 1064,    0,   67,   21,
           11,    6,   13, 4112,    0,   29,  138,   63,   25, 3263,   10, 9202,
           10,  175,  359,   17, 3805, 8487,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  29,  837, 2245,   66, 2138,   13,  325,   12,  183,  665,   85,    7,
         4608,   12,  108,  955, 1234,    0,  109,  218,   94,   11,    6,  955,
         1234,    0,   69, 7955,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 109,   66,   53, 6510,   62,    0,  159,  447,  746,   12, 7372, 2243,
         1150,   17,   26,    0, 5709,   12,  639,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 846, 1026,  156,   26,   13,  133, 3818, 8043,   22,  148,  568,   39,
         4126, 1762,   12,  283,   71, 5200,   48,  128, 1144,    0, 1320, 7988,
           96,    8, 4269,    6,   55,  384,  232,  793,    0,    2,    1,    1,
            1,    1,    1,    1,    1],
        [   9,   77,    0,    7,  664,  726,  188, 8095,   62,    0,  143,  254,
         1998,  852,   94,    0,    0,    9, 7176,   20,    0,    0,  850,  699,
          352,  521,  249,  249,   59,  240, 1144,  574,   54,    6,    0,    2,
            1,    1,    1,    1,    1],
        [  29,    7,  546,    0,    0,   17,    0,    7, 1827,  968,   80,    7,
         6900, 1532,   34,   17,   84,  506,   51,   13, 1706, 4562,   10, 2322,
           54,    9,  570,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84,   11,    6,   33, 4673,  639,  434,   13,    7,   59,  626,    6,
         3668,    0,   25, 9588,   21,    0, 2546,  436,   21,   10,    7,  859,
            0,    8,  155,  941, 6650,    9,    7,  637,  164, 8463,    0,    2,
            1,    1,    1,    1,    1],
        [1241, 6671,    0,   84,   26,   13, 6815,  634,   18,   12,  958,   46,
            8,   33,   26,   70,   11,    6,  434, 3057, 1181, 7347,    0, 1223,
          155, 2333,    6,   12, 4836,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7, 3570,  650, 1580,   35, 1533,  156,   46,   68,  200,  803,   59,
         1069, 4579,   65,   46,  166,   25,   73,  914,  116, 1598,   20,  111,
         1510,  168,    0,  368,    6, 5992, 1911,  668,  532,    6,   46,  133,
            0,  133, 5606,    0,    2],
        [1979,  122,    0,   21,   34,  432,    7, 2639,  181, 1178,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   7,  245,   26,   17,   77,  117, 2671,   63,  116,   13, 3954,   35,
         2502,   12, 2248, 5561, 2519, 1221,   17, 1308,  213,    6,   10,  150,
         1085,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 164,   25,  601,   51,  159, 3806,    0,   38,  511,   19,  246,   10,
         1222,   19,  169,  583,   33, 1361,   12,  889,  197,    7, 8195,   12,
         3806,    6,    0,   38,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:0') tensor([31, 29, 33, 18, 32, 31, 21, 34, 36, 29, 36, 31, 41, 12, 27, 29],
       device='cuda:0') tensor([1.0000, 1.0000, 0.8594, 1.0000, 1.0000, 1.0000, 0.8677, 1.0000, 0.8862,
        0.8623, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
 > at.  tensor(50.0911, device='cuda:0', grad_fn=<MulBackward0>)
2023-08-17 14:00:12 | INFO | train_inner | epoch 001:    105 / 1474 loss=20.067, trans_loss=5.876, nll_loss=4.685, w2v_ctc_loss=22.359, task_loss=1.706, contrastive_loss=3.265, total=4215.28, n_correct=125.22, ppl=25.72, accuracy=2.971, wps=18300, ups=1.46, wpb=12577.2, bsz=473, num_updates=100, lr=4.098e-06, gnorm=2.848, clip=0, loss_scale=4, train_wall=76, gb_free=19.4, wall=136
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 131 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 30, in <module>
    from fairseq import checkpoint_utils, options, quantization_utils, tasks, utils
  File "/mnt/zhangyh/fairseq-AT/fairseq/__init__.py", line 32, in <module>
    import fairseq.criterions  # noqa
  File "/mnt/zhangyh/fairseq-AT/fairseq/criterions/__init__.py", line 35, in <module>
    importlib.import_module("fairseq.criterions." + file_name)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/importlib/__init__.py", line 127, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
  File "/mnt/zhangyh/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge.py", line 581
    assert False
    ^
IndentationError: unexpected indent
2023-08-17 14:01:34 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:17964
2023-08-17 14:01:34 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:17964
2023-08-17 14:01:34 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:17964
2023-08-17 14:01:34 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:17964
2023-08-17 14:01:34 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:17964
2023-08-17 14:01:34 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:17964
2023-08-17 14:01:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-17 14:01:34 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:17964
2023-08-17 14:01:34 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:17964
2023-08-17 14:01:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-17 14:01:34 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 14:01:35 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-17 14:01:35 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 14:01:35 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-17 14:01:35 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-17 14:01:35 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 14:01:35 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 14:01:35 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 14:01:35 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-17 14:01:35 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-17 14:01:39 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17964', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-17 14:01:39 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-17 14:01:39 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-17 14:01:39 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-17 14:01:39 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-17 14:01:39 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 14:01:44 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-17 14:01:44 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-17 14:01:44 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-17 14:01:46 | INFO | root | load pretrained hubert
2023-08-17 14:01:48 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 14:01:50 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 14:01:54 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 14:01:54 | INFO | root | share the sematic adapter and textual encoder
2023-08-17 14:01:54 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-17 14:01:54 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-17 14:01:54 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-17 14:01:54 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-17 14:01:54 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-17 14:01:54 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-17 14:01:54 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 14:01:54 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 14:01:54 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 14:01:54 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 14:01:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-17 14:01:58 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-17 14:01:58 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-17 14:01:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 14:01:59 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 14:01:59 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 14:01:59 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 14:01:59 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 14:01:59 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 14:01:59 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 14:01:59 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 14:01:59 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 14:01:59 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 14:01:59 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-17 14:01:59 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-17 14:01:59 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 14:01:59 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/checkpoint_last.pt
2023-08-17 14:01:59 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-17 14:01:59 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 14:01:59 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 14:01:59 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 14:02:01 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 14:02:02 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 14:02:51 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-17 14:02:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 14:02:51 | INFO | fairseq.trainer | begin training epoch 1
2023-08-17 14:02:51 | INFO | fairseq_cli.train | Start iterating over samples
False
False
2023-08-17 14:03:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0817_AT_sentence_mixup0105_mt0102_id0_alpha1.5_mt0.5/crash.pt
False
False
False
False
False
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 830, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 620, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 488, in _per_task_train_loss
    loss_at, loss_st, loss_mt, sample_size, logging_output = criterion(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge.py", line 239, in forward
    w2v_ctc_loss, tmp_logging_output = self.compute_ctc_loss(model, sample_st, encoder_out_st, True, self.train_st_without_ctc)
  File "/mnt/zhangyh/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge.py", line 581, in compute_ctc_loss
    assert False
AssertionError

/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 44 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
