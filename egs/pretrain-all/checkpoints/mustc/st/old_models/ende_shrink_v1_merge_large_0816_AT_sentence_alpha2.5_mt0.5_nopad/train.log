2023-08-16 03:44:59 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:14074
2023-08-16 03:44:59 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14074
2023-08-16 03:44:59 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14074
2023-08-16 03:44:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-16 03:44:59 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:14074
2023-08-16 03:44:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-16 03:44:59 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:14074
2023-08-16 03:44:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-16 03:44:59 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:14074
2023-08-16 03:44:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-16 03:44:59 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:14074
2023-08-16 03:44:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-16 03:44:59 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:14074
2023-08-16 03:44:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-16 03:45:00 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-16 03:45:00 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-16 03:45:00 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-16 03:45:00 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-16 03:45:00 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-16 03:45:00 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-16 03:45:00 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-16 03:45:00 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-16 03:45:00 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-16 03:45:04 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14074', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=2.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=2.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=2.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-16 03:45:04 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-16 03:45:04 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-16 03:45:04 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-16 03:45:04 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-16 03:45:04 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-16 03:45:09 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-16 03:45:09 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-16 03:45:09 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-16 03:45:11 | INFO | root | load pretrained hubert
2023-08-16 03:45:14 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-16 03:45:15 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-16 03:45:20 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-16 03:45:20 | INFO | root | share the sematic adapter and textual encoder
2023-08-16 03:45:20 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-16 03:45:20 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-16 03:45:20 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-16 03:45:20 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-16 03:45:20 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-16 03:45:20 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-16 03:45:20 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-16 03:45:20 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-16 03:45:20 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-16 03:45:20 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-16 03:45:24 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-16 03:45:24 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-16 03:45:24 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-16 03:45:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-16 03:45:25 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-16 03:45:25 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-16 03:45:25 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-16 03:45:25 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-16 03:45:25 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-16 03:45:25 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-16 03:45:25 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-16 03:45:25 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-16 03:45:25 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-16 03:45:25 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-16 03:45:25 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-16 03:45:25 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_last.pt
2023-08-16 03:45:25 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_last.pt
2023-08-16 03:45:25 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-16 03:45:25 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-16 03:45:25 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-16 03:45:25 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-16 03:45:27 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-16 03:45:28 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-16 03:46:15 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-16 03:46:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 03:46:15 | INFO | fairseq.trainer | begin training epoch 1
2023-08-16 03:46:15 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 03:46:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-16 03:46:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-16 03:46:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-16 03:46:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-16 03:47:07 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-16 03:47:40 | INFO | train_inner | epoch 001:    105 / 1474 loss=20.038, trans_loss=5.872, nll_loss=4.68, w2v_ctc_loss=22.309, task_loss=4.185, contrastive_loss=3.274, total=4218.67, n_correct=125.81, ppl=25.64, accuracy=2.982, wps=18335.3, ups=1.46, wpb=12587.1, bsz=476.8, num_updates=100, lr=4.098e-06, gnorm=2.872, clip=0, loss_scale=4, train_wall=76, gb_free=19.4, wall=135
2023-08-16 03:48:47 | INFO | train_inner | epoch 001:    205 / 1474 loss=16.623, trans_loss=5.867, nll_loss=4.7, w2v_ctc_loss=17.119, task_loss=4.088, contrastive_loss=3.237, total=4114.86, n_correct=114.55, ppl=26, accuracy=2.784, wps=18539.1, ups=1.51, wpb=12286.8, bsz=458.8, num_updates=200, lr=8.096e-06, gnorm=7.233, clip=16, loss_scale=4, train_wall=66, gb_free=19.4, wall=202
2023-08-16 03:49:51 | INFO | train_inner | epoch 001:    305 / 1474 loss=9.929, trans_loss=5.852, nll_loss=4.719, w2v_ctc_loss=6.893, task_loss=3.982, contrastive_loss=3.176, total=4080.91, n_correct=107.81, ppl=26.34, accuracy=2.642, wps=18819.8, ups=1.54, wpb=12190.4, bsz=439.4, num_updates=300, lr=1.2094e-05, gnorm=2.264, clip=0, loss_scale=4, train_wall=64, gb_free=18.6, wall=267
2023-08-16 03:50:57 | INFO | train_inner | epoch 001:    405 / 1474 loss=9.409, trans_loss=5.792, nll_loss=4.675, w2v_ctc_loss=6.116, task_loss=3.413, contrastive_loss=3.208, total=4176.41, n_correct=98.39, ppl=25.55, accuracy=2.356, wps=19022, ups=1.53, wpb=12470, bsz=461.3, num_updates=400, lr=1.6092e-05, gnorm=1.394, clip=1, loss_scale=4, train_wall=65, gb_free=19.5, wall=332
2023-08-16 03:52:03 | INFO | train_inner | epoch 001:    505 / 1474 loss=9.224, trans_loss=5.747, nll_loss=4.639, w2v_ctc_loss=5.818, task_loss=3.062, contrastive_loss=3.306, total=4192.13, n_correct=93.78, ppl=24.91, accuracy=2.237, wps=19098.5, ups=1.52, wpb=12526, bsz=489.2, num_updates=500, lr=2.009e-05, gnorm=1.326, clip=0, loss_scale=4, train_wall=65, gb_free=19.1, wall=398
2023-08-16 03:53:09 | INFO | train_inner | epoch 001:    605 / 1474 loss=9.116, trans_loss=5.794, nll_loss=4.701, w2v_ctc_loss=5.661, task_loss=2.992, contrastive_loss=3.255, total=4131.49, n_correct=89.35, ppl=26, accuracy=2.163, wps=18635.6, ups=1.51, wpb=12320.8, bsz=473.5, num_updates=600, lr=2.4088e-05, gnorm=1.016, clip=0, loss_scale=4, train_wall=66, gb_free=18.9, wall=464
2023-08-16 03:54:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-16 03:54:14 | INFO | train_inner | epoch 001:    706 / 1474 loss=9.012, trans_loss=5.766, nll_loss=4.666, w2v_ctc_loss=5.598, task_loss=3.144, contrastive_loss=3.135, total=4141.41, n_correct=88.54, ppl=25.38, accuracy=2.138, wps=18930.2, ups=1.53, wpb=12368.7, bsz=453.2, num_updates=700, lr=2.8086e-05, gnorm=1.424, clip=0, loss_scale=2, train_wall=65, gb_free=19.5, wall=529
2023-08-16 03:55:19 | INFO | train_inner | epoch 001:    806 / 1474 loss=8.957, trans_loss=5.906, nll_loss=4.841, w2v_ctc_loss=5.418, task_loss=3.036, contrastive_loss=3.152, total=4129.2, n_correct=77.03, ppl=28.66, accuracy=1.865, wps=19073.2, ups=1.55, wpb=12318.4, bsz=463.3, num_updates=800, lr=3.2084e-05, gnorm=1.477, clip=0, loss_scale=2, train_wall=64, gb_free=19.2, wall=594
2023-08-16 03:56:24 | INFO | train_inner | epoch 001:    906 / 1474 loss=8.847, trans_loss=5.983, nll_loss=4.93, w2v_ctc_loss=5.234, task_loss=3.084, contrastive_loss=3.06, total=4167.97, n_correct=59.23, ppl=30.49, accuracy=1.421, wps=19142.2, ups=1.54, wpb=12446.3, bsz=458.8, num_updates=900, lr=3.6082e-05, gnorm=1.829, clip=0, loss_scale=2, train_wall=65, gb_free=18.6, wall=659
2023-08-16 03:57:30 | INFO | train_inner | epoch 001:   1006 / 1474 loss=8.761, trans_loss=6.134, nll_loss=5.122, w2v_ctc_loss=4.983, task_loss=3.084, contrastive_loss=3.054, total=4137.5, n_correct=42.81, ppl=34.83, accuracy=1.035, wps=18719.7, ups=1.51, wpb=12361.1, bsz=459.1, num_updates=1000, lr=4.008e-05, gnorm=1.77, clip=0, loss_scale=2, train_wall=65, gb_free=19.3, wall=725
2023-08-16 03:58:35 | INFO | train_inner | epoch 001:   1106 / 1474 loss=8.564, trans_loss=6.168, nll_loss=5.156, w2v_ctc_loss=4.764, task_loss=3.171, contrastive_loss=2.962, total=4151.84, n_correct=36.75, ppl=35.65, accuracy=0.885, wps=18992.9, ups=1.53, wpb=12382.4, bsz=452.7, num_updates=1100, lr=4.4078e-05, gnorm=2.021, clip=1, loss_scale=2, train_wall=65, gb_free=18.8, wall=790
2023-08-16 03:59:40 | INFO | train_inner | epoch 001:   1206 / 1474 loss=8.379, trans_loss=6.174, nll_loss=5.164, w2v_ctc_loss=4.586, task_loss=3.305, contrastive_loss=2.854, total=4123.25, n_correct=38, ppl=35.86, accuracy=0.922, wps=19028.1, ups=1.54, wpb=12316.7, bsz=437.7, num_updates=1200, lr=4.8076e-05, gnorm=2.192, clip=0, loss_scale=2, train_wall=64, gb_free=19.6, wall=855
2023-08-16 04:00:44 | INFO | train_inner | epoch 001:   1306 / 1474 loss=8.223, trans_loss=6.169, nll_loss=5.157, w2v_ctc_loss=4.401, task_loss=3.111, contrastive_loss=2.807, total=4066.16, n_correct=46.61, ppl=35.68, accuracy=1.146, wps=18888.7, ups=1.56, wpb=12138.7, bsz=445.8, num_updates=1300, lr=5.2074e-05, gnorm=2.544, clip=0, loss_scale=2, train_wall=64, gb_free=18.8, wall=919
2023-08-16 04:01:49 | INFO | train_inner | epoch 001:   1406 / 1474 loss=8.077, trans_loss=6.15, nll_loss=5.137, w2v_ctc_loss=4.248, task_loss=3.163, contrastive_loss=2.881, total=4119.98, n_correct=50.7, ppl=35.18, accuracy=1.231, wps=18817.9, ups=1.53, wpb=12311.1, bsz=449.5, num_updates=1400, lr=5.6072e-05, gnorm=2.594, clip=0, loss_scale=2, train_wall=65, gb_free=18.6, wall=984
2023-08-16 04:02:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 04:03:15 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 12.601 | trans_loss 14.008 | nll_loss 13.887 | w2v_ctc_loss 5.527 | task_loss 18.539 | contrastive_loss 4.121 | total 4003.4 | n_correct 25.6 | ppl 15150.8 | accuracy 0.639 | uer 71.133 | wer 69.345 | raw_wer 69.345 | bleu 0 | wps 1141.1 | wpb 4003.4 | bsz 141.8 | num_updates 1468
2023-08-16 04:03:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1468 updates
2023-08-16 04:03:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 04:03:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 04:03:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 1 @ 1468 updates, score 0.0) (writing took 6.898658061400056 seconds)
2023-08-16 04:03:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-16 04:03:22 | INFO | train | epoch 001 | loss 10.136 | trans_loss 5.965 | nll_loss 4.891 | w2v_ctc_loss 7.236 | task_loss 3.327 | contrastive_loss 3.089 | total 4139.17 | n_correct 74.876 | ppl 29.66 | accuracy 1.809 | wps 17960.5 | ups 1.45 | wpb 12357.5 | bsz 458.9 | num_updates 1468 | lr 5.87906e-05 | gnorm 2.293 | clip 1.2 | loss_scale 2 | train_wall 962 | gb_free 18.9 | wall 1077
2023-08-16 04:03:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 04:03:22 | INFO | fairseq.trainer | begin training epoch 2
2023-08-16 04:03:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 04:03:51 | INFO | train_inner | epoch 002:     32 / 1474 loss=8.004, trans_loss=6.191, nll_loss=5.186, w2v_ctc_loss=4.097, task_loss=2.991, contrastive_loss=2.863, total=4165.61, n_correct=42.44, ppl=36.41, accuracy=1.019, wps=10200, ups=0.82, wpb=12422.6, bsz=470.4, num_updates=1500, lr=6.007e-05, gnorm=2.512, clip=0, loss_scale=2, train_wall=66, gb_free=18.7, wall=1106
2023-08-16 04:04:56 | INFO | train_inner | epoch 002:    132 / 1474 loss=7.87, trans_loss=6.188, nll_loss=5.184, w2v_ctc_loss=4.004, task_loss=3.132, contrastive_loss=2.721, total=4153.7, n_correct=42.45, ppl=36.35, accuracy=1.022, wps=19168.7, ups=1.55, wpb=12391.2, bsz=453.7, num_updates=1600, lr=6.4068e-05, gnorm=2.648, clip=0, loss_scale=2, train_wall=64, gb_free=19.2, wall=1171
2023-08-16 04:06:00 | INFO | train_inner | epoch 002:    232 / 1474 loss=7.759, trans_loss=6.167, nll_loss=5.16, w2v_ctc_loss=3.843, task_loss=2.726, contrastive_loss=2.777, total=4201.44, n_correct=47.6, ppl=35.76, accuracy=1.133, wps=19469.3, ups=1.55, wpb=12547.2, bsz=493.6, num_updates=1700, lr=6.8066e-05, gnorm=2.495, clip=0, loss_scale=2, train_wall=64, gb_free=18.9, wall=1235
2023-08-16 04:07:05 | INFO | train_inner | epoch 002:    332 / 1474 loss=7.566, trans_loss=6.149, nll_loss=5.138, w2v_ctc_loss=3.788, task_loss=3.191, contrastive_loss=2.541, total=4130.13, n_correct=49.29, ppl=35.22, accuracy=1.193, wps=19050.5, ups=1.55, wpb=12330.1, bsz=445.5, num_updates=1800, lr=7.2064e-05, gnorm=2.633, clip=0, loss_scale=2, train_wall=64, gb_free=18.7, wall=1300
2023-08-16 04:08:10 | INFO | train_inner | epoch 002:    432 / 1474 loss=7.412, trans_loss=6.134, nll_loss=5.122, w2v_ctc_loss=3.724, task_loss=3.505, contrastive_loss=2.351, total=4035.12, n_correct=48.91, ppl=34.83, accuracy=1.212, wps=18592.1, ups=1.54, wpb=12062.7, bsz=413.5, num_updates=1900, lr=7.6062e-05, gnorm=2.499, clip=0, loss_scale=2, train_wall=64, gb_free=19, wall=1365
2023-08-16 04:09:15 | INFO | train_inner | epoch 002:    532 / 1474 loss=7.347, trans_loss=6.12, nll_loss=5.101, w2v_ctc_loss=3.584, task_loss=3.024, contrastive_loss=2.503, total=4183.09, n_correct=54.89, ppl=34.31, accuracy=1.312, wps=19109.7, ups=1.53, wpb=12479, bsz=468.4, num_updates=2000, lr=8.006e-05, gnorm=2.595, clip=0, loss_scale=2, train_wall=65, gb_free=18.5, wall=1430
2023-08-16 04:09:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 04:09:56 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 11.841 | trans_loss 13.595 | nll_loss 13.356 | w2v_ctc_loss 4.661 | task_loss 18.539 | contrastive_loss 3.562 | total 4003.4 | n_correct 48.8 | ppl 10487.4 | accuracy 1.219 | uer 62.711 | wer 60.859 | raw_wer 60.859 | bleu 0 | wps 1130.8 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0
2023-08-16 04:09:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-16 04:09:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_2_2000.pt
2023-08-16 04:09:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_2_2000.pt
2023-08-16 04:10:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 30.340473609045148 seconds)
2023-08-16 04:11:30 | INFO | train_inner | epoch 002:    632 / 1474 loss=7.205, trans_loss=6.111, nll_loss=5.089, w2v_ctc_loss=3.492, task_loss=3.107, contrastive_loss=2.323, total=4123.85, n_correct=53.24, ppl=34.03, accuracy=1.291, wps=9103.6, ups=0.74, wpb=12306.4, bsz=448.7, num_updates=2100, lr=8.4058e-05, gnorm=2.528, clip=0, loss_scale=2, train_wall=64, gb_free=18.6, wall=1565
2023-08-16 04:12:35 | INFO | train_inner | epoch 002:    732 / 1474 loss=7.127, trans_loss=6.099, nll_loss=5.075, w2v_ctc_loss=3.427, task_loss=3.073, contrastive_loss=2.391, total=4148.13, n_correct=57.45, ppl=33.71, accuracy=1.385, wps=19194.6, ups=1.55, wpb=12381, bsz=462.1, num_updates=2200, lr=8.8056e-05, gnorm=2.558, clip=0, loss_scale=2, train_wall=64, gb_free=18.6, wall=1630
2023-08-16 04:13:40 | INFO | train_inner | epoch 002:    832 / 1474 loss=7.032, trans_loss=6.083, nll_loss=5.057, w2v_ctc_loss=3.374, task_loss=3.108, contrastive_loss=2.341, total=4172.27, n_correct=60.55, ppl=33.29, accuracy=1.451, wps=19180.5, ups=1.54, wpb=12465.7, bsz=464.5, num_updates=2300, lr=9.2054e-05, gnorm=2.378, clip=0, loss_scale=2, train_wall=64, gb_free=18.8, wall=1695
2023-08-16 04:14:45 | INFO | train_inner | epoch 002:    932 / 1474 loss=6.897, trans_loss=6.076, nll_loss=5.043, w2v_ctc_loss=3.288, task_loss=3.254, contrastive_loss=2.262, total=4101.67, n_correct=56.26, ppl=32.97, accuracy=1.372, wps=18728, ups=1.53, wpb=12242.5, bsz=441.6, num_updates=2400, lr=9.6052e-05, gnorm=2.425, clip=0, loss_scale=2, train_wall=65, gb_free=18.9, wall=1760
2023-08-16 04:15:50 | INFO | train_inner | epoch 002:   1032 / 1474 loss=6.808, trans_loss=6.073, nll_loss=5.039, w2v_ctc_loss=3.223, task_loss=3.161, contrastive_loss=2.153, total=4091.09, n_correct=58.94, ppl=32.87, accuracy=1.441, wps=18835.9, ups=1.54, wpb=12214.8, bsz=451.5, num_updates=2500, lr=0.00010005, gnorm=2.25, clip=0, loss_scale=2, train_wall=64, gb_free=19.2, wall=1825
2023-08-16 04:16:55 | INFO | train_inner | epoch 002:   1132 / 1474 loss=6.763, trans_loss=6.054, nll_loss=5.016, w2v_ctc_loss=3.123, task_loss=2.739, contrastive_loss=2.368, total=4219.19, n_correct=64.33, ppl=32.36, accuracy=1.525, wps=19366.1, ups=1.54, wpb=12595.1, bsz=500.6, num_updates=2600, lr=0.000104048, gnorm=2.09, clip=0, loss_scale=2, train_wall=65, gb_free=19, wall=1890
2023-08-16 04:18:00 | INFO | train_inner | epoch 002:   1232 / 1474 loss=6.656, trans_loss=6.042, nll_loss=5, w2v_ctc_loss=3.089, task_loss=2.891, contrastive_loss=2.175, total=4212.91, n_correct=65.97, ppl=32, accuracy=1.566, wps=19277.3, ups=1.53, wpb=12571.7, bsz=486.8, num_updates=2700, lr=0.000108046, gnorm=2.077, clip=0, loss_scale=2, train_wall=65, gb_free=19.1, wall=1955
2023-08-16 04:19:05 | INFO | train_inner | epoch 002:   1332 / 1474 loss=6.548, trans_loss=6.034, nll_loss=4.993, w2v_ctc_loss=3.051, task_loss=3.038, contrastive_loss=1.959, total=4142.48, n_correct=65.83, ppl=31.84, accuracy=1.589, wps=19083.9, ups=1.54, wpb=12381.2, bsz=456.3, num_updates=2800, lr=0.000112044, gnorm=2.114, clip=0, loss_scale=4, train_wall=64, gb_free=18.9, wall=2020
2023-08-16 04:20:10 | INFO | train_inner | epoch 002:   1432 / 1474 loss=6.454, trans_loss=6.026, nll_loss=4.98, w2v_ctc_loss=3.004, task_loss=3.311, contrastive_loss=2.018, total=4063.28, n_correct=63.56, ppl=31.57, accuracy=1.564, wps=18651.5, ups=1.54, wpb=12131.5, bsz=444.3, num_updates=2900, lr=0.000116042, gnorm=2.097, clip=0, loss_scale=4, train_wall=64, gb_free=19.6, wall=2085
2023-08-16 04:20:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 04:21:18 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 10.94 | trans_loss 13.133 | nll_loss 12.749 | w2v_ctc_loss 3.81 | task_loss 18.538 | contrastive_loss 2.703 | total 4003.4 | n_correct 80.1 | ppl 6883.48 | accuracy 2.001 | uer 53.402 | wer 52.731 | raw_wer 52.731 | bleu 0 | wps 1096.4 | wpb 4003.4 | bsz 141.8 | num_updates 2942 | best_bleu 0
2023-08-16 04:21:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2942 updates
2023-08-16 04:21:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 04:21:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 04:21:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 2 @ 2942 updates, score 0.0) (writing took 31.801117369905114 seconds)
2023-08-16 04:21:50 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-16 04:21:50 | INFO | train | epoch 002 | loss 7.102 | trans_loss 6.097 | nll_loss 5.072 | w2v_ctc_loss 3.431 | task_loss 3.084 | contrastive_loss 2.348 | total 4138.65 | n_correct 56.2883 | ppl 33.63 | accuracy 1.36 | wps 16434.1 | ups 1.33 | wpb 12355.8 | bsz 458.5 | num_updates 2942 | lr 0.000117721 | gnorm 2.374 | clip 0 | loss_scale 4 | train_wall 948 | gb_free 19 | wall 2185
2023-08-16 04:21:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 04:21:50 | INFO | fairseq.trainer | begin training epoch 3
2023-08-16 04:21:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 04:22:36 | INFO | train_inner | epoch 003:     58 / 1474 loss=6.351, trans_loss=6.013, nll_loss=4.964, w2v_ctc_loss=2.952, task_loss=3.25, contrastive_loss=1.864, total=4048.67, n_correct=67.21, ppl=31.22, accuracy=1.66, wps=8286.8, ups=0.69, wpb=12085.6, bsz=433.9, num_updates=3000, lr=0.00012004, gnorm=1.878, clip=0, loss_scale=4, train_wall=64, gb_free=18.8, wall=2231
2023-08-16 04:22:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-16 04:24:09 | INFO | train_inner | epoch 003:    159 / 1474 loss=5.7, trans_loss=5.35, nll_loss=4.127, w2v_ctc_loss=2.707, task_loss=2.118, contrastive_loss=1.803, total=4157.08, n_correct=241.92, ppl=17.47, accuracy=5.819, wps=13395.9, ups=1.08, wpb=12413.1, bsz=461.9, num_updates=3100, lr=0.000124038, gnorm=3.979, clip=4, loss_scale=2, train_wall=92, gb_free=16.2, wall=2324
2023-08-16 04:24:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-16 04:25:43 | INFO | train_inner | epoch 003:    260 / 1474 loss=5.026, trans_loss=4.907, nll_loss=3.544, w2v_ctc_loss=2.46, task_loss=2.161, contrastive_loss=1.598, total=4161, n_correct=555.12, ppl=11.66, accuracy=13.341, wps=13154, ups=1.06, wpb=12431.2, bsz=469.1, num_updates=3200, lr=0.000128036, gnorm=4.33, clip=3, loss_scale=1, train_wall=94, gb_free=15.4, wall=2418
2023-08-16 04:27:16 | INFO | train_inner | epoch 003:    360 / 1474 loss=4.579, trans_loss=4.4, nll_loss=2.867, w2v_ctc_loss=2.415, task_loss=2.138, contrastive_loss=1.569, total=4159.93, n_correct=1059.27, ppl=7.3, accuracy=25.464, wps=13353.2, ups=1.08, wpb=12413.4, bsz=467.3, num_updates=3300, lr=0.000132034, gnorm=3.679, clip=3, loss_scale=1, train_wall=92, gb_free=15.8, wall=2511
2023-08-16 04:28:49 | INFO | train_inner | epoch 003:    460 / 1474 loss=4.313, trans_loss=4.23, nll_loss=2.648, w2v_ctc_loss=2.326, task_loss=2.146, contrastive_loss=1.395, total=4196.46, n_correct=1276.14, ppl=6.27, accuracy=30.41, wps=13482.4, ups=1.08, wpb=12527.1, bsz=468.6, num_updates=3400, lr=0.000136032, gnorm=3.152, clip=1, loss_scale=1, train_wall=92, gb_free=15.5, wall=2604
2023-08-16 04:30:21 | INFO | train_inner | epoch 003:    560 / 1474 loss=4.126, trans_loss=4.184, nll_loss=2.593, w2v_ctc_loss=2.236, task_loss=2.316, contrastive_loss=1.283, total=4085.25, n_correct=1299.98, ppl=6.03, accuracy=31.821, wps=13226, ups=1.08, wpb=12203.2, bsz=439.8, num_updates=3500, lr=0.00014003, gnorm=2.735, clip=0, loss_scale=1, train_wall=92, gb_free=15.5, wall=2697
2023-08-16 04:31:55 | INFO | train_inner | epoch 003:    660 / 1474 loss=4.049, trans_loss=4.156, nll_loss=2.552, w2v_ctc_loss=2.158, task_loss=2.05, contrastive_loss=1.356, total=4229.91, n_correct=1400.72, ppl=5.87, accuracy=33.115, wps=13431.9, ups=1.07, wpb=12610.8, bsz=484.8, num_updates=3600, lr=0.000144028, gnorm=2.657, clip=1, loss_scale=1, train_wall=93, gb_free=17.1, wall=2790
2023-08-16 04:33:28 | INFO | train_inner | epoch 003:    760 / 1474 loss=3.928, trans_loss=4.122, nll_loss=2.514, w2v_ctc_loss=2.128, task_loss=2.096, contrastive_loss=1.105, total=4157.48, n_correct=1412.42, ppl=5.71, accuracy=33.973, wps=13404.4, ups=1.08, wpb=12420.7, bsz=467.8, num_updates=3700, lr=0.000148026, gnorm=2.753, clip=1, loss_scale=1, train_wall=92, gb_free=11.1, wall=2883
2023-08-16 04:35:01 | INFO | train_inner | epoch 003:    860 / 1474 loss=3.801, trans_loss=4.114, nll_loss=2.502, w2v_ctc_loss=2.044, task_loss=2.166, contrastive_loss=1.017, total=4172.27, n_correct=1436.88, ppl=5.66, accuracy=34.439, wps=13412.3, ups=1.08, wpb=12457.6, bsz=458.8, num_updates=3800, lr=0.000152024, gnorm=1.991, clip=0, loss_scale=1, train_wall=92, gb_free=16.4, wall=2976
2023-08-16 04:36:34 | INFO | train_inner | epoch 003:    960 / 1474 loss=3.791, trans_loss=4.097, nll_loss=2.478, w2v_ctc_loss=2.055, task_loss=2.083, contrastive_loss=1.054, total=4171.53, n_correct=1471.9, ppl=5.57, accuracy=35.284, wps=13347, ups=1.07, wpb=12442.2, bsz=473.5, num_updates=3900, lr=0.000156022, gnorm=3.229, clip=3, loss_scale=1, train_wall=93, gb_free=15.9, wall=3069
2023-08-16 04:38:06 | INFO | train_inner | epoch 003:   1060 / 1474 loss=3.75, trans_loss=4.077, nll_loss=2.456, w2v_ctc_loss=2.085, task_loss=2.347, contrastive_loss=0.954, total=4051.14, n_correct=1449.93, ppl=5.49, accuracy=35.791, wps=13113.8, ups=1.08, wpb=12099.4, bsz=436.8, num_updates=4000, lr=0.00016002, gnorm=3.419, clip=3, loss_scale=1, train_wall=92, gb_free=16.5, wall=3162
2023-08-16 04:38:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 04:38:35 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.99 | trans_loss 7.115 | nll_loss 4.945 | w2v_ctc_loss 2.48 | task_loss 10.162 | contrastive_loss 1.362 | total 4003.4 | n_correct 1538.7 | ppl 30.8 | accuracy 38.435 | uer 35.798 | wer 35.614 | raw_wer 35.614 | bleu 1.66 | wps 1619.8 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 1.66
2023-08-16 04:38:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-16 04:38:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_3_4000.pt
2023-08-16 04:38:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_3_4000.pt
2023-08-16 04:39:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 1.66) (writing took 59.71502703987062 seconds)
2023-08-16 04:41:07 | INFO | train_inner | epoch 003:   1160 / 1474 loss=3.669, trans_loss=4.071, nll_loss=2.445, w2v_ctc_loss=2.029, task_loss=2.317, contrastive_loss=0.908, total=4050.25, n_correct=1466.35, ppl=5.45, accuracy=36.204, wps=6689.6, ups=0.55, wpb=12088.6, bsz=435.7, num_updates=4100, lr=0.000164018, gnorm=2.822, clip=2, loss_scale=1, train_wall=91, gb_free=16, wall=3342
2023-08-16 04:42:39 | INFO | train_inner | epoch 003:   1260 / 1474 loss=3.603, trans_loss=4.049, nll_loss=2.419, w2v_ctc_loss=1.997, task_loss=2.312, contrastive_loss=0.849, total=4058.28, n_correct=1494.76, ppl=5.35, accuracy=36.832, wps=13221.7, ups=1.09, wpb=12119.7, bsz=431.2, num_updates=4200, lr=0.000168016, gnorm=2.793, clip=3, loss_scale=1, train_wall=91, gb_free=16.2, wall=3434
2023-08-16 04:44:12 | INFO | train_inner | epoch 003:   1360 / 1474 loss=3.585, trans_loss=4.03, nll_loss=2.394, w2v_ctc_loss=1.963, task_loss=2.186, contrastive_loss=0.955, total=4134.29, n_correct=1556, ppl=5.25, accuracy=37.636, wps=13242.4, ups=1.07, wpb=12343, bsz=460.8, num_updates=4300, lr=0.000172014, gnorm=2.628, clip=2, loss_scale=1, train_wall=93, gb_free=16.5, wall=3527
2023-08-16 04:45:45 | INFO | train_inner | epoch 003:   1460 / 1474 loss=3.498, trans_loss=4.014, nll_loss=2.374, w2v_ctc_loss=1.913, task_loss=2.063, contrastive_loss=0.882, total=4206.08, n_correct=1615.87, ppl=5.18, accuracy=38.417, wps=13516.9, ups=1.08, wpb=12563.6, bsz=476.5, num_updates=4400, lr=0.000176012, gnorm=1.918, clip=0, loss_scale=1, train_wall=92, gb_free=14.1, wall=3620
2023-08-16 04:45:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 04:46:31 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.683 | trans_loss 6.874 | nll_loss 4.626 | w2v_ctc_loss 2.316 | task_loss 10.187 | contrastive_loss 1.071 | total 4003.4 | n_correct 1671.6 | ppl 24.7 | accuracy 41.755 | uer 33.778 | wer 33.63 | raw_wer 33.63 | bleu 4.09 | wps 1466 | wpb 4003.4 | bsz 141.8 | num_updates 4414 | best_bleu 4.09
2023-08-16 04:46:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4414 updates
2023-08-16 04:46:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 04:46:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 04:47:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 3 @ 4414 updates, score 4.09) (writing took 30.216055436059833 seconds)
2023-08-16 04:47:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-16 04:47:01 | INFO | train | epoch 003 | loss 4.184 | trans_loss 4.338 | nll_loss 2.794 | w2v_ctc_loss 2.207 | task_loss 2.216 | contrastive_loss 1.223 | total 4138.94 | n_correct 1222.34 | ppl 6.93 | accuracy 29.533 | wps 12036.7 | ups 0.97 | wpb 12356.6 | bsz 458.7 | num_updates 4414 | lr 0.000176572 | gnorm 2.95 | clip 1.8 | loss_scale 1 | train_wall 1342 | gb_free 16.2 | wall 3696
2023-08-16 04:47:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 04:47:01 | INFO | fairseq.trainer | begin training epoch 4
2023-08-16 04:47:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 04:48:27 | INFO | train_inner | epoch 004:     86 / 1474 loss=3.374, trans_loss=3.985, nll_loss=2.332, w2v_ctc_loss=1.872, task_loss=2.261, contrastive_loss=0.698, total=4096.88, n_correct=1617.07, ppl=5.04, accuracy=39.471, wps=7548.6, ups=0.62, wpb=12226.6, bsz=438.5, num_updates=4500, lr=0.00018001, gnorm=2.982, clip=2, loss_scale=1, train_wall=91, gb_free=13.8, wall=3782
2023-08-16 04:49:59 | INFO | train_inner | epoch 004:    186 / 1474 loss=3.388, trans_loss=3.962, nll_loss=2.305, w2v_ctc_loss=1.898, task_loss=2.064, contrastive_loss=0.731, total=4177.87, n_correct=1683.22, ppl=4.94, accuracy=40.289, wps=13545.2, ups=1.09, wpb=12473.8, bsz=469, num_updates=4600, lr=0.000184008, gnorm=2.191, clip=2, loss_scale=1, train_wall=91, gb_free=15.2, wall=3874
2023-08-16 04:51:32 | INFO | train_inner | epoch 004:    286 / 1474 loss=3.327, trans_loss=3.949, nll_loss=2.29, w2v_ctc_loss=1.838, task_loss=2.176, contrastive_loss=0.81, total=4147.79, n_correct=1693.19, ppl=4.89, accuracy=40.821, wps=13317.4, ups=1.07, wpb=12391.1, bsz=464.6, num_updates=4700, lr=0.000188006, gnorm=1.855, clip=0, loss_scale=1, train_wall=92, gb_free=15.8, wall=3967
2023-08-16 04:52:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-08-16 04:53:05 | INFO | train_inner | epoch 004:    387 / 1474 loss=3.228, trans_loss=3.934, nll_loss=2.267, w2v_ctc_loss=1.806, task_loss=2.278, contrastive_loss=0.611, total=4112.14, n_correct=1709.18, ppl=4.81, accuracy=41.564, wps=13152, ups=1.07, wpb=12269.1, bsz=439.7, num_updates=4800, lr=0.000192004, gnorm=1.636, clip=0, loss_scale=0.5, train_wall=93, gb_free=16.7, wall=4060
2023-08-16 04:54:39 | INFO | train_inner | epoch 004:    487 / 1474 loss=3.296, trans_loss=3.913, nll_loss=2.241, w2v_ctc_loss=1.784, task_loss=1.921, contrastive_loss=1.026, total=4242.53, n_correct=1798.33, ppl=4.73, accuracy=42.388, wps=13575.7, ups=1.07, wpb=12663.6, bsz=507.9, num_updates=4900, lr=0.000196002, gnorm=1.865, clip=0, loss_scale=0.5, train_wall=93, gb_free=16.9, wall=4154
2023-08-16 04:56:11 | INFO | train_inner | epoch 004:    587 / 1474 loss=3.17, trans_loss=3.883, nll_loss=2.201, w2v_ctc_loss=1.774, task_loss=2.047, contrastive_loss=0.654, total=4222.82, n_correct=1841.66, ppl=4.6, accuracy=43.612, wps=13649, ups=1.08, wpb=12606.6, bsz=484.3, num_updates=5000, lr=0.0002, gnorm=1.693, clip=1, loss_scale=0.5, train_wall=92, gb_free=14.3, wall=4246
mt_weight tensor(0.5000)
asr_weight tensor(0.5010, device='cuda:0')
2023-08-16 04:57:45 | INFO | train_inner | epoch 004:    687 / 1474 loss=3.14, trans_loss=3.885, nll_loss=2.2, w2v_ctc_loss=1.761, task_loss=2.269, contrastive_loss=0.69, total=4165.36, n_correct=1827.45, ppl=4.59, accuracy=43.873, wps=13205.2, ups=1.06, wpb=12416.4, bsz=452, num_updates=5100, lr=0.00019803, gnorm=1.212, clip=0, loss_scale=0.5, train_wall=93, gb_free=15.4, wall=4340
2023-08-16 04:59:18 | INFO | train_inner | epoch 004:    787 / 1474 loss=3.095, trans_loss=3.86, nll_loss=2.172, w2v_ctc_loss=1.78, task_loss=2.404, contrastive_loss=0.543, total=4021.88, n_correct=1794.82, ppl=4.51, accuracy=44.626, wps=12978, ups=1.08, wpb=12009.3, bsz=420.1, num_updates=5200, lr=0.000196116, gnorm=1.243, clip=0, loss_scale=0.5, train_wall=92, gb_free=17.2, wall=4433
2023-08-16 05:00:50 | INFO | train_inner | epoch 004:    887 / 1474 loss=3.112, trans_loss=3.837, nll_loss=2.143, w2v_ctc_loss=1.759, task_loss=2.171, contrastive_loss=0.72, total=4185.92, n_correct=1898.99, ppl=4.42, accuracy=45.366, wps=13495.3, ups=1.08, wpb=12500.2, bsz=466.8, num_updates=5300, lr=0.000194257, gnorm=1.123, clip=0, loss_scale=0.5, train_wall=92, gb_free=16.6, wall=4525
2023-08-16 05:02:23 | INFO | train_inner | epoch 004:    987 / 1474 loss=3.021, trans_loss=3.817, nll_loss=2.117, w2v_ctc_loss=1.72, task_loss=2.233, contrastive_loss=0.568, total=4125.22, n_correct=1906.56, ppl=4.34, accuracy=46.217, wps=13298.6, ups=1.08, wpb=12322, bsz=454.4, num_updates=5400, lr=0.00019245, gnorm=1.067, clip=0, loss_scale=0.5, train_wall=92, gb_free=16.5, wall=4618
2023-08-16 05:03:56 | INFO | train_inner | epoch 004:   1087 / 1474 loss=3.018, trans_loss=3.816, nll_loss=2.115, w2v_ctc_loss=1.739, task_loss=2.31, contrastive_loss=0.538, total=4079.97, n_correct=1894.39, ppl=4.33, accuracy=46.431, wps=13148.9, ups=1.08, wpb=12178.8, bsz=440.5, num_updates=5500, lr=0.000190693, gnorm=1.135, clip=0, loss_scale=0.5, train_wall=92, gb_free=16.4, wall=4711
2023-08-16 05:05:28 | INFO | train_inner | epoch 004:   1187 / 1474 loss=3.039, trans_loss=3.802, nll_loss=2.099, w2v_ctc_loss=1.734, task_loss=2.031, contrastive_loss=0.65, total=4171.2, n_correct=1964.59, ppl=4.28, accuracy=47.099, wps=13446.8, ups=1.08, wpb=12457.9, bsz=485.8, num_updates=5600, lr=0.000188982, gnorm=1.172, clip=0, loss_scale=0.5, train_wall=92, gb_free=15.5, wall=4803
2023-08-16 05:07:00 | INFO | train_inner | epoch 004:   1287 / 1474 loss=2.993, trans_loss=3.784, nll_loss=2.075, w2v_ctc_loss=1.72, task_loss=2.112, contrastive_loss=0.597, total=4140.63, n_correct=1974.62, ppl=4.21, accuracy=47.689, wps=13416.2, ups=1.09, wpb=12365.2, bsz=466.8, num_updates=5700, lr=0.000187317, gnorm=1.208, clip=1, loss_scale=0.5, train_wall=92, gb_free=16.6, wall=4895
2023-08-16 05:08:31 | INFO | train_inner | epoch 004:   1387 / 1474 loss=2.944, trans_loss=3.772, nll_loss=2.06, w2v_ctc_loss=1.719, task_loss=2.238, contrastive_loss=0.464, total=4100.54, n_correct=1970.26, ppl=4.17, accuracy=48.049, wps=13476.8, ups=1.1, wpb=12248.6, bsz=438.8, num_updates=5800, lr=0.000185695, gnorm=1.165, clip=0, loss_scale=0.5, train_wall=90, gb_free=17.5, wall=4986
2023-08-16 05:09:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.5010, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.5010, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.5010, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.5010, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.5010, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.5010, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.5010, device='cuda:1')
2023-08-16 05:10:17 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.815 | trans_loss 5.977 | nll_loss 3.458 | w2v_ctc_loss 1.927 | task_loss 10.811 | contrastive_loss 0.655 | total 4003.4 | n_correct 2169.2 | ppl 10.99 | accuracy 54.184 | uer 28.219 | wer 29.391 | raw_wer 29.391 | bleu 12.7 | wps 1867.4 | wpb 4003.4 | bsz 141.8 | num_updates 5887 | best_bleu 12.7
2023-08-16 05:10:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5887 updates
2023-08-16 05:10:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 05:10:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 05:10:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 4 @ 5887 updates, score 12.7) (writing took 29.185263676568866 seconds)
2023-08-16 05:10:46 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-16 05:10:46 | INFO | train | epoch 004 | loss 3.136 | trans_loss 3.864 | nll_loss 2.177 | w2v_ctc_loss 1.771 | task_loss 2.178 | contrastive_loss 0.657 | total 4138.79 | n_correct 1838.1 | ppl 4.52 | accuracy 44.412 | wps 12773.5 | ups 1.03 | wpb 12356.2 | bsz 458.6 | num_updates 5887 | lr 0.000184318 | gnorm 1.498 | clip 0.4 | loss_scale 0.5 | train_wall 1352 | gb_free 14.5 | wall 5121
2023-08-16 05:10:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 05:10:46 | INFO | fairseq.trainer | begin training epoch 5
2023-08-16 05:10:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 05:11:06 | INFO | train_inner | epoch 005:     13 / 1474 loss=2.873, trans_loss=3.755, nll_loss=2.036, w2v_ctc_loss=1.645, task_loss=2.252, contrastive_loss=0.47, total=4049.24, n_correct=1978.37, ppl=4.1, accuracy=48.858, wps=7832.3, ups=0.65, wpb=12086.7, bsz=441.2, num_updates=5900, lr=0.000184115, gnorm=0.927, clip=0, loss_scale=0.5, train_wall=91, gb_free=16.7, wall=5141
2023-08-16 05:12:38 | INFO | train_inner | epoch 005:    113 / 1474 loss=2.809, trans_loss=3.712, nll_loss=1.982, w2v_ctc_loss=1.585, task_loss=1.953, contrastive_loss=0.477, total=4250.33, n_correct=2139.79, ppl=3.95, accuracy=50.344, wps=13700.1, ups=1.08, wpb=12693.9, bsz=497.5, num_updates=6000, lr=0.000182574, gnorm=0.862, clip=0, loss_scale=0.5, train_wall=92, gb_free=16.8, wall=5233
2023-08-16 05:12:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 05:13:05 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.791 | trans_loss 5.957 | nll_loss 3.426 | w2v_ctc_loss 1.912 | task_loss 10.647 | contrastive_loss 0.632 | total 4003.4 | n_correct 2180.8 | ppl 10.75 | accuracy 54.474 | uer 28.511 | wer 29.469 | raw_wer 29.469 | bleu 12.85 | wps 1801.4 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 12.85
2023-08-16 05:13:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-16 05:13:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_5_6000.pt
2023-08-16 05:13:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_5_6000.pt
2023-08-16 05:13:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 12.85) (writing took 29.626459907740355 seconds)
2023-08-16 05:15:06 | INFO | train_inner | epoch 005:    213 / 1474 loss=2.832, trans_loss=3.718, nll_loss=1.987, w2v_ctc_loss=1.587, task_loss=2.021, contrastive_loss=0.66, total=4190.08, n_correct=2109.94, ppl=3.96, accuracy=50.356, wps=8455.3, ups=0.68, wpb=12501.7, bsz=486.7, num_updates=6100, lr=0.000181071, gnorm=0.796, clip=0, loss_scale=0.5, train_wall=91, gb_free=12.3, wall=5381
2023-08-16 05:16:37 | INFO | train_inner | epoch 005:    313 / 1474 loss=2.811, trans_loss=3.699, nll_loss=1.968, w2v_ctc_loss=1.608, task_loss=2.212, contrastive_loss=0.519, total=4096.44, n_correct=2069.51, ppl=3.91, accuracy=50.52, wps=13479.7, ups=1.1, wpb=12249.1, bsz=448.2, num_updates=6200, lr=0.000179605, gnorm=0.963, clip=1, loss_scale=0.5, train_wall=90, gb_free=16.4, wall=5472
2023-08-16 05:18:10 | INFO | train_inner | epoch 005:    413 / 1474 loss=2.793, trans_loss=3.691, nll_loss=1.956, w2v_ctc_loss=1.57, task_loss=2.164, contrastive_loss=0.591, total=4133.67, n_correct=2110.41, ppl=3.88, accuracy=51.054, wps=13323.5, ups=1.08, wpb=12352.7, bsz=463.8, num_updates=6300, lr=0.000178174, gnorm=0.913, clip=0, loss_scale=0.5, train_wall=92, gb_free=16.3, wall=5565
2023-08-16 05:19:42 | INFO | train_inner | epoch 005:    513 / 1474 loss=2.758, trans_loss=3.692, nll_loss=1.956, w2v_ctc_loss=1.568, task_loss=2.366, contrastive_loss=0.518, total=4036.61, n_correct=2062.82, ppl=3.88, accuracy=51.103, wps=13068.2, ups=1.08, wpb=12057.3, bsz=428.7, num_updates=6400, lr=0.000176777, gnorm=0.914, clip=0, loss_scale=0.5, train_wall=92, gb_free=17.6, wall=5657
2023-08-16 05:21:14 | INFO | train_inner | epoch 005:    613 / 1474 loss=2.725, trans_loss=3.689, nll_loss=1.949, w2v_ctc_loss=1.561, task_loss=2.293, contrastive_loss=0.413, total=4112.09, n_correct=2117.06, ppl=3.86, accuracy=51.484, wps=13336.2, ups=1.09, wpb=12268.1, bsz=443.6, num_updates=6500, lr=0.000175412, gnorm=0.811, clip=0, loss_scale=0.5, train_wall=91, gb_free=16.5, wall=5749
2023-08-16 05:22:46 | INFO | train_inner | epoch 005:    713 / 1474 loss=2.73, trans_loss=3.679, nll_loss=1.938, w2v_ctc_loss=1.541, task_loss=2.088, contrastive_loss=0.516, total=4160.15, n_correct=2162.01, ppl=3.83, accuracy=51.97, wps=13454, ups=1.08, wpb=12418.1, bsz=477.6, num_updates=6600, lr=0.000174078, gnorm=0.812, clip=0, loss_scale=0.5, train_wall=92, gb_free=15.3, wall=5841
2023-08-16 05:24:20 | INFO | train_inner | epoch 005:    813 / 1474 loss=2.698, trans_loss=3.669, nll_loss=1.925, w2v_ctc_loss=1.536, task_loss=2.223, contrastive_loss=0.438, total=4129.67, n_correct=2153.19, ppl=3.8, accuracy=52.14, wps=13200.9, ups=1.07, wpb=12327.3, bsz=452.6, num_updates=6700, lr=0.000172774, gnorm=0.8, clip=0, loss_scale=0.5, train_wall=93, gb_free=15.7, wall=5935
2023-08-16 05:25:52 | INFO | train_inner | epoch 005:    913 / 1474 loss=2.647, trans_loss=3.655, nll_loss=1.907, w2v_ctc_loss=1.509, task_loss=2.263, contrastive_loss=0.381, total=4107.27, n_correct=2170.23, ppl=3.75, accuracy=52.839, wps=13344.3, ups=1.09, wpb=12262.1, bsz=446.2, num_updates=6800, lr=0.000171499, gnorm=0.707, clip=0, loss_scale=0.5, train_wall=91, gb_free=12.7, wall=6027
2023-08-16 05:27:23 | INFO | train_inner | epoch 005:   1013 / 1474 loss=2.67, trans_loss=3.656, nll_loss=1.909, w2v_ctc_loss=1.517, task_loss=2.178, contrastive_loss=0.462, total=4154.85, n_correct=2194.6, ppl=3.75, accuracy=52.82, wps=13518.4, ups=1.09, wpb=12403.9, bsz=458.6, num_updates=6900, lr=0.000170251, gnorm=1.078, clip=1, loss_scale=1, train_wall=91, gb_free=15.2, wall=6118
2023-08-16 05:28:57 | INFO | train_inner | epoch 005:   1113 / 1474 loss=2.698, trans_loss=3.652, nll_loss=1.902, w2v_ctc_loss=1.539, task_loss=2.154, contrastive_loss=0.48, total=4178.83, n_correct=2217.04, ppl=3.74, accuracy=53.054, wps=13371.4, ups=1.07, wpb=12465.4, bsz=468.4, num_updates=7000, lr=0.000169031, gnorm=1.025, clip=2, loss_scale=1, train_wall=93, gb_free=12.9, wall=6212
2023-08-16 05:30:29 | INFO | train_inner | epoch 005:   1213 / 1474 loss=2.605, trans_loss=3.639, nll_loss=1.885, w2v_ctc_loss=1.482, task_loss=2.212, contrastive_loss=0.356, total=4163.71, n_correct=2232.11, ppl=3.69, accuracy=53.609, wps=13443.2, ups=1.08, wpb=12420.2, bsz=454, num_updates=7100, lr=0.000167836, gnorm=0.707, clip=0, loss_scale=1, train_wall=92, gb_free=17, wall=6304
2023-08-16 05:32:02 | INFO | train_inner | epoch 005:   1313 / 1474 loss=2.579, trans_loss=3.631, nll_loss=1.876, w2v_ctc_loss=1.474, task_loss=2.24, contrastive_loss=0.314, total=4125.59, n_correct=2226.75, ppl=3.67, accuracy=53.974, wps=13254.1, ups=1.08, wpb=12312.6, bsz=442.8, num_updates=7200, lr=0.000166667, gnorm=0.707, clip=0, loss_scale=1, train_wall=92, gb_free=14.6, wall=6397
2023-08-16 05:33:33 | INFO | train_inner | epoch 005:   1413 / 1474 loss=2.606, trans_loss=3.631, nll_loss=1.878, w2v_ctc_loss=1.475, task_loss=2.181, contrastive_loss=0.394, total=4145.41, n_correct=2233.11, ppl=3.68, accuracy=53.869, wps=13520.8, ups=1.09, wpb=12381.3, bsz=460.5, num_updates=7300, lr=0.000165521, gnorm=0.799, clip=0, loss_scale=1, train_wall=91, gb_free=16.4, wall=6488
2023-08-16 05:34:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 05:34:52 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.493 | trans_loss 5.667 | nll_loss 3.064 | w2v_ctc_loss 1.684 | task_loss 10.977 | contrastive_loss 0.539 | total 4003.4 | n_correct 2349.9 | ppl 8.36 | accuracy 58.698 | uer 26.149 | wer 27.486 | raw_wer 27.486 | bleu 15.99 | wps 2304.5 | wpb 4003.4 | bsz 141.8 | num_updates 7361 | best_bleu 15.99
2023-08-16 05:34:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7361 updates
2023-08-16 05:34:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 05:35:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 05:35:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 5 @ 7361 updates, score 15.99) (writing took 31.802234729751945 seconds)
2023-08-16 05:35:24 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-16 05:35:24 | INFO | train | epoch 005 | loss 2.708 | trans_loss 3.671 | nll_loss 1.928 | w2v_ctc_loss 1.538 | task_loss 2.18 | contrastive_loss 0.465 | total 4138.65 | n_correct 2158.36 | ppl 3.8 | accuracy 52.151 | wps 12322.3 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 7361 | lr 0.000164834 | gnorm 0.846 | clip 0.3 | loss_scale 1 | train_wall 1351 | gb_free 16 | wall 6599
2023-08-16 05:35:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 05:35:24 | INFO | fairseq.trainer | begin training epoch 6
2023-08-16 05:35:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 05:36:08 | INFO | train_inner | epoch 006:     39 / 1474 loss=2.575, trans_loss=3.612, nll_loss=1.851, w2v_ctc_loss=1.461, task_loss=2.255, contrastive_loss=0.376, total=4112.54, n_correct=2241.18, ppl=3.61, accuracy=54.496, wps=7922.6, ups=0.65, wpb=12270.9, bsz=445.9, num_updates=7400, lr=0.000164399, gnorm=0.699, clip=0, loss_scale=1, train_wall=92, gb_free=16.2, wall=6643
2023-08-16 05:37:40 | INFO | train_inner | epoch 006:    139 / 1474 loss=2.522, trans_loss=3.583, nll_loss=1.816, w2v_ctc_loss=1.409, task_loss=2.157, contrastive_loss=0.406, total=4157.02, n_correct=2297.81, ppl=3.52, accuracy=55.275, wps=13501.2, ups=1.09, wpb=12417.6, bsz=457.6, num_updates=7500, lr=0.000163299, gnorm=0.67, clip=0, loss_scale=1, train_wall=91, gb_free=14.1, wall=6735
2023-08-16 05:39:12 | INFO | train_inner | epoch 006:    239 / 1474 loss=2.52, trans_loss=3.59, nll_loss=1.826, w2v_ctc_loss=1.433, task_loss=2.291, contrastive_loss=0.33, total=4120.34, n_correct=2270.11, ppl=3.55, accuracy=55.095, wps=13439, ups=1.09, wpb=12308.9, bsz=444.8, num_updates=7600, lr=0.000162221, gnorm=0.657, clip=0, loss_scale=1, train_wall=91, gb_free=17.3, wall=6827
2023-08-16 05:40:46 | INFO | train_inner | epoch 006:    339 / 1474 loss=2.543, trans_loss=3.579, nll_loss=1.81, w2v_ctc_loss=1.382, task_loss=2.086, contrastive_loss=0.605, total=4160.74, n_correct=2312.01, ppl=3.51, accuracy=55.567, wps=13221.2, ups=1.06, wpb=12422.4, bsz=482.1, num_updates=7700, lr=0.000161165, gnorm=0.667, clip=0, loss_scale=1, train_wall=93, gb_free=15.9, wall=6921
2023-08-16 05:42:17 | INFO | train_inner | epoch 006:    439 / 1474 loss=2.471, trans_loss=3.57, nll_loss=1.799, w2v_ctc_loss=1.385, task_loss=2.089, contrastive_loss=0.321, total=4157.21, n_correct=2330.63, ppl=3.48, accuracy=56.062, wps=13612.3, ups=1.1, wpb=12413.9, bsz=470.2, num_updates=7800, lr=0.000160128, gnorm=0.647, clip=0, loss_scale=1, train_wall=91, gb_free=15.3, wall=7012
2023-08-16 05:43:49 | INFO | train_inner | epoch 006:    539 / 1474 loss=2.481, trans_loss=3.575, nll_loss=1.805, w2v_ctc_loss=1.406, task_loss=2.188, contrastive_loss=0.307, total=4168.09, n_correct=2333.24, ppl=3.49, accuracy=55.979, wps=13470.5, ups=1.08, wpb=12441.1, bsz=456, num_updates=7900, lr=0.000159111, gnorm=0.726, clip=0, loss_scale=1, train_wall=92, gb_free=15.9, wall=7104
2023-08-16 05:45:21 | INFO | train_inner | epoch 006:    639 / 1474 loss=2.463, trans_loss=3.57, nll_loss=1.799, w2v_ctc_loss=1.376, task_loss=2.061, contrastive_loss=0.353, total=4152.97, n_correct=2331.37, ppl=3.48, accuracy=56.137, wps=13550.1, ups=1.09, wpb=12396.5, bsz=473.1, num_updates=8000, lr=0.000158114, gnorm=0.632, clip=0, loss_scale=1, train_wall=91, gb_free=15.1, wall=7196
2023-08-16 05:45:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 05:45:43 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.387 | trans_loss 5.562 | nll_loss 2.929 | w2v_ctc_loss 1.655 | task_loss 11.033 | contrastive_loss 0.468 | total 4003.4 | n_correct 2413.8 | ppl 7.61 | accuracy 60.294 | uer 24.864 | wer 26.304 | raw_wer 26.304 | bleu 17.23 | wps 2345.7 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 17.23
2023-08-16 05:45:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-16 05:45:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_6_8000.pt
2023-08-16 05:45:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_6_8000.pt
2023-08-16 05:46:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 17.23) (writing took 56.69494282454252 seconds)
2023-08-16 05:48:13 | INFO | train_inner | epoch 006:    739 / 1474 loss=2.455, trans_loss=3.569, nll_loss=1.798, w2v_ctc_loss=1.388, task_loss=2.243, contrastive_loss=0.304, total=4136.55, n_correct=2330.97, ppl=3.48, accuracy=56.351, wps=7180.9, ups=0.58, wpb=12349.7, bsz=450.8, num_updates=8100, lr=0.000157135, gnorm=0.635, clip=0, loss_scale=1, train_wall=92, gb_free=16.5, wall=7368
2023-08-16 05:49:45 | INFO | train_inner | epoch 006:    839 / 1474 loss=2.436, trans_loss=3.567, nll_loss=1.796, w2v_ctc_loss=1.373, task_loss=2.256, contrastive_loss=0.281, total=4134.7, n_correct=2327.9, ppl=3.47, accuracy=56.302, wps=13395.9, ups=1.09, wpb=12344.1, bsz=447.8, num_updates=8200, lr=0.000156174, gnorm=0.621, clip=0, loss_scale=1, train_wall=92, gb_free=14.5, wall=7460
2023-08-16 05:51:18 | INFO | train_inner | epoch 006:    939 / 1474 loss=2.467, trans_loss=3.57, nll_loss=1.799, w2v_ctc_loss=1.382, task_loss=2.33, contrastive_loss=0.384, total=4074.92, n_correct=2291.72, ppl=3.48, accuracy=56.24, wps=13088.4, ups=1.08, wpb=12163, bsz=439.9, num_updates=8300, lr=0.00015523, gnorm=0.625, clip=0, loss_scale=1, train_wall=92, gb_free=16.3, wall=7553
2023-08-16 05:52:50 | INFO | train_inner | epoch 006:   1039 / 1474 loss=2.452, trans_loss=3.553, nll_loss=1.778, w2v_ctc_loss=1.351, task_loss=2.051, contrastive_loss=0.454, total=4167.38, n_correct=2368.63, ppl=3.43, accuracy=56.837, wps=13556, ups=1.09, wpb=12438.7, bsz=478.3, num_updates=8400, lr=0.000154303, gnorm=0.625, clip=0, loss_scale=1, train_wall=91, gb_free=16, wall=7645
2023-08-16 05:54:21 | INFO | train_inner | epoch 006:   1139 / 1474 loss=2.426, trans_loss=3.553, nll_loss=1.778, w2v_ctc_loss=1.371, task_loss=2.425, contrastive_loss=0.283, total=4066.48, n_correct=2304, ppl=3.43, accuracy=56.658, wps=13229.3, ups=1.09, wpb=12140, bsz=428.3, num_updates=8500, lr=0.000153393, gnorm=0.728, clip=1, loss_scale=1, train_wall=91, gb_free=15.6, wall=7737
2023-08-16 05:55:54 | INFO | train_inner | epoch 006:   1239 / 1474 loss=2.464, trans_loss=3.541, nll_loss=1.764, w2v_ctc_loss=1.339, task_loss=2.141, contrastive_loss=0.596, total=4143.59, n_correct=2369.46, ppl=3.4, accuracy=57.184, wps=13337.9, ups=1.08, wpb=12377.1, bsz=471.5, num_updates=8600, lr=0.000152499, gnorm=0.623, clip=0, loss_scale=1, train_wall=92, gb_free=16.9, wall=7829
2023-08-16 05:57:26 | INFO | train_inner | epoch 006:   1339 / 1474 loss=2.381, trans_loss=3.543, nll_loss=1.763, w2v_ctc_loss=1.336, task_loss=2.163, contrastive_loss=0.256, total=4125.75, n_correct=2367.12, ppl=3.39, accuracy=57.374, wps=13391.1, ups=1.09, wpb=12308.4, bsz=454.4, num_updates=8700, lr=0.00015162, gnorm=0.594, clip=0, loss_scale=1, train_wall=91, gb_free=15.9, wall=7921
2023-08-16 05:58:59 | INFO | train_inner | epoch 006:   1439 / 1474 loss=2.381, trans_loss=3.536, nll_loss=1.756, w2v_ctc_loss=1.335, task_loss=2.183, contrastive_loss=0.265, total=4194.06, n_correct=2410.87, ppl=3.38, accuracy=57.483, wps=13454, ups=1.07, wpb=12518.6, bsz=462, num_updates=8800, lr=0.000150756, gnorm=0.586, clip=0, loss_scale=1, train_wall=92, gb_free=16, wall=8014
2023-08-16 05:59:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 05:59:54 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.281 | trans_loss 5.47 | nll_loss 2.816 | w2v_ctc_loss 1.545 | task_loss 11.278 | contrastive_loss 0.435 | total 4003.4 | n_correct 2462.5 | ppl 7.04 | accuracy 61.51 | uer 23.667 | wer 25.323 | raw_wer 25.323 | bleu 18.2 | wps 2135.5 | wpb 4003.4 | bsz 141.8 | num_updates 8835 | best_bleu 18.2
2023-08-16 05:59:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8835 updates
2023-08-16 05:59:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 06:00:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 06:00:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 6 @ 8835 updates, score 18.2) (writing took 33.97555558755994 seconds)
2023-08-16 06:00:28 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-16 06:00:28 | INFO | train | epoch 006 | loss 2.461 | trans_loss 3.564 | nll_loss 1.792 | w2v_ctc_loss 1.376 | task_loss 2.184 | contrastive_loss 0.366 | total 4138.65 | n_correct 2331.09 | ppl 3.46 | accuracy 56.325 | wps 12107.9 | ups 0.98 | wpb 12355.8 | bsz 458.5 | num_updates 8835 | lr 0.000150457 | gnorm 0.644 | clip 0.1 | loss_scale 1 | train_wall 1350 | gb_free 14.8 | wall 8103
2023-08-16 06:00:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 06:00:29 | INFO | fairseq.trainer | begin training epoch 7
2023-08-16 06:00:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 06:01:36 | INFO | train_inner | epoch 007:     65 / 1474 loss=2.351, trans_loss=3.522, nll_loss=1.738, w2v_ctc_loss=1.305, task_loss=2.143, contrastive_loss=0.278, total=4103.49, n_correct=2378.85, ppl=3.34, accuracy=57.971, wps=7800.4, ups=0.64, wpb=12251.2, bsz=461.5, num_updates=8900, lr=0.000149906, gnorm=0.655, clip=0, loss_scale=2, train_wall=91, gb_free=16.7, wall=8171
2023-08-16 06:03:08 | INFO | train_inner | epoch 007:    165 / 1474 loss=2.351, trans_loss=3.514, nll_loss=1.728, w2v_ctc_loss=1.292, task_loss=2.208, contrastive_loss=0.347, total=4110.42, n_correct=2391.03, ppl=3.31, accuracy=58.17, wps=13385.7, ups=1.09, wpb=12272.5, bsz=455.4, num_updates=9000, lr=0.000149071, gnorm=0.606, clip=0, loss_scale=2, train_wall=91, gb_free=16.1, wall=8263
2023-08-16 06:04:40 | INFO | train_inner | epoch 007:    265 / 1474 loss=2.322, trans_loss=3.507, nll_loss=1.717, w2v_ctc_loss=1.289, task_loss=2.202, contrastive_loss=0.25, total=4130.95, n_correct=2416.16, ppl=3.29, accuracy=58.489, wps=13386.3, ups=1.09, wpb=12328.4, bsz=454.4, num_updates=9100, lr=0.00014825, gnorm=0.578, clip=0, loss_scale=2, train_wall=92, gb_free=17.4, wall=8355
2023-08-16 06:06:13 | INFO | train_inner | epoch 007:    365 / 1474 loss=2.37, trans_loss=3.51, nll_loss=1.722, w2v_ctc_loss=1.278, task_loss=2.102, contrastive_loss=0.512, total=4204.16, n_correct=2453.89, ppl=3.3, accuracy=58.368, wps=13498.5, ups=1.08, wpb=12547.1, bsz=481.5, num_updates=9200, lr=0.000147442, gnorm=0.573, clip=0, loss_scale=2, train_wall=92, gb_free=16.9, wall=8448
2023-08-16 06:07:45 | INFO | train_inner | epoch 007:    465 / 1474 loss=2.348, trans_loss=3.509, nll_loss=1.722, w2v_ctc_loss=1.274, task_loss=2.191, contrastive_loss=0.428, total=4147.32, n_correct=2420.78, ppl=3.3, accuracy=58.37, wps=13494, ups=1.09, wpb=12384.6, bsz=458.5, num_updates=9300, lr=0.000146647, gnorm=0.598, clip=0, loss_scale=2, train_wall=91, gb_free=17.6, wall=8540
2023-08-16 06:09:17 | INFO | train_inner | epoch 007:    565 / 1474 loss=2.311, trans_loss=3.506, nll_loss=1.715, w2v_ctc_loss=1.277, task_loss=2.138, contrastive_loss=0.256, total=4171.72, n_correct=2445.89, ppl=3.28, accuracy=58.63, wps=13540.9, ups=1.09, wpb=12445.8, bsz=460.5, num_updates=9400, lr=0.000145865, gnorm=0.574, clip=0, loss_scale=2, train_wall=91, gb_free=15.5, wall=8632
2023-08-16 06:10:50 | INFO | train_inner | epoch 007:    665 / 1474 loss=2.301, trans_loss=3.503, nll_loss=1.712, w2v_ctc_loss=1.271, task_loss=2.18, contrastive_loss=0.241, total=4150.49, n_correct=2441.12, ppl=3.28, accuracy=58.815, wps=13352.5, ups=1.08, wpb=12385.7, bsz=454.7, num_updates=9500, lr=0.000145095, gnorm=0.585, clip=0, loss_scale=2, train_wall=92, gb_free=16.1, wall=8725
2023-08-16 06:12:22 | INFO | train_inner | epoch 007:    765 / 1474 loss=2.293, trans_loss=3.493, nll_loss=1.701, w2v_ctc_loss=1.267, task_loss=2.265, contrastive_loss=0.238, total=4132.17, n_correct=2432.26, ppl=3.25, accuracy=58.862, wps=13373.6, ups=1.08, wpb=12338.5, bsz=450.8, num_updates=9600, lr=0.000144338, gnorm=0.572, clip=0, loss_scale=2, train_wall=92, gb_free=15.9, wall=8817
2023-08-16 06:13:54 | INFO | train_inner | epoch 007:    865 / 1474 loss=2.295, trans_loss=3.504, nll_loss=1.714, w2v_ctc_loss=1.264, task_loss=2.214, contrastive_loss=0.251, total=4140.18, n_correct=2431.68, ppl=3.28, accuracy=58.734, wps=13331.8, ups=1.08, wpb=12352.6, bsz=457.8, num_updates=9700, lr=0.000143592, gnorm=0.614, clip=0, loss_scale=2, train_wall=92, gb_free=15.3, wall=8910
2023-08-16 06:15:27 | INFO | train_inner | epoch 007:    965 / 1474 loss=2.299, trans_loss=3.489, nll_loss=1.697, w2v_ctc_loss=1.247, task_loss=2.086, contrastive_loss=0.346, total=4144.65, n_correct=2453.13, ppl=3.24, accuracy=59.188, wps=13397.3, ups=1.08, wpb=12374.9, bsz=475.4, num_updates=9800, lr=0.000142857, gnorm=0.564, clip=0, loss_scale=2, train_wall=92, gb_free=14.8, wall=9002
2023-08-16 06:16:59 | INFO | train_inner | epoch 007:   1065 / 1474 loss=2.279, trans_loss=3.499, nll_loss=1.71, w2v_ctc_loss=1.264, task_loss=2.301, contrastive_loss=0.213, total=4097.24, n_correct=2414.96, ppl=3.27, accuracy=58.941, wps=13334.8, ups=1.09, wpb=12232.2, bsz=435.9, num_updates=9900, lr=0.000142134, gnorm=0.554, clip=0, loss_scale=2, train_wall=91, gb_free=15.9, wall=9094
2023-08-16 06:18:31 | INFO | train_inner | epoch 007:   1165 / 1474 loss=2.327, trans_loss=3.483, nll_loss=1.692, w2v_ctc_loss=1.246, task_loss=2.12, contrastive_loss=0.479, total=4142.16, n_correct=2454.81, ppl=3.23, accuracy=59.264, wps=13370.7, ups=1.08, wpb=12377.5, bsz=471.9, num_updates=10000, lr=0.000141421, gnorm=0.567, clip=0, loss_scale=2, train_wall=92, gb_free=15, wall=9186
2023-08-16 06:18:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 06:18:55 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.197 | trans_loss 5.39 | nll_loss 2.718 | w2v_ctc_loss 1.476 | task_loss 11.298 | contrastive_loss 0.409 | total 4003.4 | n_correct 2517.6 | ppl 6.58 | accuracy 62.887 | uer 22.013 | wer 23.791 | raw_wer 23.791 | bleu 19.22 | wps 2183.2 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 19.22
2023-08-16 06:18:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-16 06:18:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_7_10000.pt
2023-08-16 06:18:58 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_7_10000.pt
2023-08-16 06:19:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 19.22) (writing took 32.033928679302335 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:0')
2023-08-16 06:20:58 | INFO | train_inner | epoch 007:   1265 / 1474 loss=2.266, trans_loss=3.485, nll_loss=1.693, w2v_ctc_loss=1.245, task_loss=2.241, contrastive_loss=0.241, total=4119.52, n_correct=2443.19, ppl=3.23, accuracy=59.308, wps=8369.3, ups=0.68, wpb=12302.1, bsz=446.6, num_updates=10100, lr=0.00014072, gnorm=0.439, clip=0, loss_scale=2, train_wall=91, gb_free=13.5, wall=9333
2023-08-16 06:22:30 | INFO | train_inner | epoch 007:   1365 / 1474 loss=2.282, trans_loss=3.48, nll_loss=1.686, w2v_ctc_loss=1.251, task_loss=2.03, contrastive_loss=0.279, total=4185.65, n_correct=2491.64, ppl=3.22, accuracy=59.528, wps=13633.5, ups=1.09, wpb=12496.2, bsz=480.7, num_updates=10200, lr=0.000140028, gnorm=0.463, clip=0, loss_scale=2, train_wall=91, gb_free=16.7, wall=9425
2023-08-16 06:24:04 | INFO | train_inner | epoch 007:   1465 / 1474 loss=2.285, trans_loss=3.482, nll_loss=1.69, w2v_ctc_loss=1.246, task_loss=2.34, contrastive_loss=0.339, total=4113.9, n_correct=2445.49, ppl=3.23, accuracy=59.445, wps=13042.1, ups=1.06, wpb=12290.7, bsz=446.9, num_updates=10300, lr=0.000139347, gnorm=0.446, clip=0, loss_scale=2, train_wall=94, gb_free=16.3, wall=9519
2023-08-16 06:24:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:1')
2023-08-16 06:24:36 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.169 | trans_loss 5.368 | nll_loss 2.686 | w2v_ctc_loss 1.449 | task_loss 11.298 | contrastive_loss 0.392 | total 4003.4 | n_correct 2528.1 | ppl 6.44 | accuracy 63.149 | uer 21.639 | wer 23.422 | raw_wer 23.422 | bleu 19.47 | wps 2106.7 | wpb 4003.4 | bsz 141.8 | num_updates 10309 | best_bleu 19.47
2023-08-16 06:24:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10309 updates
2023-08-16 06:24:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 06:24:52 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 06:25:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 7 @ 10309 updates, score 19.47) (writing took 32.062112007290125 seconds)
2023-08-16 06:25:08 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-16 06:25:08 | INFO | train | epoch 007 | loss 2.31 | trans_loss 3.498 | nll_loss 1.708 | w2v_ctc_loss 1.267 | task_loss 2.189 | contrastive_loss 0.315 | total 4138.65 | n_correct 2434.83 | ppl 3.27 | accuracy 58.832 | wps 12307.9 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 10309 | lr 0.000139286 | gnorm 0.558 | clip 0 | loss_scale 2 | train_wall 1351 | gb_free 12.8 | wall 9583
2023-08-16 06:25:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 06:25:08 | INFO | fairseq.trainer | begin training epoch 8
2023-08-16 06:25:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 06:26:40 | INFO | train_inner | epoch 008:     91 / 1474 loss=2.234, trans_loss=3.472, nll_loss=1.671, w2v_ctc_loss=1.217, task_loss=2.358, contrastive_loss=0.231, total=4095.89, n_correct=2456.19, ppl=3.18, accuracy=59.967, wps=7850.9, ups=0.64, wpb=12213.1, bsz=435.6, num_updates=10400, lr=0.000138675, gnorm=0.458, clip=0, loss_scale=2, train_wall=91, gb_free=17.5, wall=9675
2023-08-16 06:28:11 | INFO | train_inner | epoch 008:    191 / 1474 loss=2.229, trans_loss=3.462, nll_loss=1.658, w2v_ctc_loss=1.206, task_loss=2.374, contrastive_loss=0.254, total=4040.13, n_correct=2433.17, ppl=3.16, accuracy=60.225, wps=13160.3, ups=1.09, wpb=12049.1, bsz=429, num_updates=10500, lr=0.000138013, gnorm=0.448, clip=0, loss_scale=2, train_wall=91, gb_free=15.8, wall=9766
2023-08-16 06:29:44 | INFO | train_inner | epoch 008:    291 / 1474 loss=2.226, trans_loss=3.455, nll_loss=1.652, w2v_ctc_loss=1.207, task_loss=2.048, contrastive_loss=0.25, total=4216.54, n_correct=2547.48, ppl=3.14, accuracy=60.416, wps=13584.7, ups=1.08, wpb=12583.3, bsz=489.6, num_updates=10600, lr=0.000137361, gnorm=0.439, clip=0, loss_scale=2, train_wall=92, gb_free=15.8, wall=9859
2023-08-16 06:31:17 | INFO | train_inner | epoch 008:    391 / 1474 loss=2.236, trans_loss=3.459, nll_loss=1.656, w2v_ctc_loss=1.215, task_loss=2.298, contrastive_loss=0.272, total=4134.8, n_correct=2491.05, ppl=3.15, accuracy=60.246, wps=13230.4, ups=1.07, wpb=12337.2, bsz=446.1, num_updates=10700, lr=0.000136717, gnorm=0.434, clip=0, loss_scale=2, train_wall=93, gb_free=16.1, wall=9952
2023-08-16 06:32:50 | INFO | train_inner | epoch 008:    491 / 1474 loss=2.289, trans_loss=3.455, nll_loss=1.654, w2v_ctc_loss=1.193, task_loss=1.982, contrastive_loss=0.538, total=4193.98, n_correct=2529.53, ppl=3.15, accuracy=60.313, wps=13506.3, ups=1.08, wpb=12521.7, bsz=500.7, num_updates=10800, lr=0.000136083, gnorm=0.445, clip=0, loss_scale=2, train_wall=92, gb_free=17.5, wall=10045
2023-08-16 06:34:22 | INFO | train_inner | epoch 008:    591 / 1474 loss=2.221, trans_loss=3.454, nll_loss=1.655, w2v_ctc_loss=1.219, task_loss=2.402, contrastive_loss=0.202, total=4063.58, n_correct=2445.19, ppl=3.15, accuracy=60.173, wps=13157.7, ups=1.08, wpb=12147.8, bsz=426.8, num_updates=10900, lr=0.000135457, gnorm=0.441, clip=0, loss_scale=2, train_wall=92, gb_free=12.1, wall=10137
2023-08-16 06:35:55 | INFO | train_inner | epoch 008:    691 / 1474 loss=2.21, trans_loss=3.448, nll_loss=1.644, w2v_ctc_loss=1.208, task_loss=2.244, contrastive_loss=0.216, total=4138.77, n_correct=2513.54, ppl=3.13, accuracy=60.732, wps=13352.8, ups=1.08, wpb=12354.5, bsz=450.4, num_updates=11000, lr=0.00013484, gnorm=0.43, clip=0, loss_scale=4, train_wall=92, gb_free=16.3, wall=10230
2023-08-16 06:37:27 | INFO | train_inner | epoch 008:    791 / 1474 loss=2.221, trans_loss=3.445, nll_loss=1.643, w2v_ctc_loss=1.198, task_loss=2.239, contrastive_loss=0.302, total=4122.32, n_correct=2496.78, ppl=3.12, accuracy=60.567, wps=13364.2, ups=1.08, wpb=12319.9, bsz=449, num_updates=11100, lr=0.000134231, gnorm=0.455, clip=0, loss_scale=4, train_wall=92, gb_free=17.1, wall=10322
2023-08-16 06:38:59 | INFO | train_inner | epoch 008:    891 / 1474 loss=2.221, trans_loss=3.449, nll_loss=1.647, w2v_ctc_loss=1.185, task_loss=2.083, contrastive_loss=0.313, total=4180.85, n_correct=2536.45, ppl=3.13, accuracy=60.668, wps=13518.2, ups=1.08, wpb=12485.3, bsz=477, num_updates=11200, lr=0.000133631, gnorm=0.439, clip=0, loss_scale=4, train_wall=92, gb_free=15.4, wall=10414
2023-08-16 06:40:31 | INFO | train_inner | epoch 008:    991 / 1474 loss=2.187, trans_loss=3.445, nll_loss=1.641, w2v_ctc_loss=1.185, task_loss=2.122, contrastive_loss=0.205, total=4145.35, n_correct=2525.07, ppl=3.12, accuracy=60.913, wps=13554.6, ups=1.1, wpb=12377.4, bsz=460.4, num_updates=11300, lr=0.000133038, gnorm=0.417, clip=0, loss_scale=4, train_wall=91, gb_free=16.3, wall=10506
2023-08-16 06:42:03 | INFO | train_inner | epoch 008:   1091 / 1474 loss=2.234, trans_loss=3.448, nll_loss=1.644, w2v_ctc_loss=1.185, task_loss=2.182, contrastive_loss=0.428, total=4191.42, n_correct=2545.53, ppl=3.13, accuracy=60.732, wps=13466.9, ups=1.08, wpb=12511.3, bsz=464.9, num_updates=11400, lr=0.000132453, gnorm=0.43, clip=0, loss_scale=4, train_wall=92, gb_free=17.3, wall=10598
2023-08-16 06:43:35 | INFO | train_inner | epoch 008:   1191 / 1474 loss=2.195, trans_loss=3.441, nll_loss=1.637, w2v_ctc_loss=1.19, task_loss=2.069, contrastive_loss=0.217, total=4187.13, n_correct=2546.74, ppl=3.11, accuracy=60.823, wps=13620.1, ups=1.09, wpb=12506.2, bsz=474, num_updates=11500, lr=0.000131876, gnorm=0.428, clip=0, loss_scale=4, train_wall=91, gb_free=12.8, wall=10690
2023-08-16 06:45:07 | INFO | train_inner | epoch 008:   1291 / 1474 loss=2.2, trans_loss=3.445, nll_loss=1.641, w2v_ctc_loss=1.196, task_loss=2.309, contrastive_loss=0.237, total=4055.06, n_correct=2458.72, ppl=3.12, accuracy=60.633, wps=13253, ups=1.09, wpb=12112.2, bsz=434.6, num_updates=11600, lr=0.000131306, gnorm=0.435, clip=0, loss_scale=4, train_wall=91, gb_free=16.2, wall=10782
2023-08-16 06:46:38 | INFO | train_inner | epoch 008:   1391 / 1474 loss=2.211, trans_loss=3.445, nll_loss=1.642, w2v_ctc_loss=1.182, task_loss=2.105, contrastive_loss=0.307, total=4166, n_correct=2538.39, ppl=3.12, accuracy=60.931, wps=13583.9, ups=1.09, wpb=12439.6, bsz=471.7, num_updates=11700, lr=0.000130744, gnorm=0.443, clip=0, loss_scale=4, train_wall=91, gb_free=16.5, wall=10873
2023-08-16 06:47:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 06:48:18 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.111 | trans_loss 5.301 | nll_loss 2.601 | w2v_ctc_loss 1.421 | task_loss 11.414 | contrastive_loss 0.378 | total 4003.4 | n_correct 2567.9 | ppl 6.07 | accuracy 64.143 | uer 20.832 | wer 22.825 | raw_wer 22.825 | bleu 19.96 | wps 2117.4 | wpb 4003.4 | bsz 141.8 | num_updates 11783 | best_bleu 19.96
2023-08-16 06:48:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11783 updates
2023-08-16 06:48:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 06:48:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 06:48:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 8 @ 11783 updates, score 19.96) (writing took 30.374866975471377 seconds)
2023-08-16 06:48:49 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-16 06:48:49 | INFO | train | epoch 008 | loss 2.221 | trans_loss 3.451 | nll_loss 1.648 | w2v_ctc_loss 1.197 | task_loss 2.191 | contrastive_loss 0.289 | total 4138.65 | n_correct 2506.19 | ppl 3.13 | accuracy 60.556 | wps 12819 | ups 1.04 | wpb 12355.8 | bsz 458.5 | num_updates 11783 | lr 0.000130283 | gnorm 0.438 | clip 0 | loss_scale 4 | train_wall 1348 | gb_free 16.6 | wall 11004
2023-08-16 06:48:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 06:48:49 | INFO | fairseq.trainer | begin training epoch 9
2023-08-16 06:48:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 06:49:13 | INFO | train_inner | epoch 009:     17 / 1474 loss=2.205, trans_loss=3.439, nll_loss=1.632, w2v_ctc_loss=1.166, task_loss=2.161, contrastive_loss=0.399, total=4113.19, n_correct=2512.79, ppl=3.1, accuracy=61.091, wps=7943.4, ups=0.65, wpb=12275.6, bsz=463.4, num_updates=11800, lr=0.000130189, gnorm=0.421, clip=0, loss_scale=4, train_wall=91, gb_free=17.5, wall=11028
2023-08-16 06:50:45 | INFO | train_inner | epoch 009:    117 / 1474 loss=2.151, trans_loss=3.413, nll_loss=1.601, w2v_ctc_loss=1.145, task_loss=2.053, contrastive_loss=0.233, total=4192.68, n_correct=2590.15, ppl=3.03, accuracy=61.778, wps=13628.8, ups=1.09, wpb=12520.8, bsz=481.1, num_updates=11900, lr=0.000129641, gnorm=0.423, clip=0, loss_scale=4, train_wall=91, gb_free=15.5, wall=11120
2023-08-16 06:52:17 | INFO | train_inner | epoch 009:    217 / 1474 loss=2.137, trans_loss=3.418, nll_loss=1.606, w2v_ctc_loss=1.145, task_loss=2.374, contrastive_loss=0.184, total=4065.4, n_correct=2510.43, ppl=3.04, accuracy=61.751, wps=13104.7, ups=1.08, wpb=12138.7, bsz=429.9, num_updates=12000, lr=0.000129099, gnorm=0.433, clip=0, loss_scale=4, train_wall=92, gb_free=15.7, wall=11212
2023-08-16 06:52:17 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 06:52:41 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.101 | trans_loss 5.304 | nll_loss 2.605 | w2v_ctc_loss 1.387 | task_loss 11.361 | contrastive_loss 0.372 | total 4003.4 | n_correct 2564.3 | ppl 6.08 | accuracy 64.053 | uer 21.044 | wer 23.116 | raw_wer 23.116 | bleu 20.03 | wps 2185 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 20.03
2023-08-16 06:52:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-16 06:52:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_9_12000.pt
2023-08-16 06:52:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_9_12000.pt
2023-08-16 06:53:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 20.03) (writing took 30.898704547435045 seconds)
2023-08-16 06:54:44 | INFO | train_inner | epoch 009:    317 / 1474 loss=2.137, trans_loss=3.406, nll_loss=1.592, w2v_ctc_loss=1.129, task_loss=2.041, contrastive_loss=0.241, total=4152.87, n_correct=2577.62, ppl=3.02, accuracy=62.068, wps=8456.2, ups=0.68, wpb=12408.5, bsz=479.2, num_updates=12100, lr=0.000128565, gnorm=0.43, clip=0, loss_scale=4, train_wall=91, gb_free=16.5, wall=11359
2023-08-16 06:56:17 | INFO | train_inner | epoch 009:    417 / 1474 loss=2.14, trans_loss=3.422, nll_loss=1.611, w2v_ctc_loss=1.14, task_loss=2.171, contrastive_loss=0.203, total=4191.41, n_correct=2580.64, ppl=3.05, accuracy=61.57, wps=13440.1, ups=1.07, wpb=12514.6, bsz=464.1, num_updates=12200, lr=0.000128037, gnorm=0.421, clip=0, loss_scale=4, train_wall=93, gb_free=13.7, wall=11452
2023-08-16 06:57:48 | INFO | train_inner | epoch 009:    517 / 1474 loss=2.167, trans_loss=3.42, nll_loss=1.608, w2v_ctc_loss=1.164, task_loss=2.292, contrastive_loss=0.252, total=4119.12, n_correct=2538.09, ppl=3.05, accuracy=61.617, wps=13489.4, ups=1.1, wpb=12295.1, bsz=438.8, num_updates=12300, lr=0.000127515, gnorm=0.433, clip=0, loss_scale=4, train_wall=90, gb_free=16.4, wall=11543
2023-08-16 06:59:20 | INFO | train_inner | epoch 009:    617 / 1474 loss=2.153, trans_loss=3.413, nll_loss=1.602, w2v_ctc_loss=1.133, task_loss=2.195, contrastive_loss=0.307, total=4140.76, n_correct=2557.19, ppl=3.04, accuracy=61.757, wps=13458.3, ups=1.09, wpb=12374.8, bsz=463.2, num_updates=12400, lr=0.000127, gnorm=0.434, clip=0, loss_scale=4, train_wall=91, gb_free=15.2, wall=11635
2023-08-16 07:00:51 | INFO | train_inner | epoch 009:    717 / 1474 loss=2.143, trans_loss=3.418, nll_loss=1.608, w2v_ctc_loss=1.154, task_loss=2.272, contrastive_loss=0.198, total=4075.27, n_correct=2513.68, ppl=3.05, accuracy=61.681, wps=13364.6, ups=1.1, wpb=12176.3, bsz=443.7, num_updates=12500, lr=0.000126491, gnorm=0.432, clip=0, loss_scale=4, train_wall=91, gb_free=16.7, wall=11726
2023-08-16 07:02:25 | INFO | train_inner | epoch 009:    817 / 1474 loss=2.206, trans_loss=3.413, nll_loss=1.603, w2v_ctc_loss=1.146, task_loss=1.988, contrastive_loss=0.446, total=4215.48, n_correct=2602.96, ppl=3.04, accuracy=61.748, wps=13478.7, ups=1.07, wpb=12596.2, bsz=499.7, num_updates=12600, lr=0.000125988, gnorm=0.433, clip=0, loss_scale=4, train_wall=93, gb_free=16.6, wall=11820
2023-08-16 07:03:58 | INFO | train_inner | epoch 009:    917 / 1474 loss=2.174, trans_loss=3.416, nll_loss=1.602, w2v_ctc_loss=1.139, task_loss=2.258, contrastive_loss=0.416, total=4152.4, n_correct=2567.31, ppl=3.03, accuracy=61.827, wps=13302.7, ups=1.07, wpb=12388.2, bsz=450.8, num_updates=12700, lr=0.000125491, gnorm=0.419, clip=0, loss_scale=4, train_wall=93, gb_free=11.5, wall=11913
2023-08-16 07:05:30 | INFO | train_inner | epoch 009:   1017 / 1474 loss=2.138, trans_loss=3.422, nll_loss=1.611, w2v_ctc_loss=1.146, task_loss=2.444, contrastive_loss=0.199, total=4101.32, n_correct=2527.84, ppl=3.05, accuracy=61.635, wps=13305.9, ups=1.09, wpb=12242.4, bsz=424.9, num_updates=12800, lr=0.000125, gnorm=0.428, clip=0, loss_scale=4, train_wall=91, gb_free=15.5, wall=12005
2023-08-16 07:07:02 | INFO | train_inner | epoch 009:   1117 / 1474 loss=2.137, trans_loss=3.419, nll_loss=1.604, w2v_ctc_loss=1.137, task_loss=2.08, contrastive_loss=0.22, total=4172.83, n_correct=2586.7, ppl=3.04, accuracy=61.989, wps=13584.5, ups=1.09, wpb=12437.9, bsz=471.9, num_updates=12900, lr=0.000124515, gnorm=0.438, clip=0, loss_scale=4, train_wall=91, gb_free=16.1, wall=12097
2023-08-16 07:08:35 | INFO | train_inner | epoch 009:   1217 / 1474 loss=2.142, trans_loss=3.417, nll_loss=1.606, w2v_ctc_loss=1.153, task_loss=2.311, contrastive_loss=0.204, total=4138.15, n_correct=2553.81, ppl=3.04, accuracy=61.714, wps=13279.9, ups=1.07, wpb=12357.2, bsz=448.8, num_updates=13000, lr=0.000124035, gnorm=0.447, clip=0, loss_scale=8, train_wall=92, gb_free=15.8, wall=12190
2023-08-16 07:10:07 | INFO | train_inner | epoch 009:   1317 / 1474 loss=2.164, trans_loss=3.409, nll_loss=1.594, w2v_ctc_loss=1.128, task_loss=2.005, contrastive_loss=0.391, total=4205.27, n_correct=2613.04, ppl=3.02, accuracy=62.137, wps=13636.1, ups=1.09, wpb=12548.1, bsz=491.2, num_updates=13100, lr=0.00012356, gnorm=0.419, clip=0, loss_scale=8, train_wall=91, gb_free=16.4, wall=12282
2023-08-16 07:11:37 | INFO | train_inner | epoch 009:   1417 / 1474 loss=2.127, trans_loss=3.423, nll_loss=1.611, w2v_ctc_loss=1.141, task_loss=2.36, contrastive_loss=0.176, total=4071.37, n_correct=2515.56, ppl=3.06, accuracy=61.787, wps=13376.9, ups=1.1, wpb=12147.5, bsz=429, num_updates=13200, lr=0.000123091, gnorm=0.422, clip=0, loss_scale=8, train_wall=90, gb_free=14.5, wall=12372
2023-08-16 07:12:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 07:12:51 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.064 | trans_loss 5.27 | nll_loss 2.569 | w2v_ctc_loss 1.354 | task_loss 11.46 | contrastive_loss 0.366 | total 4003.4 | n_correct 2590.1 | ppl 5.94 | accuracy 64.698 | uer 20.012 | wer 21.752 | raw_wer 21.752 | bleu 20.17 | wps 2278 | wpb 4003.4 | bsz 141.8 | num_updates 13257 | best_bleu 20.17
2023-08-16 07:12:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13257 updates
2023-08-16 07:12:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 07:13:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 07:13:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 9 @ 13257 updates, score 20.17) (writing took 31.83903350867331 seconds)
2023-08-16 07:13:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-16 07:13:24 | INFO | train | epoch 009 | loss 2.151 | trans_loss 3.416 | nll_loss 1.604 | w2v_ctc_loss 1.143 | task_loss 2.193 | contrastive_loss 0.268 | total 4138.65 | n_correct 2557.81 | ppl 3.04 | accuracy 61.803 | wps 12346.2 | ups 1 | wpb 12355.8 | bsz 458.5 | num_updates 13257 | lr 0.000122827 | gnorm 0.429 | clip 0 | loss_scale 8 | train_wall 1348 | gb_free 11.2 | wall 12479
2023-08-16 07:13:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 07:13:24 | INFO | fairseq.trainer | begin training epoch 10
2023-08-16 07:13:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 07:14:12 | INFO | train_inner | epoch 010:     43 / 1474 loss=2.126, trans_loss=3.404, nll_loss=1.588, w2v_ctc_loss=1.116, task_loss=2.051, contrastive_loss=0.274, total=4113.02, n_correct=2564.98, ppl=3.01, accuracy=62.362, wps=7963.6, ups=0.65, wpb=12276.4, bsz=475.9, num_updates=13300, lr=0.000122628, gnorm=0.423, clip=0, loss_scale=8, train_wall=90, gb_free=16.1, wall=12527
2023-08-16 07:15:44 | INFO | train_inner | epoch 010:    143 / 1474 loss=2.08, trans_loss=3.388, nll_loss=1.568, w2v_ctc_loss=1.09, task_loss=2.112, contrastive_loss=0.195, total=4234.99, n_correct=2660.12, ppl=2.97, accuracy=62.813, wps=13710.8, ups=1.08, wpb=12647.2, bsz=473.2, num_updates=13400, lr=0.000122169, gnorm=0.408, clip=0, loss_scale=8, train_wall=92, gb_free=16.1, wall=12619
2023-08-16 07:17:16 | INFO | train_inner | epoch 010:    243 / 1474 loss=2.107, trans_loss=3.384, nll_loss=1.561, w2v_ctc_loss=1.095, task_loss=2.154, contrastive_loss=0.316, total=4131.11, n_correct=2596.28, ppl=2.95, accuracy=62.847, wps=13429.5, ups=1.09, wpb=12328.7, bsz=463.9, num_updates=13500, lr=0.000121716, gnorm=0.414, clip=0, loss_scale=8, train_wall=91, gb_free=15.9, wall=12711
2023-08-16 07:18:48 | INFO | train_inner | epoch 010:    343 / 1474 loss=2.086, trans_loss=3.385, nll_loss=1.567, w2v_ctc_loss=1.09, task_loss=2.218, contrastive_loss=0.229, total=4135.65, n_correct=2593.88, ppl=2.96, accuracy=62.72, wps=13417.3, ups=1.09, wpb=12362.4, bsz=454, num_updates=13600, lr=0.000121268, gnorm=0.416, clip=0, loss_scale=8, train_wall=91, gb_free=15.4, wall=12803
2023-08-16 07:20:21 | INFO | train_inner | epoch 010:    443 / 1474 loss=2.11, trans_loss=3.388, nll_loss=1.568, w2v_ctc_loss=1.077, task_loss=2.103, contrastive_loss=0.4, total=4199.14, n_correct=2635.07, ppl=2.97, accuracy=62.753, wps=13491.9, ups=1.08, wpb=12535.9, bsz=482.3, num_updates=13700, lr=0.000120824, gnorm=0.416, clip=0, loss_scale=8, train_wall=92, gb_free=16.6, wall=12896
2023-08-16 07:21:53 | INFO | train_inner | epoch 010:    543 / 1474 loss=2.095, trans_loss=3.399, nll_loss=1.579, w2v_ctc_loss=1.115, task_loss=2.393, contrastive_loss=0.181, total=4094.23, n_correct=2557.05, ppl=2.99, accuracy=62.455, wps=13183.1, ups=1.08, wpb=12209.6, bsz=433.2, num_updates=13800, lr=0.000120386, gnorm=0.418, clip=0, loss_scale=8, train_wall=92, gb_free=14, wall=12988
2023-08-16 07:23:26 | INFO | train_inner | epoch 010:    643 / 1474 loss=2.113, trans_loss=3.393, nll_loss=1.573, w2v_ctc_loss=1.101, task_loss=2.055, contrastive_loss=0.301, total=4182.84, n_correct=2622.55, ppl=2.98, accuracy=62.698, wps=13444.7, ups=1.08, wpb=12481.2, bsz=481.3, num_updates=13900, lr=0.000119952, gnorm=0.418, clip=0, loss_scale=8, train_wall=92, gb_free=16.4, wall=13081
2023-08-16 07:24:57 | INFO | train_inner | epoch 010:    743 / 1474 loss=2.095, trans_loss=3.391, nll_loss=1.572, w2v_ctc_loss=1.117, task_loss=2.208, contrastive_loss=0.179, total=4120.62, n_correct=2582.11, ppl=2.97, accuracy=62.663, wps=13519, ups=1.1, wpb=12301.2, bsz=451.7, num_updates=14000, lr=0.000119523, gnorm=0.423, clip=0, loss_scale=8, train_wall=90, gb_free=16.6, wall=13172
2023-08-16 07:24:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 07:25:20 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.075 | trans_loss 5.256 | nll_loss 2.544 | w2v_ctc_loss 1.426 | task_loss 11.506 | contrastive_loss 0.363 | total 4003.4 | n_correct 2601.4 | ppl 5.83 | accuracy 64.98 | uer 20.301 | wer 22.061 | raw_wer 22.061 | bleu 20.79 | wps 2202.1 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 20.79
2023-08-16 07:25:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-16 07:25:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_10_14000.pt
2023-08-16 07:25:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_10_14000.pt
2023-08-16 07:26:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 20.79) (writing took 54.28392775915563 seconds)
2023-08-16 07:27:49 | INFO | train_inner | epoch 010:    843 / 1474 loss=2.074, trans_loss=3.389, nll_loss=1.57, w2v_ctc_loss=1.092, task_loss=2.16, contrastive_loss=0.182, total=4132.62, n_correct=2592.96, ppl=2.97, accuracy=62.744, wps=7193.8, ups=0.58, wpb=12339.2, bsz=457.4, num_updates=14100, lr=0.000119098, gnorm=0.409, clip=0, loss_scale=8, train_wall=92, gb_free=16.4, wall=13344
2023-08-16 07:29:20 | INFO | train_inner | epoch 010:    943 / 1474 loss=2.09, trans_loss=3.389, nll_loss=1.567, w2v_ctc_loss=1.099, task_loss=2.108, contrastive_loss=0.22, total=4160.84, n_correct=2611.25, ppl=2.96, accuracy=62.758, wps=13572.1, ups=1.09, wpb=12411.3, bsz=467.9, num_updates=14200, lr=0.000118678, gnorm=0.416, clip=0, loss_scale=8, train_wall=90, gb_free=15.5, wall=13435
2023-08-16 07:30:53 | INFO | train_inner | epoch 010:   1043 / 1474 loss=2.083, trans_loss=3.389, nll_loss=1.57, w2v_ctc_loss=1.101, task_loss=2.399, contrastive_loss=0.193, total=4059.22, n_correct=2543.46, ppl=2.97, accuracy=62.659, wps=13116.7, ups=1.08, wpb=12120, bsz=431.2, num_updates=14300, lr=0.000118262, gnorm=0.428, clip=0, loss_scale=8, train_wall=91, gb_free=9.1, wall=13528
2023-08-16 07:32:24 | INFO | train_inner | epoch 010:   1143 / 1474 loss=2.087, trans_loss=3.396, nll_loss=1.578, w2v_ctc_loss=1.111, task_loss=2.44, contrastive_loss=0.173, total=4045.82, n_correct=2526.99, ppl=2.99, accuracy=62.459, wps=13226.2, ups=1.09, wpb=12079.3, bsz=422.8, num_updates=14400, lr=0.000117851, gnorm=0.417, clip=0, loss_scale=8, train_wall=91, gb_free=12.5, wall=13619
2023-08-16 07:33:56 | INFO | train_inner | epoch 010:   1243 / 1474 loss=2.081, trans_loss=3.381, nll_loss=1.563, w2v_ctc_loss=1.109, task_loss=2.239, contrastive_loss=0.171, total=4107.6, n_correct=2579.92, ppl=2.96, accuracy=62.808, wps=13300.5, ups=1.08, wpb=12284.6, bsz=446.5, num_updates=14500, lr=0.000117444, gnorm=0.453, clip=0, loss_scale=8, train_wall=92, gb_free=16.3, wall=13711
2023-08-16 07:35:28 | INFO | train_inner | epoch 010:   1343 / 1474 loss=2.08, trans_loss=3.389, nll_loss=1.57, w2v_ctc_loss=1.103, task_loss=2.238, contrastive_loss=0.182, total=4127.69, n_correct=2591.18, ppl=2.97, accuracy=62.776, wps=13393.8, ups=1.09, wpb=12326.4, bsz=452.2, num_updates=14600, lr=0.000117041, gnorm=0.426, clip=0, loss_scale=8, train_wall=91, gb_free=15.8, wall=13803
2023-08-16 07:37:01 | INFO | train_inner | epoch 010:   1443 / 1474 loss=2.132, trans_loss=3.396, nll_loss=1.577, w2v_ctc_loss=1.079, task_loss=2.065, contrastive_loss=0.432, total=4195.02, n_correct=2628.94, ppl=2.98, accuracy=62.668, wps=13480, ups=1.08, wpb=12514.1, bsz=483, num_updates=14700, lr=0.000116642, gnorm=0.411, clip=0, loss_scale=8, train_wall=92, gb_free=16.1, wall=13896
2023-08-16 07:37:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 07:37:53 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.043 | trans_loss 5.243 | nll_loss 2.531 | w2v_ctc_loss 1.365 | task_loss 11.5 | contrastive_loss 0.349 | total 4003.4 | n_correct 2603.5 | ppl 5.78 | accuracy 65.032 | uer 19.133 | wer 20.782 | raw_wer 20.782 | bleu 21.28 | wps 2096 | wpb 4003.4 | bsz 141.8 | num_updates 14731 | best_bleu 21.28
2023-08-16 07:37:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14731 updates
2023-08-16 07:37:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 07:38:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 07:38:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 10 @ 14731 updates, score 21.28) (writing took 34.666544331237674 seconds)
2023-08-16 07:38:29 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-16 07:38:29 | INFO | train | epoch 010 | loss 2.095 | trans_loss 3.39 | nll_loss 1.57 | w2v_ctc_loss 1.097 | task_loss 2.194 | contrastive_loss 0.251 | total 4138.65 | n_correct 2595.3 | ppl 2.97 | accuracy 62.709 | wps 12104 | ups 0.98 | wpb 12355.8 | bsz 458.5 | num_updates 14731 | lr 0.00011652 | gnorm 0.419 | clip 0 | loss_scale 8 | train_wall 1348 | gb_free 17 | wall 13984
2023-08-16 07:38:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 07:38:29 | INFO | fairseq.trainer | begin training epoch 11
2023-08-16 07:38:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 07:39:39 | INFO | train_inner | epoch 011:     69 / 1474 loss=2.062, trans_loss=3.368, nll_loss=1.542, w2v_ctc_loss=1.068, task_loss=2.051, contrastive_loss=0.255, total=4166, n_correct=2642.15, ppl=2.91, accuracy=63.422, wps=7884.9, ups=0.63, wpb=12436.1, bsz=475.7, num_updates=14800, lr=0.000116248, gnorm=0.405, clip=0, loss_scale=8, train_wall=90, gb_free=17.6, wall=14054
2023-08-16 07:41:12 | INFO | train_inner | epoch 011:    169 / 1474 loss=2.044, trans_loss=3.369, nll_loss=1.545, w2v_ctc_loss=1.066, task_loss=2.25, contrastive_loss=0.178, total=4100.74, n_correct=2595.89, ppl=2.92, accuracy=63.303, wps=13172.3, ups=1.08, wpb=12251.1, bsz=450.6, num_updates=14900, lr=0.000115857, gnorm=0.424, clip=0, loss_scale=8, train_wall=92, gb_free=14.1, wall=14147
2023-08-16 07:42:44 | INFO | train_inner | epoch 011:    269 / 1474 loss=2.033, trans_loss=3.367, nll_loss=1.541, w2v_ctc_loss=1.062, task_loss=2.267, contrastive_loss=0.162, total=4115.58, n_correct=2610.22, ppl=2.91, accuracy=63.423, wps=13366.5, ups=1.09, wpb=12290.6, bsz=444.2, num_updates=15000, lr=0.00011547, gnorm=0.425, clip=0, loss_scale=8, train_wall=91, gb_free=15.7, wall=14239
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:0')
2023-08-16 07:43:53 | INFO | train_inner | epoch 011:    369 / 1474 loss=2.087, trans_loss=5.005, nll_loss=2.293, w2v_ctc_loss=0.794, task_loss=3.38, contrastive_loss=0.13, total=4094.16, n_correct=2594.72, ppl=4.9, accuracy=63.376, wps=11945, ups=1.45, wpb=8227.1, bsz=296.5, num_updates=15100, lr=0.000115087, gnorm=0.546, clip=0, loss_scale=16, train_wall=68, gb_free=12.4, wall=14308
2023-08-16 07:45:02 | INFO | train_inner | epoch 011:    469 / 1474 loss=2.108, trans_loss=5.039, nll_loss=2.317, w2v_ctc_loss=0.796, task_loss=3.414, contrastive_loss=0.251, total=4112.8, n_correct=2591.25, ppl=4.98, accuracy=63.005, wps=11900.3, ups=1.45, wpb=8225.6, bsz=302.2, num_updates=15200, lr=0.000114708, gnorm=0.554, clip=0, loss_scale=16, train_wall=69, gb_free=16.8, wall=14377
2023-08-16 07:46:12 | INFO | train_inner | epoch 011:    569 / 1474 loss=2.107, trans_loss=5.036, nll_loss=2.313, w2v_ctc_loss=0.803, task_loss=3.547, contrastive_loss=0.246, total=4071.06, n_correct=2571.74, ppl=4.97, accuracy=63.171, wps=11629.3, ups=1.43, wpb=8142.1, bsz=292.6, num_updates=15300, lr=0.000114332, gnorm=0.564, clip=0, loss_scale=16, train_wall=69, gb_free=16.1, wall=14447
2023-08-16 07:47:21 | INFO | train_inner | epoch 011:    669 / 1474 loss=2.106, trans_loss=5.026, nll_loss=2.3, w2v_ctc_loss=0.795, task_loss=3.226, contrastive_loss=0.307, total=4156.4, n_correct=2632.2, ppl=4.93, accuracy=63.329, wps=12015.7, ups=1.45, wpb=8312.8, bsz=310.2, num_updates=15400, lr=0.000113961, gnorm=0.557, clip=0, loss_scale=16, train_wall=69, gb_free=16, wall=14516
2023-08-16 07:48:30 | INFO | train_inner | epoch 011:    769 / 1474 loss=2.099, trans_loss=5.039, nll_loss=2.317, w2v_ctc_loss=0.81, task_loss=3.337, contrastive_loss=0.13, total=4169.17, n_correct=2639.25, ppl=4.98, accuracy=63.304, wps=12011.9, ups=1.44, wpb=8338.3, bsz=304.8, num_updates=15500, lr=0.000113592, gnorm=0.561, clip=0, loss_scale=16, train_wall=69, gb_free=11.7, wall=14585
2023-08-16 07:49:39 | INFO | train_inner | epoch 011:    869 / 1474 loss=2.097, trans_loss=5.039, nll_loss=2.317, w2v_ctc_loss=0.805, task_loss=3.454, contrastive_loss=0.119, total=4120.01, n_correct=2600.04, ppl=4.98, accuracy=63.108, wps=11994.2, ups=1.46, wpb=8240, bsz=293.5, num_updates=15600, lr=0.000113228, gnorm=0.549, clip=0, loss_scale=16, train_wall=68, gb_free=13.1, wall=14654
2023-08-16 07:50:49 | INFO | train_inner | epoch 011:    969 / 1474 loss=2.094, trans_loss=5.035, nll_loss=2.313, w2v_ctc_loss=0.804, task_loss=3.329, contrastive_loss=0.132, total=4145.45, n_correct=2620.95, ppl=4.97, accuracy=63.225, wps=11936.4, ups=1.44, wpb=8290.9, bsz=303.7, num_updates=15700, lr=0.000112867, gnorm=0.552, clip=0, loss_scale=16, train_wall=69, gb_free=16.7, wall=14724
2023-08-16 07:51:57 | INFO | train_inner | epoch 011:   1069 / 1474 loss=2.094, trans_loss=5.031, nll_loss=2.308, w2v_ctc_loss=0.804, task_loss=3.228, contrastive_loss=0.15, total=4141.18, n_correct=2624.04, ppl=4.95, accuracy=63.365, wps=12042.2, ups=1.45, wpb=8282.4, bsz=309.4, num_updates=15800, lr=0.000112509, gnorm=0.536, clip=0, loss_scale=16, train_wall=68, gb_free=16.3, wall=14792
2023-08-16 07:53:06 | INFO | train_inner | epoch 011:   1169 / 1474 loss=2.096, trans_loss=5.038, nll_loss=2.317, w2v_ctc_loss=0.809, task_loss=3.301, contrastive_loss=0.135, total=4173.93, n_correct=2638.31, ppl=4.98, accuracy=63.209, wps=12090.3, ups=1.45, wpb=8347.9, bsz=307.2, num_updates=15900, lr=0.000112154, gnorm=0.539, clip=0, loss_scale=16, train_wall=69, gb_free=16.6, wall=14861
2023-08-16 07:54:16 | INFO | train_inner | epoch 011:   1269 / 1474 loss=2.101, trans_loss=5.029, nll_loss=2.305, w2v_ctc_loss=0.805, task_loss=3.16, contrastive_loss=0.207, total=4174.26, n_correct=2639.56, ppl=4.94, accuracy=63.234, wps=12081.4, ups=1.45, wpb=8348.5, bsz=314.4, num_updates=16000, lr=0.000111803, gnorm=0.541, clip=0, loss_scale=16, train_wall=69, gb_free=17.2, wall=14931
2023-08-16 07:54:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:3')
2023-08-16 07:54:39 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.054 | trans_loss 5.235 | nll_loss 2.52 | w2v_ctc_loss 1.413 | task_loss 11.532 | contrastive_loss 0.357 | total 4003.4 | n_correct 2609.5 | ppl 5.74 | accuracy 65.182 | uer 19.659 | wer 21.42 | raw_wer 21.42 | bleu 20.58 | wps 2134 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 21.28
2023-08-16 07:54:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-16 07:54:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_11_16000.pt
2023-08-16 07:54:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_11_16000.pt
2023-08-16 07:55:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 20.58) (writing took 41.09884432516992 seconds)
2023-08-16 07:56:31 | INFO | train_inner | epoch 011:   1369 / 1474 loss=2.111, trans_loss=5.031, nll_loss=2.308, w2v_ctc_loss=0.793, task_loss=3.034, contrastive_loss=0.364, total=4191.56, n_correct=2654.01, ppl=4.95, accuracy=63.318, wps=6187.2, ups=0.74, wpb=8383.1, bsz=327.7, num_updates=16100, lr=0.000111456, gnorm=0.555, clip=0, loss_scale=16, train_wall=69, gb_free=17.2, wall=15066
2023-08-16 07:57:40 | INFO | train_inner | epoch 011:   1469 / 1474 loss=2.088, trans_loss=5.033, nll_loss=2.311, w2v_ctc_loss=0.795, task_loss=3.158, contrastive_loss=0.14, total=4161.81, n_correct=2636.09, ppl=4.96, accuracy=63.34, wps=12004.7, ups=1.44, wpb=8323.6, bsz=313.2, num_updates=16200, lr=0.000111111, gnorm=0.544, clip=0, loss_scale=16, train_wall=69, gb_free=16.4, wall=15135
2023-08-16 07:57:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 07:58:07 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.034 | trans_loss 5.235 | nll_loss 2.519 | w2v_ctc_loss 1.349 | task_loss 11.495 | contrastive_loss 0.351 | total 4003.4 | n_correct 2613.6 | ppl 5.73 | accuracy 65.285 | uer 19.396 | wer 21.185 | raw_wer 21.185 | bleu 21.12 | wps 2196 | wpb 4003.4 | bsz 141.8 | num_updates 16205 | best_bleu 21.28
2023-08-16 07:58:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16205 updates
2023-08-16 07:58:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.1203.pt
2023-08-16 07:58:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.1203.pt
2023-08-16 07:58:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.1203.pt (epoch 11 @ 16205 updates, score 21.12) (writing took 22.946355184540153 seconds)
2023-08-16 07:58:30 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-16 07:58:30 | INFO | train | epoch 011 | loss 2.085 | trans_loss 4.616 | nll_loss 2.118 | w2v_ctc_loss 0.867 | task_loss 3.017 | contrastive_loss 0.188 | total 4138.65 | n_correct 2618.86 | ppl 4.34 | accuracy 63.278 | wps 11071.2 | ups 1.23 | wpb 9023.7 | bsz 333.5 | num_updates 16205 | lr 0.000111094 | gnorm 0.526 | clip 0 | loss_scale 16 | train_wall 1073 | gb_free 16.9 | wall 15185
2023-08-16 07:58:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 07:58:30 | INFO | fairseq.trainer | begin training epoch 12
2023-08-16 07:58:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 07:59:43 | INFO | train_inner | epoch 012:     95 / 1474 loss=2.071, trans_loss=4.993, nll_loss=2.258, w2v_ctc_loss=0.783, task_loss=3.172, contrastive_loss=0.169, total=4139.2, n_correct=2653.05, ppl=4.78, accuracy=64.096, wps=6754, ups=0.82, wpb=8278.4, bsz=312.5, num_updates=16300, lr=0.00011077, gnorm=0.531, clip=0, loss_scale=16, train_wall=69, gb_free=15.6, wall=15258
2023-08-16 08:00:52 | INFO | train_inner | epoch 012:    195 / 1474 loss=2.073, trans_loss=4.997, nll_loss=2.263, w2v_ctc_loss=0.788, task_loss=3.384, contrastive_loss=0.121, total=4126.87, n_correct=2637.5, ppl=4.8, accuracy=63.91, wps=11975.3, ups=1.45, wpb=8253.7, bsz=295.9, num_updates=16400, lr=0.000110432, gnorm=0.535, clip=0, loss_scale=16, train_wall=68, gb_free=16.2, wall=15327
2023-08-16 08:02:01 | INFO | train_inner | epoch 012:    295 / 1474 loss=2.068, trans_loss=4.998, nll_loss=2.266, w2v_ctc_loss=0.773, task_loss=3.073, contrastive_loss=0.151, total=4203.54, n_correct=2690.63, ppl=4.81, accuracy=64.009, wps=12133.5, ups=1.44, wpb=8407.1, bsz=321.1, num_updates=16500, lr=0.000110096, gnorm=0.551, clip=0, loss_scale=16, train_wall=69, gb_free=14.3, wall=15396
2023-08-16 08:03:10 | INFO | train_inner | epoch 012:    395 / 1474 loss=2.071, trans_loss=5.001, nll_loss=2.267, w2v_ctc_loss=0.784, task_loss=3.225, contrastive_loss=0.135, total=4149.28, n_correct=2654.65, ppl=4.81, accuracy=63.979, wps=12023.4, ups=1.45, wpb=8298.6, bsz=307.1, num_updates=16600, lr=0.000109764, gnorm=0.542, clip=0, loss_scale=16, train_wall=68, gb_free=14.8, wall=15465
2023-08-16 08:04:19 | INFO | train_inner | epoch 012:    495 / 1474 loss=2.081, trans_loss=5.015, nll_loss=2.287, w2v_ctc_loss=0.793, task_loss=3.318, contrastive_loss=0.142, total=4106.46, n_correct=2618.51, ppl=4.88, accuracy=63.766, wps=11925.9, ups=1.45, wpb=8212.9, bsz=301.2, num_updates=16700, lr=0.000109435, gnorm=0.534, clip=0, loss_scale=16, train_wall=68, gb_free=17.6, wall=15534
2023-08-16 08:05:29 | INFO | train_inner | epoch 012:    595 / 1474 loss=2.08, trans_loss=5.002, nll_loss=2.271, w2v_ctc_loss=0.787, task_loss=3.136, contrastive_loss=0.207, total=4190.91, n_correct=2678, ppl=4.83, accuracy=63.9, wps=11955.4, ups=1.43, wpb=8381.8, bsz=316.5, num_updates=16800, lr=0.000109109, gnorm=0.555, clip=0, loss_scale=16, train_wall=69, gb_free=15.6, wall=15604
2023-08-16 08:06:38 | INFO | train_inner | epoch 012:    695 / 1474 loss=2.076, trans_loss=4.998, nll_loss=2.266, w2v_ctc_loss=0.77, task_loss=3.031, contrastive_loss=0.29, total=4203.66, n_correct=2691.48, ppl=4.81, accuracy=64.027, wps=12185.1, ups=1.45, wpb=8407.3, bsz=324.4, num_updates=16900, lr=0.000108786, gnorm=0.528, clip=0, loss_scale=16, train_wall=68, gb_free=17.1, wall=15673
2023-08-16 08:07:47 | INFO | train_inner | epoch 012:    795 / 1474 loss=2.07, trans_loss=4.996, nll_loss=2.262, w2v_ctc_loss=0.786, task_loss=3.328, contrastive_loss=0.13, total=4095.72, n_correct=2624.35, ppl=4.8, accuracy=64.075, wps=11875.9, ups=1.45, wpb=8191.4, bsz=298.9, num_updates=17000, lr=0.000108465, gnorm=0.541, clip=0, loss_scale=16, train_wall=68, gb_free=16.9, wall=15742
2023-08-16 08:08:56 | INFO | train_inner | epoch 012:    895 / 1474 loss=2.079, trans_loss=5.002, nll_loss=2.271, w2v_ctc_loss=0.785, task_loss=3.372, contrastive_loss=0.184, total=4162.82, n_correct=2661.08, ppl=4.83, accuracy=63.925, wps=12034.8, ups=1.45, wpb=8325.6, bsz=305.4, num_updates=17100, lr=0.000108148, gnorm=0.547, clip=0, loss_scale=32, train_wall=68, gb_free=15.9, wall=15811
2023-08-16 08:10:05 | INFO | train_inner | epoch 012:    995 / 1474 loss=2.08, trans_loss=5.006, nll_loss=2.275, w2v_ctc_loss=0.789, task_loss=3.356, contrastive_loss=0.191, total=4117.63, n_correct=2631.21, ppl=4.84, accuracy=63.901, wps=11922.7, ups=1.45, wpb=8235.3, bsz=301.6, num_updates=17200, lr=0.000107833, gnorm=0.544, clip=0, loss_scale=32, train_wall=68, gb_free=16.6, wall=15880
2023-08-16 08:11:01 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-16 08:11:15 | INFO | train_inner | epoch 012:   1096 / 1474 loss=2.09, trans_loss=5.01, nll_loss=2.281, w2v_ctc_loss=0.792, task_loss=3.45, contrastive_loss=0.235, total=4048, n_correct=2581.63, ppl=4.86, accuracy=63.775, wps=11629.1, ups=1.44, wpb=8096, bsz=289.8, num_updates=17300, lr=0.000107521, gnorm=0.568, clip=0, loss_scale=16, train_wall=69, gb_free=16.2, wall=15950
2023-08-16 08:12:25 | INFO | train_inner | epoch 012:   1196 / 1474 loss=2.095, trans_loss=5.027, nll_loss=2.302, w2v_ctc_loss=0.805, task_loss=3.2, contrastive_loss=0.205, total=4196.85, n_correct=2664.09, ppl=4.93, accuracy=63.478, wps=12070.6, ups=1.44, wpb=8393.7, bsz=319, num_updates=17400, lr=0.000107211, gnorm=0.572, clip=0, loss_scale=16, train_wall=69, gb_free=16.2, wall=16020
2023-08-16 08:13:33 | INFO | train_inner | epoch 012:   1296 / 1474 loss=2.082, trans_loss=5.009, nll_loss=2.279, w2v_ctc_loss=0.806, task_loss=3.689, contrastive_loss=0.116, total=4067.78, n_correct=2592.49, ppl=4.85, accuracy=63.732, wps=11840.6, ups=1.46, wpb=8135.6, bsz=285.5, num_updates=17500, lr=0.000106904, gnorm=0.569, clip=0, loss_scale=16, train_wall=68, gb_free=14.8, wall=16088
2023-08-16 08:14:43 | INFO | train_inner | epoch 012:   1396 / 1474 loss=2.087, trans_loss=5.017, nll_loss=2.291, w2v_ctc_loss=0.789, task_loss=3.312, contrastive_loss=0.221, total=4142.88, n_correct=2635.98, ppl=4.89, accuracy=63.627, wps=11932.3, ups=1.44, wpb=8285.8, bsz=306.2, num_updates=17600, lr=0.0001066, gnorm=0.563, clip=0, loss_scale=16, train_wall=69, gb_free=15.3, wall=16158
2023-08-16 08:15:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 08:16:00 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.022 | trans_loss 5.218 | nll_loss 2.499 | w2v_ctc_loss 1.356 | task_loss 11.49 | contrastive_loss 0.345 | total 4003.4 | n_correct 2625.5 | ppl 5.65 | accuracy 65.582 | uer 18.984 | wer 20.879 | raw_wer 20.879 | bleu 21.22 | wps 2328.3 | wpb 4003.4 | bsz 141.8 | num_updates 17678 | best_bleu 21.28
2023-08-16 08:16:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17678 updates
2023-08-16 08:16:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.2205.pt
2023-08-16 08:16:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.2205.pt
2023-08-16 08:16:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.2205.pt (epoch 12 @ 17678 updates, score 21.22) (writing took 24.7643031347543 seconds)
2023-08-16 08:16:25 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-16 08:16:25 | INFO | train | epoch 012 | loss 2.079 | trans_loss 5.006 | nll_loss 2.275 | w2v_ctc_loss 0.788 | task_loss 3.289 | contrastive_loss 0.176 | total 4138.6 | n_correct 2643.24 | ppl 4.84 | accuracy 63.868 | wps 11345.3 | ups 1.37 | wpb 8277.2 | bsz 305.7 | num_updates 17678 | lr 0.000106365 | gnorm 0.548 | clip 0 | loss_scale 16 | train_wall 1010 | gb_free 12.5 | wall 16260
2023-08-16 08:16:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 08:16:25 | INFO | fairseq.trainer | begin training epoch 13
2023-08-16 08:16:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 08:16:48 | INFO | train_inner | epoch 013:     22 / 1474 loss=2.08, trans_loss=5.013, nll_loss=2.284, w2v_ctc_loss=0.8, task_loss=3.392, contrastive_loss=0.125, total=4097.08, n_correct=2614.31, ppl=4.87, accuracy=63.809, wps=6552.6, ups=0.8, wpb=8194.2, bsz=296.6, num_updates=17700, lr=0.000106299, gnorm=0.536, clip=0, loss_scale=16, train_wall=69, gb_free=16.1, wall=16283
2023-08-16 08:17:57 | INFO | train_inner | epoch 013:    122 / 1474 loss=2.061, trans_loss=4.982, nll_loss=2.243, w2v_ctc_loss=0.774, task_loss=3.315, contrastive_loss=0.138, total=4164.24, n_correct=2680.02, ppl=4.74, accuracy=64.358, wps=11987.3, ups=1.44, wpb=8328.5, bsz=301.9, num_updates=17800, lr=0.000106, gnorm=0.542, clip=0, loss_scale=16, train_wall=69, gb_free=16.6, wall=16352
2023-08-16 08:19:07 | INFO | train_inner | epoch 013:    222 / 1474 loss=2.077, trans_loss=4.987, nll_loss=2.251, w2v_ctc_loss=0.769, task_loss=3.05, contrastive_loss=0.35, total=4201.52, n_correct=2703, ppl=4.76, accuracy=64.334, wps=12107, ups=1.44, wpb=8403, bsz=328.5, num_updates=17900, lr=0.000105703, gnorm=0.533, clip=0, loss_scale=16, train_wall=69, gb_free=12.7, wall=16422
2023-08-16 08:20:16 | INFO | train_inner | epoch 013:    322 / 1474 loss=2.054, trans_loss=4.973, nll_loss=2.232, w2v_ctc_loss=0.769, task_loss=3.42, contrastive_loss=0.121, total=4102.53, n_correct=2651.55, ppl=4.7, accuracy=64.632, wps=11803.2, ups=1.44, wpb=8205.1, bsz=293.9, num_updates=18000, lr=0.000105409, gnorm=0.563, clip=0, loss_scale=16, train_wall=69, gb_free=15.8, wall=16491
2023-08-16 08:20:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 08:20:40 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.02 | trans_loss 5.22 | nll_loss 2.5 | w2v_ctc_loss 1.342 | task_loss 11.462 | contrastive_loss 0.348 | total 4003.4 | n_correct 2620.8 | ppl 5.66 | accuracy 65.464 | uer 19.069 | wer 20.92 | raw_wer 20.92 | bleu 21.03 | wps 2202.3 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 21.28
2023-08-16 08:20:40 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-16 08:20:40 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_13_18000.pt
2023-08-16 08:20:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_13_18000.pt
2023-08-16 08:21:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 21.03) (writing took 40.45241045765579 seconds)
2023-08-16 08:22:32 | INFO | train_inner | epoch 013:    422 / 1474 loss=2.057, trans_loss=4.977, nll_loss=2.238, w2v_ctc_loss=0.77, task_loss=3.057, contrastive_loss=0.171, total=4190.45, n_correct=2704.24, ppl=4.72, accuracy=64.533, wps=6164.4, ups=0.74, wpb=8380.9, bsz=320.3, num_updates=18100, lr=0.000105118, gnorm=0.549, clip=0, loss_scale=16, train_wall=68, gb_free=15.6, wall=16627
2023-08-16 08:23:42 | INFO | train_inner | epoch 013:    522 / 1474 loss=2.064, trans_loss=4.984, nll_loss=2.247, w2v_ctc_loss=0.775, task_loss=3.18, contrastive_loss=0.203, total=4194.45, n_correct=2695.03, ppl=4.75, accuracy=64.252, wps=12037.3, ups=1.43, wpb=8388.9, bsz=319, num_updates=18200, lr=0.000104828, gnorm=0.536, clip=0, loss_scale=16, train_wall=69, gb_free=17, wall=16697
2023-08-16 08:24:51 | INFO | train_inner | epoch 013:    622 / 1474 loss=2.05, trans_loss=4.979, nll_loss=2.241, w2v_ctc_loss=0.768, task_loss=3.217, contrastive_loss=0.116, total=4158.04, n_correct=2685.36, ppl=4.73, accuracy=64.582, wps=12054.5, ups=1.45, wpb=8316.1, bsz=306.7, num_updates=18300, lr=0.000104542, gnorm=0.543, clip=0, loss_scale=16, train_wall=68, gb_free=13.1, wall=16766
2023-08-16 08:26:01 | INFO | train_inner | epoch 013:    722 / 1474 loss=2.07, trans_loss=4.988, nll_loss=2.251, w2v_ctc_loss=0.797, task_loss=3.644, contrastive_loss=0.119, total=4099.91, n_correct=2632.73, ppl=4.76, accuracy=64.214, wps=11764.7, ups=1.43, wpb=8199.8, bsz=285.5, num_updates=18400, lr=0.000104257, gnorm=0.584, clip=0, loss_scale=16, train_wall=69, gb_free=16.4, wall=16836
2023-08-16 08:27:10 | INFO | train_inner | epoch 013:    822 / 1474 loss=2.066, trans_loss=4.987, nll_loss=2.251, w2v_ctc_loss=0.776, task_loss=3.335, contrastive_loss=0.166, total=4122.78, n_correct=2647.15, ppl=4.76, accuracy=64.208, wps=11888.1, ups=1.44, wpb=8245.6, bsz=306, num_updates=18500, lr=0.000103975, gnorm=0.555, clip=0, loss_scale=16, train_wall=69, gb_free=14.6, wall=16905
2023-08-16 08:28:19 | INFO | train_inner | epoch 013:    922 / 1474 loss=2.057, trans_loss=4.982, nll_loss=2.245, w2v_ctc_loss=0.774, task_loss=3.359, contrastive_loss=0.127, total=4102.59, n_correct=2644.48, ppl=4.74, accuracy=64.459, wps=11916.1, ups=1.45, wpb=8205.2, bsz=296.6, num_updates=18600, lr=0.000103695, gnorm=0.536, clip=0, loss_scale=16, train_wall=68, gb_free=17, wall=16974
2023-08-16 08:29:27 | INFO | train_inner | epoch 013:   1022 / 1474 loss=2.072, trans_loss=4.989, nll_loss=2.253, w2v_ctc_loss=0.786, task_loss=3.491, contrastive_loss=0.18, total=4087.8, n_correct=2624.8, ppl=4.77, accuracy=64.211, wps=11954.9, ups=1.46, wpb=8175.6, bsz=293.7, num_updates=18700, lr=0.000103418, gnorm=0.543, clip=0, loss_scale=16, train_wall=68, gb_free=16.4, wall=17042
2023-08-16 08:30:36 | INFO | train_inner | epoch 013:   1122 / 1474 loss=2.054, trans_loss=4.975, nll_loss=2.235, w2v_ctc_loss=0.769, task_loss=3.264, contrastive_loss=0.158, total=4098.77, n_correct=2645.52, ppl=4.71, accuracy=64.544, wps=11852.5, ups=1.45, wpb=8197.5, bsz=304.8, num_updates=18800, lr=0.000103142, gnorm=0.541, clip=0, loss_scale=16, train_wall=69, gb_free=13.3, wall=17111
2023-08-16 08:31:46 | INFO | train_inner | epoch 013:   1222 / 1474 loss=2.062, trans_loss=4.987, nll_loss=2.251, w2v_ctc_loss=0.782, task_loss=3.486, contrastive_loss=0.119, total=4115.57, n_correct=2645.97, ppl=4.76, accuracy=64.292, wps=11865.5, ups=1.44, wpb=8231.1, bsz=295.9, num_updates=18900, lr=0.000102869, gnorm=0.537, clip=0, loss_scale=16, train_wall=69, gb_free=14.8, wall=17181
2023-08-16 08:32:55 | INFO | train_inner | epoch 013:   1322 / 1474 loss=2.067, trans_loss=4.98, nll_loss=2.242, w2v_ctc_loss=0.779, task_loss=3.26, contrastive_loss=0.218, total=4111.02, n_correct=2651.33, ppl=4.73, accuracy=64.493, wps=11903.3, ups=1.45, wpb=8222, bsz=307.8, num_updates=19000, lr=0.000102598, gnorm=0.592, clip=0, loss_scale=16, train_wall=68, gb_free=17.6, wall=17250
2023-08-16 08:34:04 | INFO | train_inner | epoch 013:   1422 / 1474 loss=2.068, trans_loss=4.985, nll_loss=2.249, w2v_ctc_loss=0.773, task_loss=3.231, contrastive_loss=0.225, total=4179.06, n_correct=2690.21, ppl=4.75, accuracy=64.374, wps=12103.4, ups=1.45, wpb=8358.1, bsz=311.9, num_updates=19100, lr=0.000102329, gnorm=0.544, clip=0, loss_scale=16, train_wall=68, gb_free=15.8, wall=17319
2023-08-16 08:34:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 08:35:03 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.01 | trans_loss 5.207 | nll_loss 2.481 | w2v_ctc_loss 1.348 | task_loss 11.546 | contrastive_loss 0.345 | total 4003.4 | n_correct 2627.4 | ppl 5.58 | accuracy 65.629 | uer 18.992 | wer 20.726 | raw_wer 20.726 | bleu 21.55 | wps 2143.6 | wpb 4003.4 | bsz 141.8 | num_updates 19152 | best_bleu 21.55
2023-08-16 08:35:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19152 updates
2023-08-16 08:35:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 08:35:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 08:35:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 13 @ 19152 updates, score 21.55) (writing took 30.35535119473934 seconds)
2023-08-16 08:35:34 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-16 08:35:34 | INFO | train | epoch 013 | loss 2.062 | trans_loss 4.982 | nll_loss 2.244 | w2v_ctc_loss 0.776 | task_loss 3.291 | contrastive_loss 0.172 | total 4138.65 | n_correct 2665.55 | ppl 4.74 | accuracy 64.406 | wps 10618 | ups 1.28 | wpb 8277.3 | bsz 305.7 | num_updates 19152 | lr 0.00010219 | gnorm 0.549 | clip 0 | loss_scale 16 | train_wall 1011 | gb_free 17.4 | wall 17409
2023-08-16 08:35:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 08:35:34 | INFO | fairseq.trainer | begin training epoch 14
2023-08-16 08:35:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 08:36:15 | INFO | train_inner | epoch 014:     48 / 1474 loss=2.04, trans_loss=4.955, nll_loss=2.211, w2v_ctc_loss=0.762, task_loss=3.023, contrastive_loss=0.132, total=4179.66, n_correct=2716.48, ppl=4.63, accuracy=64.993, wps=6355, ups=0.76, wpb=8359.3, bsz=322, num_updates=19200, lr=0.000102062, gnorm=0.536, clip=0, loss_scale=16, train_wall=68, gb_free=10.1, wall=17450
2023-08-16 08:37:24 | INFO | train_inner | epoch 014:    148 / 1474 loss=2.034, trans_loss=4.944, nll_loss=2.195, w2v_ctc_loss=0.761, task_loss=3.299, contrastive_loss=0.114, total=4081.01, n_correct=2659.64, ppl=4.58, accuracy=65.171, wps=11874, ups=1.45, wpb=8162, bsz=300.6, num_updates=19300, lr=0.000101797, gnorm=0.538, clip=0, loss_scale=16, train_wall=68, gb_free=15.7, wall=17519
2023-08-16 08:38:33 | INFO | train_inner | epoch 014:    248 / 1474 loss=2.053, trans_loss=4.963, nll_loss=2.219, w2v_ctc_loss=0.763, task_loss=3.437, contrastive_loss=0.213, total=4109.83, n_correct=2665.22, ppl=4.66, accuracy=64.85, wps=11977.2, ups=1.46, wpb=8219.7, bsz=295.2, num_updates=19400, lr=0.000101535, gnorm=0.552, clip=0, loss_scale=32, train_wall=68, gb_free=15.4, wall=17588
2023-08-16 08:38:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-16 08:39:42 | INFO | train_inner | epoch 014:    349 / 1474 loss=2.044, trans_loss=4.962, nll_loss=2.22, w2v_ctc_loss=0.759, task_loss=3.139, contrastive_loss=0.147, total=4154.06, n_correct=2695.6, ppl=4.66, accuracy=64.891, wps=11925.3, ups=1.44, wpb=8308.1, bsz=314.4, num_updates=19500, lr=0.000101274, gnorm=0.56, clip=0, loss_scale=16, train_wall=69, gb_free=16, wall=17658
2023-08-16 08:40:52 | INFO | train_inner | epoch 014:    449 / 1474 loss=2.041, trans_loss=4.962, nll_loss=2.219, w2v_ctc_loss=0.756, task_loss=3.242, contrastive_loss=0.132, total=4155.83, n_correct=2697.53, ppl=4.66, accuracy=64.91, wps=12012.4, ups=1.45, wpb=8311.7, bsz=306.7, num_updates=19600, lr=0.000101015, gnorm=0.533, clip=0, loss_scale=16, train_wall=69, gb_free=15.7, wall=17727
2023-08-16 08:42:01 | INFO | train_inner | epoch 014:    549 / 1474 loss=2.057, trans_loss=4.967, nll_loss=2.224, w2v_ctc_loss=0.783, task_loss=3.578, contrastive_loss=0.126, total=4064.87, n_correct=2626.24, ppl=4.67, accuracy=64.608, wps=11763.7, ups=1.45, wpb=8129.7, bsz=288.5, num_updates=19700, lr=0.000100759, gnorm=0.542, clip=0, loss_scale=16, train_wall=69, gb_free=17.6, wall=17796
2023-08-16 08:43:10 | INFO | train_inner | epoch 014:    649 / 1474 loss=2.054, trans_loss=4.966, nll_loss=2.223, w2v_ctc_loss=0.766, task_loss=3.267, contrastive_loss=0.19, total=4167.34, n_correct=2700.26, ppl=4.67, accuracy=64.796, wps=12032.8, ups=1.44, wpb=8334.7, bsz=307.8, num_updates=19800, lr=0.000100504, gnorm=0.549, clip=0, loss_scale=16, train_wall=69, gb_free=16.8, wall=17865
2023-08-16 08:44:19 | INFO | train_inner | epoch 014:    749 / 1474 loss=2.043, trans_loss=4.953, nll_loss=2.208, w2v_ctc_loss=0.769, task_loss=3.232, contrastive_loss=0.123, total=4142.94, n_correct=2690, ppl=4.62, accuracy=64.93, wps=12011.6, ups=1.45, wpb=8285.9, bsz=308.6, num_updates=19900, lr=0.000100251, gnorm=0.572, clip=0, loss_scale=16, train_wall=68, gb_free=16.1, wall=17934
2023-08-16 08:45:29 | INFO | train_inner | epoch 014:    849 / 1474 loss=2.053, trans_loss=4.956, nll_loss=2.212, w2v_ctc_loss=0.761, task_loss=3.141, contrastive_loss=0.232, total=4173.06, n_correct=2705.71, ppl=4.63, accuracy=64.838, wps=12002.9, ups=1.44, wpb=8346.1, bsz=319.1, num_updates=20000, lr=0.0001, gnorm=0.567, clip=0, loss_scale=16, train_wall=69, gb_free=12.4, wall=18004
2023-08-16 08:45:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 08:45:53 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.018 | trans_loss 5.197 | nll_loss 2.47 | w2v_ctc_loss 1.397 | task_loss 11.58 | contrastive_loss 0.349 | total 4003.4 | n_correct 2641.2 | ppl 5.54 | accuracy 65.974 | uer 19.04 | wer 20.998 | raw_wer 20.998 | bleu 21.43 | wps 2041.6 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 21.55
2023-08-16 08:45:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-16 08:45:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_14_20000.pt
2023-08-16 08:45:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_14_20000.pt
2023-08-16 08:46:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 21.43) (writing took 42.777806585654616 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:0')
2023-08-16 08:47:47 | INFO | train_inner | epoch 014:    949 / 1474 loss=2.049, trans_loss=4.963, nll_loss=2.22, w2v_ctc_loss=0.761, task_loss=3.281, contrastive_loss=0.169, total=4166.71, n_correct=2696.83, ppl=4.66, accuracy=64.723, wps=6004, ups=0.72, wpb=8333.4, bsz=310.6, num_updates=20100, lr=9.97509e-05, gnorm=0.573, clip=0, loss_scale=16, train_wall=69, gb_free=16.2, wall=18142
2023-08-16 08:48:57 | INFO | train_inner | epoch 014:   1049 / 1474 loss=2.049, trans_loss=4.964, nll_loss=2.222, w2v_ctc_loss=0.764, task_loss=3.343, contrastive_loss=0.147, total=4145.57, n_correct=2688.45, ppl=4.67, accuracy=64.851, wps=11945.1, ups=1.44, wpb=8291.1, bsz=301.1, num_updates=20200, lr=9.95037e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=69, gb_free=17.1, wall=18212
2023-08-16 08:50:06 | INFO | train_inner | epoch 014:   1149 / 1474 loss=2.076, trans_loss=4.966, nll_loss=2.224, w2v_ctc_loss=0.772, task_loss=3.103, contrastive_loss=0.415, total=4219.9, n_correct=2730.57, ppl=4.67, accuracy=64.707, wps=12161.9, ups=1.44, wpb=8439.8, bsz=325.2, num_updates=20300, lr=9.92583e-05, gnorm=0.562, clip=0, loss_scale=16, train_wall=69, gb_free=16.1, wall=18281
2023-08-16 08:51:15 | INFO | train_inner | epoch 014:   1249 / 1474 loss=2.053, trans_loss=4.97, nll_loss=2.228, w2v_ctc_loss=0.781, task_loss=3.816, contrastive_loss=0.104, total=4032.06, n_correct=2606.47, ppl=4.69, accuracy=64.644, wps=11702.1, ups=1.45, wpb=8064.1, bsz=274.4, num_updates=20400, lr=9.90148e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=68, gb_free=17.4, wall=18350
2023-08-16 08:52:24 | INFO | train_inner | epoch 014:   1349 / 1474 loss=2.041, trans_loss=4.963, nll_loss=2.221, w2v_ctc_loss=0.761, task_loss=3.127, contrastive_loss=0.121, total=4205.07, n_correct=2728.44, ppl=4.66, accuracy=64.885, wps=12144.1, ups=1.44, wpb=8410.1, bsz=317.3, num_updates=20500, lr=9.8773e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=69, gb_free=16.4, wall=18419
2023-08-16 08:53:33 | INFO | train_inner | epoch 014:   1449 / 1474 loss=2.048, trans_loss=4.966, nll_loss=2.225, w2v_ctc_loss=0.762, task_loss=3.291, contrastive_loss=0.158, total=4126.44, n_correct=2673.77, ppl=4.68, accuracy=64.796, wps=11992, ups=1.45, wpb=8252.9, bsz=303.9, num_updates=20600, lr=9.85329e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=68, gb_free=15.8, wall=18488
2023-08-16 08:53:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:1')
2023-08-16 08:54:13 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.001 | trans_loss 5.196 | nll_loss 2.468 | w2v_ctc_loss 1.348 | task_loss 11.495 | contrastive_loss 0.336 | total 4003.4 | n_correct 2642.6 | ppl 5.53 | accuracy 66.009 | uer 18.87 | wer 20.708 | raw_wer 20.708 | bleu 21.36 | wps 2254 | wpb 4003.4 | bsz 141.8 | num_updates 20625 | best_bleu 21.55
2023-08-16 08:54:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20625 updates
2023-08-16 08:54:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.3601.pt
2023-08-16 08:54:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.3601.pt
2023-08-16 08:54:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.3601.pt (epoch 14 @ 20625 updates, score 21.36) (writing took 23.27131944335997 seconds)
2023-08-16 08:54:37 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-16 08:54:37 | INFO | train | epoch 014 | loss 2.049 | trans_loss 4.962 | nll_loss 2.219 | w2v_ctc_loss 0.765 | task_loss 3.296 | contrastive_loss 0.17 | total 4137.96 | n_correct 2682.73 | ppl 4.65 | accuracy 64.832 | wps 10663.8 | ups 1.29 | wpb 8275.9 | bsz 305.4 | num_updates 20625 | lr 9.84732e-05 | gnorm 0.551 | clip 0 | loss_scale 16 | train_wall 1011 | gb_free 16.1 | wall 18552
2023-08-16 08:54:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 08:54:37 | INFO | fairseq.trainer | begin training epoch 15
2023-08-16 08:54:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 08:55:36 | INFO | train_inner | epoch 015:     75 / 1474 loss=2.041, trans_loss=4.949, nll_loss=2.202, w2v_ctc_loss=0.751, task_loss=3.304, contrastive_loss=0.206, total=4090.99, n_correct=2665.47, ppl=4.6, accuracy=65.155, wps=6688.1, ups=0.82, wpb=8182, bsz=300.8, num_updates=20700, lr=9.82946e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=68, gb_free=16.6, wall=18611
2023-08-16 08:56:45 | INFO | train_inner | epoch 015:    175 / 1474 loss=2.029, trans_loss=4.938, nll_loss=2.187, w2v_ctc_loss=0.754, task_loss=3.419, contrastive_loss=0.116, total=4115.56, n_correct=2690.62, ppl=4.55, accuracy=65.377, wps=11859.2, ups=1.44, wpb=8231.1, bsz=298.5, num_updates=20800, lr=9.80581e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=69, gb_free=16.5, wall=18680
2023-08-16 08:57:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-16 08:57:54 | INFO | train_inner | epoch 015:    276 / 1474 loss=2.027, trans_loss=4.941, nll_loss=2.191, w2v_ctc_loss=0.751, task_loss=3.209, contrastive_loss=0.108, total=4179.62, n_correct=2730.66, ppl=4.57, accuracy=65.333, wps=12069.4, ups=1.44, wpb=8359.2, bsz=310.2, num_updates=20900, lr=9.78232e-05, gnorm=0.596, clip=0, loss_scale=8, train_wall=69, gb_free=16, wall=18749
2023-08-16 08:59:03 | INFO | train_inner | epoch 015:    376 / 1474 loss=2.028, trans_loss=4.934, nll_loss=2.183, w2v_ctc_loss=0.747, task_loss=3.283, contrastive_loss=0.134, total=4176.34, n_correct=2729.84, ppl=4.54, accuracy=65.364, wps=12082.3, ups=1.45, wpb=8352.7, bsz=308.5, num_updates=21000, lr=9.759e-05, gnorm=0.556, clip=0, loss_scale=8, train_wall=69, gb_free=16.1, wall=18818
2023-08-16 09:00:12 | INFO | train_inner | epoch 015:    476 / 1474 loss=2.041, trans_loss=4.943, nll_loss=2.195, w2v_ctc_loss=0.748, task_loss=3.461, contrastive_loss=0.219, total=4076.17, n_correct=2657.33, ppl=4.58, accuracy=65.192, wps=11828.2, ups=1.45, wpb=8152.3, bsz=293.1, num_updates=21100, lr=9.73585e-05, gnorm=0.617, clip=0, loss_scale=8, train_wall=68, gb_free=16.4, wall=18887
2023-08-16 09:01:22 | INFO | train_inner | epoch 015:    576 / 1474 loss=2.033, trans_loss=4.942, nll_loss=2.193, w2v_ctc_loss=0.755, task_loss=3.373, contrastive_loss=0.136, total=4151.89, n_correct=2709.61, ppl=4.57, accuracy=65.262, wps=11905, ups=1.43, wpb=8303.8, bsz=302, num_updates=21200, lr=9.71286e-05, gnorm=0.529, clip=0, loss_scale=8, train_wall=69, gb_free=13.2, wall=18957
2023-08-16 09:02:31 | INFO | train_inner | epoch 015:    676 / 1474 loss=2.033, trans_loss=4.933, nll_loss=2.181, w2v_ctc_loss=0.75, task_loss=3.36, contrastive_loss=0.179, total=4122.17, n_correct=2699.54, ppl=4.54, accuracy=65.488, wps=11963, ups=1.45, wpb=8244.3, bsz=304, num_updates=21300, lr=9.69003e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=68, gb_free=17, wall=19026
2023-08-16 09:03:41 | INFO | train_inner | epoch 015:    776 / 1474 loss=2.032, trans_loss=4.946, nll_loss=2.198, w2v_ctc_loss=0.759, task_loss=3.295, contrastive_loss=0.117, total=4181.07, n_correct=2726.22, ppl=4.59, accuracy=65.204, wps=12007.8, ups=1.44, wpb=8362.1, bsz=307.1, num_updates=21400, lr=9.66736e-05, gnorm=0.548, clip=0, loss_scale=8, train_wall=69, gb_free=16.2, wall=19096
2023-08-16 09:04:49 | INFO | train_inner | epoch 015:    876 / 1474 loss=2.032, trans_loss=4.944, nll_loss=2.196, w2v_ctc_loss=0.756, task_loss=3.563, contrastive_loss=0.109, total=4052.17, n_correct=2642.28, ppl=4.58, accuracy=65.207, wps=11791.2, ups=1.45, wpb=8104.3, bsz=286.4, num_updates=21500, lr=9.64486e-05, gnorm=0.548, clip=0, loss_scale=8, train_wall=68, gb_free=16.8, wall=19164
2023-08-16 09:05:58 | INFO | train_inner | epoch 015:    976 / 1474 loss=2.035, trans_loss=4.944, nll_loss=2.196, w2v_ctc_loss=0.747, task_loss=3.281, contrastive_loss=0.197, total=4135.95, n_correct=2700.45, ppl=4.58, accuracy=65.292, wps=12005.8, ups=1.45, wpb=8271.9, bsz=304.3, num_updates=21600, lr=9.6225e-05, gnorm=0.539, clip=0, loss_scale=8, train_wall=68, gb_free=16.3, wall=19233
2023-08-16 09:07:08 | INFO | train_inner | epoch 015:   1076 / 1474 loss=2.054, trans_loss=4.95, nll_loss=2.205, w2v_ctc_loss=0.753, task_loss=3.086, contrastive_loss=0.354, total=4187.18, n_correct=2723.12, ppl=4.61, accuracy=65.035, wps=12016.3, ups=1.43, wpb=8374.4, bsz=324.7, num_updates=21700, lr=9.60031e-05, gnorm=0.533, clip=0, loss_scale=8, train_wall=69, gb_free=13.6, wall=19303
2023-08-16 09:08:17 | INFO | train_inner | epoch 015:   1176 / 1474 loss=2.021, trans_loss=4.94, nll_loss=2.192, w2v_ctc_loss=0.734, task_loss=2.958, contrastive_loss=0.157, total=4184.18, n_correct=2741.28, ppl=4.57, accuracy=65.515, wps=12181.1, ups=1.46, wpb=8368.4, bsz=328.4, num_updates=21800, lr=9.57826e-05, gnorm=0.551, clip=0, loss_scale=8, train_wall=68, gb_free=15.5, wall=19372
2023-08-16 09:09:25 | INFO | train_inner | epoch 015:   1276 / 1474 loss=2.037, trans_loss=4.942, nll_loss=2.193, w2v_ctc_loss=0.769, task_loss=3.374, contrastive_loss=0.114, total=4141.39, n_correct=2696.86, ppl=4.57, accuracy=65.12, wps=12062.3, ups=1.46, wpb=8282.8, bsz=302.1, num_updates=21900, lr=9.55637e-05, gnorm=0.547, clip=0, loss_scale=8, train_wall=68, gb_free=16.2, wall=19440
2023-08-16 09:10:34 | INFO | train_inner | epoch 015:   1376 / 1474 loss=2.027, trans_loss=4.94, nll_loss=2.191, w2v_ctc_loss=0.753, task_loss=3.398, contrastive_loss=0.1, total=4106.11, n_correct=2683.71, ppl=4.57, accuracy=65.359, wps=11904.1, ups=1.45, wpb=8212.2, bsz=294.2, num_updates=22000, lr=9.53463e-05, gnorm=0.56, clip=0, loss_scale=8, train_wall=68, gb_free=16.6, wall=19509
2023-08-16 09:10:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 09:10:58 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.982 | trans_loss 5.197 | nll_loss 2.467 | w2v_ctc_loss 1.291 | task_loss 11.569 | contrastive_loss 0.334 | total 4003.4 | n_correct 2644.8 | ppl 5.53 | accuracy 66.064 | uer 18.392 | wer 20.283 | raw_wer 20.283 | bleu 21.77 | wps 2210.9 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 21.77
2023-08-16 09:10:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-16 09:10:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_15_22000.pt
2023-08-16 09:11:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_15_22000.pt
2023-08-16 09:11:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 21.77) (writing took 31.071872785687447 seconds)
2023-08-16 09:12:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 09:13:02 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.988 | trans_loss 5.193 | nll_loss 2.466 | w2v_ctc_loss 1.313 | task_loss 11.546 | contrastive_loss 0.341 | total 4003.4 | n_correct 2645.4 | ppl 5.53 | accuracy 66.079 | uer 18.69 | wer 20.592 | raw_wer 20.592 | bleu 21.71 | wps 2180.8 | wpb 4003.4 | bsz 141.8 | num_updates 22098 | best_bleu 21.77
2023-08-16 09:13:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22098 updates
2023-08-16 09:13:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.7100.pt
2023-08-16 09:13:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.7100.pt
2023-08-16 09:13:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.7100.pt (epoch 15 @ 22098 updates, score 21.71) (writing took 24.28326863795519 seconds)
2023-08-16 09:13:27 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-16 09:13:27 | INFO | train | epoch 015 | loss 2.033 | trans_loss 4.942 | nll_loss 2.193 | w2v_ctc_loss 0.751 | task_loss 3.292 | contrastive_loss 0.164 | total 4138.88 | n_correct 2702.11 | ppl 4.57 | accuracy 65.286 | wps 10794.6 | ups 1.3 | wpb 8277.8 | bsz 305.7 | num_updates 22098 | lr 9.51346e-05 | gnorm 0.55 | clip 0 | loss_scale 8 | train_wall 1010 | gb_free 16.7 | wall 19682
2023-08-16 09:13:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 09:13:27 | INFO | fairseq.trainer | begin training epoch 16
2023-08-16 09:13:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 09:13:36 | INFO | train_inner | epoch 016:      2 / 1474 loss=2.035, trans_loss=4.947, nll_loss=2.201, w2v_ctc_loss=0.747, task_loss=3.142, contrastive_loss=0.19, total=4152.6, n_correct=2708.92, ppl=4.6, accuracy=65.234, wps=4566.2, ups=0.55, wpb=8305.2, bsz=316.4, num_updates=22100, lr=9.51303e-05, gnorm=0.543, clip=0, loss_scale=8, train_wall=69, gb_free=16.3, wall=19691
2023-08-16 09:14:45 | INFO | train_inner | epoch 016:    102 / 1474 loss=2.016, trans_loss=4.922, nll_loss=2.167, w2v_ctc_loss=0.742, task_loss=3.17, contrastive_loss=0.131, total=4115.14, n_correct=2701.94, ppl=4.49, accuracy=65.659, wps=11981.9, ups=1.46, wpb=8230.3, bsz=313.9, num_updates=22200, lr=9.49158e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=68, gb_free=12.2, wall=19760
2023-08-16 09:15:54 | INFO | train_inner | epoch 016:    202 / 1474 loss=2.009, trans_loss=4.914, nll_loss=2.157, w2v_ctc_loss=0.731, task_loss=3.376, contrastive_loss=0.105, total=4109.58, n_correct=2705.07, ppl=4.46, accuracy=65.824, wps=11873.1, ups=1.44, wpb=8219.2, bsz=297.3, num_updates=22300, lr=9.47027e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=69, gb_free=14.7, wall=19829
2023-08-16 09:17:03 | INFO | train_inner | epoch 016:    302 / 1474 loss=2.024, trans_loss=4.923, nll_loss=2.169, w2v_ctc_loss=0.744, task_loss=3.268, contrastive_loss=0.179, total=4164.1, n_correct=2733.9, ppl=4.5, accuracy=65.654, wps=12055.9, ups=1.45, wpb=8328.2, bsz=308.6, num_updates=22400, lr=9.44911e-05, gnorm=0.533, clip=0, loss_scale=8, train_wall=68, gb_free=17, wall=19898
2023-08-16 09:18:12 | INFO | train_inner | epoch 016:    402 / 1474 loss=2.028, trans_loss=4.922, nll_loss=2.167, w2v_ctc_loss=0.747, task_loss=3.532, contrastive_loss=0.194, total=4065.22, n_correct=2664.7, ppl=4.49, accuracy=65.549, wps=11807.1, ups=1.45, wpb=8130.4, bsz=286.4, num_updates=22500, lr=9.42809e-05, gnorm=0.538, clip=0, loss_scale=8, train_wall=68, gb_free=16, wall=19967
2023-08-16 09:19:22 | INFO | train_inner | epoch 016:    502 / 1474 loss=2.015, trans_loss=4.925, nll_loss=2.171, w2v_ctc_loss=0.736, task_loss=3.159, contrastive_loss=0.136, total=4181.93, n_correct=2751.9, ppl=4.5, accuracy=65.805, wps=12026.1, ups=1.44, wpb=8363.9, bsz=320.3, num_updates=22600, lr=9.40721e-05, gnorm=0.527, clip=0, loss_scale=8, train_wall=69, gb_free=15.5, wall=20037
2023-08-16 09:20:30 | INFO | train_inner | epoch 016:    602 / 1474 loss=2.019, trans_loss=4.927, nll_loss=2.174, w2v_ctc_loss=0.746, task_loss=3.287, contrastive_loss=0.102, total=4122.97, n_correct=2704.2, ppl=4.51, accuracy=65.589, wps=12038, ups=1.46, wpb=8245.9, bsz=299, num_updates=22700, lr=9.38647e-05, gnorm=0.538, clip=0, loss_scale=8, train_wall=68, gb_free=15.8, wall=20105
2023-08-16 09:21:38 | INFO | train_inner | epoch 016:    702 / 1474 loss=2.019, trans_loss=4.926, nll_loss=2.172, w2v_ctc_loss=0.747, task_loss=3.39, contrastive_loss=0.104, total=4093.15, n_correct=2684.55, ppl=4.51, accuracy=65.586, wps=11990.1, ups=1.46, wpb=8186.3, bsz=296.5, num_updates=22800, lr=9.36586e-05, gnorm=0.528, clip=0, loss_scale=8, train_wall=68, gb_free=17.6, wall=20173
2023-08-16 09:22:48 | INFO | train_inner | epoch 016:    802 / 1474 loss=2.018, trans_loss=4.921, nll_loss=2.167, w2v_ctc_loss=0.733, task_loss=3.147, contrastive_loss=0.165, total=4183.24, n_correct=2748.62, ppl=4.49, accuracy=65.706, wps=12089.4, ups=1.44, wpb=8366.5, bsz=312.1, num_updates=22900, lr=9.34539e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=69, gb_free=17.5, wall=20243
2023-08-16 09:23:57 | INFO | train_inner | epoch 016:    902 / 1474 loss=2.019, trans_loss=4.923, nll_loss=2.17, w2v_ctc_loss=0.737, task_loss=3.237, contrastive_loss=0.156, total=4150.23, n_correct=2728.67, ppl=4.5, accuracy=65.747, wps=12021.7, ups=1.45, wpb=8300.5, bsz=306.5, num_updates=23000, lr=9.32505e-05, gnorm=0.535, clip=0, loss_scale=16, train_wall=68, gb_free=11.8, wall=20312
2023-08-16 09:25:06 | INFO | train_inner | epoch 016:   1002 / 1474 loss=2.028, trans_loss=4.93, nll_loss=2.178, w2v_ctc_loss=0.753, task_loss=3.391, contrastive_loss=0.155, total=4116.59, n_correct=2693.92, ppl=4.53, accuracy=65.441, wps=11899.9, ups=1.45, wpb=8233.2, bsz=300.6, num_updates=23100, lr=9.30484e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=69, gb_free=16.6, wall=20381
2023-08-16 09:26:15 | INFO | train_inner | epoch 016:   1102 / 1474 loss=2.029, trans_loss=4.935, nll_loss=2.185, w2v_ctc_loss=0.756, task_loss=3.495, contrastive_loss=0.129, total=4112.71, n_correct=2689.27, ppl=4.55, accuracy=65.389, wps=11819.5, ups=1.44, wpb=8225.4, bsz=295.7, num_updates=23200, lr=9.28477e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=69, gb_free=11.8, wall=20450
2023-08-16 09:27:25 | INFO | train_inner | epoch 016:   1202 / 1474 loss=2.024, trans_loss=4.928, nll_loss=2.175, w2v_ctc_loss=0.727, task_loss=3.339, contrastive_loss=0.223, total=4161.11, n_correct=2725.38, ppl=4.52, accuracy=65.496, wps=11951.3, ups=1.44, wpb=8322.2, bsz=308.2, num_updates=23300, lr=9.26482e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=69, gb_free=15.5, wall=20520
2023-08-16 09:28:34 | INFO | train_inner | epoch 016:   1302 / 1474 loss=2.027, trans_loss=4.925, nll_loss=2.171, w2v_ctc_loss=0.746, task_loss=3.242, contrastive_loss=0.204, total=4149.14, n_correct=2720.86, ppl=4.5, accuracy=65.576, wps=12024, ups=1.45, wpb=8298.3, bsz=311.9, num_updates=23400, lr=9.245e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=69, gb_free=10.9, wall=20589
2023-08-16 09:29:43 | INFO | train_inner | epoch 016:   1402 / 1474 loss=2.017, trans_loss=4.925, nll_loss=2.172, w2v_ctc_loss=0.744, task_loss=3.121, contrastive_loss=0.134, total=4200.01, n_correct=2756.65, ppl=4.51, accuracy=65.634, wps=12104.5, ups=1.44, wpb=8400, bsz=322.2, num_updates=23500, lr=9.22531e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=69, gb_free=16.7, wall=20659
2023-08-16 09:30:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 09:30:56 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 3.971 | trans_loss 5.186 | nll_loss 2.453 | w2v_ctc_loss 1.282 | task_loss 11.551 | contrastive_loss 0.328 | total 4003.4 | n_correct 2651.5 | ppl 5.48 | accuracy 66.231 | uer 18.18 | wer 20.048 | raw_wer 20.048 | bleu 21.93 | wps 2208.9 | wpb 4003.4 | bsz 141.8 | num_updates 23572 | best_bleu 21.93
2023-08-16 09:30:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23572 updates
2023-08-16 09:30:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 09:31:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 09:31:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 16 @ 23572 updates, score 21.93) (writing took 33.11140190809965 seconds)
2023-08-16 09:31:30 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-16 09:31:30 | INFO | train | epoch 016 | loss 2.021 | trans_loss 4.924 | nll_loss 2.171 | w2v_ctc_loss 0.741 | task_loss 3.293 | contrastive_loss 0.16 | total 4138.65 | n_correct 2715.87 | ppl 4.5 | accuracy 65.622 | wps 11261.1 | ups 1.36 | wpb 8277.3 | bsz 305.7 | num_updates 23572 | lr 9.21121e-05 | gnorm 0.54 | clip 0 | loss_scale 16 | train_wall 1010 | gb_free 15.1 | wall 20765
2023-08-16 09:31:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 09:31:30 | INFO | fairseq.trainer | begin training epoch 17
2023-08-16 09:31:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 09:31:58 | INFO | train_inner | epoch 017:     28 / 1474 loss=2.021, trans_loss=4.912, nll_loss=2.155, w2v_ctc_loss=0.73, task_loss=3.384, contrastive_loss=0.267, total=4141.79, n_correct=2724.61, ppl=4.45, accuracy=65.783, wps=6181.3, ups=0.75, wpb=8283.6, bsz=301.5, num_updates=23600, lr=9.20575e-05, gnorm=0.567, clip=0, loss_scale=16, train_wall=69, gb_free=15.8, wall=20793
2023-08-16 09:33:06 | INFO | train_inner | epoch 017:    128 / 1474 loss=2.003, trans_loss=4.901, nll_loss=2.14, w2v_ctc_loss=0.731, task_loss=3.411, contrastive_loss=0.103, total=4110.88, n_correct=2715.25, ppl=4.41, accuracy=66.05, wps=11934.8, ups=1.45, wpb=8221.8, bsz=295.4, num_updates=23700, lr=9.1863e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=68, gb_free=16, wall=20861
2023-08-16 09:34:15 | INFO | train_inner | epoch 017:    228 / 1474 loss=2.016, trans_loss=4.9, nll_loss=2.14, w2v_ctc_loss=0.722, task_loss=3.088, contrastive_loss=0.27, total=4171.95, n_correct=2758.84, ppl=4.41, accuracy=66.128, wps=12149.3, ups=1.46, wpb=8343.9, bsz=320.4, num_updates=23800, lr=9.16698e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=68, gb_free=15.7, wall=20930
2023-08-16 09:35:24 | INFO | train_inner | epoch 017:    328 / 1474 loss=2.016, trans_loss=4.906, nll_loss=2.146, w2v_ctc_loss=0.725, task_loss=3.29, contrastive_loss=0.271, total=4157.94, n_correct=2743.15, ppl=4.43, accuracy=65.974, wps=12022.2, ups=1.45, wpb=8315.9, bsz=305, num_updates=23900, lr=9.14779e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=69, gb_free=14.2, wall=20999
2023-08-16 09:36:34 | INFO | train_inner | epoch 017:    428 / 1474 loss=2.002, trans_loss=4.906, nll_loss=2.147, w2v_ctc_loss=0.731, task_loss=3.281, contrastive_loss=0.101, total=4141.8, n_correct=2738.17, ppl=4.43, accuracy=66.111, wps=11951.2, ups=1.44, wpb=8283.6, bsz=306.7, num_updates=24000, lr=9.12871e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=69, gb_free=17.1, wall=21069
2023-08-16 09:36:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 09:36:58 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.985 | trans_loss 5.183 | nll_loss 2.45 | w2v_ctc_loss 1.341 | task_loss 11.548 | contrastive_loss 0.32 | total 4003.4 | n_correct 2653.7 | ppl 5.46 | accuracy 66.286 | uer 18.464 | wer 20.376 | raw_wer 20.376 | bleu 22.13 | wps 2128.1 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 22.13
2023-08-16 09:36:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-16 09:36:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_17_24000.pt
2023-08-16 09:37:02 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_17_24000.pt
2023-08-16 09:37:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 22.13) (writing took 55.39373370818794 seconds)
2023-08-16 09:39:05 | INFO | train_inner | epoch 017:    528 / 1474 loss=2.011, trans_loss=4.912, nll_loss=2.155, w2v_ctc_loss=0.733, task_loss=3.431, contrastive_loss=0.147, total=4180.09, n_correct=2753.42, ppl=4.45, accuracy=65.87, wps=5539.1, ups=0.66, wpb=8360.2, bsz=307.5, num_updates=24100, lr=9.10975e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=70, gb_free=16.8, wall=21220
2023-08-16 09:40:13 | INFO | train_inner | epoch 017:    628 / 1474 loss=2.004, trans_loss=4.911, nll_loss=2.154, w2v_ctc_loss=0.731, task_loss=3.32, contrastive_loss=0.098, total=4166.6, n_correct=2750.11, ppl=4.45, accuracy=66.004, wps=12135.5, ups=1.46, wpb=8333.2, bsz=302.3, num_updates=24200, lr=9.09091e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=68, gb_free=15.4, wall=21288
2023-08-16 09:41:22 | INFO | train_inner | epoch 017:    728 / 1474 loss=2.019, trans_loss=4.914, nll_loss=2.157, w2v_ctc_loss=0.749, task_loss=3.254, contrastive_loss=0.148, total=4168.97, n_correct=2743, ppl=4.46, accuracy=65.796, wps=12088.4, ups=1.45, wpb=8337.9, bsz=308.4, num_updates=24300, lr=9.07218e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=68, gb_free=16.7, wall=21357
2023-08-16 09:42:31 | INFO | train_inner | epoch 017:    828 / 1474 loss=2.006, trans_loss=4.909, nll_loss=2.151, w2v_ctc_loss=0.734, task_loss=3.315, contrastive_loss=0.109, total=4097.38, n_correct=2703.33, ppl=4.44, accuracy=65.977, wps=11900.7, ups=1.45, wpb=8194.8, bsz=297.3, num_updates=24400, lr=9.05357e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=68, gb_free=10.1, wall=21426
2023-08-16 09:43:39 | INFO | train_inner | epoch 017:    928 / 1474 loss=2.006, trans_loss=4.912, nll_loss=2.154, w2v_ctc_loss=0.734, task_loss=3.255, contrastive_loss=0.107, total=4105.01, n_correct=2704.98, ppl=4.45, accuracy=65.895, wps=12091.2, ups=1.47, wpb=8210, bsz=304.1, num_updates=24500, lr=9.03508e-05, gnorm=0.584, clip=0, loss_scale=16, train_wall=67, gb_free=16.4, wall=21494
2023-08-16 09:44:48 | INFO | train_inner | epoch 017:   1028 / 1474 loss=2.007, trans_loss=4.909, nll_loss=2.152, w2v_ctc_loss=0.737, task_loss=3.296, contrastive_loss=0.111, total=4105.88, n_correct=2707.05, ppl=4.44, accuracy=65.931, wps=11946.1, ups=1.45, wpb=8211.8, bsz=303.4, num_updates=24600, lr=9.0167e-05, gnorm=0.557, clip=0, loss_scale=16, train_wall=68, gb_free=15.6, wall=21563
2023-08-16 09:45:56 | INFO | train_inner | epoch 017:   1128 / 1474 loss=2.001, trans_loss=4.906, nll_loss=2.147, w2v_ctc_loss=0.724, task_loss=3.371, contrastive_loss=0.102, total=4095.58, n_correct=2708.93, ppl=4.43, accuracy=66.143, wps=11917.1, ups=1.45, wpb=8191.2, bsz=298.5, num_updates=24700, lr=8.99843e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=68, gb_free=15.4, wall=21631
2023-08-16 09:47:06 | INFO | train_inner | epoch 017:   1228 / 1474 loss=2.03, trans_loss=4.918, nll_loss=2.163, w2v_ctc_loss=0.727, task_loss=3.231, contrastive_loss=0.341, total=4162.14, n_correct=2734.36, ppl=4.48, accuracy=65.696, wps=11915.5, ups=1.43, wpb=8324.3, bsz=320.2, num_updates=24800, lr=8.98027e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=69, gb_free=15.9, wall=21701
2023-08-16 09:48:16 | INFO | train_inner | epoch 017:   1328 / 1474 loss=2.011, trans_loss=4.914, nll_loss=2.158, w2v_ctc_loss=0.721, task_loss=3.286, contrastive_loss=0.181, total=4149.03, n_correct=2734.38, ppl=4.46, accuracy=65.904, wps=11961.3, ups=1.44, wpb=8298.1, bsz=306.7, num_updates=24900, lr=8.96221e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=69, gb_free=16.6, wall=21771
2023-08-16 09:49:24 | INFO | train_inner | epoch 017:   1428 / 1474 loss=2.002, trans_loss=4.911, nll_loss=2.154, w2v_ctc_loss=0.728, task_loss=3.308, contrastive_loss=0.101, total=4117.13, n_correct=2717.16, ppl=4.45, accuracy=65.996, wps=11963.4, ups=1.45, wpb=8234.3, bsz=303.7, num_updates=25000, lr=8.94427e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=68, gb_free=17.2, wall=21840
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:0')
2023-08-16 09:49:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:2')
2023-08-16 09:50:20 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.973 | trans_loss 5.181 | nll_loss 2.45 | w2v_ctc_loss 1.31 | task_loss 11.581 | contrastive_loss 0.323 | total 4003.4 | n_correct 2646.8 | ppl 5.46 | accuracy 66.114 | uer 18.132 | wer 19.962 | raw_wer 19.962 | bleu 22.08 | wps 2163.1 | wpb 4003.4 | bsz 141.8 | num_updates 25046 | best_bleu 22.13
2023-08-16 09:50:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25046 updates
2023-08-16 09:50:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.0809.pt
2023-08-16 09:50:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.0809.pt
2023-08-16 09:50:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.0809.pt (epoch 17 @ 25046 updates, score 22.08) (writing took 21.882599029690027 seconds)
2023-08-16 09:50:42 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-16 09:50:42 | INFO | train | epoch 017 | loss 2.009 | trans_loss 4.909 | nll_loss 2.151 | w2v_ctc_loss 0.731 | task_loss 3.297 | contrastive_loss 0.155 | total 4138.65 | n_correct 2730.29 | ppl 4.44 | accuracy 65.971 | wps 10591.7 | ups 1.28 | wpb 8277.3 | bsz 305.7 | num_updates 25046 | lr 8.93605e-05 | gnorm 0.545 | clip 0 | loss_scale 32 | train_wall 1010 | gb_free 16.1 | wall 21917
2023-08-16 09:50:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 09:50:42 | INFO | fairseq.trainer | begin training epoch 18
2023-08-16 09:50:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 09:51:27 | INFO | train_inner | epoch 018:     54 / 1474 loss=2, trans_loss=4.9, nll_loss=2.139, w2v_ctc_loss=0.731, task_loss=3.355, contrastive_loss=0.111, total=4138.21, n_correct=2740.6, ppl=4.41, accuracy=66.227, wps=6737.8, ups=0.81, wpb=8276.4, bsz=303.2, num_updates=25100, lr=8.92644e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=69, gb_free=17.5, wall=21962
2023-08-16 09:52:36 | INFO | train_inner | epoch 018:    154 / 1474 loss=1.996, trans_loss=4.882, nll_loss=2.116, w2v_ctc_loss=0.703, task_loss=3.131, contrastive_loss=0.232, total=4158.88, n_correct=2766.68, ppl=4.33, accuracy=66.525, wps=12035.2, ups=1.45, wpb=8317.8, bsz=314, num_updates=25200, lr=8.90871e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=69, gb_free=16.5, wall=22032
2023-08-16 09:53:46 | INFO | train_inner | epoch 018:    254 / 1474 loss=1.987, trans_loss=4.885, nll_loss=2.119, w2v_ctc_loss=0.717, task_loss=3.194, contrastive_loss=0.101, total=4164.11, n_correct=2771.26, ppl=4.35, accuracy=66.551, wps=11995.8, ups=1.44, wpb=8328.2, bsz=312.5, num_updates=25300, lr=8.89108e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=69, gb_free=14.7, wall=22101
2023-08-16 09:54:55 | INFO | train_inner | epoch 018:    354 / 1474 loss=1.99, trans_loss=4.89, nll_loss=2.126, w2v_ctc_loss=0.711, task_loss=3.345, contrastive_loss=0.115, total=4163.13, n_correct=2764.7, ppl=4.36, accuracy=66.409, wps=12034.1, ups=1.45, wpb=8326.3, bsz=301.5, num_updates=25400, lr=8.87357e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=69, gb_free=17.3, wall=22170
2023-08-16 09:55:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-16 09:56:06 | INFO | train_inner | epoch 018:    455 / 1474 loss=2.008, trans_loss=4.899, nll_loss=2.137, w2v_ctc_loss=0.722, task_loss=3.564, contrastive_loss=0.206, total=4078.15, n_correct=2696.17, ppl=4.4, accuracy=66.113, wps=11529.7, ups=1.41, wpb=8156.3, bsz=292.7, num_updates=25500, lr=8.85615e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=70, gb_free=17.6, wall=22241
2023-08-16 09:57:18 | INFO | train_inner | epoch 018:    555 / 1474 loss=1.982, trans_loss=4.881, nll_loss=2.115, w2v_ctc_loss=0.706, task_loss=2.947, contrastive_loss=0.116, total=4218.07, n_correct=2812.27, ppl=4.33, accuracy=66.672, wps=11697.3, ups=1.39, wpb=8436.1, bsz=329.6, num_updates=25600, lr=8.83883e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=71, gb_free=15.6, wall=22313
2023-08-16 09:58:29 | INFO | train_inner | epoch 018:    655 / 1474 loss=2.011, trans_loss=4.904, nll_loss=2.144, w2v_ctc_loss=0.734, task_loss=3.405, contrastive_loss=0.186, total=4093.44, n_correct=2705.44, ppl=4.42, accuracy=66.092, wps=11500.9, ups=1.4, wpb=8186.9, bsz=298.5, num_updates=25700, lr=8.82162e-05, gnorm=0.566, clip=0, loss_scale=16, train_wall=71, gb_free=14.9, wall=22384
2023-08-16 09:59:40 | INFO | train_inner | epoch 018:    755 / 1474 loss=2.012, trans_loss=4.898, nll_loss=2.137, w2v_ctc_loss=0.727, task_loss=3.14, contrastive_loss=0.274, total=4202.99, n_correct=2785.61, ppl=4.4, accuracy=66.277, wps=11864.9, ups=1.41, wpb=8406, bsz=322.5, num_updates=25800, lr=8.80451e-05, gnorm=0.535, clip=0, loss_scale=16, train_wall=70, gb_free=17.3, wall=22455
2023-08-16 10:00:55 | INFO | train_inner | epoch 018:    855 / 1474 loss=1.997, trans_loss=4.896, nll_loss=2.134, w2v_ctc_loss=0.724, task_loss=3.315, contrastive_loss=0.101, total=4177.43, n_correct=2766.14, ppl=4.39, accuracy=66.216, wps=11097.1, ups=1.33, wpb=8354.9, bsz=304.8, num_updates=25900, lr=8.7875e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=75, gb_free=15.9, wall=22530
2023-08-16 10:02:08 | INFO | train_inner | epoch 018:    955 / 1474 loss=1.985, trans_loss=4.888, nll_loss=2.124, w2v_ctc_loss=0.706, task_loss=3.07, contrastive_loss=0.109, total=4138.23, n_correct=2750.9, ppl=4.36, accuracy=66.475, wps=11312.6, ups=1.37, wpb=8276.5, bsz=314.5, num_updates=26000, lr=8.77058e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=73, gb_free=16.3, wall=22604
2023-08-16 10:02:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 10:02:34 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.983 | trans_loss 5.185 | nll_loss 2.45 | w2v_ctc_loss 1.328 | task_loss 11.592 | contrastive_loss 0.33 | total 4003.4 | n_correct 2650.8 | ppl 5.46 | accuracy 66.214 | uer 18.52 | wer 20.312 | raw_wer 20.312 | bleu 21.77 | wps 2007.7 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 22.13
2023-08-16 10:02:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-16 10:02:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_18_26000.pt
2023-08-16 10:02:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_18_26000.pt
2023-08-16 10:02:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 21.77) (writing took 25.56634896993637 seconds)
2023-08-16 10:04:10 | INFO | train_inner | epoch 018:   1055 / 1474 loss=1.994, trans_loss=4.895, nll_loss=2.133, w2v_ctc_loss=0.717, task_loss=3.47, contrastive_loss=0.101, total=4133.59, n_correct=2737.33, ppl=4.39, accuracy=66.222, wps=6825.1, ups=0.83, wpb=8267.2, bsz=298.7, num_updates=26100, lr=8.75376e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=69, gb_free=15.6, wall=22725
2023-08-16 10:05:22 | INFO | train_inner | epoch 018:   1155 / 1474 loss=2, trans_loss=4.887, nll_loss=2.124, w2v_ctc_loss=0.719, task_loss=3.117, contrastive_loss=0.207, total=4154.22, n_correct=2758.75, ppl=4.36, accuracy=66.408, wps=11467.1, ups=1.38, wpb=8308.4, bsz=315.1, num_updates=26200, lr=8.73704e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=72, gb_free=14.6, wall=22797
2023-08-16 10:06:36 | INFO | train_inner | epoch 018:   1255 / 1474 loss=1.996, trans_loss=4.903, nll_loss=2.143, w2v_ctc_loss=0.719, task_loss=3.537, contrastive_loss=0.096, total=4089.17, n_correct=2703.19, ppl=4.42, accuracy=66.106, wps=11026.2, ups=1.35, wpb=8178.3, bsz=287.6, num_updates=26300, lr=8.72041e-05, gnorm=0.553, clip=0, loss_scale=16, train_wall=74, gb_free=16.4, wall=22871
2023-08-16 10:07:50 | INFO | train_inner | epoch 018:   1355 / 1474 loss=2.009, trans_loss=4.904, nll_loss=2.145, w2v_ctc_loss=0.742, task_loss=3.52, contrastive_loss=0.124, total=4068.84, n_correct=2687.74, ppl=4.42, accuracy=66.057, wps=11084.1, ups=1.36, wpb=8137.7, bsz=291.4, num_updates=26400, lr=8.70388e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=73, gb_free=15.1, wall=22945
2023-08-16 10:09:00 | INFO | train_inner | epoch 018:   1455 / 1474 loss=2.003, trans_loss=4.903, nll_loss=2.143, w2v_ctc_loss=0.734, task_loss=3.498, contrastive_loss=0.111, total=4113.23, n_correct=2720.63, ppl=4.42, accuracy=66.143, wps=11762.9, ups=1.43, wpb=8226.5, bsz=297.2, num_updates=26500, lr=8.68744e-05, gnorm=0.569, clip=0, loss_scale=16, train_wall=69, gb_free=16.3, wall=23015
2023-08-16 10:09:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 10:09:37 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.997 | trans_loss 5.183 | nll_loss 2.45 | w2v_ctc_loss 1.382 | task_loss 11.62 | contrastive_loss 0.325 | total 4003.4 | n_correct 2651.7 | ppl 5.46 | accuracy 66.236 | uer 18.217 | wer 20.227 | raw_wer 20.227 | bleu 22.08 | wps 2046.1 | wpb 4003.4 | bsz 141.8 | num_updates 26519 | best_bleu 22.13
2023-08-16 10:09:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26519 updates
2023-08-16 10:09:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.0803.pt
2023-08-16 10:09:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.0803.pt
2023-08-16 10:10:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.0803.pt (epoch 18 @ 26519 updates, score 22.08) (writing took 24.0322956033051 seconds)
2023-08-16 10:10:02 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-16 10:10:02 | INFO | train | epoch 018 | loss 1.998 | trans_loss 4.894 | nll_loss 2.131 | w2v_ctc_loss 0.72 | task_loss 3.297 | contrastive_loss 0.152 | total 4138.61 | n_correct 2744.08 | ppl 4.38 | accuracy 66.304 | wps 10515.5 | ups 1.27 | wpb 8277.2 | bsz 305.6 | num_updates 26519 | lr 8.68433e-05 | gnorm 0.544 | clip 0 | loss_scale 16 | train_wall 1043 | gb_free 15.7 | wall 23077
2023-08-16 10:10:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 10:10:02 | INFO | fairseq.trainer | begin training epoch 19
2023-08-16 10:10:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 10:11:08 | INFO | train_inner | epoch 019:     81 / 1474 loss=1.988, trans_loss=4.875, nll_loss=2.107, w2v_ctc_loss=0.709, task_loss=3.28, contrastive_loss=0.158, total=4107.26, n_correct=2737.59, ppl=4.31, accuracy=66.652, wps=6413.3, ups=0.78, wpb=8214.5, bsz=297.5, num_updates=26600, lr=8.6711e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=70, gb_free=12.4, wall=23143
2023-08-16 10:12:17 | INFO | train_inner | epoch 019:    181 / 1474 loss=1.991, trans_loss=4.875, nll_loss=2.108, w2v_ctc_loss=0.722, task_loss=3.068, contrastive_loss=0.151, total=4222.18, n_correct=2812.75, ppl=4.31, accuracy=66.618, wps=12143, ups=1.44, wpb=8444.4, bsz=324.4, num_updates=26700, lr=8.65485e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=69, gb_free=11.3, wall=23212
2023-08-16 10:13:26 | INFO | train_inner | epoch 019:    281 / 1474 loss=1.977, trans_loss=4.87, nll_loss=2.1, w2v_ctc_loss=0.707, task_loss=3.232, contrastive_loss=0.093, total=4187.37, n_correct=2796.4, ppl=4.29, accuracy=66.782, wps=12127.4, ups=1.45, wpb=8374.7, bsz=307, num_updates=26800, lr=8.63868e-05, gnorm=0.535, clip=0, loss_scale=16, train_wall=69, gb_free=17.3, wall=23281
2023-08-16 10:14:35 | INFO | train_inner | epoch 019:    381 / 1474 loss=1.989, trans_loss=4.871, nll_loss=2.102, w2v_ctc_loss=0.703, task_loss=3.252, contrastive_loss=0.2, total=4170.67, n_correct=2779.78, ppl=4.29, accuracy=66.651, wps=12101.2, ups=1.45, wpb=8341.3, bsz=310.5, num_updates=26900, lr=8.62261e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=68, gb_free=17, wall=23350
2023-08-16 10:15:44 | INFO | train_inner | epoch 019:    481 / 1474 loss=1.99, trans_loss=4.882, nll_loss=2.115, w2v_ctc_loss=0.722, task_loss=3.379, contrastive_loss=0.109, total=4115.22, n_correct=2737.39, ppl=4.33, accuracy=66.519, wps=11932.4, ups=1.45, wpb=8230.4, bsz=301.9, num_updates=27000, lr=8.60663e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=68, gb_free=14.8, wall=23419
2023-08-16 10:16:53 | INFO | train_inner | epoch 019:    581 / 1474 loss=1.983, trans_loss=4.872, nll_loss=2.104, w2v_ctc_loss=0.703, task_loss=3.233, contrastive_loss=0.173, total=4129.22, n_correct=2754.7, ppl=4.3, accuracy=66.712, wps=12036.2, ups=1.46, wpb=8258.4, bsz=306, num_updates=27100, lr=8.59074e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=68, gb_free=15.6, wall=23488
2023-08-16 10:18:02 | INFO | train_inner | epoch 019:    681 / 1474 loss=1.975, trans_loss=4.879, nll_loss=2.112, w2v_ctc_loss=0.697, task_loss=3.011, contrastive_loss=0.1, total=4197.2, n_correct=2803.1, ppl=4.32, accuracy=66.785, wps=12203, ups=1.45, wpb=8394.4, bsz=320.8, num_updates=27200, lr=8.57493e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=68, gb_free=14.3, wall=23557
2023-08-16 10:19:11 | INFO | train_inner | epoch 019:    781 / 1474 loss=1.986, trans_loss=4.878, nll_loss=2.111, w2v_ctc_loss=0.715, task_loss=3.316, contrastive_loss=0.114, total=4142.6, n_correct=2755.52, ppl=4.32, accuracy=66.517, wps=11973.1, ups=1.45, wpb=8285.2, bsz=305.1, num_updates=27300, lr=8.55921e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=69, gb_free=16, wall=23626
2023-08-16 10:20:20 | INFO | train_inner | epoch 019:    881 / 1474 loss=1.989, trans_loss=4.887, nll_loss=2.122, w2v_ctc_loss=0.718, task_loss=3.368, contrastive_loss=0.097, total=4153.47, n_correct=2758.65, ppl=4.35, accuracy=66.418, wps=11973.7, ups=1.44, wpb=8306.9, bsz=303.1, num_updates=27400, lr=8.54358e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=69, gb_free=15.9, wall=23695
2023-08-16 10:21:30 | INFO | train_inner | epoch 019:    981 / 1474 loss=2.01, trans_loss=4.892, nll_loss=2.13, w2v_ctc_loss=0.711, task_loss=3.287, contrastive_loss=0.331, total=4101.29, n_correct=2722.51, ppl=4.38, accuracy=66.382, wps=11694.5, ups=1.43, wpb=8202.6, bsz=309.9, num_updates=27500, lr=8.52803e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=70, gb_free=16.4, wall=23765
2023-08-16 10:22:28 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-16 10:22:40 | INFO | train_inner | epoch 019:   1082 / 1474 loss=1.99, trans_loss=4.888, nll_loss=2.124, w2v_ctc_loss=0.708, task_loss=3.508, contrastive_loss=0.137, total=4045.08, n_correct=2687.64, ppl=4.36, accuracy=66.442, wps=11666.7, ups=1.44, wpb=8090.2, bsz=292.4, num_updates=27600, lr=8.51257e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=69, gb_free=15.5, wall=23835
2023-08-16 10:23:49 | INFO | train_inner | epoch 019:   1182 / 1474 loss=2.005, trans_loss=4.889, nll_loss=2.125, w2v_ctc_loss=0.721, task_loss=3.375, contrastive_loss=0.223, total=4129.82, n_correct=2737.6, ppl=4.36, accuracy=66.289, wps=11921.6, ups=1.44, wpb=8259.6, bsz=305.9, num_updates=27700, lr=8.49719e-05, gnorm=0.561, clip=0, loss_scale=16, train_wall=69, gb_free=17.5, wall=23904
2023-08-16 10:24:58 | INFO | train_inner | epoch 019:   1282 / 1474 loss=1.989, trans_loss=4.886, nll_loss=2.121, w2v_ctc_loss=0.711, task_loss=3.314, contrastive_loss=0.12, total=4147.96, n_correct=2759.6, ppl=4.35, accuracy=66.529, wps=12060.9, ups=1.45, wpb=8295.9, bsz=301.4, num_updates=27800, lr=8.48189e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=68, gb_free=15.6, wall=23973
2023-08-16 10:26:07 | INFO | train_inner | epoch 019:   1382 / 1474 loss=1.986, trans_loss=4.882, nll_loss=2.116, w2v_ctc_loss=0.714, task_loss=3.368, contrastive_loss=0.107, total=4125.32, n_correct=2745.15, ppl=4.34, accuracy=66.544, wps=11979.9, ups=1.45, wpb=8250.6, bsz=300.4, num_updates=27900, lr=8.46668e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=68, gb_free=16.2, wall=24042
2023-08-16 10:27:11 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 10:27:34 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.974 | trans_loss 5.172 | nll_loss 2.435 | w2v_ctc_loss 1.337 | task_loss 11.613 | contrastive_loss 0.318 | total 4003.4 | n_correct 2663 | ppl 5.41 | accuracy 66.518 | uer 18.026 | wer 19.943 | raw_wer 19.943 | bleu 21.89 | wps 2211.2 | wpb 4003.4 | bsz 141.8 | num_updates 27992 | best_bleu 22.13
2023-08-16 10:27:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27992 updates
2023-08-16 10:27:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.8904.pt
2023-08-16 10:27:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.8904.pt
2023-08-16 10:27:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_21.8904.pt (epoch 19 @ 27992 updates, score 21.89) (writing took 24.516689993441105 seconds)
2023-08-16 10:27:59 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-16 10:27:59 | INFO | train | epoch 019 | loss 1.989 | trans_loss 4.88 | nll_loss 2.113 | w2v_ctc_loss 0.712 | task_loss 3.293 | contrastive_loss 0.15 | total 4139.01 | n_correct 2755.48 | ppl 4.33 | accuracy 66.573 | wps 11319.8 | ups 1.37 | wpb 8278 | bsz 305.8 | num_updates 27992 | lr 8.45275e-05 | gnorm 0.543 | clip 0 | loss_scale 16 | train_wall 1012 | gb_free 17.1 | wall 24154
2023-08-16 10:27:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 10:27:59 | INFO | fairseq.trainer | begin training epoch 20
2023-08-16 10:27:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 10:28:13 | INFO | train_inner | epoch 020:      8 / 1474 loss=1.987, trans_loss=4.873, nll_loss=2.105, w2v_ctc_loss=0.706, task_loss=3.322, contrastive_loss=0.186, total=4124.63, n_correct=2752.42, ppl=4.3, accuracy=66.731, wps=6545.3, ups=0.79, wpb=8249.3, bsz=304.8, num_updates=28000, lr=8.45154e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=69, gb_free=17.1, wall=24168
2023-08-16 10:28:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 10:28:35 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.963 | trans_loss 5.177 | nll_loss 2.442 | w2v_ctc_loss 1.292 | task_loss 11.652 | contrastive_loss 0.314 | total 4003.4 | n_correct 2659.9 | ppl 5.43 | accuracy 66.441 | uer 17.925 | wer 19.779 | raw_wer 19.779 | bleu 21.76 | wps 2254.6 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 22.13
2023-08-16 10:28:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-16 10:28:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_20_28000.pt
2023-08-16 10:28:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_20_28000.pt
2023-08-16 10:29:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 21.76) (writing took 38.66642082110047 seconds)
2023-08-16 10:30:24 | INFO | train_inner | epoch 020:    108 / 1474 loss=1.966, trans_loss=4.855, nll_loss=2.081, w2v_ctc_loss=0.691, task_loss=3.184, contrastive_loss=0.113, total=4199.19, n_correct=2821.22, ppl=4.23, accuracy=67.185, wps=6407.5, ups=0.76, wpb=8398.4, bsz=314, num_updates=28100, lr=8.43649e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=69, gb_free=14.5, wall=24299
2023-08-16 10:31:33 | INFO | train_inner | epoch 020:    208 / 1474 loss=1.981, trans_loss=4.865, nll_loss=2.093, w2v_ctc_loss=0.705, task_loss=3.42, contrastive_loss=0.165, total=4148.29, n_correct=2772.91, ppl=4.27, accuracy=66.845, wps=11913.2, ups=1.44, wpb=8296.6, bsz=300.5, num_updates=28200, lr=8.42152e-05, gnorm=0.564, clip=0, loss_scale=16, train_wall=69, gb_free=15.3, wall=24368
2023-08-16 10:32:42 | INFO | train_inner | epoch 020:    308 / 1474 loss=1.967, trans_loss=4.859, nll_loss=2.087, w2v_ctc_loss=0.697, task_loss=2.984, contrastive_loss=0.103, total=4191.34, n_correct=2812.47, ppl=4.25, accuracy=67.102, wps=12169.8, ups=1.45, wpb=8382.7, bsz=326.2, num_updates=28300, lr=8.40663e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=68, gb_free=15.4, wall=24437
2023-08-16 10:33:51 | INFO | train_inner | epoch 020:    408 / 1474 loss=1.969, trans_loss=4.856, nll_loss=2.082, w2v_ctc_loss=0.696, task_loss=3.342, contrastive_loss=0.099, total=4114.19, n_correct=2759.62, ppl=4.23, accuracy=67.076, wps=11999, ups=1.46, wpb=8228.4, bsz=297.5, num_updates=28400, lr=8.39181e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=68, gb_free=15, wall=24506
2023-08-16 10:35:00 | INFO | train_inner | epoch 020:    508 / 1474 loss=1.979, trans_loss=4.867, nll_loss=2.097, w2v_ctc_loss=0.691, task_loss=3.386, contrastive_loss=0.187, total=4108.2, n_correct=2747.51, ppl=4.28, accuracy=66.879, wps=11890.4, ups=1.45, wpb=8216.4, bsz=299.5, num_updates=28500, lr=8.37708e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=69, gb_free=15.4, wall=24575
2023-08-16 10:35:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-16 10:36:10 | INFO | train_inner | epoch 020:    609 / 1474 loss=1.988, trans_loss=4.867, nll_loss=2.097, w2v_ctc_loss=0.705, task_loss=3.487, contrastive_loss=0.191, total=4090.94, n_correct=2736.6, ppl=4.28, accuracy=66.894, wps=11726.1, ups=1.43, wpb=8181.9, bsz=295, num_updates=28600, lr=8.36242e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=69, gb_free=11.9, wall=24645
2023-08-16 10:37:19 | INFO | train_inner | epoch 020:    709 / 1474 loss=1.98, trans_loss=4.872, nll_loss=2.103, w2v_ctc_loss=0.713, task_loss=3.29, contrastive_loss=0.095, total=4134.98, n_correct=2759.4, ppl=4.3, accuracy=66.733, wps=12018.7, ups=1.45, wpb=8270, bsz=300.5, num_updates=28700, lr=8.34784e-05, gnorm=0.537, clip=0, loss_scale=8, train_wall=68, gb_free=13.8, wall=24714
2023-08-16 10:38:28 | INFO | train_inner | epoch 020:    809 / 1474 loss=1.977, trans_loss=4.871, nll_loss=2.102, w2v_ctc_loss=0.706, task_loss=3.263, contrastive_loss=0.099, total=4145.82, n_correct=2773.01, ppl=4.29, accuracy=66.887, wps=11992.9, ups=1.45, wpb=8291.6, bsz=306.6, num_updates=28800, lr=8.33333e-05, gnorm=0.539, clip=0, loss_scale=8, train_wall=68, gb_free=12.1, wall=24783
2023-08-16 10:39:37 | INFO | train_inner | epoch 020:    909 / 1474 loss=2.008, trans_loss=4.877, nll_loss=2.11, w2v_ctc_loss=0.703, task_loss=3.119, contrastive_loss=0.388, total=4161.77, n_correct=2769.01, ppl=4.32, accuracy=66.534, wps=11942.1, ups=1.43, wpb=8323.5, bsz=324.4, num_updates=28900, lr=8.3189e-05, gnorm=0.541, clip=0, loss_scale=8, train_wall=69, gb_free=16.2, wall=24852
2023-08-16 10:40:47 | INFO | train_inner | epoch 020:   1009 / 1474 loss=1.974, trans_loss=4.87, nll_loss=2.101, w2v_ctc_loss=0.695, task_loss=3.299, contrastive_loss=0.103, total=4167.85, n_correct=2784.72, ppl=4.29, accuracy=66.814, wps=11945.2, ups=1.43, wpb=8335.7, bsz=306.3, num_updates=29000, lr=8.30455e-05, gnorm=0.54, clip=0, loss_scale=8, train_wall=69, gb_free=16.6, wall=24922
2023-08-16 10:41:56 | INFO | train_inner | epoch 020:   1109 / 1474 loss=1.993, trans_loss=4.872, nll_loss=2.104, w2v_ctc_loss=0.704, task_loss=3.174, contrastive_loss=0.242, total=4169.06, n_correct=2782.81, ppl=4.3, accuracy=66.749, wps=12054.1, ups=1.45, wpb=8338.1, bsz=315.8, num_updates=29100, lr=8.29027e-05, gnorm=0.572, clip=0, loss_scale=8, train_wall=69, gb_free=16.5, wall=24991
2023-08-16 10:43:06 | INFO | train_inner | epoch 020:   1209 / 1474 loss=1.981, trans_loss=4.865, nll_loss=2.094, w2v_ctc_loss=0.716, task_loss=3.665, contrastive_loss=0.091, total=4023.64, n_correct=2684.75, ppl=4.27, accuracy=66.724, wps=11613.6, ups=1.44, wpb=8047.3, bsz=282.8, num_updates=29200, lr=8.27606e-05, gnorm=0.56, clip=0, loss_scale=8, train_wall=69, gb_free=15.6, wall=25061
2023-08-16 10:44:15 | INFO | train_inner | epoch 020:   1309 / 1474 loss=1.977, trans_loss=4.872, nll_loss=2.104, w2v_ctc_loss=0.703, task_loss=3.455, contrastive_loss=0.097, total=4128.46, n_correct=2756.31, ppl=4.3, accuracy=66.764, wps=11950.5, ups=1.45, wpb=8256.9, bsz=299.2, num_updates=29300, lr=8.26192e-05, gnorm=0.55, clip=0, loss_scale=8, train_wall=69, gb_free=16.3, wall=25130
2023-08-16 10:45:24 | INFO | train_inner | epoch 020:   1409 / 1474 loss=1.98, trans_loss=4.873, nll_loss=2.104, w2v_ctc_loss=0.709, task_loss=3.479, contrastive_loss=0.093, total=4120.53, n_correct=2750.03, ppl=4.3, accuracy=66.74, wps=11874.4, ups=1.44, wpb=8241.1, bsz=294.2, num_updates=29400, lr=8.24786e-05, gnorm=0.544, clip=0, loss_scale=8, train_wall=69, gb_free=15.5, wall=25199
2023-08-16 10:46:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 10:46:32 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.958 | trans_loss 5.178 | nll_loss 2.44 | w2v_ctc_loss 1.273 | task_loss 11.608 | contrastive_loss 0.311 | total 4003.4 | n_correct 2657 | ppl 5.43 | accuracy 66.369 | uer 17.663 | wer 19.608 | raw_wer 19.608 | bleu 22.28 | wps 2142.8 | wpb 4003.4 | bsz 141.8 | num_updates 29465 | best_bleu 22.28
2023-08-16 10:46:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29465 updates
2023-08-16 10:46:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 10:46:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 10:47:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 20 @ 29465 updates, score 22.28) (writing took 31.80751238577068 seconds)
2023-08-16 10:47:05 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-16 10:47:05 | INFO | train | epoch 020 | loss 1.98 | trans_loss 4.867 | nll_loss 2.097 | w2v_ctc_loss 0.702 | task_loss 3.3 | contrastive_loss 0.148 | total 4138.46 | n_correct 2766.74 | ppl 4.28 | accuracy 66.854 | wps 10639.1 | ups 1.29 | wpb 8276.9 | bsz 305.6 | num_updates 29465 | lr 8.23876e-05 | gnorm 0.547 | clip 0 | loss_scale 8 | train_wall 1011 | gb_free 15.9 | wall 25300
2023-08-16 10:47:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 10:47:05 | INFO | fairseq.trainer | begin training epoch 21
2023-08-16 10:47:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 10:47:37 | INFO | train_inner | epoch 021:     35 / 1474 loss=1.982, trans_loss=4.867, nll_loss=2.098, w2v_ctc_loss=0.694, task_loss=3.152, contrastive_loss=0.215, total=4145.63, n_correct=2772.55, ppl=4.28, accuracy=66.879, wps=6237.7, ups=0.75, wpb=8291.3, bsz=315.4, num_updates=29500, lr=8.23387e-05, gnorm=0.547, clip=0, loss_scale=8, train_wall=69, gb_free=16.5, wall=25332
2023-08-16 10:48:46 | INFO | train_inner | epoch 021:    135 / 1474 loss=1.973, trans_loss=4.847, nll_loss=2.071, w2v_ctc_loss=0.691, task_loss=3.087, contrastive_loss=0.206, total=4194.57, n_correct=2820.32, ppl=4.2, accuracy=67.237, wps=12167.7, ups=1.45, wpb=8389.1, bsz=319.6, num_updates=29600, lr=8.21995e-05, gnorm=0.539, clip=0, loss_scale=8, train_wall=68, gb_free=16.2, wall=25401
2023-08-16 10:49:55 | INFO | train_inner | epoch 021:    235 / 1474 loss=1.962, trans_loss=4.848, nll_loss=2.072, w2v_ctc_loss=0.682, task_loss=3.169, contrastive_loss=0.16, total=4152.42, n_correct=2794.88, ppl=4.2, accuracy=67.307, wps=12121.6, ups=1.46, wpb=8304.8, bsz=312, num_updates=29700, lr=8.2061e-05, gnorm=0.536, clip=0, loss_scale=8, train_wall=68, gb_free=17.1, wall=25470
2023-08-16 10:51:04 | INFO | train_inner | epoch 021:    335 / 1474 loss=1.974, trans_loss=4.855, nll_loss=2.081, w2v_ctc_loss=0.7, task_loss=3.271, contrastive_loss=0.164, total=4157.2, n_correct=2788.88, ppl=4.23, accuracy=67.086, wps=11903.6, ups=1.43, wpb=8314.4, bsz=311.1, num_updates=29800, lr=8.19232e-05, gnorm=0.534, clip=0, loss_scale=8, train_wall=69, gb_free=14.9, wall=25539
2023-08-16 10:52:14 | INFO | train_inner | epoch 021:    435 / 1474 loss=1.958, trans_loss=4.848, nll_loss=2.072, w2v_ctc_loss=0.685, task_loss=3.166, contrastive_loss=0.087, total=4181.07, n_correct=2817.32, ppl=4.21, accuracy=67.383, wps=12082.4, ups=1.44, wpb=8362.1, bsz=308.2, num_updates=29900, lr=8.17861e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=69, gb_free=14.7, wall=25609
2023-08-16 10:53:23 | INFO | train_inner | epoch 021:    535 / 1474 loss=1.958, trans_loss=4.841, nll_loss=2.063, w2v_ctc_loss=0.691, task_loss=3.412, contrastive_loss=0.085, total=4089.72, n_correct=2756.7, ppl=4.18, accuracy=67.406, wps=11861.2, ups=1.45, wpb=8179.4, bsz=295.7, num_updates=30000, lr=8.16497e-05, gnorm=0.538, clip=0, loss_scale=8, train_wall=68, gb_free=14.1, wall=25678
2023-08-16 10:53:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-16 10:53:46 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.971 | trans_loss 5.176 | nll_loss 2.439 | w2v_ctc_loss 1.322 | task_loss 11.578 | contrastive_loss 0.308 | total 4003.4 | n_correct 2665.5 | ppl 5.42 | accuracy 66.581 | uer 17.928 | wer 19.686 | raw_wer 19.686 | bleu 22.28 | wps 2158.9 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 22.28
2023-08-16 10:53:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-16 10:53:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_21_30000.pt
2023-08-16 10:53:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_21_30000.pt
2023-08-16 10:54:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 22.28) (writing took 32.24819314852357 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:0')
2023-08-16 10:55:29 | INFO | train_inner | epoch 021:    635 / 1474 loss=1.98, trans_loss=4.852, nll_loss=2.078, w2v_ctc_loss=0.693, task_loss=3.27, contrastive_loss=0.259, total=4210.28, n_correct=2824.47, ppl=4.22, accuracy=67.085, wps=6659.9, ups=0.79, wpb=8420.6, bsz=315.7, num_updates=30100, lr=8.15139e-05, gnorm=0.56, clip=0, loss_scale=8, train_wall=69, gb_free=16.4, wall=25804
2023-08-16 10:56:38 | INFO | train_inner | epoch 021:    735 / 1474 loss=1.97, trans_loss=4.86, nll_loss=2.088, w2v_ctc_loss=0.692, task_loss=3.31, contrastive_loss=0.119, total=4149.01, n_correct=2781.79, ppl=4.25, accuracy=67.047, wps=11967.1, ups=1.44, wpb=8298, bsz=307.4, num_updates=30200, lr=8.13788e-05, gnorm=0.585, clip=0, loss_scale=8, train_wall=69, gb_free=16.3, wall=25873
2023-08-16 10:57:48 | INFO | train_inner | epoch 021:    835 / 1474 loss=1.977, trans_loss=4.865, nll_loss=2.094, w2v_ctc_loss=0.698, task_loss=3.473, contrastive_loss=0.132, total=4075.99, n_correct=2726.57, ppl=4.27, accuracy=66.893, wps=11714.6, ups=1.44, wpb=8152, bsz=295.7, num_updates=30300, lr=8.12444e-05, gnorm=0.554, clip=0, loss_scale=8, train_wall=69, gb_free=16, wall=25943
2023-08-16 10:58:57 | INFO | train_inner | epoch 021:    935 / 1474 loss=1.965, trans_loss=4.852, nll_loss=2.078, w2v_ctc_loss=0.693, task_loss=3.308, contrastive_loss=0.104, total=4091.88, n_correct=2748.16, ppl=4.22, accuracy=67.161, wps=11926.7, ups=1.46, wpb=8183.8, bsz=300, num_updates=30400, lr=8.11107e-05, gnorm=0.525, clip=0, loss_scale=8, train_wall=68, gb_free=16.8, wall=26012
2023-08-16 11:00:05 | INFO | train_inner | epoch 021:   1035 / 1474 loss=1.969, trans_loss=4.861, nll_loss=2.089, w2v_ctc_loss=0.696, task_loss=3.346, contrastive_loss=0.101, total=4107.66, n_correct=2751.46, ppl=4.25, accuracy=66.984, wps=11936.8, ups=1.45, wpb=8215.3, bsz=299.5, num_updates=30500, lr=8.09776e-05, gnorm=0.537, clip=0, loss_scale=8, train_wall=68, gb_free=14.3, wall=26080
2023-08-16 11:01:14 | INFO | train_inner | epoch 021:   1135 / 1474 loss=1.969, trans_loss=4.854, nll_loss=2.08, w2v_ctc_loss=0.697, task_loss=3.537, contrastive_loss=0.105, total=4118.94, n_correct=2765.29, ppl=4.23, accuracy=67.136, wps=11944.7, ups=1.45, wpb=8237.9, bsz=294.9, num_updates=30600, lr=8.08452e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=68, gb_free=15.6, wall=26149
2023-08-16 11:02:23 | INFO | train_inner | epoch 021:   1235 / 1474 loss=1.972, trans_loss=4.856, nll_loss=2.084, w2v_ctc_loss=0.694, task_loss=3.156, contrastive_loss=0.158, total=4151.84, n_correct=2788.93, ppl=4.24, accuracy=67.173, wps=12072.2, ups=1.45, wpb=8303.7, bsz=309.1, num_updates=30700, lr=8.07134e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=68, gb_free=14.8, wall=26218
2023-08-16 11:03:32 | INFO | train_inner | epoch 021:   1335 / 1474 loss=1.965, trans_loss=4.854, nll_loss=2.082, w2v_ctc_loss=0.689, task_loss=3.187, contrastive_loss=0.117, total=4145.91, n_correct=2789.55, ppl=4.23, accuracy=67.284, wps=12032.4, ups=1.45, wpb=8291.8, bsz=312.1, num_updates=30800, lr=8.05823e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=68, gb_free=16, wall=26287
2023-08-16 11:04:42 | INFO | train_inner | epoch 021:   1435 / 1474 loss=1.986, trans_loss=4.865, nll_loss=2.095, w2v_ctc_loss=0.713, task_loss=3.461, contrastive_loss=0.169, total=4136.27, n_correct=2762.24, ppl=4.27, accuracy=66.781, wps=11878.4, ups=1.44, wpb=8272.5, bsz=304.5, num_updates=30900, lr=8.04518e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=69, gb_free=16.3, wall=26357
2023-08-16 11:05:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2448, device='cuda:4')
2023-08-16 11:05:33 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.961 | trans_loss 5.166 | nll_loss 2.428 | w2v_ctc_loss 1.307 | task_loss 11.587 | contrastive_loss 0.319 | total 4003.4 | n_correct 2671.4 | ppl 5.38 | accuracy 66.728 | uer 17.984 | wer 19.872 | raw_wer 19.872 | bleu 22.43 | wps 2178.8 | wpb 4003.4 | bsz 141.8 | num_updates 30939 | best_bleu 22.43
2023-08-16 11:05:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30939 updates
2023-08-16 11:05:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 11:05:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-16 11:06:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 21 @ 30939 updates, score 22.43) (writing took 30.311263406649232 seconds)
2023-08-16 11:06:03 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-16 11:06:03 | INFO | train | epoch 021 | loss 1.97 | trans_loss 4.854 | nll_loss 2.081 | w2v_ctc_loss 0.694 | task_loss 3.3 | contrastive_loss 0.145 | total 4138.65 | n_correct 2778.62 | ppl 4.23 | accuracy 67.138 | wps 10714.7 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 30939 | lr 8.04011e-05 | gnorm 0.544 | clip 0 | loss_scale 16 | train_wall 1012 | gb_free 15.1 | wall 26439
2023-08-16 11:06:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-16 11:06:04 | INFO | fairseq.trainer | begin training epoch 22
2023-08-16 11:06:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-16 11:06:53 | INFO | train_inner | epoch 022:     61 / 1474 loss=1.959, trans_loss=4.841, nll_loss=2.063, w2v_ctc_loss=0.69, task_loss=3.311, contrastive_loss=0.089, total=4133.81, n_correct=2791.52, ppl=4.18, accuracy=67.529, wps=6319.7, ups=0.76, wpb=8267.6, bsz=300.5, num_updates=31000, lr=8.03219e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=69, gb_free=16.1, wall=26488
2023-08-16 11:08:02 | INFO | train_inner | epoch 022:    161 / 1474 loss=1.966, trans_loss=4.84, nll_loss=2.061, w2v_ctc_loss=0.691, task_loss=3.373, contrastive_loss=0.167, total=4116.11, n_correct=2772.07, ppl=4.17, accuracy=67.347, wps=11862.6, ups=1.44, wpb=8232.2, bsz=306.9, num_updates=31100, lr=8.01927e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=69, gb_free=16.4, wall=26557
2023-08-16 11:09:11 | INFO | train_inner | epoch 022:    261 / 1474 loss=1.95, trans_loss=4.834, nll_loss=2.054, w2v_ctc_loss=0.678, task_loss=2.885, contrastive_loss=0.112, total=4272.11, n_correct=2890.38, ppl=4.15, accuracy=67.657, wps=12364.4, ups=1.45, wpb=8544.2, bsz=331.4, num_updates=31200, lr=8.00641e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=68, gb_free=17.6, wall=26626
2023-08-16 11:10:21 | INFO | train_inner | epoch 022:    361 / 1474 loss=1.981, trans_loss=4.847, nll_loss=2.071, w2v_ctc_loss=0.691, task_loss=3.356, contrastive_loss=0.266, total=4178.4, n_correct=2806.7, ppl=4.2, accuracy=67.172, wps=11879, ups=1.42, wpb=8356.8, bsz=310, num_updates=31300, lr=7.99361e-05, gnorm=0.561, clip=0, loss_scale=16, train_wall=70, gb_free=15.1, wall=26696
2023-08-16 11:11:30 | INFO | train_inner | epoch 022:    461 / 1474 loss=1.969, trans_loss=4.845, nll_loss=2.067, w2v_ctc_loss=0.69, task_loss=3.462, contrastive_loss=0.152, total=4132.96, n_correct=2782.04, ppl=4.19, accuracy=67.313, wps=12003.9, ups=1.45, wpb=8265.9, bsz=297.5, num_updates=31400, lr=7.98087e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=68, gb_free=16.7, wall=26765
2023-08-16 11:12:40 | INFO | train_inner | epoch 022:    561 / 1474 loss=1.953, trans_loss=4.836, nll_loss=2.057, w2v_ctc_loss=0.681, task_loss=3.301, contrastive_loss=0.098, total=4158.17, n_correct=2807.11, ppl=4.16, accuracy=67.508, wps=11974.5, ups=1.44, wpb=8316.3, bsz=307.8, num_updates=31500, lr=7.96819e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=69, gb_free=16.5, wall=26835
2023-08-16 11:13:48 | INFO | train_inner | epoch 022:    661 / 1474 loss=1.956, trans_loss=4.833, nll_loss=2.053, w2v_ctc_loss=0.674, task_loss=3.143, contrastive_loss=0.18, total=4139.66, n_correct=2799.84, ppl=4.15, accuracy=67.635, wps=12145.6, ups=1.47, wpb=8279.3, bsz=311.1, num_updates=31600, lr=7.95557e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=68, gb_free=15.9, wall=26903
2023-08-16 11:14:58 | INFO | train_inner | epoch 022:    761 / 1474 loss=1.959, trans_loss=4.838, nll_loss=2.058, w2v_ctc_loss=0.691, task_loss=3.384, contrastive_loss=0.1, total=4167.89, n_correct=2811.14, ppl=4.16, accuracy=67.448, wps=11972.3, ups=1.44, wpb=8335.8, bsz=303.9, num_updates=31700, lr=7.94301e-05, gnorm=0.586, clip=0, loss_scale=16, train_wall=69, gb_free=12.4, wall=26973
2023-08-16 11:16:07 | INFO | train_inner | epoch 022:    861 / 1474 loss=1.963, trans_loss=4.85, nll_loss=2.075, w2v_ctc_loss=0.693, task_loss=3.576, contrastive_loss=0.086, total=4075.79, n_correct=2735.09, ppl=4.21, accuracy=67.106, wps=11760.5, ups=1.44, wpb=8151.6, bsz=289, num_updates=31800, lr=7.93052e-05, gnorm=0.561, clip=0, loss_scale=16, train_wall=69, gb_free=16, wall=27042
2023-08-16 11:17:16 | INFO | train_inner | epoch 022:    961 / 1474 loss=1.955, trans_loss=4.84, nll_loss=2.063, w2v_ctc_loss=0.686, task_loss=3.322, contrastive_loss=0.087, total=4134.72, n_correct=2788.31, ppl=4.18, accuracy=67.436, wps=11950, ups=1.45, wpb=8269.4, bsz=303.2, num_updates=31900, lr=7.91808e-05, gnorm=0.575, clip=0, loss_scale=16, train_wall=69, gb_free=13.9, wall=27111
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 240, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 198, in start_processes
    while not context.join():
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 3 terminated with signal SIGKILL
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1049 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-08-17 02:07:18 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:11078
2023-08-17 02:07:18 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:11078
2023-08-17 02:07:18 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:11078
2023-08-17 02:07:18 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:11078
2023-08-17 02:07:18 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:11078
2023-08-17 02:07:18 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:11078
2023-08-17 02:07:18 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:11078
2023-08-17 02:07:18 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:11078
2023-08-17 02:07:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 02:07:19 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-17 02:07:19 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 02:07:19 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-17 02:07:19 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 02:07:19 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 02:07:19 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-17 02:07:19 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-17 02:07:19 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-17 02:07:19 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-17 02:07:24 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:11078', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=2.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=2.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=True, at_scale=2.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-17 02:07:24 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-17 02:07:24 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-17 02:07:24 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-17 02:07:24 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-17 02:07:24 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 02:07:29 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-17 02:07:29 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-17 02:07:29 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-17 02:07:31 | INFO | root | load pretrained hubert
2023-08-17 02:07:33 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-17 02:07:34 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 02:07:35 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-17 02:07:35 | INFO | root | share the sematic adapter and textual encoder
2023-08-17 02:07:35 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-17 02:07:35 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-17 02:07:35 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-17 02:07:35 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-17 02:07:35 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-17 02:07:35 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-17 02:07:35 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 02:07:35 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 02:07:35 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 02:07:36 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 02:07:40 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-17 02:07:41 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-17 02:07:41 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-17 02:07:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 02:07:42 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 02:07:42 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 02:07:42 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 02:07:42 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 02:07:42 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 02:07:42 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 02:07:42 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 02:07:42 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-17 02:07:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-17 02:07:42 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-17 02:07:42 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-17 02:07:42 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_last.pt
2023-08-17 02:07:46 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
2023-08-17 02:07:46 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-17 02:07:47 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_last.pt (epoch 22 @ 30939 updates)
2023-08-17 02:07:47 | INFO | fairseq.trainer | loading train data for epoch 22
2023-08-17 02:07:47 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-17 02:07:47 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 02:07:47 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-17 02:07:52 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-17 02:07:56 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
2023-08-17 02:08:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 02:08:46 | INFO | fairseq.trainer | begin training epoch 22
2023-08-17 02:08:46 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
2023-08-17 02:09:47 | INFO | train_inner | epoch 022:     61 / 1474 loss=1.964, trans_loss=4.838, nll_loss=2.06, w2v_ctc_loss=0.703, task_loss=3.319, contrastive_loss=0.091, total=4139.28, n_correct=2796.31, ppl=4.17, accuracy=67.556, wps=11482.1, ups=1.39, wpb=8278.6, bsz=300, num_updates=31000, lr=8.03219e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=52, gb_free=16.1, wall=125
2023-08-17 02:10:58 | INFO | train_inner | epoch 022:    161 / 1474 loss=1.964, trans_loss=4.838, nll_loss=2.059, w2v_ctc_loss=0.69, task_loss=3.371, contrastive_loss=0.165, total=4116.11, n_correct=2776.64, ppl=4.17, accuracy=67.458, wps=11528.6, ups=1.4, wpb=8232.2, bsz=306.9, num_updates=31100, lr=8.01927e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=71, gb_free=16.4, wall=196
2023-08-17 02:12:09 | INFO | train_inner | epoch 022:    261 / 1474 loss=1.947, trans_loss=4.833, nll_loss=2.054, w2v_ctc_loss=0.671, task_loss=2.883, contrastive_loss=0.111, total=4272.11, n_correct=2894.42, ppl=4.15, accuracy=67.752, wps=12101.3, ups=1.42, wpb=8544.2, bsz=331.4, num_updates=31200, lr=8.00641e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=70, gb_free=17.6, wall=267
2023-08-17 02:13:20 | INFO | train_inner | epoch 022:    361 / 1474 loss=1.982, trans_loss=4.847, nll_loss=2.071, w2v_ctc_loss=0.691, task_loss=3.351, contrastive_loss=0.268, total=4178.4, n_correct=2805.51, ppl=4.2, accuracy=67.143, wps=11687.2, ups=1.4, wpb=8356.8, bsz=310, num_updates=31300, lr=7.99361e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=71, gb_free=15.1, wall=338
2023-08-17 02:14:32 | INFO | train_inner | epoch 022:    461 / 1474 loss=1.971, trans_loss=4.846, nll_loss=2.069, w2v_ctc_loss=0.695, task_loss=3.466, contrastive_loss=0.151, total=4132.96, n_correct=2779.55, ppl=4.2, accuracy=67.253, wps=11637.9, ups=1.41, wpb=8265.9, bsz=297.5, num_updates=31400, lr=7.98087e-05, gnorm=0.556, clip=0, loss_scale=16, train_wall=71, gb_free=16.7, wall=410
2023-08-17 02:15:42 | INFO | train_inner | epoch 022:    561 / 1474 loss=1.958, trans_loss=4.84, nll_loss=2.061, w2v_ctc_loss=0.69, task_loss=3.306, contrastive_loss=0.099, total=4158.17, n_correct=2806.51, ppl=4.17, accuracy=67.494, wps=11768.5, ups=1.42, wpb=8316.3, bsz=307.8, num_updates=31500, lr=7.96819e-05, gnorm=0.571, clip=0, loss_scale=16, train_wall=70, gb_free=16.5, wall=480
2023-08-17 02:16:52 | INFO | train_inner | epoch 022:    661 / 1474 loss=1.959, trans_loss=4.836, nll_loss=2.057, w2v_ctc_loss=0.678, task_loss=3.148, contrastive_loss=0.181, total=4139.66, n_correct=2796.58, ppl=4.16, accuracy=67.556, wps=11920.6, ups=1.44, wpb=8279.3, bsz=311.1, num_updates=31600, lr=7.95557e-05, gnorm=0.535, clip=0, loss_scale=16, train_wall=69, gb_free=15.9, wall=550
2023-08-17 02:18:02 | INFO | train_inner | epoch 022:    761 / 1474 loss=1.958, trans_loss=4.839, nll_loss=2.06, w2v_ctc_loss=0.687, task_loss=3.391, contrastive_loss=0.099, total=4167.89, n_correct=2806.96, ppl=4.17, accuracy=67.347, wps=11794.5, ups=1.41, wpb=8335.8, bsz=303.9, num_updates=31700, lr=7.94301e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=70, gb_free=12.4, wall=620
2023-08-17 02:19:13 | INFO | train_inner | epoch 022:    861 / 1474 loss=1.959, trans_loss=4.847, nll_loss=2.071, w2v_ctc_loss=0.686, task_loss=3.57, contrastive_loss=0.085, total=4075.79, n_correct=2740.59, ppl=4.2, accuracy=67.241, wps=11457.2, ups=1.41, wpb=8151.6, bsz=289, num_updates=31800, lr=7.93052e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=71, gb_free=16, wall=691
2023-08-17 02:20:24 | INFO | train_inner | epoch 022:    961 / 1474 loss=1.955, trans_loss=4.841, nll_loss=2.064, w2v_ctc_loss=0.682, task_loss=3.324, contrastive_loss=0.087, total=4134.72, n_correct=2787.66, ppl=4.18, accuracy=67.421, wps=11774.6, ups=1.42, wpb=8269.4, bsz=303.2, num_updates=31900, lr=7.91808e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=70, gb_free=13.9, wall=762
2023-08-17 02:21:34 | INFO | train_inner | epoch 022:   1061 / 1474 loss=1.965, trans_loss=4.838, nll_loss=2.06, w2v_ctc_loss=0.677, task_loss=3.15, contrastive_loss=0.256, total=4160.57, n_correct=2808.35, ppl=4.17, accuracy=67.499, wps=11909.3, ups=1.43, wpb=8321.1, bsz=315.5, num_updates=32000, lr=7.90569e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=69, gb_free=17, wall=832
2023-08-17 02:21:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 02:21:57 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.957 | trans_loss 5.165 | nll_loss 2.427 | w2v_ctc_loss 1.308 | task_loss 11.633 | contrastive_loss 0.305 | total 4003.4 | n_correct 2670.9 | ppl 5.38 | accuracy 66.716 | uer 17.644 | wer 19.507 | raw_wer 19.507 | bleu 22.26 | wps 2259.3 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 22.43
2023-08-17 02:21:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-17 02:21:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_22_32000.pt
2023-08-17 02:22:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_22_32000.pt
2023-08-17 02:22:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 22.26) (writing took 27.86553611047566 seconds)
2023-08-17 02:23:50 | INFO | train_inner | epoch 022:   1161 / 1474 loss=1.973, trans_loss=4.859, nll_loss=2.087, w2v_ctc_loss=0.695, task_loss=3.391, contrastive_loss=0.138, total=4099.59, n_correct=2748.73, ppl=4.25, accuracy=67.049, wps=6003.7, ups=0.73, wpb=8199.2, bsz=296.2, num_updates=32100, lr=7.89337e-05, gnorm=0.573, clip=0, loss_scale=16, train_wall=69, gb_free=14.6, wall=968
2023-08-17 02:24:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-17 02:25:01 | INFO | train_inner | epoch 022:   1262 / 1474 loss=1.964, trans_loss=4.855, nll_loss=2.083, w2v_ctc_loss=0.688, task_loss=3.081, contrastive_loss=0.127, total=4168.08, n_correct=2802.46, ppl=4.24, accuracy=67.236, wps=11711.1, ups=1.4, wpb=8336.2, bsz=319, num_updates=32200, lr=7.8811e-05, gnorm=0.548, clip=0, loss_scale=8, train_wall=71, gb_free=16.9, wall=1039
2023-08-17 02:26:10 | INFO | train_inner | epoch 022:   1362 / 1474 loss=1.961, trans_loss=4.842, nll_loss=2.065, w2v_ctc_loss=0.684, task_loss=3.292, contrastive_loss=0.156, total=4061.14, n_correct=2736.95, ppl=4.18, accuracy=67.394, wps=11761.9, ups=1.45, wpb=8122.3, bsz=299.1, num_updates=32300, lr=7.86889e-05, gnorm=0.539, clip=0, loss_scale=8, train_wall=69, gb_free=16.7, wall=1108
2023-08-17 02:27:21 | INFO | train_inner | epoch 022:   1462 / 1474 loss=1.966, trans_loss=4.855, nll_loss=2.08, w2v_ctc_loss=0.694, task_loss=3.525, contrastive_loss=0.103, total=4083.08, n_correct=2741.24, ppl=4.23, accuracy=67.137, wps=11641.7, ups=1.43, wpb=8166.2, bsz=289.3, num_updates=32400, lr=7.85674e-05, gnorm=0.578, clip=0, loss_scale=8, train_wall=70, gb_free=16, wall=1179
2023-08-17 02:27:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 02:27:53 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.958 | trans_loss 5.17 | nll_loss 2.433 | w2v_ctc_loss 1.296 | task_loss 11.49 | contrastive_loss 0.315 | total 4003.4 | n_correct 2667.1 | ppl 5.4 | accuracy 66.621 | uer 17.655 | wer 19.507 | raw_wer 19.507 | bleu 22.12 | wps 2146.6 | wpb 4003.4 | bsz 141.8 | num_updates 32412 | best_bleu 22.43
2023-08-17 02:27:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32412 updates
2023-08-17 02:27:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.1203.pt
2023-08-17 02:27:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.1203.pt
2023-08-17 02:28:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.1203.pt (epoch 22 @ 32412 updates, score 22.12) (writing took 25.533925127238035 seconds)
2023-08-17 02:28:19 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-17 02:28:19 | INFO | train | epoch 022 | loss 1.963 | trans_loss 4.844 | nll_loss 2.067 | w2v_ctc_loss 0.687 | task_loss 3.299 | contrastive_loss 0.143 | total 4137.83 | n_correct 2787.43 | ppl 4.19 | accuracy 67.365 | wps 10541.3 | ups 1.27 | wpb 8275.7 | bsz 305.4 | num_updates 32412 | lr 7.85529e-05 | gnorm 0.547 | clip 0 | loss_scale 8 | train_wall 1041 | gb_free 11.3 | wall 1237
2023-08-17 02:28:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 02:28:20 | INFO | fairseq.trainer | begin training epoch 23
2023-08-17 02:28:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 02:29:29 | INFO | train_inner | epoch 023:     88 / 1474 loss=1.95, trans_loss=4.826, nll_loss=2.044, w2v_ctc_loss=0.685, task_loss=3.338, contrastive_loss=0.093, total=4093.3, n_correct=2773.38, ppl=4.12, accuracy=67.754, wps=6373.4, ups=0.78, wpb=8186.6, bsz=301.3, num_updates=32500, lr=7.84465e-05, gnorm=0.541, clip=0, loss_scale=8, train_wall=69, gb_free=15.9, wall=1307
2023-08-17 02:30:40 | INFO | train_inner | epoch 023:    188 / 1474 loss=1.948, trans_loss=4.821, nll_loss=2.037, w2v_ctc_loss=0.678, task_loss=3.5, contrastive_loss=0.09, total=4116.26, n_correct=2789.13, ppl=4.1, accuracy=67.759, wps=11573.7, ups=1.41, wpb=8232.5, bsz=294.4, num_updates=32600, lr=7.8326e-05, gnorm=0.537, clip=0, loss_scale=8, train_wall=71, gb_free=16.7, wall=1378
2023-08-17 02:31:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-17 02:31:51 | INFO | train_inner | epoch 023:    289 / 1474 loss=1.956, trans_loss=4.834, nll_loss=2.055, w2v_ctc_loss=0.672, task_loss=3.349, contrastive_loss=0.168, total=4144.62, n_correct=2803.79, ppl=4.15, accuracy=67.649, wps=11711.9, ups=1.41, wpb=8289.2, bsz=304.3, num_updates=32700, lr=7.82062e-05, gnorm=0.54, clip=0, loss_scale=4, train_wall=70, gb_free=16.4, wall=1449
2023-08-17 02:33:00 | INFO | train_inner | epoch 023:    389 / 1474 loss=1.944, trans_loss=4.82, nll_loss=2.035, w2v_ctc_loss=0.676, task_loss=3.406, contrastive_loss=0.082, total=4115.12, n_correct=2792.41, ppl=4.1, accuracy=67.857, wps=11901.5, ups=1.45, wpb=8230.2, bsz=294.4, num_updates=32800, lr=7.80869e-05, gnorm=0.552, clip=0, loss_scale=4, train_wall=69, gb_free=12.9, wall=1518
2023-08-17 02:34:10 | INFO | train_inner | epoch 023:    489 / 1474 loss=1.954, trans_loss=4.833, nll_loss=2.053, w2v_ctc_loss=0.678, task_loss=3.225, contrastive_loss=0.139, total=4156.86, n_correct=2809.03, ppl=4.15, accuracy=67.576, wps=11840.4, ups=1.42, wpb=8313.7, bsz=312.1, num_updates=32900, lr=7.79681e-05, gnorm=0.541, clip=0, loss_scale=4, train_wall=70, gb_free=14.8, wall=1588
2023-08-17 02:35:21 | INFO | train_inner | epoch 023:    589 / 1474 loss=1.94, trans_loss=4.822, nll_loss=2.038, w2v_ctc_loss=0.67, task_loss=3.114, contrastive_loss=0.088, total=4172.99, n_correct=2833.13, ppl=4.11, accuracy=67.892, wps=11792.8, ups=1.41, wpb=8346, bsz=316.1, num_updates=33000, lr=7.78499e-05, gnorm=0.543, clip=0, loss_scale=4, train_wall=70, gb_free=15.7, wall=1659
2023-08-17 02:36:30 | INFO | train_inner | epoch 023:    689 / 1474 loss=1.955, trans_loss=4.834, nll_loss=2.055, w2v_ctc_loss=0.678, task_loss=3.312, contrastive_loss=0.128, total=4135.85, n_correct=2793.02, ppl=4.15, accuracy=67.532, wps=11935.9, ups=1.44, wpb=8271.7, bsz=301.9, num_updates=33100, lr=7.77322e-05, gnorm=0.539, clip=0, loss_scale=4, train_wall=69, gb_free=13.8, wall=1728
2023-08-17 02:37:40 | INFO | train_inner | epoch 023:    789 / 1474 loss=1.957, trans_loss=4.836, nll_loss=2.057, w2v_ctc_loss=0.69, task_loss=3.315, contrastive_loss=0.108, total=4155.54, n_correct=2801.9, ppl=4.16, accuracy=67.426, wps=11958.6, ups=1.44, wpb=8311.1, bsz=306.6, num_updates=33200, lr=7.76151e-05, gnorm=0.571, clip=0, loss_scale=4, train_wall=69, gb_free=16.5, wall=1798
2023-08-17 02:38:49 | INFO | train_inner | epoch 023:    889 / 1474 loss=1.951, trans_loss=4.826, nll_loss=2.044, w2v_ctc_loss=0.671, task_loss=3.008, contrastive_loss=0.186, total=4180.56, n_correct=2834.56, ppl=4.12, accuracy=67.803, wps=12023.7, ups=1.44, wpb=8361.1, bsz=324.5, num_updates=33300, lr=7.74984e-05, gnorm=0.539, clip=0, loss_scale=4, train_wall=69, gb_free=16.2, wall=1867
2023-08-17 02:40:01 | INFO | train_inner | epoch 023:    989 / 1474 loss=1.971, trans_loss=4.83, nll_loss=2.05, w2v_ctc_loss=0.674, task_loss=3.315, contrastive_loss=0.344, total=4163.63, n_correct=2812.07, ppl=4.14, accuracy=67.539, wps=11716.6, ups=1.41, wpb=8327.3, bsz=309.2, num_updates=33400, lr=7.73823e-05, gnorm=0.55, clip=0, loss_scale=4, train_wall=71, gb_free=11.3, wall=1938
2023-08-17 02:41:10 | INFO | train_inner | epoch 023:   1089 / 1474 loss=1.96, trans_loss=4.84, nll_loss=2.062, w2v_ctc_loss=0.69, task_loss=3.505, contrastive_loss=0.096, total=4094.62, n_correct=2759.06, ppl=4.18, accuracy=67.383, wps=11708.2, ups=1.43, wpb=8189.2, bsz=291.3, num_updates=33500, lr=7.72667e-05, gnorm=0.579, clip=0, loss_scale=4, train_wall=70, gb_free=17.1, wall=2008
2023-08-17 02:42:21 | INFO | train_inner | epoch 023:   1189 / 1474 loss=1.951, trans_loss=4.838, nll_loss=2.06, w2v_ctc_loss=0.681, task_loss=3.275, contrastive_loss=0.087, total=4161.7, n_correct=2808.47, ppl=4.17, accuracy=67.484, wps=11843, ups=1.42, wpb=8323.4, bsz=309.2, num_updates=33600, lr=7.71517e-05, gnorm=0.529, clip=0, loss_scale=4, train_wall=70, gb_free=16.7, wall=2079
2023-08-17 02:43:30 | INFO | train_inner | epoch 023:   1289 / 1474 loss=1.948, trans_loss=4.834, nll_loss=2.054, w2v_ctc_loss=0.675, task_loss=3.21, contrastive_loss=0.098, total=4133.96, n_correct=2796.72, ppl=4.15, accuracy=67.652, wps=11952.7, ups=1.45, wpb=8267.9, bsz=309.3, num_updates=33700, lr=7.70371e-05, gnorm=0.561, clip=0, loss_scale=4, train_wall=69, gb_free=16.5, wall=2148
2023-08-17 02:44:41 | INFO | train_inner | epoch 023:   1389 / 1474 loss=1.975, trans_loss=4.852, nll_loss=2.077, w2v_ctc_loss=0.696, task_loss=3.299, contrastive_loss=0.181, total=4159.95, n_correct=2791.93, ppl=4.22, accuracy=67.115, wps=11675.1, ups=1.4, wpb=8319.9, bsz=308.8, num_updates=33800, lr=7.69231e-05, gnorm=0.575, clip=0, loss_scale=4, train_wall=71, gb_free=10.4, wall=2219
2023-08-17 02:45:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 02:46:04 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 3.977 | trans_loss 5.167 | nll_loss 2.426 | w2v_ctc_loss 1.366 | task_loss 11.629 | contrastive_loss 0.31 | total 4003.4 | n_correct 2669.2 | ppl 5.38 | accuracy 66.673 | uer 17.734 | wer 19.541 | raw_wer 19.541 | bleu 22.23 | wps 2164.6 | wpb 4003.4 | bsz 141.8 | num_updates 33885 | best_bleu 22.43
2023-08-17 02:46:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33885 updates
2023-08-17 02:46:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.2307.pt
2023-08-17 02:46:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.2307.pt
2023-08-17 02:46:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.2307.pt (epoch 23 @ 33885 updates, score 22.23) (writing took 25.206842336803675 seconds)
2023-08-17 02:46:30 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-17 02:46:30 | INFO | train | epoch 023 | loss 1.955 | trans_loss 4.833 | nll_loss 2.053 | w2v_ctc_loss 0.68 | task_loss 3.3 | contrastive_loss 0.141 | total 4138.4 | n_correct 2796.97 | ppl 4.15 | accuracy 67.586 | wps 11180.2 | ups 1.35 | wpb 8276.8 | bsz 305.6 | num_updates 33885 | lr 7.68265e-05 | gnorm 0.552 | clip 0 | loss_scale 4 | train_wall 1026 | gb_free 13.4 | wall 2328
2023-08-17 02:46:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 02:46:30 | INFO | fairseq.trainer | begin training epoch 24
2023-08-17 02:46:30 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 02:46:49 | INFO | train_inner | epoch 024:     15 / 1474 loss=1.977, trans_loss=4.846, nll_loss=2.069, w2v_ctc_loss=0.679, task_loss=3.275, contrastive_loss=0.28, total=4099.91, n_correct=2759.66, ppl=4.2, accuracy=67.31, wps=6418.9, ups=0.78, wpb=8199.8, bsz=308.7, num_updates=33900, lr=7.68095e-05, gnorm=0.579, clip=0, loss_scale=4, train_wall=70, gb_free=16.7, wall=2347
2023-08-17 02:47:58 | INFO | train_inner | epoch 024:    115 / 1474 loss=1.947, trans_loss=4.813, nll_loss=2.027, w2v_ctc_loss=0.67, task_loss=3.116, contrastive_loss=0.191, total=4147.74, n_correct=2820.28, ppl=4.07, accuracy=67.996, wps=11988.3, ups=1.45, wpb=8295.5, bsz=317.7, num_updates=34000, lr=7.66965e-05, gnorm=0.569, clip=0, loss_scale=4, train_wall=69, gb_free=15.5, wall=2416
2023-08-17 02:47:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 02:48:22 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.96 | trans_loss 5.17 | nll_loss 2.429 | w2v_ctc_loss 1.3 | task_loss 11.571 | contrastive_loss 0.314 | total 4003.4 | n_correct 2661.6 | ppl 5.38 | accuracy 66.483 | uer 17.811 | wer 19.824 | raw_wer 19.824 | bleu 22.25 | wps 2122.4 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 22.43
2023-08-17 02:48:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-17 02:48:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_24_34000.pt
2023-08-17 02:48:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_24_34000.pt
2023-08-17 02:49:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 22.25) (writing took 44.654837952926755 seconds)
2023-08-17 02:50:19 | INFO | train_inner | epoch 024:    215 / 1474 loss=1.956, trans_loss=4.818, nll_loss=2.034, w2v_ctc_loss=0.661, task_loss=2.889, contrastive_loss=0.302, total=4244.96, n_correct=2882.02, ppl=4.1, accuracy=67.893, wps=6044.7, ups=0.71, wpb=8489.9, bsz=340, num_updates=34100, lr=7.6584e-05, gnorm=0.528, clip=0, loss_scale=4, train_wall=70, gb_free=15.2, wall=2557
2023-08-17 02:51:28 | INFO | train_inner | epoch 024:    315 / 1474 loss=1.937, trans_loss=4.816, nll_loss=2.031, w2v_ctc_loss=0.668, task_loss=3.174, contrastive_loss=0.084, total=4144.64, n_correct=2816.26, ppl=4.09, accuracy=67.949, wps=11871.6, ups=1.43, wpb=8289.3, bsz=309.2, num_updates=34200, lr=7.64719e-05, gnorm=0.542, clip=0, loss_scale=4, train_wall=69, gb_free=16.4, wall=2626
2023-08-17 02:52:38 | INFO | train_inner | epoch 024:    415 / 1474 loss=1.966, trans_loss=4.821, nll_loss=2.036, w2v_ctc_loss=0.683, task_loss=3.513, contrastive_loss=0.229, total=4152.59, n_correct=2808.07, ppl=4.1, accuracy=67.622, wps=11918.5, ups=1.44, wpb=8305.2, bsz=297.1, num_updates=34300, lr=7.63604e-05, gnorm=0.543, clip=0, loss_scale=4, train_wall=69, gb_free=10.5, wall=2696
2023-08-17 02:53:47 | INFO | train_inner | epoch 024:    515 / 1474 loss=1.948, trans_loss=4.818, nll_loss=2.033, w2v_ctc_loss=0.672, task_loss=3.401, contrastive_loss=0.153, total=4135.54, n_correct=2808.75, ppl=4.09, accuracy=67.917, wps=11987.8, ups=1.45, wpb=8271.1, bsz=300.8, num_updates=34400, lr=7.62493e-05, gnorm=0.532, clip=0, loss_scale=4, train_wall=69, gb_free=16.2, wall=2765
2023-08-17 02:54:57 | INFO | train_inner | epoch 024:    615 / 1474 loss=1.942, trans_loss=4.818, nll_loss=2.034, w2v_ctc_loss=0.668, task_loss=3.29, contrastive_loss=0.117, total=4163.63, n_correct=2825.57, ppl=4.1, accuracy=67.863, wps=11914.1, ups=1.43, wpb=8327.3, bsz=309.2, num_updates=34500, lr=7.61387e-05, gnorm=0.561, clip=0, loss_scale=4, train_wall=69, gb_free=16.4, wall=2835
2023-08-17 02:56:06 | INFO | train_inner | epoch 024:    715 / 1474 loss=1.954, trans_loss=4.831, nll_loss=2.05, w2v_ctc_loss=0.68, task_loss=3.394, contrastive_loss=0.129, total=4100.27, n_correct=2771.54, ppl=4.14, accuracy=67.594, wps=11832.8, ups=1.44, wpb=8200.5, bsz=295.4, num_updates=34600, lr=7.60286e-05, gnorm=0.547, clip=0, loss_scale=4, train_wall=69, gb_free=15.3, wall=2904
2023-08-17 02:57:15 | INFO | train_inner | epoch 024:    815 / 1474 loss=1.945, trans_loss=4.828, nll_loss=2.047, w2v_ctc_loss=0.672, task_loss=3.3, contrastive_loss=0.102, total=4129.03, n_correct=2799.48, ppl=4.13, accuracy=67.8, wps=11947.4, ups=1.45, wpb=8258.1, bsz=307.8, num_updates=34700, lr=7.5919e-05, gnorm=0.531, clip=0, loss_scale=4, train_wall=69, gb_free=17.2, wall=2973
2023-08-17 02:58:24 | INFO | train_inner | epoch 024:    915 / 1474 loss=1.949, trans_loss=4.826, nll_loss=2.042, w2v_ctc_loss=0.681, task_loss=3.699, contrastive_loss=0.078, total=4035.8, n_correct=2732.18, ppl=4.12, accuracy=67.699, wps=11800.4, ups=1.46, wpb=8071.6, bsz=278.8, num_updates=34800, lr=7.58098e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=68, gb_free=11.9, wall=3042
2023-08-17 02:59:34 | INFO | train_inner | epoch 024:   1015 / 1474 loss=1.943, trans_loss=4.826, nll_loss=2.043, w2v_ctc_loss=0.671, task_loss=3.446, contrastive_loss=0.083, total=4124.2, n_correct=2797.9, ppl=4.12, accuracy=67.841, wps=11779.6, ups=1.43, wpb=8248.4, bsz=295.7, num_updates=34900, lr=7.57011e-05, gnorm=0.556, clip=0, loss_scale=8, train_wall=70, gb_free=12.9, wall=3112
2023-08-17 03:00:43 | INFO | train_inner | epoch 024:   1115 / 1474 loss=1.941, trans_loss=4.811, nll_loss=2.024, w2v_ctc_loss=0.672, task_loss=3.155, contrastive_loss=0.125, total=4133.96, n_correct=2813.28, ppl=4.07, accuracy=68.053, wps=11956.8, ups=1.45, wpb=8267.9, bsz=310.6, num_updates=35000, lr=7.55929e-05, gnorm=0.535, clip=0, loss_scale=8, train_wall=69, gb_free=16.1, wall=3181
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
2023-08-17 03:01:52 | INFO | train_inner | epoch 024:   1215 / 1474 loss=1.945, trans_loss=4.824, nll_loss=2.041, w2v_ctc_loss=0.671, task_loss=3.286, contrastive_loss=0.115, total=4152.6, n_correct=2818.92, ppl=4.11, accuracy=67.883, wps=11975.9, ups=1.44, wpb=8305.2, bsz=310.7, num_updates=35100, lr=7.54851e-05, gnorm=0.545, clip=0, loss_scale=8, train_wall=69, gb_free=17.1, wall=3250
2023-08-17 03:03:02 | INFO | train_inner | epoch 024:   1315 / 1474 loss=1.952, trans_loss=4.829, nll_loss=2.047, w2v_ctc_loss=0.687, task_loss=3.534, contrastive_loss=0.087, total=4108.12, n_correct=2780.02, ppl=4.13, accuracy=67.671, wps=11842.7, ups=1.44, wpb=8216.2, bsz=293.3, num_updates=35200, lr=7.53778e-05, gnorm=0.534, clip=0, loss_scale=8, train_wall=69, gb_free=12.7, wall=3320
2023-08-17 03:04:11 | INFO | train_inner | epoch 024:   1415 / 1474 loss=1.945, trans_loss=4.828, nll_loss=2.047, w2v_ctc_loss=0.676, task_loss=3.404, contrastive_loss=0.084, total=4099.36, n_correct=2779.58, ppl=4.13, accuracy=67.805, wps=11774.1, ups=1.44, wpb=8198.7, bsz=294.7, num_updates=35300, lr=7.5271e-05, gnorm=0.533, clip=0, loss_scale=8, train_wall=69, gb_free=14.9, wall=3389
2023-08-17 03:04:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
2023-08-17 03:05:16 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.96 | trans_loss 5.168 | nll_loss 2.43 | w2v_ctc_loss 1.31 | task_loss 11.657 | contrastive_loss 0.304 | total 4003.4 | n_correct 2669.6 | ppl 5.39 | accuracy 66.683 | uer 17.474 | wer 19.19 | raw_wer 19.19 | bleu 22.54 | wps 2237.1 | wpb 4003.4 | bsz 141.8 | num_updates 35359 | best_bleu 22.54
2023-08-17 03:05:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35359 updates
2023-08-17 03:05:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-17 03:05:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt
2023-08-17 03:05:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_best.pt (epoch 24 @ 35359 updates, score 22.54) (writing took 31.58388763666153 seconds)
2023-08-17 03:05:48 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-17 03:05:48 | INFO | train | epoch 024 | loss 1.948 | trans_loss 4.822 | nll_loss 2.038 | w2v_ctc_loss 0.673 | task_loss 3.301 | contrastive_loss 0.139 | total 4138.65 | n_correct 2807.26 | ppl 4.11 | accuracy 67.83 | wps 10537.7 | ups 1.27 | wpb 8277.3 | bsz 305.7 | num_updates 35359 | lr 7.52082e-05 | gnorm 0.545 | clip 0 | loss_scale 8 | train_wall 1018 | gb_free 15.9 | wall 3486
2023-08-17 03:05:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 03:05:48 | INFO | fairseq.trainer | begin training epoch 25
2023-08-17 03:05:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 03:06:24 | INFO | train_inner | epoch 025:     41 / 1474 loss=1.936, trans_loss=4.813, nll_loss=2.027, w2v_ctc_loss=0.668, task_loss=3.2, contrastive_loss=0.093, total=4161.08, n_correct=2834.98, ppl=4.08, accuracy=68.131, wps=6267.2, ups=0.75, wpb=8322.2, bsz=309.6, num_updates=35400, lr=7.51646e-05, gnorm=0.544, clip=0, loss_scale=8, train_wall=69, gb_free=16.3, wall=3522
2023-08-17 03:07:33 | INFO | train_inner | epoch 025:    141 / 1474 loss=1.925, trans_loss=4.799, nll_loss=2.009, w2v_ctc_loss=0.654, task_loss=3.193, contrastive_loss=0.09, total=4139.23, n_correct=2827.56, ppl=4.02, accuracy=68.311, wps=12063.7, ups=1.46, wpb=8278.5, bsz=309.8, num_updates=35500, lr=7.50587e-05, gnorm=0.534, clip=0, loss_scale=8, train_wall=68, gb_free=16, wall=3591
2023-08-17 03:08:42 | INFO | train_inner | epoch 025:    241 / 1474 loss=1.933, trans_loss=4.806, nll_loss=2.018, w2v_ctc_loss=0.665, task_loss=3.383, contrastive_loss=0.094, total=4117.76, n_correct=2806.25, ppl=4.05, accuracy=68.15, wps=11891.3, ups=1.44, wpb=8235.5, bsz=302.9, num_updates=35600, lr=7.49532e-05, gnorm=0.552, clip=0, loss_scale=8, train_wall=69, gb_free=16.6, wall=3660
2023-08-17 03:09:52 | INFO | train_inner | epoch 025:    341 / 1474 loss=1.938, trans_loss=4.807, nll_loss=2.018, w2v_ctc_loss=0.664, task_loss=3.494, contrastive_loss=0.122, total=4142.17, n_correct=2820.3, ppl=4.05, accuracy=68.088, wps=11851, ups=1.43, wpb=8284.3, bsz=295.5, num_updates=35700, lr=7.48481e-05, gnorm=0.541, clip=0, loss_scale=8, train_wall=69, gb_free=15.4, wall=3730
2023-08-17 03:11:02 | INFO | train_inner | epoch 025:    441 / 1474 loss=1.955, trans_loss=4.808, nll_loss=2.021, w2v_ctc_loss=0.681, task_loss=3.469, contrastive_loss=0.2, total=4167.72, n_correct=2835.75, ppl=4.06, accuracy=68.041, wps=11898, ups=1.43, wpb=8335.4, bsz=296.8, num_updates=35800, lr=7.47435e-05, gnorm=0.603, clip=0, loss_scale=8, train_wall=70, gb_free=16.5, wall=3800
2023-08-17 03:12:11 | INFO | train_inner | epoch 025:    541 / 1474 loss=1.936, trans_loss=4.816, nll_loss=2.03, w2v_ctc_loss=0.665, task_loss=3.225, contrastive_loss=0.095, total=4154.79, n_correct=2828.01, ppl=4.09, accuracy=68.066, wps=12048.5, ups=1.45, wpb=8309.6, bsz=313.5, num_updates=35900, lr=7.46393e-05, gnorm=0.545, clip=0, loss_scale=8, train_wall=68, gb_free=17.3, wall=3869
2023-08-17 03:13:20 | INFO | train_inner | epoch 025:    641 / 1474 loss=1.938, trans_loss=4.802, nll_loss=2.013, w2v_ctc_loss=0.664, task_loss=3.276, contrastive_loss=0.162, total=4156.33, n_correct=2833.9, ppl=4.04, accuracy=68.183, wps=12069.6, ups=1.45, wpb=8312.7, bsz=309.3, num_updates=36000, lr=7.45356e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=68, gb_free=16, wall=3938
2023-08-17 03:13:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 03:13:45 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.971 | trans_loss 5.174 | nll_loss 2.435 | w2v_ctc_loss 1.332 | task_loss 11.576 | contrastive_loss 0.31 | total 4003.4 | n_correct 2664.1 | ppl 5.41 | accuracy 66.546 | uer 17.641 | wer 19.384 | raw_wer 19.384 | bleu 22.47 | wps 2177.7 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 22.54
2023-08-17 03:13:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-17 03:13:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_25_36000.pt
2023-08-17 03:13:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_25_36000.pt
2023-08-17 03:14:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 22.47) (writing took 43.912319634109735 seconds)
2023-08-17 03:15:39 | INFO | train_inner | epoch 025:    741 / 1474 loss=1.941, trans_loss=4.805, nll_loss=2.017, w2v_ctc_loss=0.663, task_loss=3.326, contrastive_loss=0.159, total=4133.94, n_correct=2817.34, ppl=4.05, accuracy=68.151, wps=5964.9, ups=0.72, wpb=8267.9, bsz=303.3, num_updates=36100, lr=7.44323e-05, gnorm=0.539, clip=0, loss_scale=8, train_wall=69, gb_free=14.8, wall=4077
2023-08-17 03:16:47 | INFO | train_inner | epoch 025:    841 / 1474 loss=1.933, trans_loss=4.811, nll_loss=2.025, w2v_ctc_loss=0.662, task_loss=3.048, contrastive_loss=0.103, total=4174.24, n_correct=2844.65, ppl=4.07, accuracy=68.148, wps=12126.1, ups=1.45, wpb=8348.5, bsz=324, num_updates=36200, lr=7.43294e-05, gnorm=0.54, clip=0, loss_scale=8, train_wall=68, gb_free=16.1, wall=4145
2023-08-17 03:17:57 | INFO | train_inner | epoch 025:    941 / 1474 loss=1.943, trans_loss=4.815, nll_loss=2.03, w2v_ctc_loss=0.668, task_loss=3.137, contrastive_loss=0.161, total=4154.13, n_correct=2825.6, ppl=4.08, accuracy=68.019, wps=12008.3, ups=1.45, wpb=8308.3, bsz=316.7, num_updates=36300, lr=7.4227e-05, gnorm=0.542, clip=0, loss_scale=8, train_wall=69, gb_free=10.5, wall=4215
2023-08-17 03:19:06 | INFO | train_inner | epoch 025:   1041 / 1474 loss=1.954, trans_loss=4.821, nll_loss=2.037, w2v_ctc_loss=0.661, task_loss=3.275, contrastive_loss=0.27, total=4178.3, n_correct=2833.5, ppl=4.1, accuracy=67.815, wps=12067.5, ups=1.44, wpb=8356.6, bsz=309.7, num_updates=36400, lr=7.41249e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=69, gb_free=16.6, wall=4284
2023-08-17 03:20:15 | INFO | train_inner | epoch 025:   1141 / 1474 loss=1.935, trans_loss=4.813, nll_loss=2.027, w2v_ctc_loss=0.661, task_loss=3.546, contrastive_loss=0.078, total=4042.33, n_correct=2750.26, ppl=4.07, accuracy=68.037, wps=11645.4, ups=1.44, wpb=8084.7, bsz=286.5, num_updates=36500, lr=7.40233e-05, gnorm=0.545, clip=0, loss_scale=8, train_wall=69, gb_free=17.5, wall=4353
2023-08-17 03:21:24 | INFO | train_inner | epoch 025:   1241 / 1474 loss=1.939, trans_loss=4.82, nll_loss=2.037, w2v_ctc_loss=0.667, task_loss=3.36, contrastive_loss=0.087, total=4087.78, n_correct=2776.87, ppl=4.1, accuracy=67.931, wps=11945.5, ups=1.46, wpb=8175.6, bsz=295.1, num_updates=36600, lr=7.39221e-05, gnorm=0.531, clip=0, loss_scale=8, train_wall=68, gb_free=17.4, wall=4422
2023-08-17 03:22:32 | INFO | train_inner | epoch 025:   1341 / 1474 loss=1.947, trans_loss=4.815, nll_loss=2.03, w2v_ctc_loss=0.671, task_loss=3.226, contrastive_loss=0.179, total=4166.64, n_correct=2831.14, ppl=4.08, accuracy=67.948, wps=12113.2, ups=1.45, wpb=8333.3, bsz=309.3, num_updates=36700, lr=7.38213e-05, gnorm=0.556, clip=0, loss_scale=8, train_wall=68, gb_free=16.2, wall=4490
2023-08-17 03:23:42 | INFO | train_inner | epoch 025:   1441 / 1474 loss=1.95, trans_loss=4.827, nll_loss=2.045, w2v_ctc_loss=0.669, task_loss=3.362, contrastive_loss=0.148, total=4114.64, n_correct=2782.74, ppl=4.13, accuracy=67.63, wps=11761.1, ups=1.43, wpb=8229.3, bsz=304.3, num_updates=36800, lr=7.3721e-05, gnorm=0.553, clip=0, loss_scale=16, train_wall=70, gb_free=16.2, wall=4560
2023-08-17 03:24:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 03:24:29 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.947 | trans_loss 5.163 | nll_loss 2.421 | w2v_ctc_loss 1.281 | task_loss 11.573 | contrastive_loss 0.308 | total 4003.4 | n_correct 2674 | ppl 5.35 | accuracy 66.793 | uer 17.357 | wer 19.231 | raw_wer 19.231 | bleu 22.29 | wps 2141 | wpb 4003.4 | bsz 141.8 | num_updates 36833 | best_bleu 22.54
2023-08-17 03:24:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36833 updates
2023-08-17 03:24:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.2902.pt
2023-08-17 03:24:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.2902.pt
2023-08-17 03:24:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.2902.pt (epoch 25 @ 36833 updates, score 22.29) (writing took 21.508168311789632 seconds)
2023-08-17 03:24:51 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-17 03:24:51 | INFO | train | epoch 025 | loss 1.94 | trans_loss 4.812 | nll_loss 2.025 | w2v_ctc_loss 0.665 | task_loss 3.298 | contrastive_loss 0.137 | total 4138.65 | n_correct 2816.17 | ppl 4.07 | accuracy 68.046 | wps 10673.5 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 36833 | lr 7.36879e-05 | gnorm 0.549 | clip 0 | loss_scale 16 | train_wall 1013 | gb_free 14 | wall 4629
2023-08-17 03:24:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 03:24:51 | INFO | fairseq.trainer | begin training epoch 26
2023-08-17 03:24:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 03:25:46 | INFO | train_inner | epoch 026:     67 / 1474 loss=1.922, trans_loss=4.794, nll_loss=2.002, w2v_ctc_loss=0.646, task_loss=3.118, contrastive_loss=0.111, total=4172.16, n_correct=2855.66, ppl=4.01, accuracy=68.446, wps=6780.1, ups=0.81, wpb=8344.3, bsz=316.9, num_updates=36900, lr=7.3621e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=69, gb_free=16.8, wall=4684
2023-08-17 03:26:55 | INFO | train_inner | epoch 026:    167 / 1474 loss=1.935, trans_loss=4.796, nll_loss=2.006, w2v_ctc_loss=0.638, task_loss=2.903, contrastive_loss=0.287, total=4265.22, n_correct=2922.32, ppl=4.02, accuracy=68.515, wps=12313.9, ups=1.44, wpb=8530.4, bsz=338.7, num_updates=37000, lr=7.35215e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=69, gb_free=15.1, wall=4753
2023-08-17 03:28:04 | INFO | train_inner | epoch 026:    267 / 1474 loss=1.936, trans_loss=4.794, nll_loss=2.003, w2v_ctc_loss=0.66, task_loss=3.279, contrastive_loss=0.177, total=4123.94, n_correct=2821.05, ppl=4.01, accuracy=68.407, wps=11967.4, ups=1.45, wpb=8247.9, bsz=306.6, num_updates=37100, lr=7.34223e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=68, gb_free=16.7, wall=4822
2023-08-17 03:29:13 | INFO | train_inner | epoch 026:    367 / 1474 loss=1.931, trans_loss=4.798, nll_loss=2.008, w2v_ctc_loss=0.659, task_loss=3.154, contrastive_loss=0.132, total=4168.11, n_correct=2847.18, ppl=4.02, accuracy=68.309, wps=12106.8, ups=1.45, wpb=8336.2, bsz=315.2, num_updates=37200, lr=7.33236e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=68, gb_free=17, wall=4891
2023-08-17 03:30:22 | INFO | train_inner | epoch 026:    467 / 1474 loss=1.932, trans_loss=4.788, nll_loss=1.995, w2v_ctc_loss=0.658, task_loss=3.158, contrastive_loss=0.177, total=4167.53, n_correct=2853.09, ppl=3.99, accuracy=68.46, wps=12005.6, ups=1.44, wpb=8335.1, bsz=314.6, num_updates=37300, lr=7.32252e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=69, gb_free=13.7, wall=4960
2023-08-17 03:31:31 | INFO | train_inner | epoch 026:    567 / 1474 loss=1.933, trans_loss=4.8, nll_loss=2.009, w2v_ctc_loss=0.668, task_loss=3.3, contrastive_loss=0.098, total=4158.48, n_correct=2837.98, ppl=4.02, accuracy=68.246, wps=12040, ups=1.45, wpb=8317, bsz=304.4, num_updates=37400, lr=7.31272e-05, gnorm=0.556, clip=0, loss_scale=16, train_wall=69, gb_free=12.5, wall=5029
2023-08-17 03:32:40 | INFO | train_inner | epoch 026:    667 / 1474 loss=1.927, trans_loss=4.801, nll_loss=2.01, w2v_ctc_loss=0.654, task_loss=3.386, contrastive_loss=0.083, total=4129.11, n_correct=2817.54, ppl=4.03, accuracy=68.236, wps=11962.7, ups=1.45, wpb=8258.2, bsz=297.7, num_updates=37500, lr=7.30297e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=69, gb_free=13.5, wall=5098
2023-08-17 03:33:49 | INFO | train_inner | epoch 026:    767 / 1474 loss=1.939, trans_loss=4.804, nll_loss=2.014, w2v_ctc_loss=0.654, task_loss=3.322, contrastive_loss=0.195, total=4096.84, n_correct=2793.47, ppl=4.04, accuracy=68.186, wps=11890.2, ups=1.45, wpb=8193.7, bsz=300.5, num_updates=37600, lr=7.29325e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=68, gb_free=16.4, wall=5167
2023-08-17 03:34:58 | INFO | train_inner | epoch 026:    867 / 1474 loss=1.934, trans_loss=4.802, nll_loss=2.012, w2v_ctc_loss=0.667, task_loss=3.286, contrastive_loss=0.098, total=4176.27, n_correct=2847.95, ppl=4.03, accuracy=68.194, wps=12038.4, ups=1.44, wpb=8352.5, bsz=306.4, num_updates=37700, lr=7.28357e-05, gnorm=0.531, clip=0, loss_scale=16, train_wall=69, gb_free=16, wall=5236
2023-08-17 03:36:08 | INFO | train_inner | epoch 026:    967 / 1474 loss=1.934, trans_loss=4.807, nll_loss=2.018, w2v_ctc_loss=0.65, task_loss=3.395, contrastive_loss=0.15, total=4141.01, n_correct=2822.78, ppl=4.05, accuracy=68.166, wps=11972.1, ups=1.45, wpb=8282, bsz=299.7, num_updates=37800, lr=7.27393e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=69, gb_free=15.2, wall=5306
2023-08-17 03:37:17 | INFO | train_inner | epoch 026:   1067 / 1474 loss=1.933, trans_loss=4.804, nll_loss=2.015, w2v_ctc_loss=0.667, task_loss=3.489, contrastive_loss=0.083, total=4113.69, n_correct=2808.07, ppl=4.04, accuracy=68.262, wps=11937.1, ups=1.45, wpb=8227.4, bsz=293, num_updates=37900, lr=7.26433e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=68, gb_free=15.3, wall=5375
2023-08-17 03:38:26 | INFO | train_inner | epoch 026:   1167 / 1474 loss=1.938, trans_loss=4.812, nll_loss=2.025, w2v_ctc_loss=0.662, task_loss=3.447, contrastive_loss=0.123, total=4116.78, n_correct=2800.8, ppl=4.07, accuracy=68.034, wps=11887.5, ups=1.44, wpb=8233.6, bsz=299, num_updates=38000, lr=7.25476e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=69, gb_free=16.3, wall=5444
2023-08-17 03:38:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 03:38:51 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.962 | trans_loss 5.167 | nll_loss 2.427 | w2v_ctc_loss 1.328 | task_loss 11.671 | contrastive_loss 0.299 | total 4003.4 | n_correct 2676.6 | ppl 5.38 | accuracy 66.858 | uer 17.678 | wer 19.518 | raw_wer 19.518 | bleu 22.38 | wps 2237.7 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 22.54
2023-08-17 03:38:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-17 03:38:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_26_38000.pt
2023-08-17 03:38:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_26_38000.pt
2023-08-17 03:39:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 22.38) (writing took 40.17571981996298 seconds)
2023-08-17 03:40:41 | INFO | train_inner | epoch 026:   1267 / 1474 loss=1.94, trans_loss=4.817, nll_loss=2.031, w2v_ctc_loss=0.671, task_loss=3.664, contrastive_loss=0.086, total=4001.06, n_correct=2719.24, ppl=4.09, accuracy=67.963, wps=5941.6, ups=0.74, wpb=8002.1, bsz=280.6, num_updates=38100, lr=7.24524e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=69, gb_free=15.4, wall=5579
2023-08-17 03:41:50 | INFO | train_inner | epoch 026:   1367 / 1474 loss=1.928, trans_loss=4.81, nll_loss=2.023, w2v_ctc_loss=0.65, task_loss=3.299, contrastive_loss=0.096, total=4157.69, n_correct=2838.69, ppl=4.07, accuracy=68.276, wps=12005.4, ups=1.44, wpb=8315.4, bsz=310.5, num_updates=38200, lr=7.23575e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=69, gb_free=15.9, wall=5648
2023-08-17 03:42:59 | INFO | train_inner | epoch 026:   1467 / 1474 loss=1.923, trans_loss=4.803, nll_loss=2.015, w2v_ctc_loss=0.649, task_loss=3.127, contrastive_loss=0.09, total=4158.47, n_correct=2840.41, ppl=4.04, accuracy=68.304, wps=12070.9, ups=1.45, wpb=8316.9, bsz=316.5, num_updates=38300, lr=7.22629e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=68, gb_free=17.1, wall=5717
2023-08-17 03:43:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 03:43:26 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.95 | trans_loss 5.167 | nll_loss 2.426 | w2v_ctc_loss 1.289 | task_loss 11.626 | contrastive_loss 0.298 | total 4003.4 | n_correct 2668.4 | ppl 5.38 | accuracy 66.653 | uer 17.639 | wer 19.522 | raw_wer 19.522 | bleu 22.02 | wps 2239.7 | wpb 4003.4 | bsz 141.8 | num_updates 38307 | best_bleu 22.54
2023-08-17 03:43:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38307 updates
2023-08-17 03:43:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_last.pt
2023-08-17 03:43:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_last.pt
2023-08-17 03:43:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_last.pt (epoch 26 @ 38307 updates, score 22.02) (writing took 17.001817613840103 seconds)
2023-08-17 03:43:43 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-17 03:43:43 | INFO | train | epoch 026 | loss 1.933 | trans_loss 4.802 | nll_loss 2.012 | w2v_ctc_loss 0.657 | task_loss 3.296 | contrastive_loss 0.134 | total 4138.65 | n_correct 2825.49 | ppl 4.03 | accuracy 68.271 | wps 10772.8 | ups 1.3 | wpb 8277.3 | bsz 305.7 | num_updates 38307 | lr 7.22563e-05 | gnorm 0.539 | clip 0 | loss_scale 16 | train_wall 1012 | gb_free 15.7 | wall 5761
2023-08-17 03:43:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 03:43:44 | INFO | fairseq.trainer | begin training epoch 27
2023-08-17 03:43:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 03:44:54 | INFO | train_inner | epoch 027:     93 / 1474 loss=1.917, trans_loss=4.775, nll_loss=1.977, w2v_ctc_loss=0.65, task_loss=3.532, contrastive_loss=0.072, total=4067.62, n_correct=2793.76, ppl=3.94, accuracy=68.683, wps=7026.8, ups=0.86, wpb=8135.2, bsz=284.4, num_updates=38400, lr=7.21688e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=68, gb_free=14.6, wall=5832
2023-08-17 03:46:04 | INFO | train_inner | epoch 027:    193 / 1474 loss=1.917, trans_loss=4.783, nll_loss=1.988, w2v_ctc_loss=0.651, task_loss=3.159, contrastive_loss=0.1, total=4185.52, n_correct=2874.11, ppl=3.97, accuracy=68.668, wps=12059.6, ups=1.44, wpb=8371, bsz=321.7, num_updates=38500, lr=7.2075e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=69, gb_free=17.4, wall=5902
2023-08-17 03:47:13 | INFO | train_inner | epoch 027:    293 / 1474 loss=1.918, trans_loss=4.786, nll_loss=1.992, w2v_ctc_loss=0.649, task_loss=3.291, contrastive_loss=0.084, total=4167.92, n_correct=2863.7, ppl=3.98, accuracy=68.708, wps=12063, ups=1.45, wpb=8335.8, bsz=306.8, num_updates=38600, lr=7.19816e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=69, gb_free=16.6, wall=5971
2023-08-17 03:48:23 | INFO | train_inner | epoch 027:    393 / 1474 loss=1.941, trans_loss=4.793, nll_loss=2, w2v_ctc_loss=0.652, task_loss=3.462, contrastive_loss=0.267, total=4075.21, n_correct=2790.27, ppl=4, accuracy=68.469, wps=11705.9, ups=1.44, wpb=8150.4, bsz=296, num_updates=38700, lr=7.18885e-05, gnorm=0.563, clip=0, loss_scale=16, train_wall=69, gb_free=17.4, wall=6041
2023-08-17 03:49:32 | INFO | train_inner | epoch 027:    493 / 1474 loss=1.937, trans_loss=4.802, nll_loss=2.013, w2v_ctc_loss=0.657, task_loss=3.016, contrastive_loss=0.203, total=4249.35, n_correct=2900.41, ppl=4.04, accuracy=68.255, wps=12174.7, ups=1.43, wpb=8498.7, bsz=331.9, num_updates=38800, lr=7.17958e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=69, gb_free=11.7, wall=6110
2023-08-17 03:50:42 | INFO | train_inner | epoch 027:    593 / 1474 loss=1.925, trans_loss=4.787, nll_loss=1.993, w2v_ctc_loss=0.652, task_loss=3.228, contrastive_loss=0.142, total=4133.39, n_correct=2835.36, ppl=3.98, accuracy=68.596, wps=11955.8, ups=1.45, wpb=8266.8, bsz=312, num_updates=38900, lr=7.17035e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=11.7, wall=6180
2023-08-17 03:51:51 | INFO | train_inner | epoch 027:    693 / 1474 loss=1.928, trans_loss=4.794, nll_loss=2.001, w2v_ctc_loss=0.657, task_loss=3.28, contrastive_loss=0.12, total=4162.71, n_correct=2847.45, ppl=4, accuracy=68.404, wps=12072.2, ups=1.45, wpb=8325.4, bsz=305.4, num_updates=39000, lr=7.16115e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=68, gb_free=16, wall=6249
2023-08-17 03:52:59 | INFO | train_inner | epoch 027:    793 / 1474 loss=1.925, trans_loss=4.792, nll_loss=1.999, w2v_ctc_loss=0.659, task_loss=3.461, contrastive_loss=0.084, total=4103.81, n_correct=2810.2, ppl=4, accuracy=68.478, wps=12041.5, ups=1.47, wpb=8207.6, bsz=294.2, num_updates=39100, lr=7.15199e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=68, gb_free=16, wall=6317
2023-08-17 03:54:08 | INFO | train_inner | epoch 027:    893 / 1474 loss=1.922, trans_loss=4.799, nll_loss=2.008, w2v_ctc_loss=0.648, task_loss=3.435, contrastive_loss=0.073, total=4101.56, n_correct=2807.27, ppl=4.02, accuracy=68.444, wps=11921.7, ups=1.45, wpb=8203.1, bsz=292.1, num_updates=39200, lr=7.14286e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=68, gb_free=17.4, wall=6386
2023-08-17 03:55:18 | INFO | train_inner | epoch 027:    993 / 1474 loss=1.94, trans_loss=4.799, nll_loss=2.009, w2v_ctc_loss=0.653, task_loss=3.188, contrastive_loss=0.263, total=4199.56, n_correct=2869.53, ppl=4.02, accuracy=68.329, wps=11965.2, ups=1.42, wpb=8399.1, bsz=316.8, num_updates=39300, lr=7.13376e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=70, gb_free=11.2, wall=6456
2023-08-17 03:56:26 | INFO | train_inner | epoch 027:   1093 / 1474 loss=1.923, trans_loss=4.794, nll_loss=2.002, w2v_ctc_loss=0.654, task_loss=3.317, contrastive_loss=0.094, total=4150.97, n_correct=2837.29, ppl=4.01, accuracy=68.352, wps=12127.2, ups=1.46, wpb=8301.9, bsz=305, num_updates=39400, lr=7.1247e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=68, gb_free=11.7, wall=6524
2023-08-17 03:57:35 | INFO | train_inner | epoch 027:   1193 / 1474 loss=1.928, trans_loss=4.796, nll_loss=2.005, w2v_ctc_loss=0.66, task_loss=3.443, contrastive_loss=0.098, total=4103.06, n_correct=2803.34, ppl=4.01, accuracy=68.323, wps=11935.5, ups=1.45, wpb=8206.1, bsz=297.7, num_updates=39500, lr=7.11568e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=68, gb_free=16.3, wall=6593
2023-08-17 03:58:44 | INFO | train_inner | epoch 027:   1293 / 1474 loss=1.934, trans_loss=4.799, nll_loss=2.008, w2v_ctc_loss=0.656, task_loss=3.531, contrastive_loss=0.149, total=4062.52, n_correct=2775.18, ppl=4.02, accuracy=68.312, wps=11813.1, ups=1.45, wpb=8125, bsz=292.2, num_updates=39600, lr=7.10669e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=68, gb_free=16.3, wall=6662
2023-08-17 03:59:53 | INFO | train_inner | epoch 027:   1393 / 1474 loss=1.925, trans_loss=4.797, nll_loss=2.007, w2v_ctc_loss=0.647, task_loss=3.113, contrastive_loss=0.132, total=4152, n_correct=2839.8, ppl=4.02, accuracy=68.396, wps=12037.7, ups=1.45, wpb=8304, bsz=312.5, num_updates=39700, lr=7.09773e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=69, gb_free=16.3, wall=6731
2023-08-17 04:00:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 04:01:12 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 3.949 | trans_loss 5.167 | nll_loss 2.427 | w2v_ctc_loss 1.282 | task_loss 11.567 | contrastive_loss 0.302 | total 4003.4 | n_correct 2671.8 | ppl 5.38 | accuracy 66.738 | uer 17.843 | wer 19.731 | raw_wer 19.731 | bleu 22.46 | wps 2147 | wpb 4003.4 | bsz 141.8 | num_updates 39781 | best_bleu 22.54
2023-08-17 04:01:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39781 updates
2023-08-17 04:01:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.4605.pt
2023-08-17 04:01:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.4605.pt
2023-08-17 04:01:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.4605.pt (epoch 27 @ 39781 updates, score 22.46) (writing took 26.14247843809426 seconds)
2023-08-17 04:01:38 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-17 04:01:38 | INFO | train | epoch 027 | loss 1.927 | trans_loss 4.792 | nll_loss 2 | w2v_ctc_loss 0.653 | task_loss 3.297 | contrastive_loss 0.133 | total 4138.65 | n_correct 2833.55 | ppl 4 | accuracy 68.466 | wps 11350.1 | ups 1.37 | wpb 8277.3 | bsz 305.7 | num_updates 39781 | lr 7.0905e-05 | gnorm 0.541 | clip 0 | loss_scale 32 | train_wall 1010 | gb_free 17.5 | wall 6836
2023-08-17 04:01:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 04:01:39 | INFO | fairseq.trainer | begin training epoch 28
2023-08-17 04:01:39 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 04:01:59 | INFO | train_inner | epoch 028:     19 / 1474 loss=1.915, trans_loss=4.788, nll_loss=1.995, w2v_ctc_loss=0.644, task_loss=3.19, contrastive_loss=0.084, total=4108.43, n_correct=2820.91, ppl=3.99, accuracy=68.662, wps=6504.1, ups=0.79, wpb=8216.9, bsz=305.1, num_updates=39800, lr=7.08881e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=68, gb_free=16, wall=6857
2023-08-17 04:02:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-17 04:03:08 | INFO | train_inner | epoch 028:    120 / 1474 loss=1.911, trans_loss=4.769, nll_loss=1.969, w2v_ctc_loss=0.646, task_loss=3.437, contrastive_loss=0.078, total=4114.98, n_correct=2837.52, ppl=3.92, accuracy=68.956, wps=11852.5, ups=1.44, wpb=8230, bsz=293.7, num_updates=39900, lr=7.07992e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=69, gb_free=13.5, wall=6926
2023-08-17 04:04:18 | INFO | train_inner | epoch 028:    220 / 1474 loss=1.908, trans_loss=4.776, nll_loss=1.979, w2v_ctc_loss=0.637, task_loss=3.109, contrastive_loss=0.087, total=4193.3, n_correct=2889.79, ppl=3.94, accuracy=68.914, wps=12100.2, ups=1.44, wpb=8386.6, bsz=316.4, num_updates=40000, lr=7.07107e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=69, gb_free=16.7, wall=6996
2023-08-17 04:04:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 04:04:41 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.95 | trans_loss 5.167 | nll_loss 2.425 | w2v_ctc_loss 1.284 | task_loss 11.574 | contrastive_loss 0.304 | total 4003.4 | n_correct 2683 | ppl 5.37 | accuracy 67.018 | uer 17.575 | wer 19.433 | raw_wer 19.433 | bleu 22.3 | wps 2169.3 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 22.54
2023-08-17 04:04:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-17 04:04:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_28_40000.pt
2023-08-17 04:04:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_28_40000.pt
2023-08-17 04:05:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 22.3) (writing took 31.3638881649822 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
2023-08-17 04:06:23 | INFO | train_inner | epoch 028:    320 / 1474 loss=1.946, trans_loss=4.787, nll_loss=1.992, w2v_ctc_loss=0.637, task_loss=3.29, contrastive_loss=0.416, total=4138.69, n_correct=2833.25, ppl=3.98, accuracy=68.458, wps=6638.3, ups=0.8, wpb=8277.4, bsz=314.4, num_updates=40100, lr=7.06225e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=69, gb_free=13, wall=7120
2023-08-17 04:07:31 | INFO | train_inner | epoch 028:    420 / 1474 loss=1.913, trans_loss=4.776, nll_loss=1.979, w2v_ctc_loss=0.647, task_loss=3.405, contrastive_loss=0.075, total=4089.84, n_correct=2814, ppl=3.94, accuracy=68.805, wps=12022.1, ups=1.47, wpb=8179.7, bsz=295.7, num_updates=40200, lr=7.05346e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=68, gb_free=16.4, wall=7189
2023-08-17 04:08:39 | INFO | train_inner | epoch 028:    520 / 1474 loss=1.913, trans_loss=4.779, nll_loss=1.982, w2v_ctc_loss=0.642, task_loss=3.428, contrastive_loss=0.085, total=4098.92, n_correct=2819.29, ppl=3.95, accuracy=68.781, wps=11948.7, ups=1.46, wpb=8197.8, bsz=295.7, num_updates=40300, lr=7.0447e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=68, gb_free=16.7, wall=7257
2023-08-17 04:09:48 | INFO | train_inner | epoch 028:    620 / 1474 loss=1.918, trans_loss=4.789, nll_loss=1.996, w2v_ctc_loss=0.647, task_loss=3.324, contrastive_loss=0.087, total=4180.1, n_correct=2868.95, ppl=3.99, accuracy=68.634, wps=12089.4, ups=1.45, wpb=8360.2, bsz=305.3, num_updates=40400, lr=7.03598e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=69, gb_free=16.2, wall=7326
2023-08-17 04:10:57 | INFO | train_inner | epoch 028:    720 / 1474 loss=1.929, trans_loss=4.792, nll_loss=2.001, w2v_ctc_loss=0.648, task_loss=2.97, contrastive_loss=0.199, total=4191.62, n_correct=2874.97, ppl=4, accuracy=68.589, wps=12149, ups=1.45, wpb=8383.2, bsz=329.2, num_updates=40500, lr=7.02728e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=69, gb_free=16.8, wall=7395
2023-08-17 04:12:06 | INFO | train_inner | epoch 028:    820 / 1474 loss=1.912, trans_loss=4.782, nll_loss=1.987, w2v_ctc_loss=0.645, task_loss=3.271, contrastive_loss=0.077, total=4088.91, n_correct=2811.52, ppl=3.96, accuracy=68.76, wps=11966.7, ups=1.46, wpb=8177.8, bsz=304.3, num_updates=40600, lr=7.01862e-05, gnorm=0.566, clip=0, loss_scale=16, train_wall=68, gb_free=16.8, wall=7464
2023-08-17 04:13:15 | INFO | train_inner | epoch 028:    920 / 1474 loss=1.925, trans_loss=4.788, nll_loss=1.994, w2v_ctc_loss=0.646, task_loss=3.418, contrastive_loss=0.139, total=4117.01, n_correct=2819.47, ppl=3.98, accuracy=68.483, wps=11842.8, ups=1.44, wpb=8234, bsz=299.7, num_updates=40700, lr=7.01e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=69, gb_free=15, wall=7533
2023-08-17 04:14:24 | INFO | train_inner | epoch 028:   1020 / 1474 loss=1.932, trans_loss=4.787, nll_loss=1.993, w2v_ctc_loss=0.653, task_loss=3.212, contrastive_loss=0.194, total=4182.85, n_correct=2868.76, ppl=3.98, accuracy=68.584, wps=12110.5, ups=1.45, wpb=8365.7, bsz=312, num_updates=40800, lr=7.0014e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=69, gb_free=16.1, wall=7602
2023-08-17 04:15:34 | INFO | train_inner | epoch 028:   1120 / 1474 loss=1.913, trans_loss=4.78, nll_loss=1.985, w2v_ctc_loss=0.64, task_loss=3.165, contrastive_loss=0.1, total=4220.16, n_correct=2898.97, ppl=3.96, accuracy=68.693, wps=12121.3, ups=1.44, wpb=8440.3, bsz=321, num_updates=40900, lr=6.99284e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=69, gb_free=15.8, wall=7672
2023-08-17 04:16:42 | INFO | train_inner | epoch 028:   1220 / 1474 loss=1.917, trans_loss=4.79, nll_loss=1.997, w2v_ctc_loss=0.646, task_loss=3.289, contrastive_loss=0.085, total=4092.46, n_correct=2807.05, ppl=3.99, accuracy=68.591, wps=11937.3, ups=1.46, wpb=8184.9, bsz=303.1, num_updates=41000, lr=6.9843e-05, gnorm=0.57, clip=0, loss_scale=16, train_wall=68, gb_free=17.3, wall=7740
2023-08-17 04:17:52 | INFO | train_inner | epoch 028:   1320 / 1474 loss=1.924, trans_loss=4.788, nll_loss=1.993, w2v_ctc_loss=0.655, task_loss=3.613, contrastive_loss=0.101, total=4084.55, n_correct=2798.72, ppl=3.98, accuracy=68.52, wps=11803.1, ups=1.44, wpb=8169.1, bsz=285.1, num_updates=41100, lr=6.9758e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=69, gb_free=15.5, wall=7810
2023-08-17 04:19:01 | INFO | train_inner | epoch 028:   1420 / 1474 loss=1.932, trans_loss=4.794, nll_loss=2.002, w2v_ctc_loss=0.66, task_loss=3.451, contrastive_loss=0.124, total=4154.09, n_correct=2838.25, ppl=4, accuracy=68.324, wps=12000.5, ups=1.44, wpb=8308.2, bsz=299.3, num_updates=41200, lr=6.96733e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=69, gb_free=15.7, wall=7879
2023-08-17 04:19:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
2023-08-17 04:20:02 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.951 | trans_loss 5.168 | nll_loss 2.426 | w2v_ctc_loss 1.282 | task_loss 11.618 | contrastive_loss 0.308 | total 4003.4 | n_correct 2675.9 | ppl 5.37 | accuracy 66.841 | uer 17.53 | wer 19.343 | raw_wer 19.343 | bleu 22.39 | wps 2178.4 | wpb 4003.4 | bsz 141.8 | num_updates 41254 | best_bleu 22.54
2023-08-17 04:20:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41254 updates
2023-08-17 04:20:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3909.pt
2023-08-17 04:20:05 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3909.pt
2023-08-17 04:20:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3909.pt (epoch 28 @ 41254 updates, score 22.39) (writing took 21.920960407704115 seconds)
2023-08-17 04:20:24 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-17 04:20:24 | INFO | train | epoch 028 | loss 1.921 | trans_loss 4.784 | nll_loss 1.989 | w2v_ctc_loss 0.646 | task_loss 3.298 | contrastive_loss 0.131 | total 4138.57 | n_correct 2841.69 | ppl 3.97 | accuracy 68.664 | wps 10829.2 | ups 1.31 | wpb 8277.1 | bsz 305.6 | num_updates 41254 | lr 6.96277e-05 | gnorm 0.544 | clip 0 | loss_scale 16 | train_wall 1010 | gb_free 16.2 | wall 7962
2023-08-17 04:20:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 04:20:24 | INFO | fairseq.trainer | begin training epoch 29
2023-08-17 04:20:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 04:21:04 | INFO | train_inner | epoch 029:     46 / 1474 loss=1.909, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.643, task_loss=3.162, contrastive_loss=0.097, total=4169.12, n_correct=2875.27, ppl=3.93, accuracy=68.966, wps=6785.1, ups=0.81, wpb=8338.2, bsz=316.4, num_updates=41300, lr=6.95889e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=68, gb_free=16.1, wall=8002
2023-08-17 04:22:13 | INFO | train_inner | epoch 029:    146 / 1474 loss=1.915, trans_loss=4.776, nll_loss=1.978, w2v_ctc_loss=0.642, task_loss=3.288, contrastive_loss=0.117, total=4105.72, n_correct=2829.05, ppl=3.94, accuracy=68.905, wps=11891.5, ups=1.45, wpb=8211.4, bsz=304.1, num_updates=41400, lr=6.95048e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=69, gb_free=15.7, wall=8071
2023-08-17 04:23:22 | INFO | train_inner | epoch 029:    246 / 1474 loss=1.917, trans_loss=4.77, nll_loss=1.971, w2v_ctc_loss=0.635, task_loss=3.005, contrastive_loss=0.198, total=4199.67, n_correct=2893.28, ppl=3.92, accuracy=68.893, wps=12126.3, ups=1.44, wpb=8399.3, bsz=330.5, num_updates=41500, lr=6.9421e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=69, gb_free=15.4, wall=8140
2023-08-17 04:24:31 | INFO | train_inner | epoch 029:    346 / 1474 loss=1.921, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.657, task_loss=3.543, contrastive_loss=0.08, total=4095.17, n_correct=2810.93, ppl=3.97, accuracy=68.64, wps=11839, ups=1.45, wpb=8190.3, bsz=291.2, num_updates=41600, lr=6.93375e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=69, gb_free=16.4, wall=8209
2023-08-17 04:25:40 | INFO | train_inner | epoch 029:    446 / 1474 loss=1.897, trans_loss=4.754, nll_loss=1.95, w2v_ctc_loss=0.629, task_loss=3.177, contrastive_loss=0.077, total=4157.44, n_correct=2882.99, ppl=3.86, accuracy=69.345, wps=12100.5, ups=1.46, wpb=8314.9, bsz=307.7, num_updates=41700, lr=6.92543e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=68, gb_free=16.2, wall=8278
2023-08-17 04:26:50 | INFO | train_inner | epoch 029:    546 / 1474 loss=1.927, trans_loss=4.782, nll_loss=1.986, w2v_ctc_loss=0.642, task_loss=3.52, contrastive_loss=0.173, total=4150.87, n_correct=2849.05, ppl=3.96, accuracy=68.637, wps=11932.3, ups=1.44, wpb=8301.7, bsz=294.9, num_updates=41800, lr=6.91714e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=69, gb_free=15.4, wall=8348
2023-08-17 04:27:58 | INFO | train_inner | epoch 029:    646 / 1474 loss=1.92, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.637, task_loss=3.121, contrastive_loss=0.239, total=4143.02, n_correct=2854.43, ppl=3.93, accuracy=68.897, wps=12050.7, ups=1.45, wpb=8286, bsz=318.6, num_updates=41900, lr=6.90889e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=68, gb_free=16.7, wall=8416
2023-08-17 04:28:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-17 04:29:09 | INFO | train_inner | epoch 029:    747 / 1474 loss=1.918, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.643, task_loss=3.036, contrastive_loss=0.16, total=4241.89, n_correct=2918.22, ppl=3.93, accuracy=68.795, wps=12070.3, ups=1.42, wpb=8483.8, bsz=330, num_updates=42000, lr=6.90066e-05, gnorm=0.562, clip=0, loss_scale=16, train_wall=70, gb_free=16.3, wall=8487
2023-08-17 04:29:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 04:29:33 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.966 | trans_loss 5.167 | nll_loss 2.423 | w2v_ctc_loss 1.338 | task_loss 11.567 | contrastive_loss 0.307 | total 4003.4 | n_correct 2674.1 | ppl 5.36 | accuracy 66.796 | uer 17.718 | wer 19.503 | raw_wer 19.503 | bleu 22.29 | wps 2167.2 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 22.54
2023-08-17 04:29:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-17 04:29:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_29_42000.pt
2023-08-17 04:29:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_29_42000.pt
2023-08-17 04:30:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 22.29) (writing took 40.88137359172106 seconds)
2023-08-17 04:31:24 | INFO | train_inner | epoch 029:    847 / 1474 loss=1.914, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.64, task_loss=3.658, contrastive_loss=0.074, total=4027.03, n_correct=2763.39, ppl=3.97, accuracy=68.621, wps=5959, ups=0.74, wpb=8054.1, bsz=280.3, num_updates=42100, lr=6.89246e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=68, gb_free=17.1, wall=8622
2023-08-17 04:32:32 | INFO | train_inner | epoch 029:    947 / 1474 loss=1.918, trans_loss=4.782, nll_loss=1.986, w2v_ctc_loss=0.654, task_loss=3.383, contrastive_loss=0.086, total=4086.72, n_correct=2810.15, ppl=3.96, accuracy=68.763, wps=11971.3, ups=1.46, wpb=8173.4, bsz=296.3, num_updates=42200, lr=6.88428e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=68, gb_free=15.1, wall=8690
2023-08-17 04:33:41 | INFO | train_inner | epoch 029:   1047 / 1474 loss=1.916, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.638, task_loss=3.302, contrastive_loss=0.159, total=4139.4, n_correct=2851.3, ppl=3.94, accuracy=68.882, wps=12095.2, ups=1.46, wpb=8278.8, bsz=307.4, num_updates=42300, lr=6.87614e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=68, gb_free=15.3, wall=8759
2023-08-17 04:34:50 | INFO | train_inner | epoch 029:   1147 / 1474 loss=1.919, trans_loss=4.788, nll_loss=1.993, w2v_ctc_loss=0.651, task_loss=3.598, contrastive_loss=0.072, total=4072.33, n_correct=2795.6, ppl=3.98, accuracy=68.649, wps=11734.5, ups=1.44, wpb=8144.7, bsz=284.1, num_updates=42400, lr=6.86803e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=69, gb_free=16.6, wall=8828
2023-08-17 04:35:59 | INFO | train_inner | epoch 029:   1247 / 1474 loss=1.913, trans_loss=4.782, nll_loss=1.986, w2v_ctc_loss=0.642, task_loss=3.345, contrastive_loss=0.078, total=4160.52, n_correct=2860.85, ppl=3.96, accuracy=68.762, wps=12077.8, ups=1.45, wpb=8321, bsz=301.5, num_updates=42500, lr=6.85994e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=68, gb_free=15.3, wall=8897
2023-08-17 04:37:08 | INFO | train_inner | epoch 029:   1347 / 1474 loss=1.917, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.642, task_loss=3.262, contrastive_loss=0.143, total=4168.02, n_correct=2868.14, ppl=3.93, accuracy=68.813, wps=12088, ups=1.45, wpb=8336, bsz=310.2, num_updates=42600, lr=6.85189e-05, gnorm=0.557, clip=0, loss_scale=16, train_wall=68, gb_free=16.3, wall=8966
2023-08-17 04:38:17 | INFO | train_inner | epoch 029:   1447 / 1474 loss=1.913, trans_loss=4.772, nll_loss=1.974, w2v_ctc_loss=0.634, task_loss=3.219, contrastive_loss=0.168, total=4166.06, n_correct=2869.18, ppl=3.93, accuracy=68.87, wps=12112.6, ups=1.45, wpb=8332.1, bsz=313.1, num_updates=42700, lr=6.84386e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=68, gb_free=16.6, wall=9035
2023-08-17 04:38:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 04:38:58 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.939 | trans_loss 5.162 | nll_loss 2.419 | w2v_ctc_loss 1.262 | task_loss 11.611 | contrastive_loss 0.304 | total 4003.4 | n_correct 2677.4 | ppl 5.35 | accuracy 66.878 | uer 17.158 | wer 19.067 | raw_wer 19.067 | bleu 22.36 | wps 2242.8 | wpb 4003.4 | bsz 141.8 | num_updates 42727 | best_bleu 22.54
2023-08-17 04:38:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42727 updates
2023-08-17 04:38:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3601.pt
2023-08-17 04:39:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3601.pt
2023-08-17 04:39:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3601.pt (epoch 29 @ 42727 updates, score 22.36) (writing took 23.02539734914899 seconds)
2023-08-17 04:39:26 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-17 04:39:26 | INFO | train | epoch 029 | loss 1.915 | trans_loss 4.776 | nll_loss 1.978 | w2v_ctc_loss 0.641 | task_loss 3.3 | contrastive_loss 0.13 | total 4138.49 | n_correct 2848.56 | ppl 3.94 | accuracy 68.831 | wps 10676.9 | ups 1.29 | wpb 8277 | bsz 305.7 | num_updates 42727 | lr 6.8417e-05 | gnorm 0.544 | clip 0 | loss_scale 16 | train_wall 1009 | gb_free 15.8 | wall 9104
2023-08-17 04:39:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 04:39:26 | INFO | fairseq.trainer | begin training epoch 30
2023-08-17 04:39:26 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 04:40:25 | INFO | train_inner | epoch 030:     73 / 1474 loss=1.913, trans_loss=4.767, nll_loss=1.967, w2v_ctc_loss=0.631, task_loss=3.135, contrastive_loss=0.189, total=4175.11, n_correct=2879.45, ppl=3.91, accuracy=68.967, wps=6513.9, ups=0.78, wpb=8350.2, bsz=318.6, num_updates=42800, lr=6.83586e-05, gnorm=0.538, clip=0, loss_scale=16, train_wall=69, gb_free=16.8, wall=9163
2023-08-17 04:41:34 | INFO | train_inner | epoch 030:    173 / 1474 loss=1.899, trans_loss=4.749, nll_loss=1.944, w2v_ctc_loss=0.63, task_loss=3.086, contrastive_loss=0.119, total=4202.64, n_correct=2917.45, ppl=3.85, accuracy=69.419, wps=12220.3, ups=1.45, wpb=8405.3, bsz=318.3, num_updates=42900, lr=6.82789e-05, gnorm=0.556, clip=0, loss_scale=16, train_wall=68, gb_free=16.4, wall=9232
2023-08-17 04:42:42 | INFO | train_inner | epoch 030:    273 / 1474 loss=1.909, trans_loss=4.768, nll_loss=1.968, w2v_ctc_loss=0.644, task_loss=3.405, contrastive_loss=0.074, total=4120.21, n_correct=2842.1, ppl=3.91, accuracy=68.979, wps=12026.3, ups=1.46, wpb=8240.4, bsz=294.9, num_updates=43000, lr=6.81994e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=68, gb_free=15, wall=9300
2023-08-17 04:43:51 | INFO | train_inner | epoch 030:    373 / 1474 loss=1.895, trans_loss=4.755, nll_loss=1.951, w2v_ctc_loss=0.626, task_loss=3.289, contrastive_loss=0.076, total=4178.23, n_correct=2894.46, ppl=3.87, accuracy=69.275, wps=12074.7, ups=1.44, wpb=8356.5, bsz=307.5, num_updates=43100, lr=6.81203e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=69, gb_free=9.6, wall=9369
2023-08-17 04:45:00 | INFO | train_inner | epoch 030:    473 / 1474 loss=1.906, trans_loss=4.764, nll_loss=1.963, w2v_ctc_loss=0.63, task_loss=3.16, contrastive_loss=0.139, total=4124.47, n_correct=2848.62, ppl=3.9, accuracy=69.066, wps=12023.3, ups=1.46, wpb=8248.9, bsz=312.6, num_updates=43200, lr=6.80414e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=68, gb_free=17.3, wall=9438
2023-08-17 04:46:09 | INFO | train_inner | epoch 030:    573 / 1474 loss=1.903, trans_loss=4.764, nll_loss=1.964, w2v_ctc_loss=0.632, task_loss=3.199, contrastive_loss=0.103, total=4168.41, n_correct=2879.95, ppl=3.9, accuracy=69.09, wps=12161.8, ups=1.46, wpb=8336.8, bsz=312.4, num_updates=43300, lr=6.79628e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=68, gb_free=17, wall=9507
2023-08-17 04:47:18 | INFO | train_inner | epoch 030:    673 / 1474 loss=1.908, trans_loss=4.765, nll_loss=1.963, w2v_ctc_loss=0.64, task_loss=3.268, contrastive_loss=0.115, total=4187.95, n_correct=2888.51, ppl=3.9, accuracy=68.972, wps=12122.5, ups=1.45, wpb=8375.9, bsz=315, num_updates=43400, lr=6.78844e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=69, gb_free=15.5, wall=9576
2023-08-17 04:48:27 | INFO | train_inner | epoch 030:    773 / 1474 loss=1.924, trans_loss=4.774, nll_loss=1.975, w2v_ctc_loss=0.648, task_loss=3.364, contrastive_loss=0.193, total=4105.32, n_correct=2825.44, ppl=3.93, accuracy=68.824, wps=11871.2, ups=1.45, wpb=8210.6, bsz=302.6, num_updates=43500, lr=6.78064e-05, gnorm=0.556, clip=0, loss_scale=16, train_wall=69, gb_free=12.6, wall=9645
2023-08-17 04:49:36 | INFO | train_inner | epoch 030:    873 / 1474 loss=1.904, trans_loss=4.768, nll_loss=1.968, w2v_ctc_loss=0.629, task_loss=3.395, contrastive_loss=0.087, total=4102.11, n_correct=2831.94, ppl=3.91, accuracy=69.036, wps=11864.6, ups=1.45, wpb=8204.2, bsz=295.6, num_updates=43600, lr=6.77285e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=69, gb_free=17.1, wall=9714
2023-08-17 04:50:45 | INFO | train_inner | epoch 030:    973 / 1474 loss=1.91, trans_loss=4.774, nll_loss=1.975, w2v_ctc_loss=0.641, task_loss=3.376, contrastive_loss=0.09, total=4129.98, n_correct=2842.27, ppl=3.93, accuracy=68.82, wps=11992.3, ups=1.45, wpb=8260, bsz=300.4, num_updates=43700, lr=6.7651e-05, gnorm=0.559, clip=0, loss_scale=16, train_wall=68, gb_free=16.1, wall=9783
2023-08-17 04:51:54 | INFO | train_inner | epoch 030:   1073 / 1474 loss=1.919, trans_loss=4.774, nll_loss=1.975, w2v_ctc_loss=0.639, task_loss=3.699, contrastive_loss=0.166, total=4101.17, n_correct=2824.83, ppl=3.93, accuracy=68.879, wps=11867.1, ups=1.45, wpb=8202.3, bsz=282.3, num_updates=43800, lr=6.75737e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=69, gb_free=15.4, wall=9852
2023-08-17 04:53:03 | INFO | train_inner | epoch 030:   1173 / 1474 loss=1.908, trans_loss=4.768, nll_loss=1.969, w2v_ctc_loss=0.63, task_loss=3.174, contrastive_loss=0.147, total=4168.36, n_correct=2877.56, ppl=3.91, accuracy=69.033, wps=12068.9, ups=1.45, wpb=8336.7, bsz=314, num_updates=43900, lr=6.74967e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=69, gb_free=15.7, wall=9921
2023-08-17 04:54:13 | INFO | train_inner | epoch 030:   1273 / 1474 loss=1.91, trans_loss=4.771, nll_loss=1.972, w2v_ctc_loss=0.643, task_loss=3.65, contrastive_loss=0.08, total=4036.17, n_correct=2781.59, ppl=3.92, accuracy=68.917, wps=11558.5, ups=1.43, wpb=8072.3, bsz=284.3, num_updates=44000, lr=6.742e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=69, gb_free=15.5, wall=9991
2023-08-17 04:54:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 04:54:36 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.951 | trans_loss 5.164 | nll_loss 2.421 | w2v_ctc_loss 1.303 | task_loss 11.668 | contrastive_loss 0.298 | total 4003.4 | n_correct 2673.9 | ppl 5.35 | accuracy 66.791 | uer 17.347 | wer 19.104 | raw_wer 19.104 | bleu 22.21 | wps 2227.9 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 22.54
2023-08-17 04:54:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-17 04:54:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_30_44000.pt
2023-08-17 04:54:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_30_44000.pt
2023-08-17 04:55:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 22.21) (writing took 25.574974179267883 seconds)
2023-08-17 04:56:11 | INFO | train_inner | epoch 030:   1373 / 1474 loss=1.902, trans_loss=4.769, nll_loss=1.971, w2v_ctc_loss=0.633, task_loss=3.107, contrastive_loss=0.09, total=4165.07, n_correct=2875.99, ppl=3.92, accuracy=69.05, wps=7036.9, ups=0.84, wpb=8330.1, bsz=321.6, num_updates=44100, lr=6.73435e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=68, gb_free=16.4, wall=10109
2023-08-17 04:56:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-17 04:57:20 | INFO | train_inner | epoch 030:   1474 / 1474 loss=1.916, trans_loss=4.773, nll_loss=1.976, w2v_ctc_loss=0.628, task_loss=3.108, contrastive_loss=0.236, total=4119.06, n_correct=2838.8, ppl=3.93, accuracy=68.919, wps=11951.5, ups=1.45, wpb=8238.1, bsz=312.4, num_updates=44200, lr=6.72673e-05, gnorm=0.555, clip=0, loss_scale=16, train_wall=68, gb_free=16.9, wall=10178
2023-08-17 04:57:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 04:57:43 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.941 | trans_loss 5.167 | nll_loss 2.425 | w2v_ctc_loss 1.265 | task_loss 11.596 | contrastive_loss 0.297 | total 4003.4 | n_correct 2669.7 | ppl 5.37 | accuracy 66.686 | uer 17.514 | wer 19.373 | raw_wer 19.373 | bleu 22.39 | wps 2243.9 | wpb 4003.4 | bsz 141.8 | num_updates 44200 | best_bleu 22.54
2023-08-17 04:57:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44200 updates
2023-08-17 04:57:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3902.pt
2023-08-17 04:57:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3902.pt
2023-08-17 04:58:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3902.pt (epoch 30 @ 44200 updates, score 22.39) (writing took 24.228594291955233 seconds)
2023-08-17 04:58:08 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-17 04:58:08 | INFO | train | epoch 030 | loss 1.908 | trans_loss 4.767 | nll_loss 1.967 | w2v_ctc_loss 0.635 | task_loss 3.296 | contrastive_loss 0.128 | total 4138.61 | n_correct 2856.47 | ppl 3.91 | accuracy 69.02 | wps 10871.2 | ups 1.31 | wpb 8277.2 | bsz 305.7 | num_updates 44200 | lr 6.72673e-05 | gnorm 0.545 | clip 0 | loss_scale 16 | train_wall 1010 | gb_free 16.9 | wall 10226
2023-08-17 04:58:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 04:58:08 | INFO | fairseq.trainer | begin training epoch 31
2023-08-17 04:58:08 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 04:59:25 | INFO | train_inner | epoch 031:    100 / 1474 loss=1.903, trans_loss=4.756, nll_loss=1.953, w2v_ctc_loss=0.64, task_loss=3.471, contrastive_loss=0.084, total=4085.38, n_correct=2829.58, ppl=3.87, accuracy=69.261, wps=6564.4, ups=0.8, wpb=8170.8, bsz=293.4, num_updates=44300, lr=6.71913e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=69, gb_free=14.6, wall=10303
2023-08-17 05:00:34 | INFO | train_inner | epoch 031:    200 / 1474 loss=1.903, trans_loss=4.758, nll_loss=1.955, w2v_ctc_loss=0.635, task_loss=3.407, contrastive_loss=0.094, total=4139.51, n_correct=2862.62, ppl=3.88, accuracy=69.154, wps=11970.9, ups=1.45, wpb=8279, bsz=299.4, num_updates=44400, lr=6.71156e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=69, gb_free=11.5, wall=10372
2023-08-17 05:01:43 | INFO | train_inner | epoch 031:    300 / 1474 loss=1.901, trans_loss=4.752, nll_loss=1.947, w2v_ctc_loss=0.625, task_loss=3.381, contrastive_loss=0.14, total=4148.01, n_correct=2875.99, ppl=3.86, accuracy=69.334, wps=11995.1, ups=1.45, wpb=8296, bsz=301.1, num_updates=44500, lr=6.70402e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=69, gb_free=13, wall=10441
2023-08-17 05:02:52 | INFO | train_inner | epoch 031:    400 / 1474 loss=1.905, trans_loss=4.767, nll_loss=1.966, w2v_ctc_loss=0.633, task_loss=3.603, contrastive_loss=0.078, total=4095.42, n_correct=2826.73, ppl=3.91, accuracy=69.022, wps=11951.7, ups=1.46, wpb=8190.8, bsz=286.3, num_updates=44600, lr=6.6965e-05, gnorm=0.547, clip=0, loss_scale=16, train_wall=68, gb_free=13.1, wall=10510
2023-08-17 05:04:01 | INFO | train_inner | epoch 031:    500 / 1474 loss=1.901, trans_loss=4.755, nll_loss=1.951, w2v_ctc_loss=0.639, task_loss=3.439, contrastive_loss=0.086, total=4115.61, n_correct=2847.72, ppl=3.87, accuracy=69.193, wps=11870.7, ups=1.44, wpb=8231.2, bsz=300.6, num_updates=44700, lr=6.689e-05, gnorm=0.553, clip=0, loss_scale=16, train_wall=69, gb_free=16.1, wall=10579
2023-08-17 05:05:10 | INFO | train_inner | epoch 031:    600 / 1474 loss=1.897, trans_loss=4.757, nll_loss=1.954, w2v_ctc_loss=0.627, task_loss=3.46, contrastive_loss=0.075, total=4075.9, n_correct=2822.56, ppl=3.87, accuracy=69.25, wps=11771.2, ups=1.44, wpb=8151.8, bsz=293.3, num_updates=44800, lr=6.68153e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=69, gb_free=11.5, wall=10648
2023-08-17 05:06:19 | INFO | train_inner | epoch 031:    700 / 1474 loss=1.89, trans_loss=4.752, nll_loss=1.947, w2v_ctc_loss=0.619, task_loss=3.139, contrastive_loss=0.077, total=4208.99, n_correct=2918.52, ppl=3.86, accuracy=69.34, wps=12275.6, ups=1.46, wpb=8418, bsz=315.1, num_updates=44900, lr=6.67409e-05, gnorm=0.542, clip=0, loss_scale=16, train_wall=68, gb_free=17.6, wall=10717
2023-08-17 05:06:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-17 05:07:28 | INFO | train_inner | epoch 031:    801 / 1474 loss=1.905, trans_loss=4.761, nll_loss=1.959, w2v_ctc_loss=0.626, task_loss=3.472, contrastive_loss=0.13, total=4089.46, n_correct=2824.12, ppl=3.89, accuracy=69.059, wps=11793, ups=1.44, wpb=8178.9, bsz=293.7, num_updates=45000, lr=6.66667e-05, gnorm=0.556, clip=0, loss_scale=8, train_wall=69, gb_free=15.4, wall=10786
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
2023-08-17 05:08:37 | INFO | train_inner | epoch 031:    901 / 1474 loss=1.903, trans_loss=4.753, nll_loss=1.948, w2v_ctc_loss=0.639, task_loss=3.486, contrastive_loss=0.093, total=4098.59, n_correct=2839.27, ppl=3.86, accuracy=69.274, wps=11903.4, ups=1.45, wpb=8197.2, bsz=293.9, num_updates=45100, lr=6.65927e-05, gnorm=0.542, clip=0, loss_scale=8, train_wall=68, gb_free=17, wall=10855
2023-08-17 05:09:46 | INFO | train_inner | epoch 031:   1001 / 1474 loss=1.91, trans_loss=4.768, nll_loss=1.97, w2v_ctc_loss=0.631, task_loss=3.084, contrastive_loss=0.175, total=4184.36, n_correct=2888.44, ppl=3.92, accuracy=69.029, wps=12078.3, ups=1.44, wpb=8368.7, bsz=320.3, num_updates=45200, lr=6.6519e-05, gnorm=0.579, clip=0, loss_scale=8, train_wall=69, gb_free=15.7, wall=10924
2023-08-17 05:10:55 | INFO | train_inner | epoch 031:   1101 / 1474 loss=1.904, trans_loss=4.762, nll_loss=1.961, w2v_ctc_loss=0.63, task_loss=3.218, contrastive_loss=0.122, total=4150.56, n_correct=2870.15, ppl=3.89, accuracy=69.151, wps=12105.3, ups=1.46, wpb=8301.1, bsz=315.1, num_updates=45300, lr=6.64455e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=68, gb_free=14.9, wall=10993
2023-08-17 05:12:03 | INFO | train_inner | epoch 031:   1201 / 1474 loss=1.911, trans_loss=4.762, nll_loss=1.961, w2v_ctc_loss=0.627, task_loss=3.069, contrastive_loss=0.239, total=4190.99, n_correct=2897.8, ppl=3.89, accuracy=69.144, wps=12271.5, ups=1.46, wpb=8382, bsz=323, num_updates=45400, lr=6.63723e-05, gnorm=0.553, clip=0, loss_scale=8, train_wall=68, gb_free=16.9, wall=11061
2023-08-17 05:13:11 | INFO | train_inner | epoch 031:   1301 / 1474 loss=1.897, trans_loss=4.763, nll_loss=1.962, w2v_ctc_loss=0.63, task_loss=2.966, contrastive_loss=0.083, total=4226.19, n_correct=2923.52, ppl=3.9, accuracy=69.176, wps=12408, ups=1.47, wpb=8452.4, bsz=325.1, num_updates=45500, lr=6.62994e-05, gnorm=0.585, clip=0, loss_scale=8, train_wall=68, gb_free=16.7, wall=11129
2023-08-17 05:14:21 | INFO | train_inner | epoch 031:   1401 / 1474 loss=1.921, trans_loss=4.764, nll_loss=1.964, w2v_ctc_loss=0.627, task_loss=3.013, contrastive_loss=0.286, total=4192.11, n_correct=2891.84, ppl=3.9, accuracy=68.983, wps=11977.1, ups=1.43, wpb=8384.2, bsz=327, num_updates=45600, lr=6.62266e-05, gnorm=0.529, clip=0, loss_scale=8, train_wall=70, gb_free=16, wall=11199
2023-08-17 05:15:11 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
mt_weight tensor(0.5000)
asr_weight tensor(0.2448)
2023-08-17 05:15:35 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 3.958 | trans_loss 5.161 | nll_loss 2.416 | w2v_ctc_loss 1.332 | task_loss 11.61 | contrastive_loss 0.302 | total 4003.4 | n_correct 2680.4 | ppl 5.34 | accuracy 66.953 | uer 17.233 | wer 19.004 | raw_wer 19.004 | bleu 22.31 | wps 2160.4 | wpb 4003.4 | bsz 141.8 | num_updates 45673 | best_bleu 22.54
2023-08-17 05:15:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45673 updates
2023-08-17 05:15:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3106.pt
2023-08-17 05:15:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3106.pt
2023-08-17 05:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3106.pt (epoch 31 @ 45673 updates, score 22.31) (writing took 25.744560597464442 seconds)
2023-08-17 05:16:01 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-17 05:16:01 | INFO | train | epoch 031 | loss 1.904 | trans_loss 4.759 | nll_loss 1.957 | w2v_ctc_loss 0.63 | task_loss 3.3 | contrastive_loss 0.125 | total 4137.67 | n_correct 2861.83 | ppl 3.88 | accuracy 69.165 | wps 11356.9 | ups 1.37 | wpb 8275.3 | bsz 305.4 | num_updates 45673 | lr 6.61737e-05 | gnorm 0.554 | clip 0 | loss_scale 8 | train_wall 1009 | gb_free 11.7 | wall 11299
2023-08-17 05:16:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 05:16:01 | INFO | fairseq.trainer | begin training epoch 32
2023-08-17 05:16:01 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 05:16:28 | INFO | train_inner | epoch 032:     27 / 1474 loss=1.897, trans_loss=4.756, nll_loss=1.952, w2v_ctc_loss=0.629, task_loss=3.44, contrastive_loss=0.074, total=4051.41, n_correct=2805.11, ppl=3.87, accuracy=69.238, wps=6412.4, ups=0.79, wpb=8102.8, bsz=291.6, num_updates=45700, lr=6.61541e-05, gnorm=0.583, clip=0, loss_scale=8, train_wall=68, gb_free=15.7, wall=11326
2023-08-17 05:17:36 | INFO | train_inner | epoch 032:    127 / 1474 loss=1.883, trans_loss=4.738, nll_loss=1.929, w2v_ctc_loss=0.612, task_loss=3.087, contrastive_loss=0.083, total=4208.32, n_correct=2932.24, ppl=3.81, accuracy=69.677, wps=12234.2, ups=1.45, wpb=8416.6, bsz=319.1, num_updates=45800, lr=6.60819e-05, gnorm=0.581, clip=0, loss_scale=8, train_wall=68, gb_free=11.2, wall=11394
2023-08-17 05:18:46 | INFO | train_inner | epoch 032:    227 / 1474 loss=1.893, trans_loss=4.754, nll_loss=1.951, w2v_ctc_loss=0.625, task_loss=3.144, contrastive_loss=0.092, total=4157.86, n_correct=2880.38, ppl=3.87, accuracy=69.276, wps=11984.3, ups=1.44, wpb=8315.7, bsz=320.3, num_updates=45900, lr=6.60098e-05, gnorm=0.545, clip=0, loss_scale=8, train_wall=69, gb_free=14, wall=11464
2023-08-17 05:19:55 | INFO | train_inner | epoch 032:    327 / 1474 loss=1.882, trans_loss=4.735, nll_loss=1.925, w2v_ctc_loss=0.611, task_loss=3.129, contrastive_loss=0.086, total=4179.17, n_correct=2916.96, ppl=3.8, accuracy=69.798, wps=12116.2, ups=1.45, wpb=8358.3, bsz=313.4, num_updates=46000, lr=6.5938e-05, gnorm=0.536, clip=0, loss_scale=8, train_wall=68, gb_free=16.8, wall=11533
2023-08-17 05:19:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 05:20:18 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.956 | trans_loss 5.166 | nll_loss 2.424 | w2v_ctc_loss 1.317 | task_loss 11.631 | contrastive_loss 0.292 | total 4003.4 | n_correct 2673.8 | ppl 5.37 | accuracy 66.788 | uer 17.54 | wer 19.324 | raw_wer 19.324 | bleu 22.31 | wps 2243.6 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 22.54
2023-08-17 05:20:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-17 05:20:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_32_46000.pt
2023-08-17 05:20:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_32_46000.pt
2023-08-17 05:20:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 22.31) (writing took 41.262721514329314 seconds)
2023-08-17 05:22:12 | INFO | train_inner | epoch 032:    427 / 1474 loss=1.888, trans_loss=4.74, nll_loss=1.933, w2v_ctc_loss=0.623, task_loss=3.185, contrastive_loss=0.083, total=4179.5, n_correct=2908.94, ppl=3.82, accuracy=69.6, wps=6114.3, ups=0.73, wpb=8359, bsz=312.2, num_updates=46100, lr=6.58665e-05, gnorm=0.553, clip=0, loss_scale=8, train_wall=68, gb_free=17.5, wall=11670
2023-08-17 05:23:21 | INFO | train_inner | epoch 032:    527 / 1474 loss=1.906, trans_loss=4.754, nll_loss=1.95, w2v_ctc_loss=0.631, task_loss=3.247, contrastive_loss=0.165, total=4188.83, n_correct=2901.31, ppl=3.87, accuracy=69.263, wps=12098.1, ups=1.44, wpb=8377.7, bsz=313.9, num_updates=46200, lr=6.57952e-05, gnorm=0.547, clip=0, loss_scale=8, train_wall=69, gb_free=12.4, wall=11739
2023-08-17 05:24:30 | INFO | train_inner | epoch 032:    627 / 1474 loss=1.899, trans_loss=4.756, nll_loss=1.952, w2v_ctc_loss=0.631, task_loss=3.455, contrastive_loss=0.09, total=4133.19, n_correct=2861.31, ppl=3.87, accuracy=69.228, wps=11893.5, ups=1.44, wpb=8266.4, bsz=298.7, num_updates=46300, lr=6.57241e-05, gnorm=0.587, clip=0, loss_scale=8, train_wall=69, gb_free=17, wall=11808
2023-08-17 05:25:40 | INFO | train_inner | epoch 032:    727 / 1474 loss=1.897, trans_loss=4.755, nll_loss=1.951, w2v_ctc_loss=0.632, task_loss=3.314, contrastive_loss=0.074, total=4162.1, n_correct=2883.47, ppl=3.87, accuracy=69.279, wps=12021.1, ups=1.44, wpb=8324.2, bsz=304.3, num_updates=46400, lr=6.56532e-05, gnorm=0.591, clip=0, loss_scale=8, train_wall=69, gb_free=15.6, wall=11878
2023-08-17 05:26:48 | INFO | train_inner | epoch 032:    827 / 1474 loss=1.895, trans_loss=4.752, nll_loss=1.948, w2v_ctc_loss=0.627, task_loss=3.433, contrastive_loss=0.073, total=4107.86, n_correct=2848.21, ppl=3.86, accuracy=69.336, wps=12028, ups=1.46, wpb=8215.7, bsz=292.6, num_updates=46500, lr=6.55826e-05, gnorm=0.642, clip=0, loss_scale=8, train_wall=68, gb_free=15.5, wall=11946
2023-08-17 05:27:57 | INFO | train_inner | epoch 032:    927 / 1474 loss=1.891, trans_loss=4.751, nll_loss=1.946, w2v_ctc_loss=0.622, task_loss=3.374, contrastive_loss=0.069, total=4146.9, n_correct=2875.64, ppl=3.85, accuracy=69.344, wps=12054.8, ups=1.45, wpb=8293.8, bsz=300.4, num_updates=46600, lr=6.55122e-05, gnorm=0.538, clip=0, loss_scale=8, train_wall=68, gb_free=16.6, wall=12015
2023-08-17 05:29:06 | INFO | train_inner | epoch 032:   1027 / 1474 loss=1.904, trans_loss=4.757, nll_loss=1.954, w2v_ctc_loss=0.627, task_loss=3.276, contrastive_loss=0.163, total=4112.45, n_correct=2848.84, ppl=3.87, accuracy=69.274, wps=11902, ups=1.45, wpb=8224.9, bsz=304.3, num_updates=46700, lr=6.5442e-05, gnorm=0.543, clip=0, loss_scale=8, train_wall=69, gb_free=16.9, wall=12084
2023-08-17 05:30:15 | INFO | train_inner | epoch 032:   1127 / 1474 loss=1.909, trans_loss=4.759, nll_loss=1.956, w2v_ctc_loss=0.637, task_loss=3.922, contrastive_loss=0.107, total=4015.2, n_correct=2772.32, ppl=3.88, accuracy=69.046, wps=11558.2, ups=1.44, wpb=8030.4, bsz=269.7, num_updates=46800, lr=6.5372e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=69, gb_free=14.1, wall=12153
2023-08-17 05:31:24 | INFO | train_inner | epoch 032:   1227 / 1474 loss=1.912, trans_loss=4.762, nll_loss=1.961, w2v_ctc_loss=0.623, task_loss=3.224, contrastive_loss=0.213, total=4158.99, n_correct=2874.2, ppl=3.89, accuracy=69.108, wps=12096.5, ups=1.45, wpb=8318, bsz=312.1, num_updates=46900, lr=6.53023e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=68, gb_free=15.9, wall=12222
2023-08-17 05:32:32 | INFO | train_inner | epoch 032:   1327 / 1474 loss=1.897, trans_loss=4.754, nll_loss=1.95, w2v_ctc_loss=0.632, task_loss=3.351, contrastive_loss=0.072, total=4079.56, n_correct=2824, ppl=3.86, accuracy=69.223, wps=12015.9, ups=1.47, wpb=8159.1, bsz=297.9, num_updates=47000, lr=6.52328e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=67, gb_free=16.9, wall=12290
2023-08-17 05:33:41 | INFO | train_inner | epoch 032:   1427 / 1474 loss=1.921, trans_loss=4.758, nll_loss=1.955, w2v_ctc_loss=0.636, task_loss=3.342, contrastive_loss=0.307, total=4107.37, n_correct=2838.86, ppl=3.88, accuracy=69.116, wps=11924.1, ups=1.45, wpb=8214.7, bsz=304.2, num_updates=47100, lr=6.51635e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=68, gb_free=17.1, wall=12359
2023-08-17 05:34:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 05:34:36 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.954 | trans_loss 5.159 | nll_loss 2.413 | w2v_ctc_loss 1.327 | task_loss 11.626 | contrastive_loss 0.297 | total 4003.4 | n_correct 2676.7 | ppl 5.33 | accuracy 66.861 | uer 17.27 | wer 19.104 | raw_wer 19.104 | bleu 22.38 | wps 2220.5 | wpb 4003.4 | bsz 141.8 | num_updates 47147 | best_bleu 22.54
2023-08-17 05:34:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47147 updates
2023-08-17 05:34:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3804.pt
2023-08-17 05:34:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3804.pt
2023-08-17 05:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.3804.pt (epoch 32 @ 47147 updates, score 22.38) (writing took 26.746611887589097 seconds)
2023-08-17 05:35:04 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-17 05:35:04 | INFO | train | epoch 032 | loss 1.898 | trans_loss 4.752 | nll_loss 1.947 | w2v_ctc_loss 0.626 | task_loss 3.294 | contrastive_loss 0.125 | total 4138.65 | n_correct 2869.23 | ppl 3.86 | accuracy 69.328 | wps 10678.6 | ups 1.29 | wpb 8277.3 | bsz 305.7 | num_updates 47147 | lr 6.5131e-05 | gnorm 0.563 | clip 0 | loss_scale 16 | train_wall 1009 | gb_free 16.2 | wall 12442
2023-08-17 05:35:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 05:35:04 | INFO | fairseq.trainer | begin training epoch 33
2023-08-17 05:35:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 05:35:48 | INFO | train_inner | epoch 033:     53 / 1474 loss=1.9, trans_loss=4.751, nll_loss=1.946, w2v_ctc_loss=0.62, task_loss=3.12, contrastive_loss=0.175, total=4146.91, n_correct=2876.56, ppl=3.85, accuracy=69.366, wps=6533.7, ups=0.79, wpb=8293.8, bsz=319.7, num_updates=47200, lr=6.50945e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=68, gb_free=15.8, wall=12486
2023-08-17 05:36:56 | INFO | train_inner | epoch 033:    153 / 1474 loss=1.883, trans_loss=4.736, nll_loss=1.926, w2v_ctc_loss=0.612, task_loss=3.535, contrastive_loss=0.062, total=4073.36, n_correct=2838.96, ppl=3.8, accuracy=69.696, wps=11904.1, ups=1.46, wpb=8146.7, bsz=285.1, num_updates=47300, lr=6.50256e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=68, gb_free=16.4, wall=12554
2023-08-17 05:38:05 | INFO | train_inner | epoch 033:    253 / 1474 loss=1.898, trans_loss=4.736, nll_loss=1.928, w2v_ctc_loss=0.61, task_loss=2.785, contrastive_loss=0.238, total=4283.64, n_correct=2982.85, ppl=3.81, accuracy=69.634, wps=12391.9, ups=1.45, wpb=8567.3, bsz=347.6, num_updates=47400, lr=6.4957e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=69, gb_free=16.1, wall=12623
2023-08-17 05:39:15 | INFO | train_inner | epoch 033:    353 / 1474 loss=1.894, trans_loss=4.745, nll_loss=1.938, w2v_ctc_loss=0.628, task_loss=3.354, contrastive_loss=0.09, total=4131.27, n_correct=2867.13, ppl=3.83, accuracy=69.401, wps=11895.9, ups=1.44, wpb=8262.5, bsz=302.3, num_updates=47500, lr=6.48886e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=69, gb_free=15.9, wall=12693
2023-08-17 05:39:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-17 05:40:23 | INFO | train_inner | epoch 033:    454 / 1474 loss=1.877, trans_loss=4.731, nll_loss=1.92, w2v_ctc_loss=0.61, task_loss=3.141, contrastive_loss=0.069, total=4131.66, n_correct=2883.04, ppl=3.78, accuracy=69.779, wps=12038.7, ups=1.46, wpb=8263.3, bsz=308.8, num_updates=47600, lr=6.48204e-05, gnorm=0.536, clip=0, loss_scale=8, train_wall=68, gb_free=15.4, wall=12761
2023-08-17 05:41:32 | INFO | train_inner | epoch 033:    554 / 1474 loss=1.895, trans_loss=4.747, nll_loss=1.94, w2v_ctc_loss=0.625, task_loss=3.423, contrastive_loss=0.093, total=4136.29, n_correct=2872.36, ppl=3.84, accuracy=69.443, wps=12026.5, ups=1.45, wpb=8272.6, bsz=294.8, num_updates=47700, lr=6.47524e-05, gnorm=0.555, clip=0, loss_scale=8, train_wall=68, gb_free=14.6, wall=12830
2023-08-17 05:42:41 | INFO | train_inner | epoch 033:    654 / 1474 loss=1.901, trans_loss=4.758, nll_loss=1.954, w2v_ctc_loss=0.623, task_loss=3.398, contrastive_loss=0.125, total=4155.56, n_correct=2878.44, ppl=3.87, accuracy=69.267, wps=12017.5, ups=1.45, wpb=8311.1, bsz=300.4, num_updates=47800, lr=6.46846e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=69, gb_free=14.9, wall=12899
2023-08-17 05:43:51 | INFO | train_inner | epoch 033:    754 / 1474 loss=1.898, trans_loss=4.749, nll_loss=1.943, w2v_ctc_loss=0.638, task_loss=3.528, contrastive_loss=0.071, total=4076.72, n_correct=2828.29, ppl=3.85, accuracy=69.377, wps=11728.1, ups=1.44, wpb=8153.4, bsz=289.6, num_updates=47900, lr=6.46171e-05, gnorm=0.551, clip=0, loss_scale=8, train_wall=69, gb_free=16.3, wall=12969
2023-08-17 05:45:00 | INFO | train_inner | epoch 033:    854 / 1474 loss=1.888, trans_loss=4.741, nll_loss=1.933, w2v_ctc_loss=0.613, task_loss=3.152, contrastive_loss=0.141, total=4126.23, n_correct=2874.02, ppl=3.82, accuracy=69.652, wps=12009, ups=1.46, wpb=8252.5, bsz=314.4, num_updates=48000, lr=6.45497e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=68, gb_free=15.7, wall=13038
2023-08-17 05:45:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 05:45:22 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.957 | trans_loss 5.173 | nll_loss 2.431 | w2v_ctc_loss 1.304 | task_loss 11.562 | contrastive_loss 0.297 | total 4003.4 | n_correct 2674.7 | ppl 5.39 | accuracy 66.811 | uer 17.328 | wer 19.097 | raw_wer 19.097 | bleu 22.14 | wps 2255.4 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 22.54
2023-08-17 05:45:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-17 05:45:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_33_48000.pt
2023-08-17 05:45:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_33_48000.pt
2023-08-17 05:45:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 22.14) (writing took 26.180449228733778 seconds)
2023-08-17 05:46:57 | INFO | train_inner | epoch 033:    954 / 1474 loss=1.894, trans_loss=4.747, nll_loss=1.941, w2v_ctc_loss=0.633, task_loss=3.25, contrastive_loss=0.085, total=4161.72, n_correct=2889.78, ppl=3.84, accuracy=69.437, wps=7075.3, ups=0.85, wpb=8323.4, bsz=312, num_updates=48100, lr=6.44826e-05, gnorm=0.546, clip=0, loss_scale=8, train_wall=68, gb_free=15.7, wall=13155
2023-08-17 05:48:07 | INFO | train_inner | epoch 033:   1054 / 1474 loss=1.899, trans_loss=4.741, nll_loss=1.933, w2v_ctc_loss=0.621, task_loss=3.342, contrastive_loss=0.185, total=4134.8, n_correct=2874.43, ppl=3.82, accuracy=69.518, wps=11946.9, ups=1.44, wpb=8269.6, bsz=304.7, num_updates=48200, lr=6.44157e-05, gnorm=0.551, clip=0, loss_scale=8, train_wall=69, gb_free=16.5, wall=13225
2023-08-17 05:49:17 | INFO | train_inner | epoch 033:   1154 / 1474 loss=1.899, trans_loss=4.754, nll_loss=1.949, w2v_ctc_loss=0.615, task_loss=3.311, contrastive_loss=0.173, total=4177.62, n_correct=2898.29, ppl=3.86, accuracy=69.377, wps=11913.2, ups=1.43, wpb=8355.2, bsz=309.6, num_updates=48300, lr=6.43489e-05, gnorm=0.56, clip=0, loss_scale=8, train_wall=70, gb_free=16.2, wall=13295
2023-08-17 05:50:25 | INFO | train_inner | epoch 033:   1254 / 1474 loss=1.892, trans_loss=4.745, nll_loss=1.939, w2v_ctc_loss=0.625, task_loss=3.455, contrastive_loss=0.074, total=4115.15, n_correct=2860.01, ppl=3.83, accuracy=69.5, wps=11965.1, ups=1.45, wpb=8230.3, bsz=294.4, num_updates=48400, lr=6.42824e-05, gnorm=0.58, clip=0, loss_scale=8, train_wall=68, gb_free=16, wall=13363
2023-08-17 05:51:34 | INFO | train_inner | epoch 033:   1354 / 1474 loss=1.89, trans_loss=4.746, nll_loss=1.94, w2v_ctc_loss=0.624, task_loss=3.233, contrastive_loss=0.094, total=4121.6, n_correct=2868.24, ppl=3.84, accuracy=69.59, wps=11949.8, ups=1.45, wpb=8243.2, bsz=311.7, num_updates=48500, lr=6.42161e-05, gnorm=0.533, clip=0, loss_scale=8, train_wall=69, gb_free=15.1, wall=13432
2023-08-17 05:52:43 | INFO | train_inner | epoch 033:   1454 / 1474 loss=1.903, trans_loss=4.748, nll_loss=1.943, w2v_ctc_loss=0.618, task_loss=3.273, contrastive_loss=0.237, total=4131.62, n_correct=2867.91, ppl=3.85, accuracy=69.414, wps=12040.6, ups=1.46, wpb=8263.2, bsz=309.9, num_updates=48600, lr=6.415e-05, gnorm=0.565, clip=0, loss_scale=8, train_wall=68, gb_free=16.4, wall=13501
2023-08-17 05:52:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 05:53:20 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.964 | trans_loss 5.164 | nll_loss 2.42 | w2v_ctc_loss 1.349 | task_loss 11.71 | contrastive_loss 0.295 | total 4003.4 | n_correct 2677 | ppl 5.35 | accuracy 66.868 | uer 17.437 | wer 19.265 | raw_wer 19.265 | bleu 22.46 | wps 2129.6 | wpb 4003.4 | bsz 141.8 | num_updates 48620 | best_bleu 22.54
2023-08-17 05:53:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48620 updates
2023-08-17 05:53:20 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.4600.pt
2023-08-17 05:53:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.4600.pt
2023-08-17 05:53:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint.best_bleu_22.4600.pt (epoch 33 @ 48620 updates, score 22.46) (writing took 24.22829613648355 seconds)
2023-08-17 05:53:45 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-17 05:53:45 | INFO | train | epoch 033 | loss 1.893 | trans_loss 4.744 | nll_loss 1.938 | w2v_ctc_loss 0.621 | task_loss 3.294 | contrastive_loss 0.123 | total 4138.71 | n_correct 2876.74 | ppl 3.83 | accuracy 69.508 | wps 10874.8 | ups 1.31 | wpb 8277.4 | bsz 305.7 | num_updates 48620 | lr 6.41368e-05 | gnorm 0.552 | clip 0 | loss_scale 8 | train_wall 1009 | gb_free 17.6 | wall 13563
2023-08-17 05:53:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-17 05:53:45 | INFO | fairseq.trainer | begin training epoch 34
2023-08-17 05:53:45 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-17 05:54:49 | INFO | train_inner | epoch 034:     80 / 1474 loss=1.885, trans_loss=4.734, nll_loss=1.924, w2v_ctc_loss=0.62, task_loss=3.28, contrastive_loss=0.076, total=4123.05, n_correct=2874.66, ppl=3.79, accuracy=69.722, wps=6560.5, ups=0.8, wpb=8246.1, bsz=300.6, num_updates=48700, lr=6.40841e-05, gnorm=0.552, clip=0, loss_scale=8, train_wall=69, gb_free=15.7, wall=13627
2023-08-17 05:55:58 | INFO | train_inner | epoch 034:    180 / 1474 loss=1.878, trans_loss=4.723, nll_loss=1.909, w2v_ctc_loss=0.613, task_loss=3.438, contrastive_loss=0.077, total=4066.35, n_correct=2848.88, ppl=3.76, accuracy=70.06, wps=11816.7, ups=1.45, wpb=8132.7, bsz=295.2, num_updates=48800, lr=6.40184e-05, gnorm=0.567, clip=0, loss_scale=8, train_wall=68, gb_free=16.4, wall=13696
2023-08-17 05:57:07 | INFO | train_inner | epoch 034:    280 / 1474 loss=1.905, trans_loss=4.743, nll_loss=1.936, w2v_ctc_loss=0.609, task_loss=3.07, contrastive_loss=0.281, total=4247.33, n_correct=2952.46, ppl=3.83, accuracy=69.513, wps=12298.5, ups=1.45, wpb=8494.7, bsz=329.5, num_updates=48900, lr=6.39529e-05, gnorm=0.549, clip=0, loss_scale=8, train_wall=69, gb_free=16.2, wall=13765
2023-08-17 05:58:16 | INFO | train_inner | epoch 034:    380 / 1474 loss=1.886, trans_loss=4.727, nll_loss=1.915, w2v_ctc_loss=0.609, task_loss=3.143, contrastive_loss=0.177, total=4152.22, n_correct=2898.97, ppl=3.77, accuracy=69.817, wps=12028.1, ups=1.45, wpb=8304.4, bsz=316.1, num_updates=49000, lr=6.38877e-05, gnorm=0.536, clip=0, loss_scale=8, train_wall=69, gb_free=15.6, wall=13834
2023-08-17 05:59:25 | INFO | train_inner | epoch 034:    480 / 1474 loss=1.888, trans_loss=4.737, nll_loss=1.926, w2v_ctc_loss=0.624, task_loss=3.586, contrastive_loss=0.07, total=4080.7, n_correct=2842.72, ppl=3.8, accuracy=69.663, wps=11710.4, ups=1.43, wpb=8161.4, bsz=286.7, num_updates=49100, lr=6.38226e-05, gnorm=0.55, clip=0, loss_scale=8, train_wall=69, gb_free=15.4, wall=13903
2023-08-17 06:00:34 | INFO | train_inner | epoch 034:    580 / 1474 loss=1.879, trans_loss=4.728, nll_loss=1.916, w2v_ctc_loss=0.613, task_loss=3.345, contrastive_loss=0.071, total=4126.98, n_correct=2884.45, ppl=3.77, accuracy=69.893, wps=12109.6, ups=1.47, wpb=8254, bsz=300.1, num_updates=49200, lr=6.37577e-05, gnorm=0.598, clip=0, loss_scale=8, train_wall=68, gb_free=16, wall=13972
2023-08-17 06:01:42 | INFO | train_inner | epoch 034:    680 / 1474 loss=1.878, trans_loss=4.733, nll_loss=1.923, w2v_ctc_loss=0.609, task_loss=3.398, contrastive_loss=0.066, total=4110.23, n_correct=2869.9, ppl=3.79, accuracy=69.823, wps=12045.5, ups=1.47, wpb=8220.5, bsz=297.1, num_updates=49300, lr=6.3693e-05, gnorm=0.545, clip=0, loss_scale=8, train_wall=68, gb_free=16, wall=14040
2023-08-17 06:02:50 | INFO | train_inner | epoch 034:    780 / 1474 loss=1.892, trans_loss=4.752, nll_loss=1.947, w2v_ctc_loss=0.608, task_loss=3.419, contrastive_loss=0.136, total=4087.05, n_correct=2838.25, ppl=3.86, accuracy=69.445, wps=11978, ups=1.47, wpb=8174.1, bsz=297.4, num_updates=49400, lr=6.36285e-05, gnorm=0.567, clip=0, loss_scale=8, train_wall=68, gb_free=15.8, wall=14108
2023-08-17 06:04:00 | INFO | train_inner | epoch 034:    880 / 1474 loss=1.891, trans_loss=4.744, nll_loss=1.936, w2v_ctc_loss=0.619, task_loss=3.51, contrastive_loss=0.094, total=4088.94, n_correct=2841.25, ppl=3.83, accuracy=69.486, wps=11736.3, ups=1.44, wpb=8177.9, bsz=294.2, num_updates=49500, lr=6.35642e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=69, gb_free=14.5, wall=14178
2023-08-17 06:05:09 | INFO | train_inner | epoch 034:    980 / 1474 loss=1.891, trans_loss=4.742, nll_loss=1.935, w2v_ctc_loss=0.626, task_loss=3.226, contrastive_loss=0.091, total=4175.9, n_correct=2902.25, ppl=3.82, accuracy=69.5, wps=12149.3, ups=1.45, wpb=8351.8, bsz=312.7, num_updates=49600, lr=6.35001e-05, gnorm=0.544, clip=0, loss_scale=16, train_wall=68, gb_free=13.4, wall=14246
2023-08-17 06:06:17 | INFO | train_inner | epoch 034:   1080 / 1474 loss=1.885, trans_loss=4.739, nll_loss=1.93, w2v_ctc_loss=0.621, task_loss=3.177, contrastive_loss=0.072, total=4152.17, n_correct=2896.3, ppl=3.81, accuracy=69.754, wps=12174.1, ups=1.47, wpb=8304.3, bsz=309, num_updates=49700, lr=6.34361e-05, gnorm=0.577, clip=0, loss_scale=16, train_wall=68, gb_free=14.1, wall=14315
2023-08-17 06:07:25 | INFO | train_inner | epoch 034:   1180 / 1474 loss=1.889, trans_loss=4.742, nll_loss=1.935, w2v_ctc_loss=0.621, task_loss=3.388, contrastive_loss=0.085, total=4101.68, n_correct=2854.74, ppl=3.82, accuracy=69.599, wps=12003, ups=1.46, wpb=8203.4, bsz=298, num_updates=49800, lr=6.33724e-05, gnorm=0.557, clip=0, loss_scale=16, train_wall=68, gb_free=15.9, wall=14383
2023-08-17 06:08:35 | INFO | train_inner | epoch 034:   1280 / 1474 loss=1.886, trans_loss=4.738, nll_loss=1.929, w2v_ctc_loss=0.619, task_loss=3.316, contrastive_loss=0.07, total=4146.01, n_correct=2881.79, ppl=3.81, accuracy=69.508, wps=11920.9, ups=1.44, wpb=8292, bsz=300.6, num_updates=49900, lr=6.33089e-05, gnorm=0.559, clip=0, loss_scale=16, train_wall=69, gb_free=17, wall=14453
2023-08-17 06:09:44 | INFO | train_inner | epoch 034:   1380 / 1474 loss=1.893, trans_loss=4.743, nll_loss=1.936, w2v_ctc_loss=0.621, task_loss=3.151, contrastive_loss=0.133, total=4197.99, n_correct=2916.89, ppl=3.83, accuracy=69.483, wps=12170.6, ups=1.45, wpb=8396, bsz=321.1, num_updates=50000, lr=6.32456e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=68, gb_free=17, wall=14522
2023-08-17 06:09:44 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-17 06:09:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-17 06:10:07 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 3.957 | trans_loss 5.169 | nll_loss 2.424 | w2v_ctc_loss 1.317 | task_loss 11.643 | contrastive_loss 0.295 | total 4003.4 | n_correct 2675.9 | ppl 5.37 | accuracy 66.841 | uer 17.339 | wer 19.22 | raw_wer 19.22 | bleu 22.81 | wps 2207.4 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 22.81
2023-08-17 06:10:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-17 06:10:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_34_50000.pt
2023-08-17 06:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_34_50000.pt
2023-08-17 06:10:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0816_AT_sentence_alpha2.5_mt0.5_nopad/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 22.81) (writing took 44.455249501392245 seconds)
2023-08-17 06:10:52 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-17 06:10:52 | INFO | train | epoch 034 | loss 1.888 | trans_loss 4.737 | nll_loss 1.928 | w2v_ctc_loss 0.617 | task_loss 3.315 | contrastive_loss 0.109 | total 4133.03 | n_correct 2879.21 | ppl 3.81 | accuracy 69.663 | wps 11104.2 | ups 1.34 | wpb 8266.1 | bsz 304.3 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.557 | clip 0 | loss_scale 16 | train_wall 944 | gb_free 17 | wall 14590
2023-08-17 06:10:52 | INFO | fairseq_cli.train | done training in 14525.7 seconds
Exception in thread Thread-9:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 560 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
