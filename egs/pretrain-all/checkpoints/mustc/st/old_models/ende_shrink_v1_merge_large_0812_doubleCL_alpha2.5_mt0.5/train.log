2023-08-12 12:20:59 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:10837
2023-08-12 12:21:00 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:10837
2023-08-12 12:21:00 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:10837
2023-08-12 12:21:00 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:10837
2023-08-12 12:21:00 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:10837
2023-08-12 12:21:00 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:10837
2023-08-12 12:21:00 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:10837
2023-08-12 12:21:00 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:10837
2023-08-12 12:21:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-08-12 12:21:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-08-12 12:21:00 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 12:21:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 0
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 12:21:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 5
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 12:21:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 3
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 12:21:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 6
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 12:21:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 2
2023-08-12 12:21:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 4
2023-08-12 12:21:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 7
2023-08-12 12:21:01 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-08-12 12:21:01 | INFO | fairseq.distributed.utils | initialized host cpc1-finc11-0-0-cust154.4-2.cable.virginm.net as rank 1
2023-08-12 12:21:05 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:10837', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=2.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=2.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=False, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_nomute=False, at_nopad=False, at_scale=1.0, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=2.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=0.0, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=True, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-08-12 12:21:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-08-12 12:21:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-08-12 12:21:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-08-12 12:21:05 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-08-12 12:21:05 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-12 12:21:09 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-08-12 12:21:09 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-08-12 12:21:09 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-08-12 12:21:12 | INFO | root | load pretrained hubert
2023-08-12 12:21:14 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-08-12 12:21:14 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-12 12:21:16 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-08-12 12:21:16 | INFO | root | share the sematic adapter and textual encoder
2023-08-12 12:21:17 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (2): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (3): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (4): Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (6): Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (1): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (2): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (3): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (4): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (5): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (6): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (7): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (8): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (9): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (10): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
          (11): TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (1): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (2): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (3): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (4): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
        (5): TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (1): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (2): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (3): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (4): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
      (5): TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-08-12 12:21:17 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-08-12 12:21:17 | INFO | fairseq_cli.train | model: S2TJoint
2023-08-12 12:21:17 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-08-12 12:21:17 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-08-12 12:21:17 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-08-12 12:21:17 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-12 12:21:17 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-12 12:21:17 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-12 12:21:17 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-12 12:21:18 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-08-12 12:21:18 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-08-12 12:21:18 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-08-12 12:21:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-12 12:21:19 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 12:21:19 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 12:21:19 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 12:21:19 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 12:21:19 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 12:21:19 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 12:21:19 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 12:21:19 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-08-12 12:21:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-08-12 12:21:19 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-08-12 12:21:19 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-08-12 12:21:19 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_last.pt
2023-08-12 12:21:19 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_last.pt
2023-08-12 12:21:19 | INFO | fairseq.trainer | loading train data for epoch 1
2023-08-12 12:21:19 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-08-12 12:21:19 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-12 12:21:19 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-08-12 12:21:20 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-12 12:21:22 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-08-12 12:22:09 | INFO | fairseq.optim.adam | using FusedAdam
2023-08-12 12:22:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 12:22:09 | INFO | fairseq.trainer | begin training epoch 1
2023-08-12 12:22:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 12:22:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-08-12 12:22:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-08-12 12:22:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-12 12:22:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-12 12:22:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 12:23:28 | INFO | train_inner | epoch 001:    105 / 1474 loss=22.402, trans_loss=5.873, nll_loss=4.682, w2v_ctc_loss=22.973, task_loss=0, contrastive_loss=3.258, total=4207.25, n_correct=124.5, ppl=25.66, accuracy=2.959, wps=19510.2, ups=1.56, wpb=12553.5, bsz=469.7, num_updates=100, lr=4.098e-06, gnorm=3.319, clip=1, loss_scale=4, train_wall=70, gb_free=19.5, wall=129
2023-08-12 12:23:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-08-12 12:24:31 | INFO | train_inner | epoch 001:    206 / 1474 loss=19.217, trans_loss=5.862, nll_loss=4.694, w2v_ctc_loss=18.17, task_loss=0, contrastive_loss=3.235, total=4112.15, n_correct=115.78, ppl=25.88, accuracy=2.816, wps=19268.1, ups=1.57, wpb=12277.5, bsz=458.3, num_updates=200, lr=8.096e-06, gnorm=7.307, clip=21, loss_scale=2, train_wall=63, gb_free=18.9, wall=193
2023-08-12 12:25:34 | INFO | train_inner | epoch 001:    306 / 1474 loss=12.045, trans_loss=5.852, nll_loss=4.719, w2v_ctc_loss=7.244, task_loss=0, contrastive_loss=3.181, total=4091.69, n_correct=107.35, ppl=26.33, accuracy=2.624, wps=19591.7, ups=1.6, wpb=12222.8, bsz=442.4, num_updates=300, lr=1.2094e-05, gnorm=3.28, clip=2, loss_scale=2, train_wall=62, gb_free=18.6, wall=255
2023-08-12 12:26:36 | INFO | train_inner | epoch 001:    406 / 1474 loss=11.468, trans_loss=5.819, nll_loss=4.712, w2v_ctc_loss=6.329, task_loss=0, contrastive_loss=3.204, total=4172.39, n_correct=98.4, ppl=26.2, accuracy=2.358, wps=20019, ups=1.61, wpb=12458.9, bsz=460.4, num_updates=400, lr=1.6092e-05, gnorm=2.038, clip=1, loss_scale=2, train_wall=62, gb_free=18.6, wall=317
2023-08-12 12:26:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-08-12 12:27:39 | INFO | train_inner | epoch 001:    507 / 1474 loss=11.288, trans_loss=5.74, nll_loss=4.626, w2v_ctc_loss=6.025, task_loss=0, contrastive_loss=3.304, total=4179.78, n_correct=101.45, ppl=24.7, accuracy=2.427, wps=19802.9, ups=1.59, wpb=12490.1, bsz=488, num_updates=500, lr=2.009e-05, gnorm=1.869, clip=0, loss_scale=1, train_wall=63, gb_free=19.2, wall=380
2023-08-12 12:28:43 | INFO | train_inner | epoch 001:    607 / 1474 loss=11.109, trans_loss=5.737, nll_loss=4.614, w2v_ctc_loss=5.855, task_loss=0, contrastive_loss=3.251, total=4146.02, n_correct=100.16, ppl=24.49, accuracy=2.416, wps=19327.7, ups=1.56, wpb=12362.3, bsz=474.7, num_updates=600, lr=2.4088e-05, gnorm=1.947, clip=0, loss_scale=1, train_wall=63, gb_free=19.2, wall=444
2023-08-12 12:29:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-08-12 12:29:46 | INFO | train_inner | epoch 001:    708 / 1474 loss=11.113, trans_loss=5.916, nll_loss=4.86, w2v_ctc_loss=5.808, task_loss=0, contrastive_loss=3.129, total=4142.55, n_correct=72.44, ppl=29.05, accuracy=1.749, wps=19724.1, ups=1.59, wpb=12368.8, bsz=453.6, num_updates=700, lr=2.8086e-05, gnorm=2.278, clip=3, loss_scale=0.5, train_wall=62, gb_free=18.8, wall=507
2023-08-12 12:30:48 | INFO | train_inner | epoch 001:    808 / 1474 loss=11.01, trans_loss=5.945, nll_loss=4.89, w2v_ctc_loss=5.688, task_loss=0, contrastive_loss=3.147, total=4130.73, n_correct=65.44, ppl=29.64, accuracy=1.584, wps=19866.4, ups=1.61, wpb=12327.4, bsz=464.5, num_updates=800, lr=3.2084e-05, gnorm=2.848, clip=4, loss_scale=0.5, train_wall=62, gb_free=19.1, wall=569
2023-08-12 12:31:50 | INFO | train_inner | epoch 001:    908 / 1474 loss=10.963, trans_loss=6.085, nll_loss=5.064, w2v_ctc_loss=5.6, task_loss=0, contrastive_loss=3.056, total=4162.34, n_correct=46.82, ppl=33.45, accuracy=1.125, wps=20069.6, ups=1.61, wpb=12430.9, bsz=458.6, num_updates=900, lr=3.6082e-05, gnorm=2.346, clip=0, loss_scale=0.5, train_wall=61, gb_free=19.5, wall=631
2023-08-12 12:32:53 | INFO | train_inner | epoch 001:   1008 / 1474 loss=10.778, trans_loss=6.06, nll_loss=5.025, w2v_ctc_loss=5.419, task_loss=0, contrastive_loss=3.043, total=4138.67, n_correct=59.04, ppl=32.56, accuracy=1.427, wps=19622.3, ups=1.59, wpb=12362.2, bsz=457.6, num_updates=1000, lr=4.008e-05, gnorm=3.018, clip=0, loss_scale=0.5, train_wall=63, gb_free=18.9, wall=694
2023-08-12 12:33:55 | INFO | train_inner | epoch 001:   1108 / 1474 loss=10.561, trans_loss=6.124, nll_loss=5.1, w2v_ctc_loss=5.235, task_loss=0, contrastive_loss=2.951, total=4151.9, n_correct=58.04, ppl=34.29, accuracy=1.398, wps=19725.9, ups=1.59, wpb=12383.7, bsz=453.3, num_updates=1100, lr=4.4078e-05, gnorm=2.951, clip=1, loss_scale=0.5, train_wall=62, gb_free=18.6, wall=757
2023-08-12 12:34:58 | INFO | train_inner | epoch 001:   1208 / 1474 loss=10.34, trans_loss=6.159, nll_loss=5.145, w2v_ctc_loss=5.06, task_loss=0, contrastive_loss=2.843, total=4124.46, n_correct=56.93, ppl=35.37, accuracy=1.38, wps=19832.3, ups=1.61, wpb=12318.6, bsz=438.9, num_updates=1200, lr=4.8076e-05, gnorm=3.24, clip=0, loss_scale=0.5, train_wall=62, gb_free=18.7, wall=819
2023-08-12 12:35:59 | INFO | train_inner | epoch 001:   1308 / 1474 loss=10.158, trans_loss=6.125, nll_loss=5.097, w2v_ctc_loss=4.863, task_loss=0, contrastive_loss=2.801, total=4073.4, n_correct=67.91, ppl=34.22, accuracy=1.667, wps=19704.2, ups=1.62, wpb=12160.2, bsz=447.2, num_updates=1300, lr=5.2074e-05, gnorm=3.568, clip=0, loss_scale=0.5, train_wall=61, gb_free=19.1, wall=881
2023-08-12 12:37:02 | INFO | train_inner | epoch 001:   1408 / 1474 loss=9.987, trans_loss=6.146, nll_loss=5.129, w2v_ctc_loss=4.673, task_loss=0, contrastive_loss=2.893, total=4132.87, n_correct=62.97, ppl=34.99, accuracy=1.524, wps=19823.6, ups=1.61, wpb=12350.3, bsz=455.3, num_updates=1400, lr=5.6072e-05, gnorm=3.279, clip=0, loss_scale=0.5, train_wall=62, gb_free=18.8, wall=943
2023-08-12 12:37:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 12:38:23 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 14.182 | trans_loss 14.005 | nll_loss 13.892 | w2v_ctc_loss 6.214 | task_loss 0 | contrastive_loss 4.101 | total 4003.4 | n_correct 38.9 | ppl 15198.9 | accuracy 0.972 | uer 75.521 | wer 74.445 | raw_wer 74.445 | bleu 0 | wps 1150.5 | wpb 4003.4 | bsz 141.8 | num_updates 1466
2023-08-12 12:38:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1466 updates
2023-08-12 12:38:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 12:38:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 12:38:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 1 @ 1466 updates, score 0.0) (writing took 7.367701338604093 seconds)
2023-08-12 12:38:30 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-08-12 12:38:30 | INFO | train | epoch 001 | loss 12.216 | trans_loss 5.969 | nll_loss 4.894 | w2v_ctc_loss 7.65 | task_loss 0 | contrastive_loss 3.082 | total 4138.77 | n_correct 80.0764 | ppl 29.73 | accuracy 1.935 | wps 18728.6 | ups 1.52 | wpb 12356.3 | bsz 458.6 | num_updates 1466 | lr 5.87107e-05 | gnorm 3.127 | clip 2.3 | loss_scale 0.5 | train_wall 919 | gb_free 18.9 | wall 1032
2023-08-12 12:38:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 12:38:31 | INFO | fairseq.trainer | begin training epoch 2
2023-08-12 12:38:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 12:39:00 | INFO | train_inner | epoch 002:     34 / 1474 loss=9.83, trans_loss=6.156, nll_loss=5.138, w2v_ctc_loss=4.503, task_loss=0, contrastive_loss=2.813, total=4142.16, n_correct=57.92, ppl=35.22, accuracy=1.398, wps=10408.5, ups=0.84, wpb=12351.5, bsz=460.8, num_updates=1500, lr=6.007e-05, gnorm=3.922, clip=0, loss_scale=0.5, train_wall=62, gb_free=19.7, wall=1062
2023-08-12 12:40:02 | INFO | train_inner | epoch 002:    134 / 1474 loss=9.629, trans_loss=6.094, nll_loss=5.061, w2v_ctc_loss=4.389, task_loss=0, contrastive_loss=2.714, total=4162.24, n_correct=69.49, ppl=33.39, accuracy=1.67, wps=19965.1, ups=1.61, wpb=12417.9, bsz=457.2, num_updates=1600, lr=6.4068e-05, gnorm=3.446, clip=0, loss_scale=0.5, train_wall=62, gb_free=19.1, wall=1124
2023-08-12 12:41:05 | INFO | train_inner | epoch 002:    234 / 1474 loss=9.575, trans_loss=6.125, nll_loss=5.107, w2v_ctc_loss=4.22, task_loss=0, contrastive_loss=2.774, total=4199.56, n_correct=63.62, ppl=34.45, accuracy=1.515, wps=20150.5, ups=1.61, wpb=12540.3, bsz=490.3, num_updates=1700, lr=6.8066e-05, gnorm=3.504, clip=0, loss_scale=0.5, train_wall=62, gb_free=18.7, wall=1186
2023-08-12 12:42:07 | INFO | train_inner | epoch 002:    334 / 1474 loss=9.292, trans_loss=6.125, nll_loss=5.109, w2v_ctc_loss=4.141, task_loss=0, contrastive_loss=2.553, total=4122.83, n_correct=60.35, ppl=34.52, accuracy=1.464, wps=19866.4, ups=1.61, wpb=12311.3, bsz=447.3, num_updates=1800, lr=7.2064e-05, gnorm=3.896, clip=0, loss_scale=0.5, train_wall=62, gb_free=18.8, wall=1248
2023-08-12 12:43:09 | INFO | train_inner | epoch 002:    434 / 1474 loss=9.036, trans_loss=6.122, nll_loss=5.107, w2v_ctc_loss=4.065, task_loss=0, contrastive_loss=2.359, total=4035.27, n_correct=61.35, ppl=34.47, accuracy=1.52, wps=19326.4, ups=1.6, wpb=12058.4, bsz=412.7, num_updates=1900, lr=7.6062e-05, gnorm=3.539, clip=0, loss_scale=0.5, train_wall=62, gb_free=19.1, wall=1310
2023-08-12 12:44:12 | INFO | train_inner | epoch 002:    534 / 1474 loss=8.992, trans_loss=6.102, nll_loss=5.078, w2v_ctc_loss=3.912, task_loss=0, contrastive_loss=2.496, total=4188.78, n_correct=65.54, ppl=33.79, accuracy=1.565, wps=19942.5, ups=1.6, wpb=12497.3, bsz=468.8, num_updates=2000, lr=8.006e-05, gnorm=3.454, clip=0, loss_scale=0.5, train_wall=62, gb_free=19, wall=1373
2023-08-12 12:44:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 12:44:50 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 13.173 | trans_loss 13.529 | nll_loss 13.291 | w2v_ctc_loss 5.161 | task_loss 0 | contrastive_loss 3.542 | total 4003.4 | n_correct 62.7 | ppl 10020 | accuracy 1.566 | uer 65.891 | wer 63.663 | raw_wer 63.663 | bleu 0 | wps 1203.4 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0
2023-08-12 12:44:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-08-12 12:44:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_2_2000.pt
2023-08-12 12:44:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_2_2000.pt
2023-08-12 12:45:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 45.267091708257794 seconds)
2023-08-12 12:46:37 | INFO | train_inner | epoch 002:    634 / 1474 loss=8.806, trans_loss=6.101, nll_loss=5.077, w2v_ctc_loss=3.805, task_loss=0, contrastive_loss=2.347, total=4130.32, n_correct=64.23, ppl=33.75, accuracy=1.555, wps=8472.5, ups=0.69, wpb=12325.7, bsz=454.3, num_updates=2100, lr=8.4058e-05, gnorm=3.739, clip=0, loss_scale=0.5, train_wall=61, gb_free=18.6, wall=1518
2023-08-12 12:47:39 | INFO | train_inner | epoch 002:    734 / 1474 loss=8.68, trans_loss=6.097, nll_loss=5.072, w2v_ctc_loss=3.729, task_loss=0, contrastive_loss=2.387, total=4147.87, n_correct=67.8, ppl=33.64, accuracy=1.635, wps=20050.1, ups=1.62, wpb=12382.5, bsz=461.6, num_updates=2200, lr=8.8056e-05, gnorm=3.317, clip=0, loss_scale=0.5, train_wall=61, gb_free=18.7, wall=1580
2023-08-12 12:48:41 | INFO | train_inner | epoch 002:    834 / 1474 loss=8.507, trans_loss=6.085, nll_loss=5.059, w2v_ctc_loss=3.654, task_loss=0, contrastive_loss=2.285, total=4150.23, n_correct=68.74, ppl=33.33, accuracy=1.656, wps=20058.2, ups=1.62, wpb=12399.7, bsz=455.5, num_updates=2300, lr=9.2054e-05, gnorm=3.329, clip=0, loss_scale=0.5, train_wall=61, gb_free=19.6, wall=1642
2023-08-12 12:49:43 | INFO | train_inner | epoch 002:    934 / 1474 loss=8.325, trans_loss=6.066, nll_loss=5.03, w2v_ctc_loss=3.549, task_loss=0, contrastive_loss=2.249, total=4108.25, n_correct=68.03, ppl=32.67, accuracy=1.656, wps=19571.9, ups=1.6, wpb=12261.4, bsz=444.1, num_updates=2400, lr=9.6052e-05, gnorm=3.402, clip=0, loss_scale=0.5, train_wall=62, gb_free=19, wall=1705
2023-08-12 12:50:46 | INFO | train_inner | epoch 002:   1034 / 1474 loss=8.202, trans_loss=6.054, nll_loss=5.015, w2v_ctc_loss=3.479, task_loss=0, contrastive_loss=2.144, total=4102.66, n_correct=70.97, ppl=32.33, accuracy=1.73, wps=19676.2, ups=1.61, wpb=12248.4, bsz=455.1, num_updates=2500, lr=0.00010005, gnorm=3.347, clip=0, loss_scale=0.5, train_wall=62, gb_free=18.8, wall=1767
2023-08-12 12:51:49 | INFO | train_inner | epoch 002:   1134 / 1474 loss=8.196, trans_loss=6.042, nll_loss=5.002, w2v_ctc_loss=3.381, task_loss=0, contrastive_loss=2.352, total=4205.12, n_correct=72.79, ppl=32.05, accuracy=1.731, wps=19972.3, ups=1.59, wpb=12555, bsz=496.9, num_updates=2600, lr=0.000104048, gnorm=3.359, clip=0, loss_scale=0.5, train_wall=62, gb_free=19.2, wall=1830
2023-08-12 12:52:51 | INFO | train_inner | epoch 002:   1234 / 1474 loss=8.015, trans_loss=6.028, nll_loss=4.982, w2v_ctc_loss=3.334, task_loss=0, contrastive_loss=2.158, total=4219.75, n_correct=72.08, ppl=31.6, accuracy=1.708, wps=20295.7, ups=1.61, wpb=12592.6, bsz=486.7, num_updates=2700, lr=0.000108046, gnorm=3.107, clip=0, loss_scale=1, train_wall=62, gb_free=19.3, wall=1892
2023-08-12 12:53:53 | INFO | train_inner | epoch 002:   1334 / 1474 loss=7.809, trans_loss=6.005, nll_loss=4.957, w2v_ctc_loss=3.288, task_loss=0, contrastive_loss=1.934, total=4134.59, n_correct=75.85, ppl=31.07, accuracy=1.835, wps=19918.1, ups=1.61, wpb=12358.8, bsz=454.2, num_updates=2800, lr=0.000112044, gnorm=3.028, clip=0, loss_scale=1, train_wall=62, gb_free=18.7, wall=1954
2023-08-12 12:54:55 | INFO | train_inner | epoch 002:   1434 / 1474 loss=7.675, trans_loss=6.002, nll_loss=4.949, w2v_ctc_loss=3.223, task_loss=0, contrastive_loss=1.993, total=4061.91, n_correct=72.79, ppl=30.9, accuracy=1.792, wps=19388, ups=1.6, wpb=12123.6, bsz=444.9, num_updates=2900, lr=0.000116042, gnorm=3.104, clip=0, loss_scale=1, train_wall=62, gb_free=19.4, wall=2016
2023-08-12 12:55:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 12:55:59 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 11.925 | trans_loss 13.015 | nll_loss 12.606 | w2v_ctc_loss 4.149 | task_loss 0 | contrastive_loss 2.689 | total 4003.4 | n_correct 86.4 | ppl 6232.78 | accuracy 2.158 | uer 56.128 | wer 54.793 | raw_wer 54.793 | bleu 0 | wps 1167.2 | wpb 4003.4 | bsz 141.8 | num_updates 2940 | best_bleu 0
2023-08-12 12:55:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2940 updates
2023-08-12 12:55:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 12:56:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 12:56:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 2 @ 2940 updates, score 0.0) (writing took 26.927540939301252 seconds)
2023-08-12 12:56:26 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-08-12 12:56:26 | INFO | train | epoch 002 | loss 8.624 | trans_loss 6.074 | nll_loss 5.042 | w2v_ctc_loss 3.729 | task_loss 0 | contrastive_loss 2.339 | total 4138.65 | n_correct 68.1404 | ppl 32.95 | accuracy 1.646 | wps 16924.7 | ups 1.37 | wpb 12355.8 | bsz 458.5 | num_updates 2940 | lr 0.000117641 | gnorm 3.397 | clip 0 | loss_scale 1 | train_wall 910 | gb_free 19 | wall 2108
2023-08-12 12:56:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 12:56:27 | INFO | fairseq.trainer | begin training epoch 3
2023-08-12 12:56:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 12:57:12 | INFO | train_inner | epoch 003:     60 / 1474 loss=7.517, trans_loss=5.98, nll_loss=4.923, w2v_ctc_loss=3.163, task_loss=0, contrastive_loss=1.847, total=4056.16, n_correct=74.02, ppl=30.34, accuracy=1.825, wps=8862.5, ups=0.73, wpb=12110.2, bsz=436.1, num_updates=3000, lr=0.00012004, gnorm=2.876, clip=0, loss_scale=1, train_wall=62, gb_free=18.8, wall=2153
2023-08-12 12:58:31 | INFO | train_inner | epoch 003:    160 / 1474 loss=6.787, trans_loss=5.375, nll_loss=4.158, w2v_ctc_loss=2.902, task_loss=0, contrastive_loss=1.735, total=4157.91, n_correct=235.94, ppl=17.85, accuracy=5.674, wps=15593.7, ups=1.26, wpb=12415, bsz=462.6, num_updates=3100, lr=0.000124038, gnorm=5.195, clip=9, loss_scale=1, train_wall=79, gb_free=17, wall=2233
2023-08-12 12:59:53 | INFO | train_inner | epoch 003:    260 / 1474 loss=5.953, trans_loss=5.082, nll_loss=3.781, w2v_ctc_loss=2.573, task_loss=0, contrastive_loss=1.491, total=4160.57, n_correct=414.53, ppl=13.75, accuracy=9.963, wps=15297.7, ups=1.23, wpb=12430.5, bsz=467.1, num_updates=3200, lr=0.000128036, gnorm=3.426, clip=1, loss_scale=1, train_wall=81, gb_free=15.6, wall=2314
2023-08-12 13:01:14 | INFO | train_inner | epoch 003:    360 / 1474 loss=5.615, trans_loss=4.948, nll_loss=3.597, w2v_ctc_loss=2.457, task_loss=0, contrastive_loss=1.422, total=4159.93, n_correct=524.41, ppl=12.1, accuracy=12.606, wps=15336, ups=1.24, wpb=12413.4, bsz=467.3, num_updates=3300, lr=0.000132034, gnorm=3.3, clip=2, loss_scale=1, train_wall=80, gb_free=16, wall=2395
2023-08-12 13:01:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-08-12 13:02:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-08-12 13:02:36 | INFO | train_inner | epoch 003:    462 / 1474 loss=5.378, trans_loss=4.661, nll_loss=3.213, w2v_ctc_loss=2.465, task_loss=0, contrastive_loss=1.319, total=4198.62, n_correct=773.86, ppl=9.27, accuracy=18.431, wps=15279.1, ups=1.22, wpb=12531.5, bsz=470.6, num_updates=3400, lr=0.000136032, gnorm=6.361, clip=16, loss_scale=0.25, train_wall=81, gb_free=14.5, wall=2477
2023-08-12 13:03:56 | INFO | train_inner | epoch 003:    562 / 1474 loss=4.987, trans_loss=4.335, nll_loss=2.787, w2v_ctc_loss=2.446, task_loss=0, contrastive_loss=1.23, total=4083.21, n_correct=1111.5, ppl=6.9, accuracy=27.221, wps=15207.5, ups=1.25, wpb=12199, bsz=437.9, num_updates=3500, lr=0.00014003, gnorm=5.983, clip=9, loss_scale=0.25, train_wall=80, gb_free=15.8, wall=2557
2023-08-12 13:05:17 | INFO | train_inner | epoch 003:    662 / 1474 loss=4.954, trans_loss=4.273, nll_loss=2.703, w2v_ctc_loss=2.411, task_loss=0, contrastive_loss=1.334, total=4232.39, n_correct=1257.64, ppl=6.51, accuracy=29.715, wps=15502.2, ups=1.23, wpb=12618.9, bsz=487.6, num_updates=3600, lr=0.000144028, gnorm=9.701, clip=19, loss_scale=0.25, train_wall=81, gb_free=16.3, wall=2639
2023-08-12 13:06:38 | INFO | train_inner | epoch 003:    762 / 1474 loss=4.695, trans_loss=4.178, nll_loss=2.585, w2v_ctc_loss=2.364, task_loss=0, contrastive_loss=1.063, total=4155.31, n_correct=1338.64, ppl=6, accuracy=32.215, wps=15399.2, ups=1.24, wpb=12412.9, bsz=465.8, num_updates=3700, lr=0.000148026, gnorm=4.778, clip=6, loss_scale=0.25, train_wall=80, gb_free=16.3, wall=2719
2023-08-12 13:07:58 | INFO | train_inner | epoch 003:    862 / 1474 loss=4.54, trans_loss=4.155, nll_loss=2.554, w2v_ctc_loss=2.299, task_loss=0, contrastive_loss=0.99, total=4170.95, n_correct=1385.5, ppl=5.87, accuracy=33.218, wps=15544.9, ups=1.25, wpb=12453.7, bsz=458.1, num_updates=3800, lr=0.000152024, gnorm=4.86, clip=5, loss_scale=0.25, train_wall=80, gb_free=17.3, wall=2799
2023-08-12 13:09:18 | INFO | train_inner | epoch 003:    962 / 1474 loss=4.505, trans_loss=4.142, nll_loss=2.534, w2v_ctc_loss=2.297, task_loss=0, contrastive_loss=1.011, total=4174.19, n_correct=1418.04, ppl=5.79, accuracy=33.972, wps=15517, ups=1.25, wpb=12449.2, bsz=475.7, num_updates=3900, lr=0.000156022, gnorm=5.645, clip=8, loss_scale=0.25, train_wall=80, gb_free=13.7, wall=2880
2023-08-12 13:10:38 | INFO | train_inner | epoch 003:   1062 / 1474 loss=4.358, trans_loss=4.111, nll_loss=2.498, w2v_ctc_loss=2.278, task_loss=0, contrastive_loss=0.891, total=4049.41, n_correct=1403.43, ppl=5.65, accuracy=34.658, wps=15177.5, ups=1.25, wpb=12096, bsz=436.3, num_updates=4000, lr=0.00016002, gnorm=4.48, clip=8, loss_scale=0.25, train_wall=79, gb_free=16.4, wall=2959
2023-08-12 13:10:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 13:11:09 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 6.66 | trans_loss 7.285 | nll_loss 5.158 | w2v_ctc_loss 2.853 | task_loss 0 | contrastive_loss 1.27 | total 4003.4 | n_correct 1450.2 | ppl 35.71 | accuracy 36.224 | uer 38.999 | wer 38.336 | raw_wer 38.336 | bleu 1.77 | wps 1604 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 1.77
2023-08-12 13:11:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-08-12 13:11:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_3_4000.pt
2023-08-12 13:11:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_3_4000.pt
2023-08-12 13:11:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 1.77) (writing took 46.943548081442714 seconds)
2023-08-12 13:13:15 | INFO | train_inner | epoch 003:   1162 / 1474 loss=4.313, trans_loss=4.117, nll_loss=2.504, w2v_ctc_loss=2.263, task_loss=0, contrastive_loss=0.862, total=4044.1, n_correct=1408.76, ppl=5.67, accuracy=34.835, wps=7664, ups=0.63, wpb=12070.2, bsz=433.6, num_updates=4100, lr=0.000164018, gnorm=5.428, clip=8, loss_scale=0.25, train_wall=79, gb_free=15.6, wall=3117
2023-08-12 13:14:35 | INFO | train_inner | epoch 003:   1262 / 1474 loss=4.227, trans_loss=4.078, nll_loss=2.455, w2v_ctc_loss=2.228, task_loss=0, contrastive_loss=0.826, total=4065.1, n_correct=1457.94, ppl=5.48, accuracy=35.865, wps=15259.8, ups=1.26, wpb=12140.5, bsz=432.3, num_updates=4200, lr=0.000168016, gnorm=4.736, clip=7, loss_scale=0.25, train_wall=79, gb_free=17, wall=3196
2023-08-12 13:15:56 | INFO | train_inner | epoch 003:   1362 / 1474 loss=4.212, trans_loss=4.053, nll_loss=2.422, w2v_ctc_loss=2.186, task_loss=0, contrastive_loss=0.927, total=4132.35, n_correct=1526.97, ppl=5.36, accuracy=36.952, wps=15323.2, ups=1.24, wpb=12335.6, bsz=462.1, num_updates=4300, lr=0.000172014, gnorm=5.369, clip=8, loss_scale=0.25, train_wall=80, gb_free=17.5, wall=3277
2023-08-12 13:17:17 | INFO | train_inner | epoch 003:   1462 / 1474 loss=4.148, trans_loss=4.036, nll_loss=2.402, w2v_ctc_loss=2.167, task_loss=0, contrastive_loss=0.872, total=4206.88, n_correct=1581.1, ppl=5.29, accuracy=37.584, wps=15493.3, ups=1.23, wpb=12566, bsz=476, num_updates=4400, lr=0.000176012, gnorm=3.665, clip=5, loss_scale=0.25, train_wall=81, gb_free=15, wall=3358
2023-08-12 13:17:26 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 13:17:56 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 6.209 | trans_loss 6.885 | nll_loss 4.637 | w2v_ctc_loss 2.681 | task_loss 0 | contrastive_loss 1.079 | total 4003.4 | n_correct 1655.5 | ppl 24.88 | accuracy 41.352 | uer 36.541 | wer 36.188 | raw_wer 36.188 | bleu 3.39 | wps 1702.2 | wpb 4003.4 | bsz 141.8 | num_updates 4412 | best_bleu 3.39
2023-08-12 13:17:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4412 updates
2023-08-12 13:17:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 13:18:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 13:18:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 3 @ 4412 updates, score 3.39) (writing took 27.90188466385007 seconds)
2023-08-12 13:18:24 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-08-12 13:18:24 | INFO | train | epoch 003 | loss 5.006 | trans_loss 4.458 | nll_loss 2.952 | w2v_ctc_loss 2.41 | task_loss 0 | contrastive_loss 1.171 | total 4138.41 | n_correct 1091.2 | ppl 7.74 | accuracy 26.368 | wps 13805.3 | ups 1.12 | wpb 12355.1 | bsz 458.5 | num_updates 4412 | lr 0.000176492 | gnorm 5.094 | clip 7.5 | loss_scale 0.25 | train_wall 1167 | gb_free 16.4 | wall 3425
2023-08-12 13:18:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 13:18:24 | INFO | fairseq.trainer | begin training epoch 4
2023-08-12 13:18:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 13:19:42 | INFO | train_inner | epoch 004:     88 / 1474 loss=3.949, trans_loss=4.013, nll_loss=2.368, w2v_ctc_loss=2.119, task_loss=0, contrastive_loss=0.683, total=4082.27, n_correct=1568.98, ppl=5.16, accuracy=38.434, wps=8398.5, ups=0.69, wpb=12182.1, bsz=434.2, num_updates=4500, lr=0.00018001, gnorm=4.3, clip=3, loss_scale=0.25, train_wall=79, gb_free=17.7, wall=3503
2023-08-12 13:21:01 | INFO | train_inner | epoch 004:    188 / 1474 loss=3.901, trans_loss=3.979, nll_loss=2.326, w2v_ctc_loss=2.073, task_loss=0, contrastive_loss=0.704, total=4184.92, n_correct=1655, ppl=5.01, accuracy=39.547, wps=15754.5, ups=1.26, wpb=12496.9, bsz=470.3, num_updates=4600, lr=0.000184008, gnorm=2.823, clip=1, loss_scale=0.25, train_wall=79, gb_free=13.2, wall=3582
2023-08-12 13:22:21 | INFO | train_inner | epoch 004:    288 / 1474 loss=3.87, trans_loss=3.975, nll_loss=2.322, w2v_ctc_loss=2.045, task_loss=0, contrastive_loss=0.788, total=4150, n_correct=1657.49, ppl=5, accuracy=39.94, wps=15471, ups=1.25, wpb=12397.9, bsz=465.2, num_updates=4700, lr=0.000188006, gnorm=3.589, clip=6, loss_scale=0.25, train_wall=80, gb_free=16.8, wall=3662
2023-08-12 13:23:41 | INFO | train_inner | epoch 004:    388 / 1474 loss=3.813, trans_loss=3.972, nll_loss=2.315, w2v_ctc_loss=2.064, task_loss=0, contrastive_loss=0.626, total=4114.32, n_correct=1649.44, ppl=4.98, accuracy=40.09, wps=15285.8, ups=1.25, wpb=12277.1, bsz=440, num_updates=4800, lr=0.000192004, gnorm=3.035, clip=2, loss_scale=0.25, train_wall=80, gb_free=12.3, wall=3743
2023-08-12 13:25:03 | INFO | train_inner | epoch 004:    488 / 1474 loss=3.875, trans_loss=3.945, nll_loss=2.281, w2v_ctc_loss=1.98, task_loss=0, contrastive_loss=1.011, total=4239.74, n_correct=1751.5, ppl=4.86, accuracy=41.311, wps=15603.6, ups=1.23, wpb=12652.2, bsz=506.4, num_updates=4900, lr=0.000196002, gnorm=2.902, clip=2, loss_scale=0.25, train_wall=81, gb_free=17, wall=3824
2023-08-12 13:26:23 | INFO | train_inner | epoch 004:    588 / 1474 loss=3.701, trans_loss=3.915, nll_loss=2.243, w2v_ctc_loss=1.983, task_loss=0, contrastive_loss=0.657, total=4219.26, n_correct=1788.71, ppl=4.73, accuracy=42.394, wps=15663.3, ups=1.24, wpb=12596.5, bsz=483.8, num_updates=5000, lr=0.0002, gnorm=2.545, clip=1, loss_scale=0.25, train_wall=80, gb_free=16, wall=3904
mt_weight tensor(0.5000)
asr_weight tensor(0.5023, device='cuda:0')
2023-08-12 13:27:44 | INFO | train_inner | epoch 004:    688 / 1474 loss=3.612, trans_loss=3.912, nll_loss=2.234, w2v_ctc_loss=1.94, task_loss=0, contrastive_loss=0.675, total=4171.93, n_correct=1786.96, ppl=4.71, accuracy=42.833, wps=15275, ups=1.23, wpb=12436.9, bsz=453.3, num_updates=5100, lr=0.00019803, gnorm=1.583, clip=0, loss_scale=0.25, train_wall=81, gb_free=15.6, wall=3986
2023-08-12 13:29:05 | INFO | train_inner | epoch 004:    788 / 1474 loss=3.562, trans_loss=3.89, nll_loss=2.211, w2v_ctc_loss=1.968, task_loss=0, contrastive_loss=0.542, total=4029.4, n_correct=1747.31, ppl=4.63, accuracy=43.364, wps=14890.2, ups=1.24, wpb=12032.2, bsz=423.7, num_updates=5200, lr=0.000196116, gnorm=2.106, clip=2, loss_scale=0.25, train_wall=80, gb_free=16.3, wall=4067
2023-08-12 13:30:26 | INFO | train_inner | epoch 004:    888 / 1474 loss=3.622, trans_loss=3.878, nll_loss=2.196, w2v_ctc_loss=1.967, task_loss=0, contrastive_loss=0.706, total=4175.28, n_correct=1830.67, ppl=4.58, accuracy=43.845, wps=15445.3, ups=1.24, wpb=12468.9, bsz=463.7, num_updates=5300, lr=0.000194257, gnorm=1.886, clip=1, loss_scale=0.25, train_wall=80, gb_free=15.6, wall=4147
2023-08-12 13:31:47 | INFO | train_inner | epoch 004:    988 / 1474 loss=3.486, trans_loss=3.854, nll_loss=2.165, w2v_ctc_loss=1.913, task_loss=0, contrastive_loss=0.566, total=4132.46, n_correct=1852.38, ppl=4.48, accuracy=44.825, wps=15230.1, ups=1.23, wpb=12343.2, bsz=455.3, num_updates=5400, lr=0.00019245, gnorm=1.44, clip=0, loss_scale=0.25, train_wall=81, gb_free=11, wall=4228
2023-08-12 13:33:07 | INFO | train_inner | epoch 004:   1088 / 1474 loss=3.45, trans_loss=3.848, nll_loss=2.156, w2v_ctc_loss=1.913, task_loss=0, contrastive_loss=0.535, total=4073.98, n_correct=1844.34, ppl=4.46, accuracy=45.271, wps=15203.1, ups=1.25, wpb=12161.7, bsz=437.9, num_updates=5500, lr=0.000190693, gnorm=1.482, clip=0, loss_scale=0.5, train_wall=79, gb_free=15.9, wall=4308
2023-08-12 13:34:27 | INFO | train_inner | epoch 004:   1188 / 1474 loss=3.494, trans_loss=3.832, nll_loss=2.137, w2v_ctc_loss=1.897, task_loss=0, contrastive_loss=0.644, total=4172.46, n_correct=1918.83, ppl=4.4, accuracy=45.988, wps=15481.8, ups=1.24, wpb=12459.8, bsz=487.1, num_updates=5600, lr=0.000188982, gnorm=1.506, clip=0, loss_scale=0.5, train_wall=80, gb_free=11.5, wall=4389
2023-08-12 13:35:48 | INFO | train_inner | epoch 004:   1288 / 1474 loss=3.421, trans_loss=3.812, nll_loss=2.111, w2v_ctc_loss=1.872, task_loss=0, contrastive_loss=0.592, total=4140.32, n_correct=1926.77, ppl=4.32, accuracy=46.537, wps=15433.1, ups=1.25, wpb=12365.2, bsz=467.7, num_updates=5700, lr=0.000187317, gnorm=1.378, clip=1, loss_scale=0.5, train_wall=80, gb_free=17.1, wall=4469
2023-08-12 13:37:07 | INFO | train_inner | epoch 004:   1388 / 1474 loss=3.289, trans_loss=3.792, nll_loss=2.086, w2v_ctc_loss=1.835, task_loss=0, contrastive_loss=0.439, total=4092.66, n_correct=1938.76, ppl=4.25, accuracy=47.372, wps=15477.9, ups=1.27, wpb=12224.9, bsz=436.3, num_updates=5800, lr=0.000185695, gnorm=1.198, clip=0, loss_scale=0.5, train_wall=79, gb_free=16.4, wall=4548
2023-08-12 13:38:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.5023, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.5023, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.5023, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.5023, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.5023, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.5023, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.5023, device='cuda:5')
2023-08-12 13:38:41 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 5.214 | trans_loss 6.05 | nll_loss 3.554 | w2v_ctc_loss 2.194 | task_loss 0 | contrastive_loss 0.628 | total 4003.4 | n_correct 2118.9 | ppl 11.74 | accuracy 52.928 | uer 30.6 | wer 30.946 | raw_wer 30.946 | bleu 11.07 | wps 1867.2 | wpb 4003.4 | bsz 141.8 | num_updates 5886 | best_bleu 11.07
2023-08-12 13:38:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5886 updates
2023-08-12 13:38:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 13:38:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 13:39:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 4 @ 5886 updates, score 11.07) (writing took 29.083267411217093 seconds)
2023-08-12 13:39:10 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-08-12 13:39:10 | INFO | train | epoch 004 | loss 3.622 | trans_loss 3.894 | nll_loss 2.216 | w2v_ctc_loss 1.958 | task_loss 0 | contrastive_loss 0.648 | total 4138.65 | n_correct 1791.17 | ppl 4.64 | accuracy 43.279 | wps 14613.1 | ups 1.18 | wpb 12355.8 | bsz 458.5 | num_updates 5886 | lr 0.000184334 | gnorm 2.201 | clip 1.3 | loss_scale 0.5 | train_wall 1176 | gb_free 14.8 | wall 4671
2023-08-12 13:39:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 13:39:10 | INFO | fairseq.trainer | begin training epoch 5
2023-08-12 13:39:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 13:39:29 | INFO | train_inner | epoch 005:     14 / 1474 loss=3.262, trans_loss=3.781, nll_loss=2.069, w2v_ctc_loss=1.789, task_loss=0, contrastive_loss=0.484, total=4073.09, n_correct=1950.51, ppl=4.2, accuracy=47.888, wps=8542.3, ups=0.7, wpb=12158.4, bsz=451.4, num_updates=5900, lr=0.000184115, gnorm=1.119, clip=0, loss_scale=0.5, train_wall=79, gb_free=17.2, wall=4690
2023-08-12 13:40:50 | INFO | train_inner | epoch 005:    114 / 1474 loss=3.176, trans_loss=3.743, nll_loss=2.021, w2v_ctc_loss=1.73, task_loss=0, contrastive_loss=0.442, total=4233.69, n_correct=2083.48, ppl=4.06, accuracy=49.212, wps=15660, ups=1.24, wpb=12643.8, bsz=488.6, num_updates=6000, lr=0.000182574, gnorm=1.112, clip=0, loss_scale=0.5, train_wall=80, gb_free=15.7, wall=4771
2023-08-12 13:40:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 13:41:17 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 5.17 | trans_loss 6.042 | nll_loss 3.535 | w2v_ctc_loss 2.069 | task_loss 0 | contrastive_loss 0.624 | total 4003.4 | n_correct 2132.4 | ppl 11.59 | accuracy 53.265 | uer 30.242 | wer 30.994 | raw_wer 30.994 | bleu 11.94 | wps 1878 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 11.94
2023-08-12 13:41:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-08-12 13:41:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_5_6000.pt
2023-08-12 13:41:20 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_5_6000.pt
2023-08-12 13:41:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 11.94) (writing took 42.623544454574585 seconds)
2023-08-12 13:43:19 | INFO | train_inner | epoch 005:    214 / 1474 loss=3.233, trans_loss=3.749, nll_loss=2.027, w2v_ctc_loss=1.738, task_loss=0, contrastive_loss=0.646, total=4185.02, n_correct=2061.19, ppl=4.07, accuracy=49.252, wps=8354.5, ups=0.67, wpb=12486.5, bsz=485.7, num_updates=6100, lr=0.000181071, gnorm=1.126, clip=0, loss_scale=0.5, train_wall=79, gb_free=16.7, wall=4920
2023-08-12 13:44:39 | INFO | train_inner | epoch 005:    314 / 1474 loss=3.179, trans_loss=3.731, nll_loss=2.009, w2v_ctc_loss=1.753, task_loss=0, contrastive_loss=0.505, total=4097.42, n_correct=2022.05, ppl=4.02, accuracy=49.349, wps=15424.3, ups=1.26, wpb=12251.6, bsz=448, num_updates=6200, lr=0.000179605, gnorm=1.21, clip=0, loss_scale=0.5, train_wall=79, gb_free=14.3, wall=5000
2023-08-12 13:45:59 | INFO | train_inner | epoch 005:    414 / 1474 loss=3.171, trans_loss=3.72, nll_loss=1.993, w2v_ctc_loss=1.716, task_loss=0, contrastive_loss=0.577, total=4135.49, n_correct=2064.22, ppl=3.98, accuracy=49.915, wps=15318, ups=1.24, wpb=12359.1, bsz=465.9, num_updates=6300, lr=0.000178174, gnorm=1.198, clip=0, loss_scale=0.5, train_wall=80, gb_free=17.3, wall=5081
2023-08-12 13:47:20 | INFO | train_inner | epoch 005:    514 / 1474 loss=3.089, trans_loss=3.718, nll_loss=1.989, w2v_ctc_loss=1.696, task_loss=0, contrastive_loss=0.494, total=4035.21, n_correct=2024.77, ppl=3.97, accuracy=50.178, wps=15010.5, ups=1.25, wpb=12051.5, bsz=427.7, num_updates=6400, lr=0.000176777, gnorm=1.052, clip=0, loss_scale=0.5, train_wall=80, gb_free=17, wall=5161
2023-08-12 13:48:40 | INFO | train_inner | epoch 005:    614 / 1474 loss=3.041, trans_loss=3.714, nll_loss=1.981, w2v_ctc_loss=1.684, task_loss=0, contrastive_loss=0.399, total=4117.64, n_correct=2083.83, ppl=3.95, accuracy=50.607, wps=15327.3, ups=1.25, wpb=12284.8, bsz=443.7, num_updates=6500, lr=0.000175412, gnorm=0.945, clip=0, loss_scale=0.5, train_wall=80, gb_free=17.1, wall=5241
2023-08-12 13:50:00 | INFO | train_inner | epoch 005:    714 / 1474 loss=3.087, trans_loss=3.706, nll_loss=1.973, w2v_ctc_loss=1.682, task_loss=0, contrastive_loss=0.508, total=4153.99, n_correct=2118.68, ppl=3.93, accuracy=51.003, wps=15515.1, ups=1.25, wpb=12400.6, bsz=476.1, num_updates=6600, lr=0.000174078, gnorm=1.037, clip=0, loss_scale=0.5, train_wall=79, gb_free=17.5, wall=5321
2023-08-12 13:51:20 | INFO | train_inner | epoch 005:    814 / 1474 loss=3.02, trans_loss=3.689, nll_loss=1.951, w2v_ctc_loss=1.659, task_loss=0, contrastive_loss=0.427, total=4131.08, n_correct=2124, ppl=3.87, accuracy=51.415, wps=15297, ups=1.24, wpb=12332.3, bsz=455.1, num_updates=6700, lr=0.000172774, gnorm=0.986, clip=0, loss_scale=0.5, train_wall=80, gb_free=17.6, wall=5402
2023-08-12 13:52:41 | INFO | train_inner | epoch 005:    914 / 1474 loss=2.959, trans_loss=3.682, nll_loss=1.941, w2v_ctc_loss=1.636, task_loss=0, contrastive_loss=0.372, total=4109.29, n_correct=2126.26, ppl=3.84, accuracy=51.743, wps=15271, ups=1.24, wpb=12268.1, bsz=445.5, num_updates=6800, lr=0.000171499, gnorm=0.93, clip=0, loss_scale=0.5, train_wall=80, gb_free=12.1, wall=5482
2023-08-12 13:54:01 | INFO | train_inner | epoch 005:   1014 / 1474 loss=2.96, trans_loss=3.676, nll_loss=1.935, w2v_ctc_loss=1.627, task_loss=0, contrastive_loss=0.438, total=4163.73, n_correct=2168.64, ppl=3.82, accuracy=52.084, wps=15553.5, ups=1.25, wpb=12429.6, bsz=462.1, num_updates=6900, lr=0.000170251, gnorm=0.854, clip=0, loss_scale=0.5, train_wall=79, gb_free=16.3, wall=5562
2023-08-12 13:55:21 | INFO | train_inner | epoch 005:   1114 / 1474 loss=2.969, trans_loss=3.668, nll_loss=1.923, w2v_ctc_loss=1.627, task_loss=0, contrastive_loss=0.446, total=4172.75, n_correct=2193.63, ppl=3.79, accuracy=52.57, wps=15434.5, ups=1.24, wpb=12448.4, bsz=464, num_updates=7000, lr=0.000169031, gnorm=0.833, clip=0, loss_scale=0.5, train_wall=80, gb_free=12.9, wall=5642
2023-08-12 13:56:42 | INFO | train_inner | epoch 005:   1214 / 1474 loss=2.892, trans_loss=3.66, nll_loss=1.911, w2v_ctc_loss=1.592, task_loss=0, contrastive_loss=0.344, total=4164.91, n_correct=2199.96, ppl=3.76, accuracy=52.821, wps=15323.6, ups=1.23, wpb=12422.5, bsz=455.7, num_updates=7100, lr=0.000167836, gnorm=0.845, clip=0, loss_scale=0.5, train_wall=81, gb_free=14.9, wall=5724
2023-08-12 13:58:03 | INFO | train_inner | epoch 005:   1314 / 1474 loss=2.854, trans_loss=3.655, nll_loss=1.907, w2v_ctc_loss=1.585, task_loss=0, contrastive_loss=0.307, total=4127.88, n_correct=2186.13, ppl=3.75, accuracy=52.96, wps=15319.3, ups=1.24, wpb=12320.7, bsz=445.3, num_updates=7200, lr=0.000166667, gnorm=0.865, clip=0, loss_scale=0.5, train_wall=80, gb_free=16.3, wall=5804
2023-08-12 13:59:23 | INFO | train_inner | epoch 005:   1414 / 1474 loss=2.851, trans_loss=3.647, nll_loss=1.898, w2v_ctc_loss=1.561, task_loss=0, contrastive_loss=0.356, total=4135.9, n_correct=2205.28, ppl=3.73, accuracy=53.32, wps=15473.8, ups=1.25, wpb=12351.8, bsz=456.9, num_updates=7300, lr=0.000165521, gnorm=0.831, clip=0, loss_scale=0.5, train_wall=79, gb_free=16.8, wall=5884
2023-08-12 14:00:11 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 14:00:35 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.804 | trans_loss 5.722 | nll_loss 3.133 | w2v_ctc_loss 1.824 | task_loss 0 | contrastive_loss 0.516 | total 4003.4 | n_correct 2317.7 | ppl 8.77 | accuracy 57.893 | uer 27.598 | wer 28.668 | raw_wer 28.668 | bleu 15.79 | wps 1984.7 | wpb 4003.4 | bsz 141.8 | num_updates 7360 | best_bleu 15.79
2023-08-12 14:00:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7360 updates
2023-08-12 14:00:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 14:00:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 14:01:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 5 @ 7360 updates, score 15.79) (writing took 28.576737264171243 seconds)
2023-08-12 14:01:04 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-08-12 14:01:04 | INFO | train | epoch 005 | loss 3.03 | trans_loss 3.695 | nll_loss 1.959 | w2v_ctc_loss 1.661 | task_loss 0 | contrastive_loss 0.448 | total 4138.65 | n_correct 2120.97 | ppl 3.89 | accuracy 51.248 | wps 13860.8 | ups 1.12 | wpb 12355.8 | bsz 458.5 | num_updates 7360 | lr 0.000164845 | gnorm 0.979 | clip 0 | loss_scale 0.5 | train_wall 1176 | gb_free 16.2 | wall 5985
2023-08-12 14:01:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 14:01:04 | INFO | fairseq.trainer | begin training epoch 6
2023-08-12 14:01:04 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 14:01:44 | INFO | train_inner | epoch 006:     40 / 1474 loss=2.835, trans_loss=3.631, nll_loss=1.875, w2v_ctc_loss=1.561, task_loss=0, contrastive_loss=0.355, total=4114.92, n_correct=2213.68, ppl=3.67, accuracy=53.796, wps=8681.8, ups=0.71, wpb=12278.2, bsz=446.4, num_updates=7400, lr=0.000164399, gnorm=0.813, clip=0, loss_scale=0.5, train_wall=80, gb_free=16.5, wall=6025
2023-08-12 14:03:05 | INFO | train_inner | epoch 006:    140 / 1474 loss=2.8, trans_loss=3.609, nll_loss=1.849, w2v_ctc_loss=1.519, task_loss=0, contrastive_loss=0.391, total=4157.03, n_correct=2257.41, ppl=3.6, accuracy=54.303, wps=15402.8, ups=1.24, wpb=12418.5, bsz=455.9, num_updates=7500, lr=0.000163299, gnorm=0.885, clip=0, loss_scale=1, train_wall=80, gb_free=15, wall=6106
2023-08-12 14:04:24 | INFO | train_inner | epoch 006:    240 / 1474 loss=2.772, trans_loss=3.613, nll_loss=1.855, w2v_ctc_loss=1.536, task_loss=0, contrastive_loss=0.311, total=4121.73, n_correct=2236.87, ppl=3.62, accuracy=54.27, wps=15429.7, ups=1.25, wpb=12312.8, bsz=446.5, num_updates=7600, lr=0.000162221, gnorm=0.777, clip=0, loss_scale=1, train_wall=79, gb_free=16.2, wall=6186
2023-08-12 14:05:46 | INFO | train_inner | epoch 006:    340 / 1474 loss=2.836, trans_loss=3.599, nll_loss=1.835, w2v_ctc_loss=1.478, task_loss=0, contrastive_loss=0.584, total=4170.63, n_correct=2291.76, ppl=3.57, accuracy=54.95, wps=15304.6, ups=1.23, wpb=12451, bsz=486.5, num_updates=7700, lr=0.000161165, gnorm=0.82, clip=0, loss_scale=1, train_wall=81, gb_free=12.9, wall=6267
2023-08-12 14:07:05 | INFO | train_inner | epoch 006:    440 / 1474 loss=2.716, trans_loss=3.592, nll_loss=1.827, w2v_ctc_loss=1.483, task_loss=0, contrastive_loss=0.3, total=4147.89, n_correct=2289.9, ppl=3.55, accuracy=55.206, wps=15618.2, ups=1.26, wpb=12386.2, bsz=467, num_updates=7800, lr=0.000160128, gnorm=0.768, clip=0, loss_scale=1, train_wall=79, gb_free=16.9, wall=6346
2023-08-12 14:08:26 | INFO | train_inner | epoch 006:    540 / 1474 loss=2.707, trans_loss=3.593, nll_loss=1.827, w2v_ctc_loss=1.487, task_loss=0, contrastive_loss=0.286, total=4170.36, n_correct=2309.84, ppl=3.55, accuracy=55.387, wps=15470.5, ups=1.24, wpb=12447.9, bsz=455.9, num_updates=7900, lr=0.000159111, gnorm=0.72, clip=0, loss_scale=1, train_wall=80, gb_free=15.6, wall=6427
2023-08-12 14:09:46 | INFO | train_inner | epoch 006:    640 / 1474 loss=2.722, trans_loss=3.592, nll_loss=1.828, w2v_ctc_loss=1.475, task_loss=0, contrastive_loss=0.341, total=4144.5, n_correct=2295.3, ppl=3.55, accuracy=55.382, wps=15414.2, ups=1.25, wpb=12372.4, bsz=470, num_updates=8000, lr=0.000158114, gnorm=0.8, clip=0, loss_scale=1, train_wall=80, gb_free=17.2, wall=6507
2023-08-12 14:09:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 14:10:13 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.686 | trans_loss 5.605 | nll_loss 2.986 | w2v_ctc_loss 1.821 | task_loss 0 | contrastive_loss 0.452 | total 4003.4 | n_correct 2387.4 | ppl 7.92 | accuracy 59.634 | uer 26.048 | wer 27.296 | raw_wer 27.296 | bleu 16.85 | wps 1701.5 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 16.85
2023-08-12 14:10:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-08-12 14:10:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_6_8000.pt
2023-08-12 14:10:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_6_8000.pt
2023-08-12 14:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 16.85) (writing took 48.36755234375596 seconds)
2023-08-12 14:12:22 | INFO | train_inner | epoch 006:    740 / 1474 loss=2.708, trans_loss=3.592, nll_loss=1.826, w2v_ctc_loss=1.489, task_loss=0, contrastive_loss=0.295, total=4145.29, n_correct=2298.76, ppl=3.55, accuracy=55.455, wps=7944.5, ups=0.64, wpb=12375, bsz=454.7, num_updates=8100, lr=0.000157135, gnorm=0.815, clip=0, loss_scale=1, train_wall=79, gb_free=17.6, wall=6663
2023-08-12 14:13:42 | INFO | train_inner | epoch 006:    840 / 1474 loss=2.671, trans_loss=3.588, nll_loss=1.823, w2v_ctc_loss=1.466, task_loss=0, contrastive_loss=0.268, total=4131.83, n_correct=2292.57, ppl=3.54, accuracy=55.486, wps=15404.4, ups=1.25, wpb=12335.3, bsz=446.9, num_updates=8200, lr=0.000156174, gnorm=0.728, clip=0, loss_scale=1, train_wall=80, gb_free=16.6, wall=6743
2023-08-12 14:15:02 | INFO | train_inner | epoch 006:    940 / 1474 loss=2.722, trans_loss=3.593, nll_loss=1.828, w2v_ctc_loss=1.48, task_loss=0, contrastive_loss=0.368, total=4068.88, n_correct=2254.47, ppl=3.55, accuracy=55.408, wps=15057.8, ups=1.24, wpb=12145.6, bsz=436.9, num_updates=8300, lr=0.00015523, gnorm=0.804, clip=0, loss_scale=1, train_wall=80, gb_free=16.1, wall=6824
2023-08-12 14:16:22 | INFO | train_inner | epoch 006:   1040 / 1474 loss=2.715, trans_loss=3.576, nll_loss=1.806, w2v_ctc_loss=1.442, task_loss=0, contrastive_loss=0.438, total=4176.69, n_correct=2341.08, ppl=3.5, accuracy=56.051, wps=15581.4, ups=1.25, wpb=12465.8, bsz=480, num_updates=8400, lr=0.000154303, gnorm=0.733, clip=0, loss_scale=1, train_wall=80, gb_free=16.5, wall=6904
2023-08-12 14:17:42 | INFO | train_inner | epoch 006:   1140 / 1474 loss=2.655, trans_loss=3.572, nll_loss=1.802, w2v_ctc_loss=1.451, task_loss=0, contrastive_loss=0.294, total=4078.12, n_correct=2282.3, ppl=3.49, accuracy=55.965, wps=15267.9, ups=1.25, wpb=12175.4, bsz=435.5, num_updates=8500, lr=0.000153393, gnorm=0.719, clip=0, loss_scale=1, train_wall=79, gb_free=15, wall=6983
2023-08-12 14:19:03 | INFO | train_inner | epoch 006:   1240 / 1474 loss=2.726, trans_loss=3.561, nll_loss=1.789, w2v_ctc_loss=1.426, task_loss=0, contrastive_loss=0.564, total=4126.61, n_correct=2325.68, ppl=3.46, accuracy=56.358, wps=15309.4, ups=1.24, wpb=12325.4, bsz=463.4, num_updates=8600, lr=0.000152499, gnorm=0.683, clip=0, loss_scale=1, train_wall=80, gb_free=13.8, wall=7064
2023-08-12 14:20:23 | INFO | train_inner | epoch 006:   1340 / 1474 loss=2.6, trans_loss=3.562, nll_loss=1.787, w2v_ctc_loss=1.423, task_loss=0, contrastive_loss=0.242, total=4127.1, n_correct=2343.25, ppl=3.45, accuracy=56.777, wps=15369.8, ups=1.25, wpb=12313, bsz=454.8, num_updates=8700, lr=0.00015162, gnorm=0.688, clip=0, loss_scale=1, train_wall=80, gb_free=16.7, wall=7144
2023-08-12 14:21:43 | INFO | train_inner | epoch 006:   1440 / 1474 loss=2.598, trans_loss=3.553, nll_loss=1.778, w2v_ctc_loss=1.419, task_loss=0, contrastive_loss=0.251, total=4197.14, n_correct=2389.65, ppl=3.43, accuracy=56.935, wps=15598.4, ups=1.24, wpb=12529.7, bsz=463.5, num_updates=8800, lr=0.000150756, gnorm=0.669, clip=0, loss_scale=1, train_wall=80, gb_free=15.4, wall=7224
2023-08-12 14:22:10 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 14:22:32 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.557 | trans_loss 5.505 | nll_loss 2.862 | w2v_ctc_loss 1.691 | task_loss 0 | contrastive_loss 0.42 | total 4003.4 | n_correct 2443.1 | ppl 7.27 | accuracy 61.026 | uer 24.771 | wer 26.241 | raw_wer 26.241 | bleu 17.82 | wps 2262.3 | wpb 4003.4 | bsz 141.8 | num_updates 8834 | best_bleu 17.82
2023-08-12 14:22:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8834 updates
2023-08-12 14:22:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 14:22:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 14:23:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 6 @ 8834 updates, score 17.82) (writing took 29.819910237565637 seconds)
2023-08-12 14:23:02 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-08-12 14:23:02 | INFO | train | epoch 006 | loss 2.71 | trans_loss 3.585 | nll_loss 1.819 | w2v_ctc_loss 1.47 | task_loss 0 | contrastive_loss 0.351 | total 4138.65 | n_correct 2299.82 | ppl 3.53 | accuracy 55.569 | wps 13815.9 | ups 1.12 | wpb 12355.8 | bsz 458.5 | num_updates 8834 | lr 0.000150465 | gnorm 0.758 | clip 0 | loss_scale 1 | train_wall 1175 | gb_free 15.1 | wall 7304
2023-08-12 14:23:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 14:23:03 | INFO | fairseq.trainer | begin training epoch 7
2023-08-12 14:23:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 14:24:03 | INFO | train_inner | epoch 007:     66 / 1474 loss=2.564, trans_loss=3.541, nll_loss=1.761, w2v_ctc_loss=1.387, task_loss=0, contrastive_loss=0.26, total=4099.48, n_correct=2354.59, ppl=3.39, accuracy=57.436, wps=8748.5, ups=0.71, wpb=12236.9, bsz=460.3, num_updates=8900, lr=0.000149906, gnorm=0.679, clip=0, loss_scale=1, train_wall=80, gb_free=16.2, wall=7364
2023-08-12 14:25:22 | INFO | train_inner | epoch 007:    166 / 1474 loss=2.579, trans_loss=3.535, nll_loss=1.755, w2v_ctc_loss=1.381, task_loss=0, contrastive_loss=0.33, total=4107.4, n_correct=2359.31, ppl=3.37, accuracy=57.44, wps=15435.2, ups=1.26, wpb=12263.9, bsz=455.4, num_updates=9000, lr=0.000149071, gnorm=0.713, clip=0, loss_scale=1, train_wall=79, gb_free=16.9, wall=7444
2023-08-12 14:26:43 | INFO | train_inner | epoch 007:    266 / 1474 loss=2.529, trans_loss=3.527, nll_loss=1.743, w2v_ctc_loss=1.372, task_loss=0, contrastive_loss=0.235, total=4137.13, n_correct=2387.62, ppl=3.35, accuracy=57.712, wps=15363.1, ups=1.24, wpb=12346.5, bsz=454.9, num_updates=9100, lr=0.00014825, gnorm=0.654, clip=0, loss_scale=1, train_wall=80, gb_free=16.4, wall=7524
2023-08-12 14:28:03 | INFO | train_inner | epoch 007:    366 / 1474 loss=2.612, trans_loss=3.528, nll_loss=1.745, w2v_ctc_loss=1.358, task_loss=0, contrastive_loss=0.501, total=4199.72, n_correct=2423.88, ppl=3.35, accuracy=57.715, wps=15599.3, ups=1.24, wpb=12534, bsz=480.8, num_updates=9200, lr=0.000147442, gnorm=0.657, clip=0, loss_scale=1, train_wall=80, gb_free=17.4, wall=7604
2023-08-12 14:29:23 | INFO | train_inner | epoch 007:    466 / 1474 loss=2.583, trans_loss=3.527, nll_loss=1.745, w2v_ctc_loss=1.359, task_loss=0, contrastive_loss=0.418, total=4149.08, n_correct=2395.57, ppl=3.35, accuracy=57.737, wps=15505.3, ups=1.25, wpb=12391, bsz=459, num_updates=9300, lr=0.000146647, gnorm=0.699, clip=0, loss_scale=1, train_wall=79, gb_free=17, wall=7684
2023-08-12 14:30:43 | INFO | train_inner | epoch 007:    566 / 1474 loss=2.506, trans_loss=3.524, nll_loss=1.738, w2v_ctc_loss=1.351, task_loss=0, contrastive_loss=0.238, total=4168.84, n_correct=2418.96, ppl=3.34, accuracy=58.025, wps=15627.9, ups=1.26, wpb=12436.4, bsz=460, num_updates=9400, lr=0.000145865, gnorm=0.651, clip=0, loss_scale=1, train_wall=79, gb_free=16.3, wall=7764
2023-08-12 14:32:04 | INFO | train_inner | epoch 007:    666 / 1474 loss=2.485, trans_loss=3.517, nll_loss=1.731, w2v_ctc_loss=1.341, task_loss=0, contrastive_loss=0.221, total=4164.13, n_correct=2426.79, ppl=3.32, accuracy=58.278, wps=15334.2, ups=1.23, wpb=12427.2, bsz=458.5, num_updates=9500, lr=0.000145095, gnorm=0.636, clip=0, loss_scale=1, train_wall=81, gb_free=15.6, wall=7845
2023-08-12 14:33:24 | INFO | train_inner | epoch 007:    766 / 1474 loss=2.485, trans_loss=3.513, nll_loss=1.726, w2v_ctc_loss=1.341, task_loss=0, contrastive_loss=0.224, total=4126.5, n_correct=2401.78, ppl=3.31, accuracy=58.204, wps=15296.3, ups=1.24, wpb=12320.2, bsz=450.3, num_updates=9600, lr=0.000144338, gnorm=0.63, clip=0, loss_scale=2, train_wall=80, gb_free=15.3, wall=7925
2023-08-12 14:34:45 | INFO | train_inner | epoch 007:    866 / 1474 loss=2.485, trans_loss=3.519, nll_loss=1.733, w2v_ctc_loss=1.34, task_loss=0, contrastive_loss=0.228, total=4133.53, n_correct=2406.07, ppl=3.32, accuracy=58.209, wps=15260.2, ups=1.24, wpb=12333.5, bsz=453.5, num_updates=9700, lr=0.000143592, gnorm=0.655, clip=0, loss_scale=2, train_wall=80, gb_free=12.9, wall=8006
2023-08-12 14:36:06 | INFO | train_inner | epoch 007:    966 / 1474 loss=2.515, trans_loss=3.507, nll_loss=1.72, w2v_ctc_loss=1.324, task_loss=0, contrastive_loss=0.332, total=4144.45, n_correct=2428.14, ppl=3.29, accuracy=58.588, wps=15357.3, ups=1.24, wpb=12374.7, bsz=476.9, num_updates=9800, lr=0.000142857, gnorm=0.683, clip=0, loss_scale=2, train_wall=80, gb_free=16.1, wall=8087
2023-08-12 14:37:26 | INFO | train_inner | epoch 007:   1066 / 1474 loss=2.475, trans_loss=3.519, nll_loss=1.735, w2v_ctc_loss=1.35, task_loss=0, contrastive_loss=0.201, total=4100.65, n_correct=2388.05, ppl=3.33, accuracy=58.236, wps=15255.3, ups=1.25, wpb=12242.5, bsz=435.4, num_updates=9900, lr=0.000142134, gnorm=0.793, clip=1, loss_scale=2, train_wall=80, gb_free=15.8, wall=8167
2023-08-12 14:38:47 | INFO | train_inner | epoch 007:   1166 / 1474 loss=2.563, trans_loss=3.501, nll_loss=1.715, w2v_ctc_loss=1.323, task_loss=0, contrastive_loss=0.465, total=4138.96, n_correct=2428.99, ppl=3.28, accuracy=58.686, wps=15307.4, ups=1.24, wpb=12367.7, bsz=471.9, num_updates=10000, lr=0.000141421, gnorm=0.696, clip=0, loss_scale=2, train_wall=80, gb_free=16.6, wall=8248
2023-08-12 14:38:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 14:39:10 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.45 | trans_loss 5.426 | nll_loss 2.764 | w2v_ctc_loss 1.595 | task_loss 0 | contrastive_loss 0.383 | total 4003.4 | n_correct 2493.5 | ppl 6.79 | accuracy 62.285 | uer 23.492 | wer 25.402 | raw_wer 25.402 | bleu 18.56 | wps 2243.5 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 18.56
2023-08-12 14:39:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-08-12 14:39:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_7_10000.pt
2023-08-12 14:39:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_7_10000.pt
2023-08-12 14:40:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 18.56) (writing took 50.85456250794232 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-12 14:41:21 | INFO | train_inner | epoch 007:   1266 / 1474 loss=2.447, trans_loss=3.502, nll_loss=1.714, w2v_ctc_loss=1.315, task_loss=0, contrastive_loss=0.223, total=4120.42, n_correct=2421, ppl=3.28, accuracy=58.756, wps=7991.3, ups=0.65, wpb=12304.2, bsz=446.6, num_updates=10100, lr=0.00014072, gnorm=0.524, clip=0, loss_scale=2, train_wall=79, gb_free=15.5, wall=8402
2023-08-12 14:42:41 | INFO | train_inner | epoch 007:   1366 / 1474 loss=2.482, trans_loss=3.498, nll_loss=1.709, w2v_ctc_loss=1.328, task_loss=0, contrastive_loss=0.262, total=4186.45, n_correct=2468.49, ppl=3.27, accuracy=58.964, wps=15600.3, ups=1.25, wpb=12499.3, bsz=479.7, num_updates=10200, lr=0.000140028, gnorm=0.504, clip=0, loss_scale=2, train_wall=80, gb_free=12.3, wall=8482
2023-08-12 14:44:02 | INFO | train_inner | epoch 007:   1466 / 1474 loss=2.486, trans_loss=3.498, nll_loss=1.711, w2v_ctc_loss=1.324, task_loss=0, contrastive_loss=0.321, total=4117.01, n_correct=2422.68, ppl=3.27, accuracy=58.846, wps=15133.6, ups=1.23, wpb=12300.4, bsz=448.4, num_updates=10300, lr=0.000139347, gnorm=0.517, clip=0, loss_scale=2, train_wall=81, gb_free=16, wall=8563
2023-08-12 14:44:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
2023-08-12 14:44:30 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.443 | trans_loss 5.403 | nll_loss 2.732 | w2v_ctc_loss 1.632 | task_loss 0 | contrastive_loss 0.382 | total 4003.4 | n_correct 2503.2 | ppl 6.64 | accuracy 62.527 | uer 23.571 | wer 25.379 | raw_wer 25.379 | bleu 18.97 | wps 2377.2 | wpb 4003.4 | bsz 141.8 | num_updates 10308 | best_bleu 18.97
2023-08-12 14:44:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10308 updates
2023-08-12 14:44:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 14:44:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 14:44:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 7 @ 10308 updates, score 18.97) (writing took 27.788208624348044 seconds)
2023-08-12 14:44:58 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-08-12 14:44:58 | INFO | train | epoch 007 | loss 2.518 | trans_loss 3.516 | nll_loss 1.731 | w2v_ctc_loss 1.345 | task_loss 0 | contrastive_loss 0.299 | total 4138.65 | n_correct 2409.34 | ppl 3.32 | accuracy 58.216 | wps 13840.6 | ups 1.12 | wpb 12355.8 | bsz 458.5 | num_updates 10308 | lr 0.000139293 | gnorm 0.644 | clip 0.1 | loss_scale 2 | train_wall 1177 | gb_free 13.3 | wall 8620
2023-08-12 14:44:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 14:44:59 | INFO | fairseq.trainer | begin training epoch 8
2023-08-12 14:44:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 14:46:21 | INFO | train_inner | epoch 008:     92 / 1474 loss=2.411, trans_loss=3.491, nll_loss=1.694, w2v_ctc_loss=1.289, task_loss=0, contrastive_loss=0.214, total=4097.76, n_correct=2432.2, ppl=3.24, accuracy=59.354, wps=8810.9, ups=0.72, wpb=12216.9, bsz=437, num_updates=10400, lr=0.000138675, gnorm=0.503, clip=0, loss_scale=2, train_wall=80, gb_free=15.8, wall=8702
2023-08-12 14:47:41 | INFO | train_inner | epoch 008:    192 / 1474 loss=2.408, trans_loss=3.478, nll_loss=1.68, w2v_ctc_loss=1.284, task_loss=0, contrastive_loss=0.231, total=4039, n_correct=2409.27, ppl=3.2, accuracy=59.65, wps=15088.1, ups=1.25, wpb=12046.3, bsz=428.9, num_updates=10500, lr=0.000138013, gnorm=0.519, clip=0, loss_scale=2, train_wall=79, gb_free=14.8, wall=8782
2023-08-12 14:49:01 | INFO | train_inner | epoch 008:    292 / 1474 loss=2.409, trans_loss=3.47, nll_loss=1.671, w2v_ctc_loss=1.284, task_loss=0, contrastive_loss=0.227, total=4210.6, n_correct=2519.57, ppl=3.19, accuracy=59.839, wps=15548.6, ups=1.24, wpb=12566.5, bsz=486.9, num_updates=10600, lr=0.000137361, gnorm=0.495, clip=0, loss_scale=2, train_wall=80, gb_free=15.9, wall=8863
2023-08-12 14:50:22 | INFO | train_inner | epoch 008:    392 / 1474 loss=2.416, trans_loss=3.476, nll_loss=1.677, w2v_ctc_loss=1.287, task_loss=0, contrastive_loss=0.251, total=4139.64, n_correct=2468.26, ppl=3.2, accuracy=59.625, wps=15266.5, ups=1.24, wpb=12350.6, bsz=447.4, num_updates=10700, lr=0.000136717, gnorm=0.504, clip=0, loss_scale=2, train_wall=80, gb_free=15, wall=8944
2023-08-12 14:51:43 | INFO | train_inner | epoch 008:    492 / 1474 loss=2.536, trans_loss=3.474, nll_loss=1.678, w2v_ctc_loss=1.279, task_loss=0, contrastive_loss=0.519, total=4189.5, n_correct=2498.93, ppl=3.2, accuracy=59.647, wps=15427.4, ups=1.23, wpb=12508.9, bsz=499.4, num_updates=10800, lr=0.000136083, gnorm=0.556, clip=0, loss_scale=2, train_wall=81, gb_free=12.8, wall=9025
2023-08-12 14:53:03 | INFO | train_inner | epoch 008:    592 / 1474 loss=2.39, trans_loss=3.469, nll_loss=1.674, w2v_ctc_loss=1.288, task_loss=0, contrastive_loss=0.186, total=4061.09, n_correct=2423.97, ppl=3.19, accuracy=59.688, wps=15232.6, ups=1.25, wpb=12140.1, bsz=427.7, num_updates=10900, lr=0.000135457, gnorm=0.486, clip=0, loss_scale=2, train_wall=79, gb_free=16.4, wall=9104
2023-08-12 14:54:24 | INFO | train_inner | epoch 008:    692 / 1474 loss=2.381, trans_loss=3.464, nll_loss=1.664, w2v_ctc_loss=1.28, task_loss=0, contrastive_loss=0.196, total=4144.17, n_correct=2490.14, ppl=3.17, accuracy=60.088, wps=15322.7, ups=1.24, wpb=12370.7, bsz=450.4, num_updates=11000, lr=0.00013484, gnorm=0.473, clip=0, loss_scale=2, train_wall=80, gb_free=13.4, wall=9185
2023-08-12 14:55:43 | INFO | train_inner | epoch 008:    792 / 1474 loss=2.407, trans_loss=3.46, nll_loss=1.663, w2v_ctc_loss=1.274, task_loss=0, contrastive_loss=0.281, total=4117.36, n_correct=2474.51, ppl=3.17, accuracy=60.099, wps=15474.2, ups=1.26, wpb=12305.6, bsz=448.1, num_updates=11100, lr=0.000134231, gnorm=0.508, clip=0, loss_scale=2, train_wall=79, gb_free=16.2, wall=9265
2023-08-12 14:57:04 | INFO | train_inner | epoch 008:    892 / 1474 loss=2.404, trans_loss=3.462, nll_loss=1.664, w2v_ctc_loss=1.251, task_loss=0, contrastive_loss=0.29, total=4182.5, n_correct=2518.36, ppl=3.17, accuracy=60.212, wps=15552.1, ups=1.25, wpb=12489.6, bsz=478.7, num_updates=11200, lr=0.000133631, gnorm=0.472, clip=0, loss_scale=2, train_wall=80, gb_free=17.3, wall=9345
2023-08-12 14:58:23 | INFO | train_inner | epoch 008:    992 / 1474 loss=2.361, trans_loss=3.46, nll_loss=1.66, w2v_ctc_loss=1.26, task_loss=0, contrastive_loss=0.19, total=4155.49, n_correct=2512.27, ppl=3.16, accuracy=60.457, wps=15588.8, ups=1.26, wpb=12408.4, bsz=464, num_updates=11300, lr=0.000133038, gnorm=0.503, clip=0, loss_scale=2, train_wall=79, gb_free=17, wall=9425
2023-08-12 14:59:44 | INFO | train_inner | epoch 008:   1092 / 1474 loss=2.43, trans_loss=3.463, nll_loss=1.662, w2v_ctc_loss=1.254, task_loss=0, contrastive_loss=0.415, total=4186.39, n_correct=2518.52, ppl=3.17, accuracy=60.16, wps=15384.8, ups=1.23, wpb=12493.6, bsz=461.7, num_updates=11400, lr=0.000132453, gnorm=0.491, clip=0, loss_scale=2, train_wall=81, gb_free=15.2, wall=9506
2023-08-12 15:01:04 | INFO | train_inner | epoch 008:   1192 / 1474 loss=2.367, trans_loss=3.455, nll_loss=1.655, w2v_ctc_loss=1.262, task_loss=0, contrastive_loss=0.197, total=4178.48, n_correct=2520.94, ppl=3.15, accuracy=60.332, wps=15610.1, ups=1.25, wpb=12482, bsz=472.1, num_updates=11500, lr=0.000131876, gnorm=0.488, clip=0, loss_scale=2, train_wall=79, gb_free=15.8, wall=9586
2023-08-12 15:02:24 | INFO | train_inner | epoch 008:   1292 / 1474 loss=2.367, trans_loss=3.458, nll_loss=1.658, w2v_ctc_loss=1.263, task_loss=0, contrastive_loss=0.218, total=4064.4, n_correct=2446.76, ppl=3.16, accuracy=60.2, wps=15177.3, ups=1.25, wpb=12140.7, bsz=436, num_updates=11600, lr=0.000131306, gnorm=0.477, clip=0, loss_scale=4, train_wall=80, gb_free=15.8, wall=9666
2023-08-12 15:03:44 | INFO | train_inner | epoch 008:   1392 / 1474 loss=2.395, trans_loss=3.46, nll_loss=1.66, w2v_ctc_loss=1.251, task_loss=0, contrastive_loss=0.287, total=4163.91, n_correct=2516.56, ppl=3.16, accuracy=60.437, wps=15711.7, ups=1.26, wpb=12433, bsz=472.8, num_updates=11700, lr=0.000130744, gnorm=0.485, clip=0, loss_scale=4, train_wall=79, gb_free=16.4, wall=9745
2023-08-12 15:04:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 15:05:11 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.353 | trans_loss 5.327 | nll_loss 2.635 | w2v_ctc_loss 1.559 | task_loss 0 | contrastive_loss 0.356 | total 4003.4 | n_correct 2547.3 | ppl 6.21 | accuracy 63.628 | uer 22.451 | wer 24.369 | raw_wer 24.369 | bleu 19.87 | wps 2419.1 | wpb 4003.4 | bsz 141.8 | num_updates 11782 | best_bleu 19.87
2023-08-12 15:05:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11782 updates
2023-08-12 15:05:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 15:05:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 15:05:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 8 @ 11782 updates, score 19.87) (writing took 28.764190528541803 seconds)
2023-08-12 15:05:41 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-08-12 15:05:41 | INFO | train | epoch 008 | loss 2.404 | trans_loss 3.467 | nll_loss 1.668 | w2v_ctc_loss 1.27 | task_loss 0 | contrastive_loss 0.269 | total 4138.65 | n_correct 2483.91 | ppl 3.18 | accuracy 60.017 | wps 14660.8 | ups 1.19 | wpb 12355.8 | bsz 458.5 | num_updates 11782 | lr 0.000130288 | gnorm 0.496 | clip 0 | loss_scale 4 | train_wall 1176 | gb_free 16.9 | wall 9862
2023-08-12 15:05:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 15:05:41 | INFO | fairseq.trainer | begin training epoch 9
2023-08-12 15:05:41 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 15:06:03 | INFO | train_inner | epoch 009:     18 / 1474 loss=2.392, trans_loss=3.454, nll_loss=1.651, w2v_ctc_loss=1.234, task_loss=0, contrastive_loss=0.377, total=4111.15, n_correct=2491.29, ppl=3.14, accuracy=60.598, wps=8771.1, ups=0.71, wpb=12269.8, bsz=460.9, num_updates=11800, lr=0.000130189, gnorm=0.469, clip=0, loss_scale=4, train_wall=80, gb_free=16.9, wall=9885
2023-08-12 15:07:23 | INFO | train_inner | epoch 009:    118 / 1474 loss=2.316, trans_loss=3.427, nll_loss=1.619, w2v_ctc_loss=1.213, task_loss=0, contrastive_loss=0.21, total=4186.63, n_correct=2568.97, ppl=3.07, accuracy=61.361, wps=15730.3, ups=1.26, wpb=12503, bsz=481.5, num_updates=11900, lr=0.000129641, gnorm=0.465, clip=0, loss_scale=4, train_wall=79, gb_free=16.4, wall=9964
2023-08-12 15:08:43 | INFO | train_inner | epoch 009:    218 / 1474 loss=2.29, trans_loss=3.432, nll_loss=1.623, w2v_ctc_loss=1.211, task_loss=0, contrastive_loss=0.166, total=4071.82, n_correct=2492.78, ppl=3.08, accuracy=61.22, wps=15130.6, ups=1.24, wpb=12157.1, bsz=430.9, num_updates=12000, lr=0.000129099, gnorm=0.464, clip=0, loss_scale=4, train_wall=80, gb_free=16.7, wall=10045
2023-08-12 15:08:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 15:09:06 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.342 | trans_loss 5.325 | nll_loss 2.63 | w2v_ctc_loss 1.53 | task_loss 0 | contrastive_loss 0.356 | total 4003.4 | n_correct 2556.2 | ppl 6.19 | accuracy 63.851 | uer 22.032 | wer 23.985 | raw_wer 23.985 | bleu 19.92 | wps 2253.6 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 19.92
2023-08-12 15:09:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-08-12 15:09:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_9_12000.pt
2023-08-12 15:09:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_9_12000.pt
2023-08-12 15:09:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 19.92) (writing took 29.941128136590123 seconds)
2023-08-12 15:10:57 | INFO | train_inner | epoch 009:    318 / 1474 loss=2.309, trans_loss=3.421, nll_loss=1.613, w2v_ctc_loss=1.199, task_loss=0, contrastive_loss=0.222, total=4167.06, n_correct=2563.51, ppl=3.06, accuracy=61.518, wps=9331.5, ups=0.75, wpb=12450.2, bsz=485.4, num_updates=12100, lr=0.000128565, gnorm=0.502, clip=0, loss_scale=4, train_wall=79, gb_free=14.9, wall=10178
2023-08-12 15:12:17 | INFO | train_inner | epoch 009:    418 / 1474 loss=2.299, trans_loss=3.438, nll_loss=1.631, w2v_ctc_loss=1.213, task_loss=0, contrastive_loss=0.178, total=4175.09, n_correct=2549.3, ppl=3.1, accuracy=61.06, wps=15497.9, ups=1.24, wpb=12466.4, bsz=457.5, num_updates=12200, lr=0.000128037, gnorm=0.473, clip=0, loss_scale=4, train_wall=80, gb_free=17.2, wall=10258
2023-08-12 15:13:36 | INFO | train_inner | epoch 009:    518 / 1474 loss=2.326, trans_loss=3.434, nll_loss=1.625, w2v_ctc_loss=1.225, task_loss=0, contrastive_loss=0.232, total=4118.47, n_correct=2519.18, ppl=3.08, accuracy=61.168, wps=15497, ups=1.26, wpb=12292.7, bsz=439.7, num_updates=12300, lr=0.000127515, gnorm=0.461, clip=0, loss_scale=4, train_wall=79, gb_free=17.3, wall=10338
2023-08-12 15:14:57 | INFO | train_inner | epoch 009:    618 / 1474 loss=2.328, trans_loss=3.426, nll_loss=1.619, w2v_ctc_loss=1.201, task_loss=0, contrastive_loss=0.288, total=4141.76, n_correct=2538.3, ppl=3.07, accuracy=61.286, wps=15381, ups=1.24, wpb=12378.7, bsz=462.3, num_updates=12400, lr=0.000127, gnorm=0.509, clip=0, loss_scale=4, train_wall=80, gb_free=16.8, wall=10418
2023-08-12 15:16:16 | INFO | train_inner | epoch 009:    718 / 1474 loss=2.298, trans_loss=3.432, nll_loss=1.626, w2v_ctc_loss=1.223, task_loss=0, contrastive_loss=0.178, total=4075.02, n_correct=2492.28, ppl=3.09, accuracy=61.16, wps=15324.7, ups=1.26, wpb=12176, bsz=443.4, num_updates=12500, lr=0.000126491, gnorm=0.467, clip=0, loss_scale=4, train_wall=79, gb_free=17.4, wall=10498
2023-08-12 15:17:37 | INFO | train_inner | epoch 009:    818 / 1474 loss=2.406, trans_loss=3.426, nll_loss=1.619, w2v_ctc_loss=1.211, task_loss=0, contrastive_loss=0.423, total=4214.28, n_correct=2585.35, ppl=3.07, accuracy=61.347, wps=15540.8, ups=1.23, wpb=12593.1, bsz=499.2, num_updates=12600, lr=0.000125988, gnorm=0.483, clip=0, loss_scale=4, train_wall=81, gb_free=16.8, wall=10579
2023-08-12 15:18:58 | INFO | train_inner | epoch 009:    918 / 1474 loss=2.361, trans_loss=3.432, nll_loss=1.621, w2v_ctc_loss=1.211, task_loss=0, contrastive_loss=0.4, total=4157.54, n_correct=2550.4, ppl=3.08, accuracy=61.344, wps=15311.5, ups=1.23, wpb=12400.4, bsz=452, num_updates=12700, lr=0.000125491, gnorm=0.487, clip=0, loss_scale=4, train_wall=80, gb_free=16.3, wall=10660
2023-08-12 15:20:18 | INFO | train_inner | epoch 009:   1018 / 1474 loss=2.278, trans_loss=3.434, nll_loss=1.625, w2v_ctc_loss=1.204, task_loss=0, contrastive_loss=0.178, total=4093.77, n_correct=2510.3, ppl=3.08, accuracy=61.32, wps=15318, ups=1.25, wpb=12221, bsz=423.7, num_updates=12800, lr=0.000125, gnorm=0.463, clip=0, loss_scale=4, train_wall=79, gb_free=17.2, wall=10740
2023-08-12 15:21:38 | INFO | train_inner | epoch 009:   1118 / 1474 loss=2.296, trans_loss=3.432, nll_loss=1.621, w2v_ctc_loss=1.202, task_loss=0, contrastive_loss=0.201, total=4173.97, n_correct=2568.2, ppl=3.08, accuracy=61.529, wps=15579.7, ups=1.25, wpb=12442.6, bsz=471.4, num_updates=12900, lr=0.000124515, gnorm=0.467, clip=0, loss_scale=4, train_wall=79, gb_free=16.1, wall=10819
2023-08-12 15:22:59 | INFO | train_inner | epoch 009:   1218 / 1474 loss=2.305, trans_loss=3.431, nll_loss=1.623, w2v_ctc_loss=1.225, task_loss=0, contrastive_loss=0.186, total=4144.88, n_correct=2543.59, ppl=3.08, accuracy=61.367, wps=15292.2, ups=1.24, wpb=12376.1, bsz=450, num_updates=13000, lr=0.000124035, gnorm=0.543, clip=0, loss_scale=4, train_wall=80, gb_free=16.3, wall=10900
2023-08-12 15:24:19 | INFO | train_inner | epoch 009:   1318 / 1474 loss=2.354, trans_loss=3.421, nll_loss=1.61, w2v_ctc_loss=1.19, task_loss=0, contrastive_loss=0.375, total=4200.61, n_correct=2593.49, ppl=3.05, accuracy=61.741, wps=15640.3, ups=1.25, wpb=12534.4, bsz=490.7, num_updates=13100, lr=0.00012356, gnorm=0.463, clip=0, loss_scale=4, train_wall=80, gb_free=15.5, wall=10981
2023-08-12 15:25:39 | INFO | train_inner | epoch 009:   1418 / 1474 loss=2.279, trans_loss=3.437, nll_loss=1.63, w2v_ctc_loss=1.21, task_loss=0, contrastive_loss=0.162, total=4075.96, n_correct=2498.87, ppl=3.09, accuracy=61.308, wps=15194.2, ups=1.25, wpb=12161.1, bsz=430.6, num_updates=13200, lr=0.000123091, gnorm=0.496, clip=0, loss_scale=4, train_wall=80, gb_free=17.5, wall=11061
2023-08-12 15:26:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 15:26:46 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.292 | trans_loss 5.284 | nll_loss 2.585 | w2v_ctc_loss 1.488 | task_loss 0 | contrastive_loss 0.335 | total 4003.4 | n_correct 2570.9 | ppl 6 | accuracy 64.218 | uer 21.535 | wer 23.418 | raw_wer 23.418 | bleu 20.11 | wps 2259.4 | wpb 4003.4 | bsz 141.8 | num_updates 13256 | best_bleu 20.11
2023-08-12 15:26:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13256 updates
2023-08-12 15:26:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 15:27:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 15:27:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 9 @ 13256 updates, score 20.11) (writing took 28.75356973335147 seconds)
2023-08-12 15:27:16 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-08-12 15:27:16 | INFO | train | epoch 009 | loss 2.318 | trans_loss 3.43 | nll_loss 1.621 | w2v_ctc_loss 1.209 | task_loss 0 | contrastive_loss 0.249 | total 4138.65 | n_correct 2539.36 | ppl 3.08 | accuracy 61.357 | wps 14063.4 | ups 1.14 | wpb 12355.8 | bsz 458.5 | num_updates 13256 | lr 0.000122831 | gnorm 0.48 | clip 0 | loss_scale 4 | train_wall 1174 | gb_free 11.7 | wall 11157
2023-08-12 15:27:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 15:27:16 | INFO | fairseq.trainer | begin training epoch 10
2023-08-12 15:27:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 15:27:59 | INFO | train_inner | epoch 010:     44 / 1474 loss=2.29, trans_loss=3.414, nll_loss=1.601, w2v_ctc_loss=1.175, task_loss=0, contrastive_loss=0.256, total=4112.83, n_correct=2549.94, ppl=3.03, accuracy=62, wps=8813.7, ups=0.72, wpb=12275.4, bsz=475.1, num_updates=13300, lr=0.000122628, gnorm=0.459, clip=0, loss_scale=4, train_wall=79, gb_free=16.3, wall=11200
2023-08-12 15:29:19 | INFO | train_inner | epoch 010:    144 / 1474 loss=2.228, trans_loss=3.4, nll_loss=1.584, w2v_ctc_loss=1.152, task_loss=0, contrastive_loss=0.174, total=4230.29, n_correct=2637.98, ppl=3, accuracy=62.359, wps=15794.5, ups=1.25, wpb=12634.2, bsz=472.7, num_updates=13400, lr=0.000122169, gnorm=0.448, clip=0, loss_scale=4, train_wall=79, gb_free=16.5, wall=11280
2023-08-12 15:30:39 | INFO | train_inner | epoch 010:    244 / 1474 loss=2.275, trans_loss=3.397, nll_loss=1.578, w2v_ctc_loss=1.16, task_loss=0, contrastive_loss=0.298, total=4131.59, n_correct=2581.2, ppl=2.99, accuracy=62.475, wps=15418.9, ups=1.25, wpb=12331.3, bsz=463.6, num_updates=13500, lr=0.000121716, gnorm=0.467, clip=0, loss_scale=4, train_wall=80, gb_free=10.4, wall=11360
2023-08-12 15:31:59 | INFO | train_inner | epoch 010:    344 / 1474 loss=2.24, trans_loss=3.395, nll_loss=1.581, w2v_ctc_loss=1.153, task_loss=0, contrastive_loss=0.214, total=4135.23, n_correct=2580.27, ppl=2.99, accuracy=62.397, wps=15450.3, ups=1.25, wpb=12360.3, bsz=454.8, num_updates=13600, lr=0.000121268, gnorm=0.454, clip=0, loss_scale=4, train_wall=80, gb_free=17.2, wall=11440
2023-08-12 15:33:20 | INFO | train_inner | epoch 010:    444 / 1474 loss=2.293, trans_loss=3.402, nll_loss=1.586, w2v_ctc_loss=1.145, task_loss=0, contrastive_loss=0.383, total=4197.95, n_correct=2616.38, ppl=3, accuracy=62.325, wps=15362.9, ups=1.23, wpb=12531.8, bsz=481.2, num_updates=13700, lr=0.000120824, gnorm=0.477, clip=0, loss_scale=8, train_wall=81, gb_free=15.5, wall=11521
2023-08-12 15:34:41 | INFO | train_inner | epoch 010:    544 / 1474 loss=2.238, trans_loss=3.411, nll_loss=1.594, w2v_ctc_loss=1.176, task_loss=0, contrastive_loss=0.162, total=4097.61, n_correct=2545.12, ppl=3.02, accuracy=62.112, wps=15163.2, ups=1.24, wpb=12219.2, bsz=434.6, num_updates=13800, lr=0.000120386, gnorm=0.457, clip=0, loss_scale=8, train_wall=80, gb_free=17.7, wall=11602
2023-08-12 15:36:01 | INFO | train_inner | epoch 010:    644 / 1474 loss=2.283, trans_loss=3.405, nll_loss=1.589, w2v_ctc_loss=1.164, task_loss=0, contrastive_loss=0.282, total=4187.04, n_correct=2609.99, ppl=3.01, accuracy=62.335, wps=15530.7, ups=1.24, wpb=12494.9, bsz=483.5, num_updates=13900, lr=0.000119952, gnorm=0.47, clip=0, loss_scale=8, train_wall=80, gb_free=16.4, wall=11682
2023-08-12 15:37:21 | INFO | train_inner | epoch 010:    744 / 1474 loss=2.239, trans_loss=3.404, nll_loss=1.588, w2v_ctc_loss=1.18, task_loss=0, contrastive_loss=0.159, total=4112.31, n_correct=2558.42, ppl=3.01, accuracy=62.214, wps=15396.3, ups=1.25, wpb=12277.4, bsz=448.4, num_updates=14000, lr=0.000119523, gnorm=0.452, clip=0, loss_scale=8, train_wall=79, gb_free=12.5, wall=11762
2023-08-12 15:37:21 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 15:37:43 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.271 | trans_loss 5.27 | nll_loss 2.563 | w2v_ctc_loss 1.458 | task_loss 0 | contrastive_loss 0.336 | total 4003.4 | n_correct 2588.3 | ppl 5.91 | accuracy 64.653 | uer 21.1 | wer 22.859 | raw_wer 22.859 | bleu 20.35 | wps 2235.4 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 20.35
2023-08-12 15:37:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-08-12 15:37:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_10_14000.pt
2023-08-12 15:37:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_10_14000.pt
2023-08-12 15:38:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 20.35) (writing took 48.03231820464134 seconds)
2023-08-12 15:39:53 | INFO | train_inner | epoch 010:    844 / 1474 loss=2.219, trans_loss=3.402, nll_loss=1.585, w2v_ctc_loss=1.152, task_loss=0, contrastive_loss=0.164, total=4135.95, n_correct=2578.96, ppl=3, accuracy=62.355, wps=8140.8, ups=0.66, wpb=12346.8, bsz=456.9, num_updates=14100, lr=0.000119098, gnorm=0.448, clip=0, loss_scale=8, train_wall=80, gb_free=15.8, wall=11914
2023-08-12 15:41:12 | INFO | train_inner | epoch 010:    944 / 1474 loss=2.246, trans_loss=3.399, nll_loss=1.581, w2v_ctc_loss=1.16, task_loss=0, contrastive_loss=0.206, total=4169.57, n_correct=2605.33, ppl=2.99, accuracy=62.484, wps=15624.1, ups=1.26, wpb=12439, bsz=473.3, num_updates=14200, lr=0.000118678, gnorm=0.481, clip=0, loss_scale=8, train_wall=79, gb_free=13.6, wall=11993
2023-08-12 15:41:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 15:42:34 | INFO | train_inner | epoch 010:   1045 / 1474 loss=2.225, trans_loss=3.402, nll_loss=1.586, w2v_ctc_loss=1.162, task_loss=0, contrastive_loss=0.172, total=4048.19, n_correct=2517.02, ppl=3, accuracy=62.176, wps=14863.5, ups=1.23, wpb=12087.4, bsz=428.7, num_updates=14300, lr=0.000118262, gnorm=0.464, clip=0, loss_scale=4, train_wall=81, gb_free=17, wall=12075
2023-08-12 15:43:53 | INFO | train_inner | epoch 010:   1145 / 1474 loss=2.228, trans_loss=3.409, nll_loss=1.594, w2v_ctc_loss=1.174, task_loss=0, contrastive_loss=0.155, total=4033.34, n_correct=2505.26, ppl=3.02, accuracy=62.114, wps=15195.4, ups=1.26, wpb=12041.8, bsz=418.7, num_updates=14400, lr=0.000117851, gnorm=0.465, clip=0, loss_scale=4, train_wall=79, gb_free=16.7, wall=12154
2023-08-12 15:45:13 | INFO | train_inner | epoch 010:   1245 / 1474 loss=2.229, trans_loss=3.394, nll_loss=1.581, w2v_ctc_loss=1.174, task_loss=0, contrastive_loss=0.156, total=4107.17, n_correct=2559.57, ppl=2.99, accuracy=62.32, wps=15298.1, ups=1.25, wpb=12283.9, bsz=445.4, num_updates=14500, lr=0.000117444, gnorm=0.538, clip=0, loss_scale=4, train_wall=80, gb_free=16.4, wall=12234
2023-08-12 15:46:34 | INFO | train_inner | epoch 010:   1345 / 1474 loss=2.228, trans_loss=3.401, nll_loss=1.586, w2v_ctc_loss=1.165, task_loss=0, contrastive_loss=0.168, total=4148.52, n_correct=2589.44, ppl=3, accuracy=62.418, wps=15363.8, ups=1.24, wpb=12389, bsz=458.7, num_updates=14600, lr=0.000117041, gnorm=0.456, clip=0, loss_scale=4, train_wall=80, gb_free=16.7, wall=12315
2023-08-12 15:47:54 | INFO | train_inner | epoch 010:   1445 / 1474 loss=2.326, trans_loss=3.409, nll_loss=1.592, w2v_ctc_loss=1.14, task_loss=0, contrastive_loss=0.419, total=4181.64, n_correct=2605.26, ppl=3.02, accuracy=62.302, wps=15442.8, ups=1.24, wpb=12471.6, bsz=479.6, num_updates=14700, lr=0.000116642, gnorm=0.498, clip=0, loss_scale=4, train_wall=80, gb_free=16, wall=12396
2023-08-12 15:48:17 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 15:48:41 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 4.274 | trans_loss 5.258 | nll_loss 2.549 | w2v_ctc_loss 1.499 | task_loss 0 | contrastive_loss 0.337 | total 4003.4 | n_correct 2592.6 | ppl 5.85 | accuracy 64.76 | uer 21.121 | wer 23.187 | raw_wer 23.187 | bleu 20.67 | wps 2096.1 | wpb 4003.4 | bsz 141.8 | num_updates 14729 | best_bleu 20.67
2023-08-12 15:48:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14729 updates
2023-08-12 15:48:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 15:48:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 15:49:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 10 @ 14729 updates, score 20.67) (writing took 29.69365666806698 seconds)
2023-08-12 15:49:11 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-08-12 15:49:11 | INFO | train | epoch 010 | loss 2.252 | trans_loss 3.402 | nll_loss 1.586 | w2v_ctc_loss 1.16 | task_loss 0 | contrastive_loss 0.233 | total 4138.34 | n_correct 2579.04 | ppl 3 | accuracy 62.321 | wps 13832.4 | ups 1.12 | wpb 12355 | bsz 458.5 | num_updates 14729 | lr 0.000116527 | gnorm 0.469 | clip 0 | loss_scale 4 | train_wall 1175 | gb_free 17.1 | wall 12473
2023-08-12 15:49:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 15:49:12 | INFO | fairseq.trainer | begin training epoch 11
2023-08-12 15:49:12 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 15:50:15 | INFO | train_inner | epoch 011:     71 / 1474 loss=2.215, trans_loss=3.378, nll_loss=1.555, w2v_ctc_loss=1.127, task_loss=0, contrastive_loss=0.239, total=4161.32, n_correct=2625.14, ppl=2.94, accuracy=63.084, wps=8840.7, ups=0.71, wpb=12424.2, bsz=475.2, num_updates=14800, lr=0.000116248, gnorm=0.453, clip=0, loss_scale=4, train_wall=79, gb_free=11.8, wall=12536
2023-08-12 15:51:36 | INFO | train_inner | epoch 011:    171 / 1474 loss=2.184, trans_loss=3.381, nll_loss=1.56, w2v_ctc_loss=1.128, task_loss=0, contrastive_loss=0.16, total=4105.22, n_correct=2586.04, ppl=2.95, accuracy=62.994, wps=15214.1, ups=1.24, wpb=12264.3, bsz=450.5, num_updates=14900, lr=0.000115857, gnorm=0.448, clip=0, loss_scale=4, train_wall=80, gb_free=16.3, wall=12617
2023-08-12 15:52:55 | INFO | train_inner | epoch 011:    271 / 1474 loss=2.166, trans_loss=3.377, nll_loss=1.554, w2v_ctc_loss=1.121, task_loss=0, contrastive_loss=0.145, total=4109.58, n_correct=2593.02, ppl=2.94, accuracy=63.097, wps=15401.5, ups=1.25, wpb=12274.9, bsz=441.4, num_updates=15000, lr=0.00011547, gnorm=0.456, clip=0, loss_scale=4, train_wall=79, gb_free=16.8, wall=12697
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-12 15:53:53 | INFO | train_inner | epoch 011:    371 / 1474 loss=2.2, trans_loss=5.019, nll_loss=2.313, w2v_ctc_loss=0.848, task_loss=0, contrastive_loss=0.119, total=4103.77, n_correct=2586.67, ppl=4.97, accuracy=63.032, wps=14366.3, ups=1.74, wpb=8253.5, bsz=299, num_updates=15100, lr=0.000115087, gnorm=0.582, clip=0, loss_scale=4, train_wall=57, gb_free=14.4, wall=12754
2023-08-12 15:54:50 | INFO | train_inner | epoch 011:    471 / 1474 loss=2.226, trans_loss=5.053, nll_loss=2.334, w2v_ctc_loss=0.841, task_loss=0, contrastive_loss=0.242, total=4117.47, n_correct=2586.31, ppl=5.04, accuracy=62.813, wps=14327.8, ups=1.74, wpb=8234.9, bsz=303.7, num_updates=15200, lr=0.000114708, gnorm=0.662, clip=0, loss_scale=4, train_wall=57, gb_free=16.4, wall=12812
2023-08-12 15:55:48 | INFO | train_inner | epoch 011:    571 / 1474 loss=2.227, trans_loss=5.053, nll_loss=2.335, w2v_ctc_loss=0.858, task_loss=0, contrastive_loss=0.234, total=4070.21, n_correct=2558.11, ppl=5.04, accuracy=62.85, wps=14007.5, ups=1.72, wpb=8140.4, bsz=292.9, num_updates=15300, lr=0.000114332, gnorm=0.616, clip=0, loss_scale=4, train_wall=58, gb_free=13.5, wall=12870
2023-08-12 15:56:46 | INFO | train_inner | epoch 011:    671 / 1474 loss=2.234, trans_loss=5.043, nll_loss=2.323, w2v_ctc_loss=0.848, task_loss=0, contrastive_loss=0.294, total=4161.7, n_correct=2619.43, ppl=5, accuracy=62.941, wps=14424.7, ups=1.73, wpb=8323.4, bsz=311.8, num_updates=15400, lr=0.000113961, gnorm=0.593, clip=0, loss_scale=4, train_wall=57, gb_free=17.2, wall=12927
2023-08-12 15:57:44 | INFO | train_inner | epoch 011:    771 / 1474 loss=2.213, trans_loss=5.058, nll_loss=2.341, w2v_ctc_loss=0.865, task_loss=0, contrastive_loss=0.117, total=4151.24, n_correct=2608.62, ppl=5.07, accuracy=62.84, wps=14342.2, ups=1.73, wpb=8302.5, bsz=301, num_updates=15500, lr=0.000113592, gnorm=0.587, clip=0, loss_scale=4, train_wall=57, gb_free=16, wall=12985
2023-08-12 15:58:41 | INFO | train_inner | epoch 011:    871 / 1474 loss=2.204, trans_loss=5.052, nll_loss=2.333, w2v_ctc_loss=0.852, task_loss=0, contrastive_loss=0.107, total=4128.97, n_correct=2595.68, ppl=5.04, accuracy=62.865, wps=14445.7, ups=1.75, wpb=8257.9, bsz=294.6, num_updates=15600, lr=0.000113228, gnorm=0.577, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=13042
2023-08-12 15:59:39 | INFO | train_inner | epoch 011:    971 / 1474 loss=2.207, trans_loss=5.051, nll_loss=2.332, w2v_ctc_loss=0.857, task_loss=0, contrastive_loss=0.12, total=4144.6, n_correct=2608.1, ppl=5.04, accuracy=62.928, wps=14225, ups=1.72, wpb=8289.2, bsz=304.5, num_updates=15700, lr=0.000112867, gnorm=0.595, clip=0, loss_scale=4, train_wall=58, gb_free=17.9, wall=13101
2023-08-12 16:00:37 | INFO | train_inner | epoch 011:   1071 / 1474 loss=2.203, trans_loss=5.042, nll_loss=2.322, w2v_ctc_loss=0.85, task_loss=0, contrastive_loss=0.134, total=4153.33, n_correct=2627.88, ppl=5, accuracy=63.272, wps=14476.5, ups=1.74, wpb=8306.7, bsz=310.5, num_updates=15800, lr=0.000112509, gnorm=0.584, clip=0, loss_scale=4, train_wall=57, gb_free=15.9, wall=13158
2023-08-12 16:01:35 | INFO | train_inner | epoch 011:   1171 / 1474 loss=2.212, trans_loss=5.054, nll_loss=2.337, w2v_ctc_loss=0.861, task_loss=0, contrastive_loss=0.132, total=4177.49, n_correct=2625.6, ppl=5.05, accuracy=62.851, wps=14467.3, ups=1.73, wpb=8355, bsz=311.7, num_updates=15900, lr=0.000112154, gnorm=0.588, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=13216
2023-08-12 16:02:32 | INFO | train_inner | epoch 011:   1271 / 1474 loss=2.225, trans_loss=5.049, nll_loss=2.331, w2v_ctc_loss=0.869, task_loss=0, contrastive_loss=0.182, total=4150.45, n_correct=2610.43, ppl=5.03, accuracy=62.895, wps=14476.9, ups=1.74, wpb=8300.9, bsz=307.4, num_updates=16000, lr=0.000111803, gnorm=0.599, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=13273
2023-08-12 16:02:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
2023-08-12 16:02:55 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.271 | trans_loss 5.249 | nll_loss 2.539 | w2v_ctc_loss 1.533 | task_loss 0 | contrastive_loss 0.318 | total 4003.4 | n_correct 2605.1 | ppl 5.81 | accuracy 65.072 | uer 21.111 | wer 23.206 | raw_wer 23.206 | bleu 20.58 | wps 2217.1 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 20.67
2023-08-12 16:02:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-08-12 16:02:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_11_16000.pt
2023-08-12 16:02:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_11_16000.pt
2023-08-12 16:03:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 20.58) (writing took 44.081793673336506 seconds)
2023-08-12 16:04:39 | INFO | train_inner | epoch 011:   1371 / 1474 loss=2.244, trans_loss=5.047, nll_loss=2.329, w2v_ctc_loss=0.845, task_loss=0, contrastive_loss=0.353, total=4191.95, n_correct=2639.89, ppl=5.03, accuracy=62.975, wps=6608.7, ups=0.79, wpb=8383.9, bsz=326.8, num_updates=16100, lr=0.000111456, gnorm=0.598, clip=0, loss_scale=4, train_wall=58, gb_free=15.8, wall=13400
2023-08-12 16:05:36 | INFO | train_inner | epoch 011:   1471 / 1474 loss=2.203, trans_loss=5.045, nll_loss=2.326, w2v_ctc_loss=0.853, task_loss=0, contrastive_loss=0.128, total=4172.29, n_correct=2636.38, ppl=5.01, accuracy=63.188, wps=14497.1, ups=1.74, wpb=8344.6, bsz=315.6, num_updates=16200, lr=0.000111111, gnorm=0.597, clip=0, loss_scale=4, train_wall=57, gb_free=16.4, wall=13458
2023-08-12 16:05:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 16:06:01 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 4.248 | trans_loss 5.246 | nll_loss 2.533 | w2v_ctc_loss 1.46 | task_loss 0 | contrastive_loss 0.325 | total 4003.4 | n_correct 2602.8 | ppl 5.79 | accuracy 65.015 | uer 20.059 | wer 21.949 | raw_wer 21.949 | bleu 20.74 | wps 2322.8 | wpb 4003.4 | bsz 141.8 | num_updates 16203 | best_bleu 20.74
2023-08-12 16:06:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16203 updates
2023-08-12 16:06:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 16:06:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 16:06:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 11 @ 16203 updates, score 20.74) (writing took 27.96181927807629 seconds)
2023-08-12 16:06:29 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-08-12 16:06:29 | INFO | train | epoch 011 | loss 2.207 | trans_loss 4.628 | nll_loss 2.135 | w2v_ctc_loss 0.922 | task_loss 0 | contrastive_loss 0.174 | total 4138.65 | n_correct 2606.43 | ppl 4.39 | accuracy 62.978 | wps 12823.4 | ups 1.42 | wpb 9029.1 | bsz 333.7 | num_updates 16203 | lr 0.000111101 | gnorm 0.571 | clip 0 | loss_scale 4 | train_wall 904 | gb_free 17.3 | wall 13511
2023-08-12 16:06:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 16:06:29 | INFO | fairseq.trainer | begin training epoch 12
2023-08-12 16:06:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 16:07:33 | INFO | train_inner | epoch 012:     97 / 1474 loss=2.183, trans_loss=5.005, nll_loss=2.273, w2v_ctc_loss=0.83, task_loss=0, contrastive_loss=0.155, total=4138.25, n_correct=2641.95, ppl=4.83, accuracy=63.842, wps=7114.9, ups=0.86, wpb=8276.5, bsz=312.6, num_updates=16300, lr=0.00011077, gnorm=0.579, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=13574
2023-08-12 16:08:30 | INFO | train_inner | epoch 012:    197 / 1474 loss=2.182, trans_loss=5.014, nll_loss=2.284, w2v_ctc_loss=0.84, task_loss=0, contrastive_loss=0.11, total=4126.75, n_correct=2627.33, ppl=4.87, accuracy=63.666, wps=14332, ups=1.74, wpb=8253.5, bsz=296.6, num_updates=16400, lr=0.000110432, gnorm=0.583, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=13632
2023-08-12 16:09:28 | INFO | train_inner | epoch 012:    297 / 1474 loss=2.188, trans_loss=5.016, nll_loss=2.287, w2v_ctc_loss=0.829, task_loss=0, contrastive_loss=0.143, total=4221.98, n_correct=2685.99, ppl=4.88, accuracy=63.619, wps=14523, ups=1.72, wpb=8444, bsz=326, num_updates=16500, lr=0.000110096, gnorm=0.596, clip=0, loss_scale=8, train_wall=58, gb_free=16.5, wall=13690
2023-08-12 16:10:26 | INFO | train_inner | epoch 012:    397 / 1474 loss=2.184, trans_loss=5.019, nll_loss=2.291, w2v_ctc_loss=0.839, task_loss=0, contrastive_loss=0.115, total=4136.43, n_correct=2631.54, ppl=4.89, accuracy=63.619, wps=14287.6, ups=1.73, wpb=8272.9, bsz=301.9, num_updates=16600, lr=0.000109764, gnorm=0.588, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=13748
2023-08-12 16:11:24 | INFO | train_inner | epoch 012:    497 / 1474 loss=2.193, trans_loss=5.029, nll_loss=2.305, w2v_ctc_loss=0.847, task_loss=0, contrastive_loss=0.127, total=4082.65, n_correct=2590.54, ppl=4.94, accuracy=63.452, wps=14237.5, ups=1.74, wpb=8165.3, bsz=297.7, num_updates=16700, lr=0.000109435, gnorm=0.574, clip=0, loss_scale=8, train_wall=57, gb_free=17.5, wall=13805
2023-08-12 16:12:22 | INFO | train_inner | epoch 012:    597 / 1474 loss=2.2, trans_loss=5.018, nll_loss=2.29, w2v_ctc_loss=0.84, task_loss=0, contrastive_loss=0.195, total=4217.4, n_correct=2681.17, ppl=4.89, accuracy=63.574, wps=14446, ups=1.71, wpb=8434.8, bsz=320.8, num_updates=16800, lr=0.000109109, gnorm=0.574, clip=0, loss_scale=8, train_wall=58, gb_free=16, wall=13863
2023-08-12 16:13:20 | INFO | train_inner | epoch 012:    697 / 1474 loss=2.203, trans_loss=5.013, nll_loss=2.284, w2v_ctc_loss=0.825, task_loss=0, contrastive_loss=0.279, total=4197.24, n_correct=2675.26, ppl=4.87, accuracy=63.739, wps=14531.7, ups=1.73, wpb=8394.5, bsz=323.6, num_updates=16900, lr=0.000108786, gnorm=0.595, clip=0, loss_scale=8, train_wall=57, gb_free=15.4, wall=13921
2023-08-12 16:14:17 | INFO | train_inner | epoch 012:    797 / 1474 loss=2.177, trans_loss=5.01, nll_loss=2.279, w2v_ctc_loss=0.834, task_loss=0, contrastive_loss=0.116, total=4083.9, n_correct=2602.59, ppl=4.85, accuracy=63.728, wps=14234.8, ups=1.74, wpb=8167.8, bsz=296.3, num_updates=17000, lr=0.000108465, gnorm=0.573, clip=0, loss_scale=8, train_wall=57, gb_free=13.1, wall=13978
2023-08-12 16:15:15 | INFO | train_inner | epoch 012:    897 / 1474 loss=2.192, trans_loss=5.015, nll_loss=2.287, w2v_ctc_loss=0.836, task_loss=0, contrastive_loss=0.17, total=4168.41, n_correct=2656.15, ppl=4.88, accuracy=63.721, wps=14471.8, ups=1.74, wpb=8336.8, bsz=306.2, num_updates=17100, lr=0.000108148, gnorm=0.571, clip=0, loss_scale=8, train_wall=57, gb_free=15.6, wall=14036
2023-08-12 16:16:13 | INFO | train_inner | epoch 012:    997 / 1474 loss=2.195, trans_loss=5.021, nll_loss=2.295, w2v_ctc_loss=0.843, task_loss=0, contrastive_loss=0.179, total=4121.91, n_correct=2616.59, ppl=4.91, accuracy=63.48, wps=14227.4, ups=1.73, wpb=8243.8, bsz=301.5, num_updates=17200, lr=0.000107833, gnorm=0.59, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=14094
2023-08-12 16:16:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 16:17:11 | INFO | train_inner | epoch 012:   1098 / 1474 loss=2.182, trans_loss=5.024, nll_loss=2.299, w2v_ctc_loss=0.845, task_loss=0, contrastive_loss=0.1, total=4033.63, n_correct=2564.7, ppl=4.92, accuracy=63.583, wps=13855.1, ups=1.72, wpb=8067.3, bsz=281.9, num_updates=17300, lr=0.000107521, gnorm=0.588, clip=0, loss_scale=4, train_wall=58, gb_free=16, wall=14152
2023-08-12 16:18:09 | INFO | train_inner | epoch 012:   1198 / 1474 loss=2.219, trans_loss=5.042, nll_loss=2.322, w2v_ctc_loss=0.865, task_loss=0, contrastive_loss=0.193, total=4184.7, n_correct=2642.26, ppl=5, accuracy=63.141, wps=14515.9, ups=1.73, wpb=8369.4, bsz=317.6, num_updates=17400, lr=0.000107211, gnorm=0.596, clip=0, loss_scale=4, train_wall=57, gb_free=13.8, wall=14210
2023-08-12 16:19:06 | INFO | train_inner | epoch 012:   1298 / 1474 loss=2.185, trans_loss=5.023, nll_loss=2.297, w2v_ctc_loss=0.849, task_loss=0, contrastive_loss=0.103, total=4070.11, n_correct=2583.78, ppl=4.92, accuracy=63.482, wps=14210.8, ups=1.75, wpb=8140.2, bsz=285.4, num_updates=17500, lr=0.000106904, gnorm=0.585, clip=0, loss_scale=4, train_wall=57, gb_free=14.6, wall=14267
2023-08-12 16:20:04 | INFO | train_inner | epoch 012:   1398 / 1474 loss=2.205, trans_loss=5.032, nll_loss=2.309, w2v_ctc_loss=0.843, task_loss=0, contrastive_loss=0.211, total=4135.31, n_correct=2617.88, ppl=4.95, accuracy=63.306, wps=14268.6, ups=1.73, wpb=8270.6, bsz=305.4, num_updates=17600, lr=0.0001066, gnorm=0.653, clip=0, loss_scale=4, train_wall=58, gb_free=16.2, wall=14325
2023-08-12 16:20:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 16:21:11 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 4.24 | trans_loss 5.227 | nll_loss 2.51 | w2v_ctc_loss 1.487 | task_loss 0 | contrastive_loss 0.319 | total 4003.4 | n_correct 2613.2 | ppl 5.7 | accuracy 65.275 | uer 20.259 | wer 22.139 | raw_wer 22.139 | bleu 21.05 | wps 2148.5 | wpb 4003.4 | bsz 141.8 | num_updates 17676 | best_bleu 21.05
2023-08-12 16:21:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17676 updates
2023-08-12 16:21:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 16:21:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 16:21:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 12 @ 17676 updates, score 21.05) (writing took 29.828041737899184 seconds)
2023-08-12 16:21:42 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-08-12 16:21:42 | INFO | train | epoch 012 | loss 2.192 | trans_loss 5.02 | nll_loss 2.293 | w2v_ctc_loss 0.84 | task_loss 0 | contrastive_loss 0.156 | total 4136.49 | n_correct 2629.44 | ppl 4.9 | accuracy 63.567 | wps 13355.1 | ups 1.61 | wpb 8273 | bsz 305 | num_updates 17676 | lr 0.000106371 | gnorm 0.588 | clip 0 | loss_scale 4 | train_wall 844 | gb_free 13.1 | wall 14423
2023-08-12 16:21:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 16:21:42 | INFO | fairseq.trainer | begin training epoch 13
2023-08-12 16:21:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 16:22:03 | INFO | train_inner | epoch 013:     24 / 1474 loss=2.184, trans_loss=5.023, nll_loss=2.297, w2v_ctc_loss=0.845, task_loss=0, contrastive_loss=0.112, total=4097.19, n_correct=2603.29, ppl=4.91, accuracy=63.538, wps=6906.8, ups=0.84, wpb=8194.4, bsz=297.7, num_updates=17700, lr=0.000106299, gnorm=0.583, clip=0, loss_scale=4, train_wall=57, gb_free=16.2, wall=14444
2023-08-12 16:23:00 | INFO | train_inner | epoch 013:    124 / 1474 loss=2.167, trans_loss=4.995, nll_loss=2.261, w2v_ctc_loss=0.821, task_loss=0, contrastive_loss=0.124, total=4172.67, n_correct=2674.77, ppl=4.79, accuracy=64.102, wps=14509.6, ups=1.74, wpb=8345.3, bsz=303.7, num_updates=17800, lr=0.000106, gnorm=0.581, clip=0, loss_scale=4, train_wall=57, gb_free=16.1, wall=14501
2023-08-12 16:23:58 | INFO | train_inner | epoch 013:    224 / 1474 loss=2.211, trans_loss=5.004, nll_loss=2.273, w2v_ctc_loss=0.827, task_loss=0, contrastive_loss=0.338, total=4191.23, n_correct=2679.4, ppl=4.83, accuracy=63.929, wps=14483.7, ups=1.73, wpb=8382.5, bsz=326.3, num_updates=17900, lr=0.000105703, gnorm=0.574, clip=0, loss_scale=4, train_wall=57, gb_free=16.9, wall=14559
2023-08-12 16:24:56 | INFO | train_inner | epoch 013:    324 / 1474 loss=2.155, trans_loss=4.987, nll_loss=2.25, w2v_ctc_loss=0.813, task_loss=0, contrastive_loss=0.105, total=4102.25, n_correct=2639.36, ppl=4.76, accuracy=64.339, wps=14201.2, ups=1.73, wpb=8204.5, bsz=292.6, num_updates=18000, lr=0.000105409, gnorm=0.568, clip=0, loss_scale=4, train_wall=57, gb_free=16.4, wall=14617
2023-08-12 16:24:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 16:25:18 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.247 | trans_loss 5.237 | nll_loss 2.522 | w2v_ctc_loss 1.477 | task_loss 0 | contrastive_loss 0.327 | total 4003.4 | n_correct 2614.6 | ppl 5.74 | accuracy 65.309 | uer 20.71 | wer 22.628 | raw_wer 22.628 | bleu 21.08 | wps 2321.5 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 21.08
2023-08-12 16:25:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-08-12 16:25:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_13_18000.pt
2023-08-12 16:25:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_13_18000.pt
2023-08-12 16:26:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 21.08) (writing took 45.49180428870022 seconds)
2023-08-12 16:27:02 | INFO | train_inner | epoch 013:    424 / 1474 loss=2.173, trans_loss=4.991, nll_loss=2.256, w2v_ctc_loss=0.825, task_loss=0, contrastive_loss=0.157, total=4205.83, n_correct=2704.6, ppl=4.78, accuracy=64.306, wps=6667.1, ups=0.79, wpb=8411.7, bsz=324.4, num_updates=18100, lr=0.000105118, gnorm=0.573, clip=0, loss_scale=4, train_wall=57, gb_free=16.6, wall=14743
2023-08-12 16:27:59 | INFO | train_inner | epoch 013:    524 / 1474 loss=2.179, trans_loss=5, nll_loss=2.267, w2v_ctc_loss=0.822, task_loss=0, contrastive_loss=0.188, total=4186.08, n_correct=2677.67, ppl=4.81, accuracy=63.966, wps=14563.7, ups=1.74, wpb=8372.2, bsz=317, num_updates=18200, lr=0.000104828, gnorm=0.595, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=14801
2023-08-12 16:28:57 | INFO | train_inner | epoch 013:    624 / 1474 loss=2.164, trans_loss=4.996, nll_loss=2.263, w2v_ctc_loss=0.829, task_loss=0, contrastive_loss=0.103, total=4152.83, n_correct=2665.55, ppl=4.8, accuracy=64.186, wps=14494, ups=1.75, wpb=8305.7, bsz=304.5, num_updates=18300, lr=0.000104542, gnorm=0.571, clip=0, loss_scale=4, train_wall=57, gb_free=16.1, wall=14858
2023-08-12 16:29:54 | INFO | train_inner | epoch 013:    724 / 1474 loss=2.169, trans_loss=4.999, nll_loss=2.265, w2v_ctc_loss=0.837, task_loss=0, contrastive_loss=0.106, total=4109.56, n_correct=2628.92, ppl=4.81, accuracy=63.971, wps=14272.7, ups=1.74, wpb=8219.1, bsz=289.8, num_updates=18400, lr=0.000104257, gnorm=0.587, clip=0, loss_scale=4, train_wall=57, gb_free=15, wall=14916
2023-08-12 16:30:52 | INFO | train_inner | epoch 013:    824 / 1474 loss=2.176, trans_loss=5.001, nll_loss=2.269, w2v_ctc_loss=0.826, task_loss=0, contrastive_loss=0.148, total=4114.63, n_correct=2630.26, ppl=4.82, accuracy=63.925, wps=14152, ups=1.72, wpb=8229.3, bsz=302.5, num_updates=18500, lr=0.000103975, gnorm=0.629, clip=0, loss_scale=4, train_wall=58, gb_free=17, wall=14974
2023-08-12 16:31:50 | INFO | train_inner | epoch 013:    924 / 1474 loss=2.166, trans_loss=4.999, nll_loss=2.266, w2v_ctc_loss=0.825, task_loss=0, contrastive_loss=0.113, total=4102.25, n_correct=2631.18, ppl=4.81, accuracy=64.14, wps=14367.6, ups=1.75, wpb=8204.5, bsz=297.5, num_updates=18600, lr=0.000103695, gnorm=0.598, clip=0, loss_scale=4, train_wall=57, gb_free=15.9, wall=15031
2023-08-12 16:32:47 | INFO | train_inner | epoch 013:   1024 / 1474 loss=2.186, trans_loss=5.005, nll_loss=2.275, w2v_ctc_loss=0.839, task_loss=0, contrastive_loss=0.167, total=4097.02, n_correct=2613.77, ppl=4.84, accuracy=63.797, wps=14309.9, ups=1.75, wpb=8194, bsz=297.1, num_updates=18700, lr=0.000103418, gnorm=0.616, clip=0, loss_scale=4, train_wall=57, gb_free=16.8, wall=15088
2023-08-12 16:33:44 | INFO | train_inner | epoch 013:   1124 / 1474 loss=2.167, trans_loss=4.99, nll_loss=2.255, w2v_ctc_loss=0.822, task_loss=0, contrastive_loss=0.146, total=4082.89, n_correct=2625.01, ppl=4.77, accuracy=64.293, wps=14170.8, ups=1.74, wpb=8165.8, bsz=300.3, num_updates=18800, lr=0.000103142, gnorm=0.592, clip=0, loss_scale=4, train_wall=57, gb_free=15.6, wall=15146
2023-08-12 16:34:42 | INFO | train_inner | epoch 013:   1224 / 1474 loss=2.17, trans_loss=5.002, nll_loss=2.271, w2v_ctc_loss=0.833, task_loss=0, contrastive_loss=0.107, total=4122.93, n_correct=2639.16, ppl=4.83, accuracy=64.012, wps=14342.3, ups=1.74, wpb=8245.9, bsz=296.7, num_updates=18900, lr=0.000102869, gnorm=0.602, clip=0, loss_scale=4, train_wall=57, gb_free=16.5, wall=15203
2023-08-12 16:35:40 | INFO | train_inner | epoch 013:   1324 / 1474 loss=2.174, trans_loss=4.988, nll_loss=2.253, w2v_ctc_loss=0.817, task_loss=0, contrastive_loss=0.205, total=4113.08, n_correct=2649.86, ppl=4.77, accuracy=64.425, wps=14281.1, ups=1.74, wpb=8226.2, bsz=309.2, num_updates=19000, lr=0.000102598, gnorm=0.586, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=15261
2023-08-12 16:36:37 | INFO | train_inner | epoch 013:   1424 / 1474 loss=2.182, trans_loss=4.997, nll_loss=2.265, w2v_ctc_loss=0.818, task_loss=0, contrastive_loss=0.215, total=4174.18, n_correct=2673.79, ppl=4.81, accuracy=64.055, wps=14503.2, ups=1.74, wpb=8348.4, bsz=310.4, num_updates=19100, lr=0.000102329, gnorm=0.569, clip=0, loss_scale=4, train_wall=57, gb_free=16.6, wall=15318
2023-08-12 16:37:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 16:37:27 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 4.226 | trans_loss 5.223 | nll_loss 2.501 | w2v_ctc_loss 1.469 | task_loss 0 | contrastive_loss 0.31 | total 4003.4 | n_correct 2619.5 | ppl 5.66 | accuracy 65.432 | uer 20.359 | wer 22.247 | raw_wer 22.247 | bleu 20.91 | wps 2494.4 | wpb 4003.4 | bsz 141.8 | num_updates 19150 | best_bleu 21.08
2023-08-12 16:37:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19150 updates
2023-08-12 16:37:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_20.9100.pt
2023-08-12 16:37:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_20.9100.pt
2023-08-12 16:37:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_20.9100.pt (epoch 13 @ 19150 updates, score 20.91) (writing took 16.11321426741779 seconds)
2023-08-12 16:37:46 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-08-12 16:37:46 | INFO | train | epoch 013 | loss 2.174 | trans_loss 4.996 | nll_loss 2.263 | w2v_ctc_loss 0.826 | task_loss 0 | contrastive_loss 0.159 | total 4138.65 | n_correct 2653.49 | ppl 4.8 | accuracy 64.115 | wps 12647.1 | ups 1.53 | wpb 8277.3 | bsz 305.7 | num_updates 19150 | lr 0.000102195 | gnorm 0.588 | clip 0 | loss_scale 4 | train_wall 841 | gb_free 17.7 | wall 15388
2023-08-12 16:37:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 16:37:47 | INFO | fairseq.trainer | begin training epoch 14
2023-08-12 16:37:47 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 16:38:24 | INFO | train_inner | epoch 014:     50 / 1474 loss=2.154, trans_loss=4.97, nll_loss=2.23, w2v_ctc_loss=0.819, task_loss=0, contrastive_loss=0.118, total=4177.38, n_correct=2702.73, ppl=4.69, accuracy=64.699, wps=7818.4, ups=0.94, wpb=8354.8, bsz=321.6, num_updates=19200, lr=0.000102062, gnorm=0.575, clip=0, loss_scale=4, train_wall=57, gb_free=17.2, wall=15425
2023-08-12 16:39:21 | INFO | train_inner | epoch 014:    150 / 1474 loss=2.14, trans_loss=4.959, nll_loss=2.214, w2v_ctc_loss=0.81, task_loss=0, contrastive_loss=0.099, total=4098.74, n_correct=2659.08, ppl=4.64, accuracy=64.876, wps=14399.8, ups=1.76, wpb=8197.5, bsz=303.4, num_updates=19300, lr=0.000101797, gnorm=0.569, clip=0, loss_scale=8, train_wall=56, gb_free=16.7, wall=15482
2023-08-12 16:40:18 | INFO | train_inner | epoch 014:    250 / 1474 loss=2.165, trans_loss=4.978, nll_loss=2.239, w2v_ctc_loss=0.818, task_loss=0, contrastive_loss=0.201, total=4099.27, n_correct=2643.23, ppl=4.72, accuracy=64.481, wps=14317.1, ups=1.75, wpb=8198.5, bsz=294.5, num_updates=19400, lr=0.000101535, gnorm=0.618, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=15539
2023-08-12 16:41:16 | INFO | train_inner | epoch 014:    350 / 1474 loss=2.153, trans_loss=4.976, nll_loss=2.237, w2v_ctc_loss=0.808, task_loss=0, contrastive_loss=0.133, total=4161.31, n_correct=2691.97, ppl=4.71, accuracy=64.69, wps=14358.3, ups=1.73, wpb=8322.6, bsz=316.7, num_updates=19500, lr=0.000101274, gnorm=0.58, clip=0, loss_scale=8, train_wall=58, gb_free=16.8, wall=15597
2023-08-12 16:42:14 | INFO | train_inner | epoch 014:    450 / 1474 loss=2.147, trans_loss=4.975, nll_loss=2.235, w2v_ctc_loss=0.805, task_loss=0, contrastive_loss=0.115, total=4153.74, n_correct=2685.49, ppl=4.71, accuracy=64.652, wps=14456, ups=1.74, wpb=8307.5, bsz=305.8, num_updates=19600, lr=0.000101015, gnorm=0.562, clip=0, loss_scale=8, train_wall=57, gb_free=15.8, wall=15655
2023-08-12 16:43:11 | INFO | train_inner | epoch 014:    550 / 1474 loss=2.153, trans_loss=4.975, nll_loss=2.235, w2v_ctc_loss=0.82, task_loss=0, contrastive_loss=0.111, total=4064.6, n_correct=2617.23, ppl=4.71, accuracy=64.391, wps=14152.6, ups=1.74, wpb=8129.2, bsz=288.5, num_updates=19700, lr=0.000100759, gnorm=0.569, clip=0, loss_scale=8, train_wall=57, gb_free=16, wall=15712
2023-08-12 16:44:08 | INFO | train_inner | epoch 014:    650 / 1474 loss=2.163, trans_loss=4.979, nll_loss=2.24, w2v_ctc_loss=0.812, task_loss=0, contrastive_loss=0.176, total=4170.44, n_correct=2691.94, ppl=4.72, accuracy=64.548, wps=14590.7, ups=1.75, wpb=8340.9, bsz=308.9, num_updates=19800, lr=0.000100504, gnorm=0.576, clip=0, loss_scale=8, train_wall=57, gb_free=14.8, wall=15770
2023-08-12 16:45:06 | INFO | train_inner | epoch 014:    750 / 1474 loss=2.146, trans_loss=4.966, nll_loss=2.225, w2v_ctc_loss=0.809, task_loss=0, contrastive_loss=0.108, total=4131.24, n_correct=2671.01, ppl=4.67, accuracy=64.654, wps=14289.7, ups=1.73, wpb=8262.5, bsz=306.7, num_updates=19900, lr=0.000100251, gnorm=0.585, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=15827
2023-08-12 16:46:04 | INFO | train_inner | epoch 014:    850 / 1474 loss=2.168, trans_loss=4.966, nll_loss=2.225, w2v_ctc_loss=0.808, task_loss=0, contrastive_loss=0.221, total=4186.83, n_correct=2709.82, ppl=4.67, accuracy=64.722, wps=14483.7, ups=1.73, wpb=8373.7, bsz=321.3, num_updates=20000, lr=0.0001, gnorm=0.591, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=15885
2023-08-12 16:46:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 16:46:28 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.216 | trans_loss 5.212 | nll_loss 2.488 | w2v_ctc_loss 1.454 | task_loss 0 | contrastive_loss 0.314 | total 4003.4 | n_correct 2631.2 | ppl 5.61 | accuracy 65.724 | uer 19.359 | wer 21.297 | raw_wer 21.297 | bleu 21.1 | wps 2103.4 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 21.1
2023-08-12 16:46:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-08-12 16:46:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_14_20000.pt
2023-08-12 16:46:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_14_20000.pt
2023-08-12 16:47:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 21.1) (writing took 50.10416246578097 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-12 16:48:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 16:48:17 | INFO | train_inner | epoch 014:    951 / 1474 loss=2.148, trans_loss=4.975, nll_loss=2.236, w2v_ctc_loss=0.809, task_loss=0, contrastive_loss=0.105, total=4150.37, n_correct=2677.15, ppl=4.71, accuracy=64.504, wps=6248.2, ups=0.75, wpb=8300.7, bsz=304.2, num_updates=20100, lr=9.97509e-05, gnorm=0.58, clip=0, loss_scale=4, train_wall=57, gb_free=17.1, wall=16018
2023-08-12 16:49:14 | INFO | train_inner | epoch 014:   1051 / 1474 loss=2.147, trans_loss=4.971, nll_loss=2.231, w2v_ctc_loss=0.803, task_loss=0, contrastive_loss=0.129, total=4148.16, n_correct=2682.35, ppl=4.7, accuracy=64.664, wps=14364.4, ups=1.73, wpb=8296.3, bsz=300.7, num_updates=20200, lr=9.95037e-05, gnorm=0.574, clip=0, loss_scale=4, train_wall=57, gb_free=15.8, wall=16076
2023-08-12 16:50:13 | INFO | train_inner | epoch 014:   1151 / 1474 loss=2.205, trans_loss=4.978, nll_loss=2.24, w2v_ctc_loss=0.815, task_loss=0, contrastive_loss=0.408, total=4223.98, n_correct=2719.89, ppl=4.73, accuracy=64.392, wps=14485.8, ups=1.71, wpb=8448, bsz=326.4, num_updates=20300, lr=9.92583e-05, gnorm=0.583, clip=0, loss_scale=4, train_wall=58, gb_free=17.1, wall=16134
2023-08-12 16:51:10 | INFO | train_inner | epoch 014:   1251 / 1474 loss=2.151, trans_loss=4.983, nll_loss=2.246, w2v_ctc_loss=0.825, task_loss=0, contrastive_loss=0.09, total=4028.78, n_correct=2593.28, ppl=4.74, accuracy=64.369, wps=13970.4, ups=1.73, wpb=8057.6, bsz=274.5, num_updates=20400, lr=9.90148e-05, gnorm=0.569, clip=0, loss_scale=4, train_wall=57, gb_free=15.9, wall=16192
2023-08-12 16:52:08 | INFO | train_inner | epoch 014:   1351 / 1474 loss=2.144, trans_loss=4.974, nll_loss=2.235, w2v_ctc_loss=0.804, task_loss=0, contrastive_loss=0.104, total=4193.2, n_correct=2713.08, ppl=4.71, accuracy=64.702, wps=14577.5, ups=1.74, wpb=8386.4, bsz=315.4, num_updates=20500, lr=9.8773e-05, gnorm=0.554, clip=0, loss_scale=4, train_wall=57, gb_free=17.1, wall=16249
2023-08-12 16:53:05 | INFO | train_inner | epoch 014:   1451 / 1474 loss=2.156, trans_loss=4.981, nll_loss=2.245, w2v_ctc_loss=0.808, task_loss=0, contrastive_loss=0.143, total=4132.58, n_correct=2665.75, ppl=4.74, accuracy=64.506, wps=14414.8, ups=1.74, wpb=8265.2, bsz=304.1, num_updates=20600, lr=9.85329e-05, gnorm=0.587, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=16307
2023-08-12 16:53:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
2023-08-12 16:53:41 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 4.229 | trans_loss 5.207 | nll_loss 2.483 | w2v_ctc_loss 1.515 | task_loss 0 | contrastive_loss 0.313 | total 4003.4 | n_correct 2632.2 | ppl 5.59 | accuracy 65.749 | uer 20.325 | wer 22.412 | raw_wer 22.412 | bleu 21.4 | wps 2313.2 | wpb 4003.4 | bsz 141.8 | num_updates 20623 | best_bleu 21.4
2023-08-12 16:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20623 updates
2023-08-12 16:53:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 16:53:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 16:54:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 14 @ 20623 updates, score 21.4) (writing took 28.27838877774775 seconds)
2023-08-12 16:54:10 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-08-12 16:54:10 | INFO | train | epoch 014 | loss 2.156 | trans_loss 4.974 | nll_loss 2.235 | w2v_ctc_loss 0.812 | task_loss 0 | contrastive_loss 0.153 | total 4137.74 | n_correct 2672.2 | ppl 4.71 | accuracy 64.581 | wps 12392.9 | ups 1.5 | wpb 8275.5 | bsz 305.3 | num_updates 20623 | lr 9.8478e-05 | gnorm 0.579 | clip 0 | loss_scale 4 | train_wall 842 | gb_free 16.4 | wall 16371
2023-08-12 16:54:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 16:54:10 | INFO | fairseq.trainer | begin training epoch 15
2023-08-12 16:54:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 16:55:02 | INFO | train_inner | epoch 015:     77 / 1474 loss=2.153, trans_loss=4.962, nll_loss=2.218, w2v_ctc_loss=0.804, task_loss=0, contrastive_loss=0.194, total=4093.65, n_correct=2655.9, ppl=4.65, accuracy=64.879, wps=7028.3, ups=0.86, wpb=8187.3, bsz=302.1, num_updates=20700, lr=9.82946e-05, gnorm=0.577, clip=0, loss_scale=4, train_wall=57, gb_free=16.4, wall=16423
2023-08-12 16:56:00 | INFO | train_inner | epoch 015:    177 / 1474 loss=2.133, trans_loss=4.953, nll_loss=2.206, w2v_ctc_loss=0.802, task_loss=0, contrastive_loss=0.101, total=4112.43, n_correct=2678.64, ppl=4.61, accuracy=65.135, wps=14266.1, ups=1.73, wpb=8224.9, bsz=298, num_updates=20800, lr=9.80581e-05, gnorm=0.566, clip=0, loss_scale=4, train_wall=57, gb_free=16, wall=16481
2023-08-12 16:56:57 | INFO | train_inner | epoch 015:    277 / 1474 loss=2.133, trans_loss=4.956, nll_loss=2.211, w2v_ctc_loss=0.802, task_loss=0, contrastive_loss=0.096, total=4185.73, n_correct=2724.16, ppl=4.63, accuracy=65.082, wps=14573.8, ups=1.74, wpb=8371.5, bsz=311, num_updates=20900, lr=9.78232e-05, gnorm=0.709, clip=1, loss_scale=4, train_wall=57, gb_free=13.5, wall=16538
2023-08-12 16:57:54 | INFO | train_inner | epoch 015:    377 / 1474 loss=2.134, trans_loss=4.948, nll_loss=2.2, w2v_ctc_loss=0.796, task_loss=0, contrastive_loss=0.118, total=4173.44, n_correct=2717.07, ppl=4.6, accuracy=65.104, wps=14538.8, ups=1.74, wpb=8346.9, bsz=307.4, num_updates=21000, lr=9.759e-05, gnorm=0.574, clip=0, loss_scale=4, train_wall=57, gb_free=15, wall=16596
2023-08-12 16:58:53 | INFO | train_inner | epoch 015:    477 / 1474 loss=2.149, trans_loss=4.955, nll_loss=2.209, w2v_ctc_loss=0.792, task_loss=0, contrastive_loss=0.209, total=4076.89, n_correct=2645.1, ppl=4.62, accuracy=64.88, wps=13996, ups=1.72, wpb=8153.8, bsz=294.4, num_updates=21100, lr=9.73585e-05, gnorm=0.59, clip=0, loss_scale=4, train_wall=58, gb_free=16.7, wall=16654
2023-08-12 16:59:50 | INFO | train_inner | epoch 015:    577 / 1474 loss=2.138, trans_loss=4.955, nll_loss=2.21, w2v_ctc_loss=0.803, task_loss=0, contrastive_loss=0.12, total=4145.22, n_correct=2692.97, ppl=4.63, accuracy=64.966, wps=14334.1, ups=1.73, wpb=8290.4, bsz=300.4, num_updates=21200, lr=9.71286e-05, gnorm=0.58, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=16712
2023-08-12 17:00:48 | INFO | train_inner | epoch 015:    677 / 1474 loss=2.149, trans_loss=4.953, nll_loss=2.207, w2v_ctc_loss=0.804, task_loss=0, contrastive_loss=0.165, total=4127.9, n_correct=2682.69, ppl=4.62, accuracy=64.989, wps=14359.7, ups=1.74, wpb=8255.8, bsz=304.9, num_updates=21300, lr=9.69003e-05, gnorm=0.59, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=16769
2023-08-12 17:01:46 | INFO | train_inner | epoch 015:    777 / 1474 loss=2.138, trans_loss=4.962, nll_loss=2.218, w2v_ctc_loss=0.804, task_loss=0, contrastive_loss=0.105, total=4182.81, n_correct=2711.51, ppl=4.65, accuracy=64.825, wps=14491.1, ups=1.73, wpb=8365.6, bsz=306.5, num_updates=21400, lr=9.66736e-05, gnorm=0.574, clip=0, loss_scale=4, train_wall=57, gb_free=13.2, wall=16827
2023-08-12 17:02:43 | INFO | train_inner | epoch 015:    877 / 1474 loss=2.133, trans_loss=4.956, nll_loss=2.212, w2v_ctc_loss=0.806, task_loss=0, contrastive_loss=0.097, total=4047.26, n_correct=2628.21, ppl=4.63, accuracy=64.938, wps=14055.1, ups=1.74, wpb=8094.5, bsz=286.2, num_updates=21500, lr=9.64486e-05, gnorm=0.574, clip=0, loss_scale=4, train_wall=57, gb_free=15.2, wall=16885
2023-08-12 17:03:41 | INFO | train_inner | epoch 015:    977 / 1474 loss=2.146, trans_loss=4.958, nll_loss=2.214, w2v_ctc_loss=0.796, task_loss=0, contrastive_loss=0.182, total=4140.07, n_correct=2690.91, ppl=4.64, accuracy=64.997, wps=14314.9, ups=1.73, wpb=8280.1, bsz=304.7, num_updates=21600, lr=9.6225e-05, gnorm=0.567, clip=0, loss_scale=4, train_wall=57, gb_free=15.5, wall=16942
2023-08-12 17:04:39 | INFO | train_inner | epoch 015:   1077 / 1474 loss=2.174, trans_loss=4.96, nll_loss=2.216, w2v_ctc_loss=0.794, task_loss=0, contrastive_loss=0.342, total=4182.98, n_correct=2716.21, ppl=4.65, accuracy=64.935, wps=14407.6, ups=1.72, wpb=8366, bsz=325, num_updates=21700, lr=9.60031e-05, gnorm=0.661, clip=0, loss_scale=4, train_wall=58, gb_free=13.5, wall=17001
2023-08-12 17:05:37 | INFO | train_inner | epoch 015:   1177 / 1474 loss=2.138, trans_loss=4.953, nll_loss=2.209, w2v_ctc_loss=0.79, task_loss=0, contrastive_loss=0.146, total=4187.88, n_correct=2730.18, ppl=4.62, accuracy=65.192, wps=14615.9, ups=1.75, wpb=8375.8, bsz=329.1, num_updates=21800, lr=9.57826e-05, gnorm=0.586, clip=0, loss_scale=4, train_wall=57, gb_free=16.9, wall=17058
2023-08-12 17:06:34 | INFO | train_inner | epoch 015:   1277 / 1474 loss=2.131, trans_loss=4.95, nll_loss=2.203, w2v_ctc_loss=0.801, task_loss=0, contrastive_loss=0.1, total=4136.5, n_correct=2691.29, ppl=4.6, accuracy=65.062, wps=14390.5, ups=1.74, wpb=8273, bsz=300.6, num_updates=21900, lr=9.55637e-05, gnorm=0.572, clip=0, loss_scale=4, train_wall=57, gb_free=12.6, wall=17115
2023-08-12 17:07:32 | INFO | train_inner | epoch 015:   1377 / 1474 loss=2.127, trans_loss=4.953, nll_loss=2.207, w2v_ctc_loss=0.798, task_loss=0, contrastive_loss=0.088, total=4103.74, n_correct=2669.05, ppl=4.62, accuracy=65.039, wps=14275.4, ups=1.74, wpb=8207.5, bsz=294.1, num_updates=22000, lr=9.53463e-05, gnorm=0.564, clip=0, loss_scale=4, train_wall=57, gb_free=16.5, wall=17173
2023-08-12 17:07:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 17:07:53 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.191 | trans_loss 5.197 | nll_loss 2.467 | w2v_ctc_loss 1.423 | task_loss 0 | contrastive_loss 0.303 | total 4003.4 | n_correct 2634.8 | ppl 5.53 | accuracy 65.814 | uer 19.393 | wer 21.263 | raw_wer 21.263 | bleu 21.42 | wps 2435.4 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 21.42
2023-08-12 17:07:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-08-12 17:07:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_15_22000.pt
2023-08-12 17:07:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_15_22000.pt
2023-08-12 17:08:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 21.42) (writing took 43.68179398775101 seconds)
2023-08-12 17:09:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 17:09:56 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 4.194 | trans_loss 5.207 | nll_loss 2.485 | w2v_ctc_loss 1.39 | task_loss 0 | contrastive_loss 0.321 | total 4003.4 | n_correct 2626.5 | ppl 5.6 | accuracy 65.607 | uer 19.176 | wer 21.129 | raw_wer 21.129 | bleu 21.47 | wps 2393.2 | wpb 4003.4 | bsz 141.8 | num_updates 22097 | best_bleu 21.47
2023-08-12 17:09:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22097 updates
2023-08-12 17:09:56 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 17:10:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 17:10:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 15 @ 22097 updates, score 21.47) (writing took 27.77867135964334 seconds)
2023-08-12 17:10:24 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-08-12 17:10:24 | INFO | train | epoch 015 | loss 2.142 | trans_loss 4.955 | nll_loss 2.21 | w2v_ctc_loss 0.799 | task_loss 0 | contrastive_loss 0.151 | total 4138.65 | n_correct 2690.52 | ppl 4.63 | accuracy 65.01 | wps 12522.7 | ups 1.51 | wpb 8277.3 | bsz 305.7 | num_updates 22097 | lr 9.51368e-05 | gnorm 0.591 | clip 0.1 | loss_scale 4 | train_wall 844 | gb_free 16.9 | wall 17346
2023-08-12 17:10:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 17:10:25 | INFO | fairseq.trainer | begin training epoch 16
2023-08-12 17:10:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 17:10:34 | INFO | train_inner | epoch 016:      3 / 1474 loss=2.154, trans_loss=4.964, nll_loss=2.223, w2v_ctc_loss=0.803, task_loss=0, contrastive_loss=0.178, total=4154.85, n_correct=2697.12, ppl=4.67, accuracy=64.915, wps=4564.5, ups=0.55, wpb=8309.7, bsz=316.5, num_updates=22100, lr=9.51303e-05, gnorm=0.584, clip=0, loss_scale=4, train_wall=58, gb_free=16.8, wall=17355
2023-08-12 17:11:31 | INFO | train_inner | epoch 016:    103 / 1474 loss=2.116, trans_loss=4.931, nll_loss=2.178, w2v_ctc_loss=0.776, task_loss=0, contrastive_loss=0.115, total=4116.45, n_correct=2700.54, ppl=4.53, accuracy=65.604, wps=14334.3, ups=1.74, wpb=8232.9, bsz=314.3, num_updates=22200, lr=9.49158e-05, gnorm=0.567, clip=0, loss_scale=8, train_wall=57, gb_free=17.1, wall=17412
2023-08-12 17:12:29 | INFO | train_inner | epoch 016:    203 / 1474 loss=2.109, trans_loss=4.926, nll_loss=2.172, w2v_ctc_loss=0.773, task_loss=0, contrastive_loss=0.093, total=4112.07, n_correct=2700.12, ppl=4.51, accuracy=65.663, wps=14269.1, ups=1.74, wpb=8224.1, bsz=297.4, num_updates=22300, lr=9.47027e-05, gnorm=0.554, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=17470
2023-08-12 17:13:26 | INFO | train_inner | epoch 016:    303 / 1474 loss=2.133, trans_loss=4.936, nll_loss=2.185, w2v_ctc_loss=0.79, task_loss=0, contrastive_loss=0.166, total=4160.84, n_correct=2717.91, ppl=4.55, accuracy=65.321, wps=14541.6, ups=1.75, wpb=8321.7, bsz=308.1, num_updates=22400, lr=9.44911e-05, gnorm=0.59, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=17527
2023-08-12 17:14:23 | INFO | train_inner | epoch 016:    403 / 1474 loss=2.134, trans_loss=4.935, nll_loss=2.183, w2v_ctc_loss=0.794, task_loss=0, contrastive_loss=0.182, total=4066.97, n_correct=2659.19, ppl=4.54, accuracy=65.385, wps=14196.1, ups=1.75, wpb=8133.9, bsz=287, num_updates=22500, lr=9.42809e-05, gnorm=0.575, clip=0, loss_scale=8, train_wall=57, gb_free=15, wall=17584
2023-08-12 17:15:21 | INFO | train_inner | epoch 016:    503 / 1474 loss=2.126, trans_loss=4.94, nll_loss=2.191, w2v_ctc_loss=0.785, task_loss=0, contrastive_loss=0.124, total=4168.86, n_correct=2727.57, ppl=4.57, accuracy=65.427, wps=14338, ups=1.72, wpb=8337.7, bsz=318.4, num_updates=22600, lr=9.40721e-05, gnorm=0.577, clip=0, loss_scale=8, train_wall=58, gb_free=16.5, wall=17643
2023-08-12 17:16:19 | INFO | train_inner | epoch 016:    603 / 1474 loss=2.115, trans_loss=4.936, nll_loss=2.186, w2v_ctc_loss=0.783, task_loss=0, contrastive_loss=0.09, total=4132.12, n_correct=2705.75, ppl=4.55, accuracy=65.481, wps=14380.3, ups=1.74, wpb=8264.2, bsz=300.3, num_updates=22700, lr=9.38647e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=57, gb_free=16.1, wall=17700
2023-08-12 17:17:16 | INFO | train_inner | epoch 016:    703 / 1474 loss=2.114, trans_loss=4.934, nll_loss=2.183, w2v_ctc_loss=0.786, task_loss=0, contrastive_loss=0.09, total=4102.33, n_correct=2686.37, ppl=4.54, accuracy=65.484, wps=14319.4, ups=1.75, wpb=8204.7, bsz=298.1, num_updates=22800, lr=9.36586e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=17757
2023-08-12 17:18:14 | INFO | train_inner | epoch 016:    803 / 1474 loss=2.12, trans_loss=4.931, nll_loss=2.179, w2v_ctc_loss=0.768, task_loss=0, contrastive_loss=0.152, total=4176.5, n_correct=2737.09, ppl=4.53, accuracy=65.535, wps=14535.9, ups=1.74, wpb=8353, bsz=311.3, num_updates=22900, lr=9.34539e-05, gnorm=0.558, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=17815
2023-08-12 17:19:11 | INFO | train_inner | epoch 016:    903 / 1474 loss=2.124, trans_loss=4.935, nll_loss=2.184, w2v_ctc_loss=0.78, task_loss=0, contrastive_loss=0.144, total=4150.45, n_correct=2719.28, ppl=4.55, accuracy=65.518, wps=14396.9, ups=1.73, wpb=8300.9, bsz=305.8, num_updates=23000, lr=9.32505e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=57, gb_free=12.1, wall=17873
2023-08-12 17:20:09 | INFO | train_inner | epoch 016:   1003 / 1474 loss=2.129, trans_loss=4.94, nll_loss=2.191, w2v_ctc_loss=0.793, task_loss=0, contrastive_loss=0.14, total=4118.26, n_correct=2686.43, ppl=4.56, accuracy=65.232, wps=14232.1, ups=1.73, wpb=8236.5, bsz=301.7, num_updates=23100, lr=9.30484e-05, gnorm=0.568, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=17930
2023-08-12 17:21:06 | INFO | train_inner | epoch 016:   1103 / 1474 loss=2.132, trans_loss=4.948, nll_loss=2.201, w2v_ctc_loss=0.8, task_loss=0, contrastive_loss=0.117, total=4113.57, n_correct=2679.76, ppl=4.6, accuracy=65.144, wps=14339.5, ups=1.74, wpb=8227.1, bsz=296.2, num_updates=23200, lr=9.28477e-05, gnorm=0.578, clip=0, loss_scale=8, train_wall=57, gb_free=17.1, wall=17988
2023-08-12 17:22:04 | INFO | train_inner | epoch 016:   1203 / 1474 loss=2.141, trans_loss=4.944, nll_loss=2.196, w2v_ctc_loss=0.78, task_loss=0, contrastive_loss=0.213, total=4157.18, n_correct=2707.72, ppl=4.58, accuracy=65.134, wps=14348.4, ups=1.73, wpb=8314.4, bsz=306.3, num_updates=23300, lr=9.26482e-05, gnorm=0.577, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=18046
2023-08-12 17:23:02 | INFO | train_inner | epoch 016:   1303 / 1474 loss=2.143, trans_loss=4.941, nll_loss=2.193, w2v_ctc_loss=0.801, task_loss=0, contrastive_loss=0.193, total=4150.54, n_correct=2710.44, ppl=4.57, accuracy=65.303, wps=14318.1, ups=1.72, wpb=8301.1, bsz=312.3, num_updates=23400, lr=9.245e-05, gnorm=0.571, clip=0, loss_scale=8, train_wall=58, gb_free=13.1, wall=18104
2023-08-12 17:24:01 | INFO | train_inner | epoch 016:   1403 / 1474 loss=2.122, trans_loss=4.937, nll_loss=2.187, w2v_ctc_loss=0.785, task_loss=0, contrastive_loss=0.119, total=4198.78, n_correct=2749.01, ppl=4.55, accuracy=65.472, wps=14425.5, ups=1.72, wpb=8397.6, bsz=322.4, num_updates=23500, lr=9.22531e-05, gnorm=0.582, clip=0, loss_scale=8, train_wall=58, gb_free=17.8, wall=18162
2023-08-12 17:24:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 17:25:04 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 4.19 | trans_loss 5.189 | nll_loss 2.457 | w2v_ctc_loss 1.438 | task_loss 0 | contrastive_loss 0.304 | total 4003.4 | n_correct 2643.5 | ppl 5.49 | accuracy 66.031 | uer 18.833 | wer 20.685 | raw_wer 20.685 | bleu 21.59 | wps 2288.3 | wpb 4003.4 | bsz 141.8 | num_updates 23571 | best_bleu 21.59
2023-08-12 17:25:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23571 updates
2023-08-12 17:25:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 17:25:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 17:25:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 16 @ 23571 updates, score 21.59) (writing took 28.805519307032228 seconds)
2023-08-12 17:25:33 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-08-12 17:25:33 | INFO | train | epoch 016 | loss 2.127 | trans_loss 4.937 | nll_loss 2.186 | w2v_ctc_loss 0.785 | task_loss 0 | contrastive_loss 0.147 | total 4138.65 | n_correct 2706.84 | ppl 4.55 | accuracy 65.404 | wps 13422 | ups 1.62 | wpb 8277.3 | bsz 305.7 | num_updates 23571 | lr 9.21141e-05 | gnorm 0.57 | clip 0 | loss_scale 8 | train_wall 843 | gb_free 15.6 | wall 18255
2023-08-12 17:25:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 17:25:34 | INFO | fairseq.trainer | begin training epoch 17
2023-08-12 17:25:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 17:25:58 | INFO | train_inner | epoch 017:     29 / 1474 loss=2.131, trans_loss=4.924, nll_loss=2.17, w2v_ctc_loss=0.773, task_loss=0, contrastive_loss=0.258, total=4138.06, n_correct=2713.12, ppl=4.5, accuracy=65.565, wps=7064.9, ups=0.85, wpb=8276.1, bsz=300.4, num_updates=23600, lr=9.20575e-05, gnorm=0.564, clip=0, loss_scale=8, train_wall=57, gb_free=17.7, wall=18279
2023-08-12 17:26:56 | INFO | train_inner | epoch 017:    129 / 1474 loss=2.103, trans_loss=4.913, nll_loss=2.155, w2v_ctc_loss=0.778, task_loss=0, contrastive_loss=0.091, total=4110.37, n_correct=2708.74, ppl=4.45, accuracy=65.9, wps=14235, ups=1.73, wpb=8220.7, bsz=295.6, num_updates=23700, lr=9.1863e-05, gnorm=0.565, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=18337
2023-08-12 17:27:53 | INFO | train_inner | epoch 017:    229 / 1474 loss=2.137, trans_loss=4.915, nll_loss=2.158, w2v_ctc_loss=0.768, task_loss=0, contrastive_loss=0.261, total=4181.59, n_correct=2748.2, ppl=4.46, accuracy=65.721, wps=14565.1, ups=1.74, wpb=8363.2, bsz=322.2, num_updates=23800, lr=9.16698e-05, gnorm=0.592, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=18394
2023-08-12 17:28:51 | INFO | train_inner | epoch 017:    329 / 1474 loss=2.136, trans_loss=4.923, nll_loss=2.168, w2v_ctc_loss=0.781, task_loss=0, contrastive_loss=0.267, total=4157.97, n_correct=2727.87, ppl=4.49, accuracy=65.606, wps=14301.6, ups=1.72, wpb=8315.9, bsz=304, num_updates=23900, lr=9.14779e-05, gnorm=0.8, clip=0, loss_scale=8, train_wall=58, gb_free=16, wall=18452
2023-08-12 17:29:49 | INFO | train_inner | epoch 017:    429 / 1474 loss=2.103, trans_loss=4.917, nll_loss=2.161, w2v_ctc_loss=0.776, task_loss=0, contrastive_loss=0.091, total=4135.12, n_correct=2723.32, ppl=4.47, accuracy=65.858, wps=14372.4, ups=1.74, wpb=8270.2, bsz=306.1, num_updates=24000, lr=9.12871e-05, gnorm=0.591, clip=0, loss_scale=8, train_wall=57, gb_free=13, wall=18510
2023-08-12 17:29:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 17:30:11 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.186 | trans_loss 5.194 | nll_loss 2.463 | w2v_ctc_loss 1.415 | task_loss 0 | contrastive_loss 0.308 | total 4003.4 | n_correct 2639.4 | ppl 5.51 | accuracy 65.929 | uer 18.517 | wer 20.368 | raw_wer 20.368 | bleu 21.51 | wps 2306.9 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 21.59
2023-08-12 17:30:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-08-12 17:30:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_17_24000.pt
2023-08-12 17:30:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_17_24000.pt
2023-08-12 17:30:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 21.51) (writing took 41.56775324605405 seconds)
2023-08-12 17:31:52 | INFO | train_inner | epoch 017:    529 / 1474 loss=2.121, trans_loss=4.927, nll_loss=2.174, w2v_ctc_loss=0.787, task_loss=0, contrastive_loss=0.139, total=4185.81, n_correct=2745.53, ppl=4.51, accuracy=65.591, wps=6814.8, ups=0.81, wpb=8371.6, bsz=308.6, num_updates=24100, lr=9.10975e-05, gnorm=0.663, clip=0, loss_scale=8, train_wall=58, gb_free=16.1, wall=18633
2023-08-12 17:32:49 | INFO | train_inner | epoch 017:    629 / 1474 loss=2.101, trans_loss=4.921, nll_loss=2.166, w2v_ctc_loss=0.767, task_loss=0, contrastive_loss=0.088, total=4168.62, n_correct=2747.98, ppl=4.49, accuracy=65.921, wps=14486.3, ups=1.74, wpb=8337.2, bsz=303.2, num_updates=24200, lr=9.09091e-05, gnorm=0.559, clip=0, loss_scale=16, train_wall=57, gb_free=14.5, wall=18690
2023-08-12 17:32:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-12 17:33:48 | INFO | train_inner | epoch 017:    730 / 1474 loss=2.122, trans_loss=4.927, nll_loss=2.173, w2v_ctc_loss=0.789, task_loss=0, contrastive_loss=0.131, total=4171.52, n_correct=2734.89, ppl=4.51, accuracy=65.561, wps=14265.2, ups=1.71, wpb=8343, bsz=307.4, num_updates=24300, lr=9.07218e-05, gnorm=0.597, clip=0, loss_scale=8, train_wall=58, gb_free=12.1, wall=18749
2023-08-12 17:34:44 | INFO | train_inner | epoch 017:    830 / 1474 loss=2.11, trans_loss=4.923, nll_loss=2.168, w2v_ctc_loss=0.783, task_loss=0, contrastive_loss=0.1, total=4083.91, n_correct=2683.81, ppl=4.49, accuracy=65.717, wps=14406.9, ups=1.76, wpb=8167.8, bsz=294.5, num_updates=24400, lr=9.05357e-05, gnorm=0.586, clip=0, loss_scale=8, train_wall=56, gb_free=17.1, wall=18806
2023-08-12 17:35:41 | INFO | train_inner | epoch 017:    930 / 1474 loss=2.102, trans_loss=4.918, nll_loss=2.163, w2v_ctc_loss=0.768, task_loss=0, contrastive_loss=0.099, total=4114.87, n_correct=2706.79, ppl=4.48, accuracy=65.781, wps=14430.4, ups=1.75, wpb=8229.7, bsz=307.5, num_updates=24500, lr=9.03508e-05, gnorm=0.572, clip=0, loss_scale=8, train_wall=57, gb_free=17.8, wall=18863
2023-08-12 17:36:39 | INFO | train_inner | epoch 017:   1030 / 1474 loss=2.106, trans_loss=4.92, nll_loss=2.166, w2v_ctc_loss=0.78, task_loss=0, contrastive_loss=0.097, total=4091.03, n_correct=2690.8, ppl=4.49, accuracy=65.773, wps=14215.4, ups=1.74, wpb=8182.1, bsz=298.8, num_updates=24600, lr=9.0167e-05, gnorm=0.584, clip=0, loss_scale=8, train_wall=57, gb_free=15.8, wall=18920
2023-08-12 17:37:36 | INFO | train_inner | epoch 017:   1130 / 1474 loss=2.1, trans_loss=4.917, nll_loss=2.161, w2v_ctc_loss=0.766, task_loss=0, contrastive_loss=0.091, total=4112.54, n_correct=2709.13, ppl=4.47, accuracy=65.875, wps=14336.3, ups=1.74, wpb=8225.1, bsz=301.7, num_updates=24700, lr=8.99843e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=57, gb_free=15.8, wall=18978
2023-08-12 17:38:35 | INFO | train_inner | epoch 017:   1230 / 1474 loss=2.165, trans_loss=4.932, nll_loss=2.18, w2v_ctc_loss=0.772, task_loss=0, contrastive_loss=0.39, total=4171.58, n_correct=2727.82, ppl=4.53, accuracy=65.391, wps=14316.3, ups=1.72, wpb=8343.2, bsz=325.2, num_updates=24800, lr=8.98027e-05, gnorm=0.59, clip=0, loss_scale=8, train_wall=58, gb_free=16.6, wall=19036
2023-08-12 17:39:32 | INFO | train_inner | epoch 017:   1330 / 1474 loss=2.103, trans_loss=4.921, nll_loss=2.167, w2v_ctc_loss=0.768, task_loss=0, contrastive_loss=0.1, total=4138.5, n_correct=2721.93, ppl=4.49, accuracy=65.771, wps=14402, ups=1.74, wpb=8277, bsz=301.4, num_updates=24900, lr=8.96221e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=19093
2023-08-12 17:40:30 | INFO | train_inner | epoch 017:   1430 / 1474 loss=2.101, trans_loss=4.923, nll_loss=2.169, w2v_ctc_loss=0.768, task_loss=0, contrastive_loss=0.091, total=4118.28, n_correct=2709.59, ppl=4.5, accuracy=65.794, wps=14254.6, ups=1.73, wpb=8236.6, bsz=303.9, num_updates=25000, lr=8.94427e-05, gnorm=0.564, clip=0, loss_scale=8, train_wall=57, gb_free=16.1, wall=19151
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-12 17:40:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
2023-08-12 17:41:18 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 4.172 | trans_loss 5.185 | nll_loss 2.455 | w2v_ctc_loss 1.399 | task_loss 0 | contrastive_loss 0.303 | total 4003.4 | n_correct 2649.3 | ppl 5.48 | accuracy 66.176 | uer 18.286 | wer 20.245 | raw_wer 20.245 | bleu 21.6 | wps 2292.7 | wpb 4003.4 | bsz 141.8 | num_updates 25044 | best_bleu 21.6
2023-08-12 17:41:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25044 updates
2023-08-12 17:41:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 17:41:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 17:41:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 17 @ 25044 updates, score 21.6) (writing took 27.79345068708062 seconds)
2023-08-12 17:41:46 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-08-12 17:41:46 | INFO | train | epoch 017 | loss 2.115 | trans_loss 4.921 | nll_loss 2.166 | w2v_ctc_loss 0.775 | task_loss 0 | contrastive_loss 0.145 | total 4138.47 | n_correct 2720.64 | ppl 4.49 | accuracy 65.74 | wps 12532.5 | ups 1.51 | wpb 8276.9 | bsz 305.5 | num_updates 25044 | lr 8.93641e-05 | gnorm 0.598 | clip 0 | loss_scale 8 | train_wall 843 | gb_free 16.4 | wall 19228
2023-08-12 17:41:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 17:41:46 | INFO | fairseq.trainer | begin training epoch 18
2023-08-12 17:41:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 17:42:27 | INFO | train_inner | epoch 018:     56 / 1474 loss=2.104, trans_loss=4.914, nll_loss=2.157, w2v_ctc_loss=0.781, task_loss=0, contrastive_loss=0.099, total=4131.1, n_correct=2721.53, ppl=4.46, accuracy=65.879, wps=7068.2, ups=0.86, wpb=8262.2, bsz=301.5, num_updates=25100, lr=8.92644e-05, gnorm=0.567, clip=0, loss_scale=8, train_wall=58, gb_free=16.9, wall=19268
2023-08-12 17:43:24 | INFO | train_inner | epoch 018:    156 / 1474 loss=2.111, trans_loss=4.894, nll_loss=2.131, w2v_ctc_loss=0.75, task_loss=0, contrastive_loss=0.224, total=4161.38, n_correct=2757.24, ppl=4.38, accuracy=66.258, wps=14496.4, ups=1.74, wpb=8322.8, bsz=315, num_updates=25200, lr=8.90871e-05, gnorm=0.602, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=19325
2023-08-12 17:44:22 | INFO | train_inner | epoch 018:    256 / 1474 loss=2.089, trans_loss=4.896, nll_loss=2.134, w2v_ctc_loss=0.762, task_loss=0, contrastive_loss=0.091, total=4153.17, n_correct=2753.73, ppl=4.39, accuracy=66.304, wps=14432.8, ups=1.74, wpb=8306.3, bsz=311.2, num_updates=25300, lr=8.89108e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=19383
2023-08-12 17:44:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 17:45:21 | INFO | train_inner | epoch 018:    357 / 1474 loss=2.094, trans_loss=4.906, nll_loss=2.146, w2v_ctc_loss=0.76, task_loss=0, contrastive_loss=0.103, total=4167.75, n_correct=2752.24, ppl=4.43, accuracy=66.037, wps=14114.2, ups=1.69, wpb=8335.5, bsz=298.3, num_updates=25400, lr=8.87357e-05, gnorm=0.571, clip=0, loss_scale=4, train_wall=59, gb_free=16.5, wall=19442
2023-08-12 17:46:19 | INFO | train_inner | epoch 018:    457 / 1474 loss=2.116, trans_loss=4.911, nll_loss=2.153, w2v_ctc_loss=0.77, task_loss=0, contrastive_loss=0.2, total=4072.48, n_correct=2682.15, ppl=4.45, accuracy=65.86, wps=13966.9, ups=1.71, wpb=8145, bsz=294.3, num_updates=25500, lr=8.85615e-05, gnorm=0.69, clip=1, loss_scale=4, train_wall=58, gb_free=16.5, wall=19500
2023-08-12 17:47:16 | INFO | train_inner | epoch 018:    557 / 1474 loss=2.087, trans_loss=4.892, nll_loss=2.13, w2v_ctc_loss=0.753, task_loss=0, contrastive_loss=0.102, total=4215.05, n_correct=2796.37, ppl=4.38, accuracy=66.343, wps=14711.8, ups=1.75, wpb=8430.1, bsz=328.2, num_updates=25600, lr=8.83883e-05, gnorm=0.57, clip=0, loss_scale=4, train_wall=57, gb_free=17.6, wall=19558
2023-08-12 17:48:14 | INFO | train_inner | epoch 018:    657 / 1474 loss=2.109, trans_loss=4.912, nll_loss=2.155, w2v_ctc_loss=0.765, task_loss=0, contrastive_loss=0.173, total=4092.43, n_correct=2700.49, ppl=4.45, accuracy=65.987, wps=14242, ups=1.74, wpb=8184.9, bsz=298.2, num_updates=25700, lr=8.82162e-05, gnorm=0.567, clip=0, loss_scale=4, train_wall=57, gb_free=15.9, wall=19615
2023-08-12 17:49:12 | INFO | train_inner | epoch 018:    757 / 1474 loss=2.129, trans_loss=4.91, nll_loss=2.152, w2v_ctc_loss=0.772, task_loss=0, contrastive_loss=0.266, total=4200.52, n_correct=2771.41, ppl=4.44, accuracy=65.978, wps=14408.2, ups=1.72, wpb=8401, bsz=323, num_updates=25800, lr=8.80451e-05, gnorm=0.576, clip=0, loss_scale=4, train_wall=58, gb_free=14.6, wall=19673
2023-08-12 17:50:10 | INFO | train_inner | epoch 018:    857 / 1474 loss=2.093, trans_loss=4.907, nll_loss=2.149, w2v_ctc_loss=0.761, task_loss=0, contrastive_loss=0.092, total=4191.43, n_correct=2768.93, ppl=4.43, accuracy=66.062, wps=14556.5, ups=1.74, wpb=8382.9, bsz=309, num_updates=25900, lr=8.7875e-05, gnorm=0.567, clip=0, loss_scale=4, train_wall=57, gb_free=16, wall=19731
2023-08-12 17:51:06 | INFO | train_inner | epoch 018:    957 / 1474 loss=2.088, trans_loss=4.9, nll_loss=2.14, w2v_ctc_loss=0.752, task_loss=0, contrastive_loss=0.094, total=4121.06, n_correct=2730.94, ppl=4.41, accuracy=66.268, wps=14518.3, ups=1.76, wpb=8242.1, bsz=308.9, num_updates=26000, lr=8.77058e-05, gnorm=0.566, clip=0, loss_scale=4, train_wall=56, gb_free=14.5, wall=19788
2023-08-12 17:51:06 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 17:51:28 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.169 | trans_loss 5.189 | nll_loss 2.456 | w2v_ctc_loss 1.377 | task_loss 0 | contrastive_loss 0.304 | total 4003.4 | n_correct 2653.9 | ppl 5.49 | accuracy 66.291 | uer 18.355 | wer 20.245 | raw_wer 20.245 | bleu 21.53 | wps 2404.3 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 21.6
2023-08-12 17:51:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-08-12 17:51:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_18_26000.pt
2023-08-12 17:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_18_26000.pt
2023-08-12 17:52:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 21.53) (writing took 32.31592430360615 seconds)
2023-08-12 17:52:59 | INFO | train_inner | epoch 018:   1057 / 1474 loss=2.091, trans_loss=4.905, nll_loss=2.146, w2v_ctc_loss=0.757, task_loss=0, contrastive_loss=0.091, total=4133.99, n_correct=2734.09, ppl=4.43, accuracy=66.137, wps=7354.2, ups=0.89, wpb=8268, bsz=299.3, num_updates=26100, lr=8.75376e-05, gnorm=0.562, clip=0, loss_scale=4, train_wall=57, gb_free=17.9, wall=19900
2023-08-12 17:53:57 | INFO | train_inner | epoch 018:   1157 / 1474 loss=2.116, trans_loss=4.903, nll_loss=2.144, w2v_ctc_loss=0.769, task_loss=0, contrastive_loss=0.198, total=4159.99, n_correct=2748.18, ppl=4.42, accuracy=66.062, wps=14329.8, ups=1.72, wpb=8320, bsz=316.2, num_updates=26200, lr=8.73704e-05, gnorm=0.574, clip=0, loss_scale=4, train_wall=58, gb_free=16.6, wall=19958
2023-08-12 17:54:55 | INFO | train_inner | epoch 018:   1257 / 1474 loss=2.093, trans_loss=4.914, nll_loss=2.158, w2v_ctc_loss=0.763, task_loss=0, contrastive_loss=0.086, total=4104.17, n_correct=2705.46, ppl=4.46, accuracy=65.92, wps=14251.4, ups=1.74, wpb=8208.3, bsz=290.1, num_updates=26300, lr=8.72041e-05, gnorm=0.589, clip=0, loss_scale=4, train_wall=57, gb_free=16.1, wall=20016
2023-08-12 17:55:52 | INFO | train_inner | epoch 018:   1357 / 1474 loss=2.112, trans_loss=4.92, nll_loss=2.165, w2v_ctc_loss=0.79, task_loss=0, contrastive_loss=0.112, total=4041.44, n_correct=2655.03, ppl=4.48, accuracy=65.695, wps=14127.6, ups=1.75, wpb=8082.9, bsz=285.4, num_updates=26400, lr=8.70388e-05, gnorm=0.582, clip=0, loss_scale=4, train_wall=57, gb_free=17.8, wall=20073
2023-08-12 17:56:50 | INFO | train_inner | epoch 018:   1457 / 1474 loss=2.099, trans_loss=4.909, nll_loss=2.151, w2v_ctc_loss=0.772, task_loss=0, contrastive_loss=0.099, total=4130.96, n_correct=2726.07, ppl=4.44, accuracy=65.991, wps=14251.4, ups=1.72, wpb=8261.9, bsz=300.5, num_updates=26500, lr=8.68744e-05, gnorm=0.583, clip=0, loss_scale=4, train_wall=58, gb_free=11.1, wall=20131
2023-08-12 17:57:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 17:57:23 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 4.176 | trans_loss 5.187 | nll_loss 2.457 | w2v_ctc_loss 1.409 | task_loss 0 | contrastive_loss 0.299 | total 4003.4 | n_correct 2650.6 | ppl 5.49 | accuracy 66.209 | uer 18.509 | wer 20.469 | raw_wer 20.469 | bleu 21.9 | wps 2120.1 | wpb 4003.4 | bsz 141.8 | num_updates 26517 | best_bleu 21.9
2023-08-12 17:57:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26517 updates
2023-08-12 17:57:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 17:58:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 17:58:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 18 @ 26517 updates, score 21.9) (writing took 64.27180169709027 seconds)
2023-08-12 17:58:28 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-08-12 17:58:28 | INFO | train | epoch 018 | loss 2.102 | trans_loss 4.906 | nll_loss 2.147 | w2v_ctc_loss 0.764 | task_loss 0 | contrastive_loss 0.141 | total 4137.67 | n_correct 2733.37 | ppl 4.43 | accuracy 66.061 | wps 12170 | ups 1.47 | wpb 8275.3 | bsz 305.4 | num_updates 26517 | lr 8.68466e-05 | gnorm 0.583 | clip 0.1 | loss_scale 4 | train_wall 844 | gb_free 16 | wall 20229
2023-08-12 17:58:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 17:58:28 | INFO | fairseq.trainer | begin training epoch 19
2023-08-12 17:58:28 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 17:59:24 | INFO | train_inner | epoch 019:     83 / 1474 loss=2.092, trans_loss=4.888, nll_loss=2.124, w2v_ctc_loss=0.758, task_loss=0, contrastive_loss=0.148, total=4100.13, n_correct=2722.2, ppl=4.36, accuracy=66.393, wps=5302.4, ups=0.65, wpb=8200.3, bsz=297.1, num_updates=26600, lr=8.6711e-05, gnorm=0.636, clip=0, loss_scale=4, train_wall=58, gb_free=16.1, wall=20286
2023-08-12 18:00:22 | INFO | train_inner | epoch 019:    183 / 1474 loss=2.097, trans_loss=4.887, nll_loss=2.122, w2v_ctc_loss=0.763, task_loss=0, contrastive_loss=0.139, total=4219.78, n_correct=2805.79, ppl=4.35, accuracy=66.491, wps=14563.1, ups=1.73, wpb=8439.6, bsz=323.2, num_updates=26700, lr=8.65485e-05, gnorm=0.652, clip=0, loss_scale=4, train_wall=57, gb_free=13, wall=20344
2023-08-12 18:01:20 | INFO | train_inner | epoch 019:    283 / 1474 loss=2.077, trans_loss=4.881, nll_loss=2.114, w2v_ctc_loss=0.756, task_loss=0, contrastive_loss=0.083, total=4192.4, n_correct=2793.71, ppl=4.33, accuracy=66.637, wps=14559, ups=1.74, wpb=8384.8, bsz=308.2, num_updates=26800, lr=8.63868e-05, gnorm=0.557, clip=0, loss_scale=4, train_wall=57, gb_free=16.2, wall=20401
2023-08-12 18:02:17 | INFO | train_inner | epoch 019:    383 / 1474 loss=2.096, trans_loss=4.882, nll_loss=2.116, w2v_ctc_loss=0.747, task_loss=0, contrastive_loss=0.192, total=4172.45, n_correct=2773.7, ppl=4.34, accuracy=66.477, wps=14558.3, ups=1.74, wpb=8344.9, bsz=312.3, num_updates=26900, lr=8.62261e-05, gnorm=0.581, clip=0, loss_scale=4, train_wall=57, gb_free=14.5, wall=20459
2023-08-12 18:03:15 | INFO | train_inner | epoch 019:    483 / 1474 loss=2.082, trans_loss=4.89, nll_loss=2.126, w2v_ctc_loss=0.755, task_loss=0, contrastive_loss=0.094, total=4100.34, n_correct=2725.69, ppl=4.36, accuracy=66.475, wps=14229.7, ups=1.74, wpb=8200.7, bsz=298.5, num_updates=27000, lr=8.60663e-05, gnorm=0.561, clip=0, loss_scale=4, train_wall=57, gb_free=14.4, wall=20516
2023-08-12 18:04:13 | INFO | train_inner | epoch 019:    583 / 1474 loss=2.087, trans_loss=4.885, nll_loss=2.12, w2v_ctc_loss=0.746, task_loss=0, contrastive_loss=0.16, total=4135.81, n_correct=2753.15, ppl=4.35, accuracy=66.569, wps=14322.1, ups=1.73, wpb=8271.6, bsz=306.5, num_updates=27100, lr=8.59074e-05, gnorm=0.576, clip=0, loss_scale=4, train_wall=57, gb_free=16.9, wall=20574
2023-08-12 18:05:10 | INFO | train_inner | epoch 019:    683 / 1474 loss=2.078, trans_loss=4.893, nll_loss=2.13, w2v_ctc_loss=0.741, task_loss=0, contrastive_loss=0.089, total=4201.16, n_correct=2792.49, ppl=4.38, accuracy=66.469, wps=14725.5, ups=1.75, wpb=8402.3, bsz=321.9, num_updates=27200, lr=8.57493e-05, gnorm=0.594, clip=0, loss_scale=4, train_wall=57, gb_free=17.2, wall=20631
2023-08-12 18:06:07 | INFO | train_inner | epoch 019:    783 / 1474 loss=2.088, trans_loss=4.891, nll_loss=2.127, w2v_ctc_loss=0.763, task_loss=0, contrastive_loss=0.101, total=4144.63, n_correct=2752.7, ppl=4.37, accuracy=66.416, wps=14497.5, ups=1.75, wpb=8289.3, bsz=305.6, num_updates=27300, lr=8.55921e-05, gnorm=0.569, clip=0, loss_scale=4, train_wall=57, gb_free=12.7, wall=20688
2023-08-12 18:07:05 | INFO | train_inner | epoch 019:    883 / 1474 loss=2.084, trans_loss=4.895, nll_loss=2.133, w2v_ctc_loss=0.758, task_loss=0, contrastive_loss=0.084, total=4151.05, n_correct=2750.9, ppl=4.39, accuracy=66.27, wps=14253, ups=1.72, wpb=8302.1, bsz=303.2, num_updates=27400, lr=8.54358e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=58, gb_free=16.2, wall=20746
2023-08-12 18:08:03 | INFO | train_inner | epoch 019:    983 / 1474 loss=2.123, trans_loss=4.902, nll_loss=2.143, w2v_ctc_loss=0.748, task_loss=0, contrastive_loss=0.321, total=4099.07, n_correct=2711.25, ppl=4.42, accuracy=66.143, wps=14126.6, ups=1.72, wpb=8198.1, bsz=307.5, num_updates=27500, lr=8.52803e-05, gnorm=0.595, clip=0, loss_scale=8, train_wall=58, gb_free=14.4, wall=20805
2023-08-12 18:08:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 18:09:01 | INFO | train_inner | epoch 019:   1084 / 1474 loss=2.092, trans_loss=4.901, nll_loss=2.14, w2v_ctc_loss=0.753, task_loss=0, contrastive_loss=0.127, total=4035.11, n_correct=2669.69, ppl=4.41, accuracy=66.162, wps=13897.3, ups=1.72, wpb=8070.2, bsz=290.8, num_updates=27600, lr=8.51257e-05, gnorm=0.595, clip=0, loss_scale=4, train_wall=58, gb_free=13.5, wall=20863
2023-08-12 18:09:59 | INFO | train_inner | epoch 019:   1184 / 1474 loss=2.107, trans_loss=4.896, nll_loss=2.133, w2v_ctc_loss=0.753, task_loss=0, contrastive_loss=0.209, total=4139.45, n_correct=2740.11, ppl=4.39, accuracy=66.195, wps=14352.2, ups=1.73, wpb=8278.9, bsz=306.3, num_updates=27700, lr=8.49719e-05, gnorm=0.57, clip=0, loss_scale=4, train_wall=57, gb_free=15.8, wall=20920
2023-08-12 18:10:57 | INFO | train_inner | epoch 019:   1284 / 1474 loss=2.088, trans_loss=4.898, nll_loss=2.138, w2v_ctc_loss=0.751, task_loss=0, contrastive_loss=0.109, total=4145.11, n_correct=2745.08, ppl=4.4, accuracy=66.225, wps=14233.6, ups=1.72, wpb=8290.2, bsz=302.9, num_updates=27800, lr=8.48189e-05, gnorm=0.562, clip=0, loss_scale=4, train_wall=58, gb_free=16.6, wall=20979
2023-08-12 18:11:55 | INFO | train_inner | epoch 019:   1384 / 1474 loss=2.086, trans_loss=4.895, nll_loss=2.133, w2v_ctc_loss=0.756, task_loss=0, contrastive_loss=0.096, total=4130.68, n_correct=2737.34, ppl=4.39, accuracy=66.269, wps=14318, ups=1.73, wpb=8261.4, bsz=302.6, num_updates=27900, lr=8.46668e-05, gnorm=0.571, clip=0, loss_scale=4, train_wall=57, gb_free=12.2, wall=21036
2023-08-12 18:12:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 18:13:09 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 4.176 | trans_loss 5.184 | nll_loss 2.451 | w2v_ctc_loss 1.416 | task_loss 0 | contrastive_loss 0.305 | total 4003.4 | n_correct 2654.8 | ppl 5.47 | accuracy 66.314 | uer 18.416 | wer 20.458 | raw_wer 20.458 | bleu 21.77 | wps 2286.1 | wpb 4003.4 | bsz 141.8 | num_updates 27990 | best_bleu 21.9
2023-08-12 18:13:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27990 updates
2023-08-12 18:13:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.7705.pt
2023-08-12 18:13:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.7705.pt
2023-08-12 18:13:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.7705.pt (epoch 19 @ 27990 updates, score 21.77) (writing took 20.52765665203333 seconds)
2023-08-12 18:13:30 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-08-12 18:13:30 | INFO | train | epoch 019 | loss 2.091 | trans_loss 4.891 | nll_loss 2.128 | w2v_ctc_loss 0.754 | task_loss 0 | contrastive_loss 0.139 | total 4138.41 | n_correct 2746.9 | ppl 4.37 | accuracy 66.376 | wps 13510.2 | ups 1.63 | wpb 8276.8 | bsz 305.5 | num_updates 27990 | lr 8.45305e-05 | gnorm 0.584 | clip 0 | loss_scale 4 | train_wall 844 | gb_free 17.4 | wall 21132
2023-08-12 18:13:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 18:13:31 | INFO | fairseq.trainer | begin training epoch 20
2023-08-12 18:13:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 18:13:44 | INFO | train_inner | epoch 020:     10 / 1474 loss=2.092, trans_loss=4.887, nll_loss=2.122, w2v_ctc_loss=0.749, task_loss=0, contrastive_loss=0.175, total=4111.13, n_correct=2731.12, ppl=4.35, accuracy=66.432, wps=7549.4, ups=0.92, wpb=8222.3, bsz=300.9, num_updates=28000, lr=8.45154e-05, gnorm=0.575, clip=0, loss_scale=4, train_wall=57, gb_free=16.8, wall=21145
2023-08-12 18:13:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 18:14:06 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.16 | trans_loss 5.185 | nll_loss 2.452 | w2v_ctc_loss 1.368 | task_loss 0 | contrastive_loss 0.298 | total 4003.4 | n_correct 2655.6 | ppl 5.47 | accuracy 66.334 | uer 18.273 | wer 20.339 | raw_wer 20.339 | bleu 21.8 | wps 2388.7 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 21.9
2023-08-12 18:14:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-08-12 18:14:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_20_28000.pt
2023-08-12 18:14:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_20_28000.pt
2023-08-12 18:14:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 21.8) (writing took 42.35643528215587 seconds)
2023-08-12 18:15:48 | INFO | train_inner | epoch 020:    110 / 1474 loss=2.07, trans_loss=4.87, nll_loss=2.1, w2v_ctc_loss=0.737, task_loss=0, contrastive_loss=0.101, total=4202.23, n_correct=2806.55, ppl=4.29, accuracy=66.787, wps=6787.7, ups=0.81, wpb=8404.5, bsz=314.4, num_updates=28100, lr=8.43649e-05, gnorm=0.618, clip=0, loss_scale=4, train_wall=58, gb_free=16.2, wall=21269
2023-08-12 18:16:46 | INFO | train_inner | epoch 020:    210 / 1474 loss=2.076, trans_loss=4.874, nll_loss=2.105, w2v_ctc_loss=0.735, task_loss=0, contrastive_loss=0.153, total=4155.61, n_correct=2770.95, ppl=4.3, accuracy=66.68, wps=14283, ups=1.72, wpb=8311.2, bsz=301.4, num_updates=28200, lr=8.42152e-05, gnorm=0.554, clip=0, loss_scale=4, train_wall=58, gb_free=16.9, wall=21327
2023-08-12 18:17:43 | INFO | train_inner | epoch 020:    310 / 1474 loss=2.066, trans_loss=4.866, nll_loss=2.096, w2v_ctc_loss=0.739, task_loss=0, contrastive_loss=0.088, total=4189.88, n_correct=2803.99, ppl=4.28, accuracy=66.923, wps=14642.1, ups=1.75, wpb=8379.8, bsz=324.9, num_updates=28300, lr=8.40663e-05, gnorm=0.552, clip=0, loss_scale=4, train_wall=57, gb_free=16.6, wall=21384
2023-08-12 18:18:40 | INFO | train_inner | epoch 020:    410 / 1474 loss=2.065, trans_loss=4.867, nll_loss=2.096, w2v_ctc_loss=0.737, task_loss=0, contrastive_loss=0.088, total=4101.46, n_correct=2742.23, ppl=4.28, accuracy=66.86, wps=14333.3, ups=1.75, wpb=8202.9, bsz=295.5, num_updates=28400, lr=8.39181e-05, gnorm=0.567, clip=0, loss_scale=4, train_wall=57, gb_free=15.9, wall=21442
2023-08-12 18:19:38 | INFO | train_inner | epoch 020:    510 / 1474 loss=2.085, trans_loss=4.883, nll_loss=2.117, w2v_ctc_loss=0.738, task_loss=0, contrastive_loss=0.175, total=4127.02, n_correct=2750.23, ppl=4.34, accuracy=66.64, wps=14223.4, ups=1.72, wpb=8254, bsz=303.8, num_updates=28500, lr=8.37708e-05, gnorm=0.566, clip=0, loss_scale=4, train_wall=58, gb_free=15.9, wall=21500
2023-08-12 18:20:36 | INFO | train_inner | epoch 020:    610 / 1474 loss=2.092, trans_loss=4.88, nll_loss=2.113, w2v_ctc_loss=0.745, task_loss=0, contrastive_loss=0.181, total=4079.57, n_correct=2714.07, ppl=4.33, accuracy=66.528, wps=14095.3, ups=1.73, wpb=8159.1, bsz=293.6, num_updates=28600, lr=8.36242e-05, gnorm=0.583, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=21558
2023-08-12 18:21:33 | INFO | train_inner | epoch 020:    710 / 1474 loss=2.076, trans_loss=4.882, nll_loss=2.116, w2v_ctc_loss=0.754, task_loss=0, contrastive_loss=0.082, total=4134.59, n_correct=2748.98, ppl=4.33, accuracy=66.487, wps=14481.9, ups=1.75, wpb=8269.2, bsz=299.7, num_updates=28700, lr=8.34784e-05, gnorm=0.564, clip=0, loss_scale=4, train_wall=57, gb_free=12, wall=21615
2023-08-12 18:22:31 | INFO | train_inner | epoch 020:    810 / 1474 loss=2.072, trans_loss=4.88, nll_loss=2.113, w2v_ctc_loss=0.746, task_loss=0, contrastive_loss=0.086, total=4157.39, n_correct=2772.01, ppl=4.33, accuracy=66.677, wps=14465.2, ups=1.74, wpb=8314.8, bsz=310.2, num_updates=28800, lr=8.33333e-05, gnorm=0.558, clip=0, loss_scale=4, train_wall=57, gb_free=13.5, wall=21672
2023-08-12 18:23:29 | INFO | train_inner | epoch 020:    910 / 1474 loss=2.129, trans_loss=4.885, nll_loss=2.121, w2v_ctc_loss=0.745, task_loss=0, contrastive_loss=0.377, total=4155.35, n_correct=2759.3, ppl=4.35, accuracy=66.404, wps=14204.7, ups=1.71, wpb=8310.7, bsz=322.5, num_updates=28900, lr=8.3189e-05, gnorm=0.62, clip=0, loss_scale=4, train_wall=58, gb_free=17.3, wall=21731
2023-08-12 18:24:28 | INFO | train_inner | epoch 020:   1010 / 1474 loss=2.084, trans_loss=4.882, nll_loss=2.116, w2v_ctc_loss=0.739, task_loss=0, contrastive_loss=0.156, total=4185.22, n_correct=2788.59, ppl=4.34, accuracy=66.629, wps=14247.2, ups=1.7, wpb=8370.4, bsz=311.8, num_updates=29000, lr=8.30455e-05, gnorm=0.559, clip=0, loss_scale=4, train_wall=58, gb_free=15.7, wall=21789
2023-08-12 18:25:25 | INFO | train_inner | epoch 020:   1110 / 1474 loss=2.09, trans_loss=4.88, nll_loss=2.114, w2v_ctc_loss=0.748, task_loss=0, contrastive_loss=0.163, total=4143.91, n_correct=2761.02, ppl=4.33, accuracy=66.628, wps=14515.7, ups=1.75, wpb=8287.8, bsz=308.6, num_updates=29100, lr=8.29027e-05, gnorm=0.584, clip=0, loss_scale=4, train_wall=57, gb_free=16.4, wall=21847
2023-08-12 18:26:23 | INFO | train_inner | epoch 020:   1210 / 1474 loss=2.075, trans_loss=4.876, nll_loss=2.108, w2v_ctc_loss=0.759, task_loss=0, contrastive_loss=0.079, total=4027.04, n_correct=2680.91, ppl=4.31, accuracy=66.573, wps=14051.1, ups=1.74, wpb=8054.1, bsz=282.7, num_updates=29200, lr=8.27606e-05, gnorm=0.572, clip=0, loss_scale=4, train_wall=57, gb_free=16.4, wall=21904
2023-08-12 18:27:20 | INFO | train_inner | epoch 020:   1310 / 1474 loss=2.071, trans_loss=4.882, nll_loss=2.116, w2v_ctc_loss=0.744, task_loss=0, contrastive_loss=0.084, total=4135.87, n_correct=2758.2, ppl=4.33, accuracy=66.69, wps=14272.8, ups=1.73, wpb=8271.7, bsz=300.8, num_updates=29300, lr=8.26192e-05, gnorm=0.588, clip=0, loss_scale=4, train_wall=58, gb_free=16.6, wall=21962
2023-08-12 18:28:19 | INFO | train_inner | epoch 020:   1410 / 1474 loss=2.073, trans_loss=4.88, nll_loss=2.113, w2v_ctc_loss=0.749, task_loss=0, contrastive_loss=0.082, total=4122.3, n_correct=2744.55, ppl=4.33, accuracy=66.578, wps=14187.6, ups=1.72, wpb=8244.6, bsz=294.5, num_updates=29400, lr=8.24786e-05, gnorm=0.57, clip=0, loss_scale=4, train_wall=58, gb_free=16.3, wall=22020
2023-08-12 18:28:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 18:29:18 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 4.166 | trans_loss 5.189 | nll_loss 2.456 | w2v_ctc_loss 1.371 | task_loss 0 | contrastive_loss 0.305 | total 4003.4 | n_correct 2646.1 | ppl 5.49 | accuracy 66.096 | uer 18.979 | wer 21.14 | raw_wer 21.14 | bleu 21.81 | wps 2308.3 | wpb 4003.4 | bsz 141.8 | num_updates 29464 | best_bleu 21.9
2023-08-12 18:29:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29464 updates
2023-08-12 18:29:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.8107.pt
2023-08-12 18:29:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.8107.pt
2023-08-12 18:29:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.8107.pt (epoch 20 @ 29464 updates, score 21.81) (writing took 19.772289806976914 seconds)
2023-08-12 18:29:38 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-08-12 18:29:38 | INFO | train | epoch 020 | loss 2.08 | trans_loss 4.878 | nll_loss 2.111 | w2v_ctc_loss 0.744 | task_loss 0 | contrastive_loss 0.136 | total 4138.65 | n_correct 2758.29 | ppl 4.32 | accuracy 66.647 | wps 12608.2 | ups 1.52 | wpb 8277.3 | bsz 305.7 | num_updates 29464 | lr 8.2389e-05 | gnorm 0.576 | clip 0 | loss_scale 4 | train_wall 845 | gb_free 16.2 | wall 22099
2023-08-12 18:29:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 18:29:38 | INFO | fairseq.trainer | begin training epoch 21
2023-08-12 18:29:38 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 18:30:06 | INFO | train_inner | epoch 021:     36 / 1474 loss=2.093, trans_loss=4.88, nll_loss=2.114, w2v_ctc_loss=0.74, task_loss=0, contrastive_loss=0.203, total=4143.81, n_correct=2759.37, ppl=4.33, accuracy=66.59, wps=7683, ups=0.93, wpb=8287.6, bsz=315, num_updates=29500, lr=8.23387e-05, gnorm=0.583, clip=0, loss_scale=4, train_wall=57, gb_free=15.6, wall=22128
2023-08-12 18:31:04 | INFO | train_inner | epoch 021:    136 / 1474 loss=2.081, trans_loss=4.86, nll_loss=2.087, w2v_ctc_loss=0.731, task_loss=0, contrastive_loss=0.193, total=4195.35, n_correct=2811.29, ppl=4.25, accuracy=67.01, wps=14572.9, ups=1.74, wpb=8390.7, bsz=319.8, num_updates=29600, lr=8.21995e-05, gnorm=0.572, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=22185
2023-08-12 18:32:02 | INFO | train_inner | epoch 021:    236 / 1474 loss=2.063, trans_loss=4.86, nll_loss=2.087, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.149, total=4148.29, n_correct=2782.39, ppl=4.25, accuracy=67.073, wps=14344.3, ups=1.73, wpb=8296.6, bsz=311.4, num_updates=29700, lr=8.2061e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=57, gb_free=12.8, wall=22243
2023-08-12 18:33:00 | INFO | train_inner | epoch 021:    336 / 1474 loss=2.077, trans_loss=4.865, nll_loss=2.094, w2v_ctc_loss=0.742, task_loss=0, contrastive_loss=0.151, total=4164.34, n_correct=2784.22, ppl=4.27, accuracy=66.859, wps=14393.5, ups=1.73, wpb=8328.7, bsz=312.6, num_updates=29800, lr=8.19232e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=57, gb_free=15.1, wall=22301
2023-08-12 18:33:57 | INFO | train_inner | epoch 021:    436 / 1474 loss=2.056, trans_loss=4.861, nll_loss=2.088, w2v_ctc_loss=0.728, task_loss=0, contrastive_loss=0.078, total=4170.15, n_correct=2798.58, ppl=4.25, accuracy=67.11, wps=14653.8, ups=1.76, wpb=8340.3, bsz=306.1, num_updates=29900, lr=8.17861e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=56, gb_free=16.7, wall=22358
2023-08-12 18:34:54 | INFO | train_inner | epoch 021:    536 / 1474 loss=2.057, trans_loss=4.855, nll_loss=2.081, w2v_ctc_loss=0.74, task_loss=0, contrastive_loss=0.076, total=4086.43, n_correct=2743.09, ppl=4.23, accuracy=67.127, wps=14224.9, ups=1.74, wpb=8172.9, bsz=295.4, num_updates=30000, lr=8.16497e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=57, gb_free=13.2, wall=22415
2023-08-12 18:34:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 18:35:16 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.171 | trans_loss 5.18 | nll_loss 2.444 | w2v_ctc_loss 1.418 | task_loss 0 | contrastive_loss 0.295 | total 4003.4 | n_correct 2654.1 | ppl 5.44 | accuracy 66.296 | uer 18.52 | wer 20.592 | raw_wer 20.592 | bleu 21.64 | wps 2457.3 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 21.9
2023-08-12 18:35:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-08-12 18:35:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_21_30000.pt
2023-08-12 18:35:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_21_30000.pt
2023-08-12 18:36:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 21.64) (writing took 43.92962779663503 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-12 18:36:59 | INFO | train_inner | epoch 021:    636 / 1474 loss=2.085, trans_loss=4.861, nll_loss=2.088, w2v_ctc_loss=0.73, task_loss=0, contrastive_loss=0.247, total=4214.93, n_correct=2821.58, ppl=4.25, accuracy=66.943, wps=6728, ups=0.8, wpb=8429.9, bsz=317.2, num_updates=30100, lr=8.15139e-05, gnorm=0.558, clip=0, loss_scale=8, train_wall=59, gb_free=17.1, wall=22541
2023-08-12 18:37:57 | INFO | train_inner | epoch 021:    736 / 1474 loss=2.066, trans_loss=4.868, nll_loss=2.098, w2v_ctc_loss=0.73, task_loss=0, contrastive_loss=0.108, total=4151.84, n_correct=2780.06, ppl=4.28, accuracy=66.96, wps=14351.2, ups=1.73, wpb=8303.7, bsz=307.5, num_updates=30200, lr=8.13788e-05, gnorm=0.573, clip=0, loss_scale=8, train_wall=57, gb_free=15.3, wall=22599
2023-08-12 18:38:55 | INFO | train_inner | epoch 021:    836 / 1474 loss=2.077, trans_loss=4.878, nll_loss=2.111, w2v_ctc_loss=0.738, task_loss=0, contrastive_loss=0.121, total=4075.02, n_correct=2713.9, ppl=4.32, accuracy=66.598, wps=14184.3, ups=1.74, wpb=8150, bsz=296.2, num_updates=30300, lr=8.12444e-05, gnorm=0.627, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=22656
2023-08-12 18:39:52 | INFO | train_inner | epoch 021:    936 / 1474 loss=2.068, trans_loss=4.867, nll_loss=2.096, w2v_ctc_loss=0.743, task_loss=0, contrastive_loss=0.095, total=4090.62, n_correct=2735.05, ppl=4.28, accuracy=66.862, wps=14323.8, ups=1.75, wpb=8181.2, bsz=300.5, num_updates=30400, lr=8.11107e-05, gnorm=0.568, clip=0, loss_scale=8, train_wall=57, gb_free=16, wall=22713
2023-08-12 18:40:50 | INFO | train_inner | epoch 021:   1036 / 1474 loss=2.067, trans_loss=4.873, nll_loss=2.105, w2v_ctc_loss=0.74, task_loss=0, contrastive_loss=0.09, total=4109.99, n_correct=2747.03, ppl=4.3, accuracy=66.838, wps=14259.7, ups=1.73, wpb=8220, bsz=298.5, num_updates=30500, lr=8.09776e-05, gnorm=0.569, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=22771
2023-08-12 18:41:47 | INFO | train_inner | epoch 021:   1136 / 1474 loss=2.062, trans_loss=4.863, nll_loss=2.091, w2v_ctc_loss=0.734, task_loss=0, contrastive_loss=0.094, total=4112.95, n_correct=2754.76, ppl=4.26, accuracy=66.978, wps=14267.4, ups=1.73, wpb=8225.9, bsz=293.6, num_updates=30600, lr=8.08452e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=22828
2023-08-12 18:42:44 | INFO | train_inner | epoch 021:   1236 / 1474 loss=2.076, trans_loss=4.868, nll_loss=2.099, w2v_ctc_loss=0.738, task_loss=0, contrastive_loss=0.146, total=4161.16, n_correct=2781.23, ppl=4.28, accuracy=66.838, wps=14610.8, ups=1.76, wpb=8322.3, bsz=311.6, num_updates=30700, lr=8.07134e-05, gnorm=0.568, clip=0, loss_scale=8, train_wall=56, gb_free=16.5, wall=22885
2023-08-12 18:43:41 | INFO | train_inner | epoch 021:   1336 / 1474 loss=2.064, trans_loss=4.865, nll_loss=2.094, w2v_ctc_loss=0.73, task_loss=0, contrastive_loss=0.106, total=4138.28, n_correct=2774.74, ppl=4.27, accuracy=67.051, wps=14449.6, ups=1.75, wpb=8276.6, bsz=309.9, num_updates=30800, lr=8.05823e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=57, gb_free=16.5, wall=22943
2023-08-12 18:44:40 | INFO | train_inner | epoch 021:   1436 / 1474 loss=2.088, trans_loss=4.877, nll_loss=2.11, w2v_ctc_loss=0.751, task_loss=0, contrastive_loss=0.155, total=4136.94, n_correct=2755.83, ppl=4.32, accuracy=66.615, wps=14144.4, ups=1.71, wpb=8273.9, bsz=305, num_updates=30900, lr=8.04518e-05, gnorm=0.572, clip=0, loss_scale=8, train_wall=58, gb_free=16.7, wall=23001
2023-08-12 18:45:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
2023-08-12 18:45:24 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 4.164 | trans_loss 5.18 | nll_loss 2.445 | w2v_ctc_loss 1.393 | task_loss 0 | contrastive_loss 0.299 | total 4003.4 | n_correct 2660 | ppl 5.44 | accuracy 66.444 | uer 19.176 | wer 21.218 | raw_wer 21.218 | bleu 21.78 | wps 2322.9 | wpb 4003.4 | bsz 141.8 | num_updates 30938 | best_bleu 21.9
2023-08-12 18:45:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30938 updates
2023-08-12 18:45:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.7805.pt
2023-08-12 18:45:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.7805.pt
2023-08-12 18:45:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_21.7805.pt (epoch 21 @ 30938 updates, score 21.78) (writing took 22.576692402362823 seconds)
2023-08-12 18:45:48 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-08-12 18:45:48 | INFO | train | epoch 021 | loss 2.071 | trans_loss 4.866 | nll_loss 2.095 | w2v_ctc_loss 0.735 | task_loss 0 | contrastive_loss 0.134 | total 4138.65 | n_correct 2769.39 | ppl 4.27 | accuracy 66.915 | wps 12585.5 | ups 1.52 | wpb 8277.3 | bsz 305.7 | num_updates 30938 | lr 8.04024e-05 | gnorm 0.572 | clip 0 | loss_scale 8 | train_wall 843 | gb_free 15.5 | wall 23069
2023-08-12 18:45:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 18:45:48 | INFO | fairseq.trainer | begin training epoch 22
2023-08-12 18:45:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 18:46:30 | INFO | train_inner | epoch 022:     62 / 1474 loss=2.055, trans_loss=4.853, nll_loss=2.078, w2v_ctc_loss=0.733, task_loss=0, contrastive_loss=0.079, total=4137.1, n_correct=2784.27, ppl=4.22, accuracy=67.3, wps=7493.7, ups=0.91, wpb=8274.2, bsz=300.4, num_updates=31000, lr=8.03219e-05, gnorm=0.571, clip=0, loss_scale=8, train_wall=57, gb_free=12.8, wall=23112
2023-08-12 18:47:28 | INFO | train_inner | epoch 022:    162 / 1474 loss=2.062, trans_loss=4.846, nll_loss=2.069, w2v_ctc_loss=0.722, task_loss=0, contrastive_loss=0.153, total=4112.98, n_correct=2766.71, ppl=4.2, accuracy=67.268, wps=14334.1, ups=1.74, wpb=8226, bsz=307, num_updates=31100, lr=8.01927e-05, gnorm=0.564, clip=0, loss_scale=8, train_wall=57, gb_free=17.6, wall=23169
2023-08-12 18:48:25 | INFO | train_inner | epoch 022:    262 / 1474 loss=2.049, trans_loss=4.844, nll_loss=2.068, w2v_ctc_loss=0.718, task_loss=0, contrastive_loss=0.098, total=4271.21, n_correct=2880.83, ppl=4.19, accuracy=67.448, wps=14877.5, ups=1.74, wpb=8542.4, bsz=330.6, num_updates=31200, lr=8.00641e-05, gnorm=0.551, clip=0, loss_scale=8, train_wall=57, gb_free=14.4, wall=23226
2023-08-12 18:49:25 | INFO | train_inner | epoch 022:    362 / 1474 loss=2.089, trans_loss=4.856, nll_loss=2.082, w2v_ctc_loss=0.732, task_loss=0, contrastive_loss=0.252, total=4179.51, n_correct=2801.96, ppl=4.23, accuracy=67.04, wps=14056, ups=1.68, wpb=8359, bsz=310.4, num_updates=31300, lr=7.99361e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=59, gb_free=17.7, wall=23286
2023-08-12 18:50:23 | INFO | train_inner | epoch 022:    462 / 1474 loss=2.07, trans_loss=4.857, nll_loss=2.083, w2v_ctc_loss=0.733, task_loss=0, contrastive_loss=0.141, total=4143.46, n_correct=2776.92, ppl=4.24, accuracy=67.019, wps=14315.9, ups=1.73, wpb=8286.9, bsz=300.6, num_updates=31400, lr=7.98087e-05, gnorm=0.582, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=23344
2023-08-12 18:51:20 | INFO | train_inner | epoch 022:    562 / 1474 loss=2.052, trans_loss=4.85, nll_loss=2.074, w2v_ctc_loss=0.729, task_loss=0, contrastive_loss=0.084, total=4143.14, n_correct=2786.24, ppl=4.21, accuracy=67.249, wps=14437.3, ups=1.74, wpb=8286.3, bsz=304.4, num_updates=31500, lr=7.96819e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=23401
2023-08-12 18:52:17 | INFO | train_inner | epoch 022:    662 / 1474 loss=2.056, trans_loss=4.843, nll_loss=2.065, w2v_ctc_loss=0.71, task_loss=0, contrastive_loss=0.169, total=4146.54, n_correct=2798.07, ppl=4.19, accuracy=67.48, wps=14574.2, ups=1.76, wpb=8293.1, bsz=311.9, num_updates=31600, lr=7.95557e-05, gnorm=0.591, clip=0, loss_scale=8, train_wall=56, gb_free=13.5, wall=23458
2023-08-12 18:53:15 | INFO | train_inner | epoch 022:    762 / 1474 loss=2.053, trans_loss=4.849, nll_loss=2.073, w2v_ctc_loss=0.73, task_loss=0, contrastive_loss=0.087, total=4170.82, n_correct=2803.28, ppl=4.21, accuracy=67.212, wps=14385.9, ups=1.72, wpb=8341.6, bsz=303.9, num_updates=31700, lr=7.94301e-05, gnorm=0.571, clip=0, loss_scale=16, train_wall=58, gb_free=12.2, wall=23516
2023-08-12 18:54:12 | INFO | train_inner | epoch 022:    862 / 1474 loss=2.053, trans_loss=4.858, nll_loss=2.085, w2v_ctc_loss=0.727, task_loss=0, contrastive_loss=0.076, total=4077.65, n_correct=2731.5, ppl=4.24, accuracy=66.987, wps=14159, ups=1.74, wpb=8155.3, bsz=290.5, num_updates=31800, lr=7.93052e-05, gnorm=0.57, clip=0, loss_scale=16, train_wall=57, gb_free=17.1, wall=23574
2023-08-12 18:55:09 | INFO | train_inner | epoch 022:    962 / 1474 loss=2.049, trans_loss=4.85, nll_loss=2.074, w2v_ctc_loss=0.721, task_loss=0, contrastive_loss=0.084, total=4136.66, n_correct=2781.91, ppl=4.21, accuracy=67.25, wps=14511.2, ups=1.75, wpb=8273.3, bsz=304.7, num_updates=31900, lr=7.91808e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=57, gb_free=14.9, wall=23631
2023-08-12 18:56:07 | INFO | train_inner | epoch 022:   1062 / 1474 loss=2.066, trans_loss=4.844, nll_loss=2.068, w2v_ctc_loss=0.71, task_loss=0, contrastive_loss=0.238, total=4152.13, n_correct=2800.11, ppl=4.19, accuracy=67.438, wps=14558.5, ups=1.75, wpb=8304.3, bsz=313.8, num_updates=32000, lr=7.90569e-05, gnorm=0.556, clip=0, loss_scale=16, train_wall=57, gb_free=16.1, wall=23688
2023-08-12 18:56:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 18:56:29 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.163 | trans_loss 5.172 | nll_loss 2.437 | w2v_ctc_loss 1.412 | task_loss 0 | contrastive_loss 0.297 | total 4003.4 | n_correct 2654.9 | ppl 5.42 | accuracy 66.316 | uer 18.552 | wer 20.417 | raw_wer 20.417 | bleu 22.32 | wps 2305 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 22.32
2023-08-12 18:56:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-08-12 18:56:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_22_32000.pt
2023-08-12 18:56:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_22_32000.pt
2023-08-12 18:57:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 22.32) (writing took 48.80471231974661 seconds)
2023-08-12 18:58:17 | INFO | train_inner | epoch 022:   1162 / 1474 loss=2.073, trans_loss=4.871, nll_loss=2.102, w2v_ctc_loss=0.741, task_loss=0, contrastive_loss=0.128, total=4102.27, n_correct=2739.4, ppl=4.29, accuracy=66.778, wps=6310.4, ups=0.77, wpb=8204.5, bsz=296.1, num_updates=32100, lr=7.89337e-05, gnorm=0.593, clip=0, loss_scale=16, train_wall=57, gb_free=16.9, wall=23818
2023-08-12 18:59:14 | INFO | train_inner | epoch 022:   1262 / 1474 loss=2.06, trans_loss=4.862, nll_loss=2.09, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.118, total=4179.1, n_correct=2805.42, ppl=4.26, accuracy=67.13, wps=14568.4, ups=1.74, wpb=8358.2, bsz=321.7, num_updates=32200, lr=7.8811e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=57, gb_free=17.2, wall=23875
2023-08-12 19:00:11 | INFO | train_inner | epoch 022:   1362 / 1474 loss=2.055, trans_loss=4.848, nll_loss=2.072, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.144, total=4061.14, n_correct=2734.76, ppl=4.2, accuracy=67.34, wps=14248, ups=1.75, wpb=8122.3, bsz=299.1, num_updates=32300, lr=7.86889e-05, gnorm=0.565, clip=0, loss_scale=16, train_wall=57, gb_free=17, wall=23932
2023-08-12 19:01:08 | INFO | train_inner | epoch 022:   1462 / 1474 loss=2.062, trans_loss=4.866, nll_loss=2.094, w2v_ctc_loss=0.738, task_loss=0, contrastive_loss=0.092, total=4083.08, n_correct=2731.83, ppl=4.27, accuracy=66.906, wps=14242.6, ups=1.74, wpb=8166.2, bsz=289.3, num_updates=32400, lr=7.85674e-05, gnorm=0.566, clip=0, loss_scale=16, train_wall=57, gb_free=16.2, wall=23990
2023-08-12 19:01:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 19:01:38 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 4.141 | trans_loss 5.169 | nll_loss 2.43 | w2v_ctc_loss 1.353 | task_loss 0 | contrastive_loss 0.291 | total 4003.4 | n_correct 2664.4 | ppl 5.39 | accuracy 66.553 | uer 18.223 | wer 20.148 | raw_wer 20.148 | bleu 22.3 | wps 2171.8 | wpb 4003.4 | bsz 141.8 | num_updates 32412 | best_bleu 22.32
2023-08-12 19:01:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32412 updates
2023-08-12 19:01:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.3008.pt
2023-08-12 19:01:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.3008.pt
2023-08-12 19:02:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.3008.pt (epoch 22 @ 32412 updates, score 22.3) (writing took 41.92792543210089 seconds)
2023-08-12 19:02:23 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-08-12 19:02:23 | INFO | train | epoch 022 | loss 2.06 | trans_loss 4.853 | nll_loss 2.078 | w2v_ctc_loss 0.725 | task_loss 0 | contrastive_loss 0.131 | total 4138.65 | n_correct 2780.98 | ppl 4.22 | accuracy 67.195 | wps 12252.5 | ups 1.48 | wpb 8277.3 | bsz 305.7 | num_updates 32412 | lr 7.85529e-05 | gnorm 0.568 | clip 0 | loss_scale 16 | train_wall 841 | gb_free 12 | wall 24065
2023-08-12 19:02:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 19:02:24 | INFO | fairseq.trainer | begin training epoch 23
2023-08-12 19:02:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 19:03:22 | INFO | train_inner | epoch 023:     88 / 1474 loss=2.046, trans_loss=4.837, nll_loss=2.058, w2v_ctc_loss=0.729, task_loss=0, contrastive_loss=0.081, total=4093.3, n_correct=2763.41, ppl=4.16, accuracy=67.511, wps=6105.3, ups=0.75, wpb=8186.6, bsz=301.3, num_updates=32500, lr=7.84465e-05, gnorm=0.572, clip=0, loss_scale=16, train_wall=57, gb_free=16.1, wall=24124
2023-08-12 19:04:20 | INFO | train_inner | epoch 023:    188 / 1474 loss=2.037, trans_loss=4.83, nll_loss=2.048, w2v_ctc_loss=0.713, task_loss=0, contrastive_loss=0.079, total=4116.26, n_correct=2782.07, ppl=4.14, accuracy=67.587, wps=14319.4, ups=1.74, wpb=8232.5, bsz=294.4, num_updates=32600, lr=7.8326e-05, gnorm=0.567, clip=0, loss_scale=16, train_wall=57, gb_free=17.1, wall=24181
2023-08-12 19:05:17 | INFO | train_inner | epoch 023:    288 / 1474 loss=2.054, trans_loss=4.843, nll_loss=2.065, w2v_ctc_loss=0.708, task_loss=0, contrastive_loss=0.155, total=4148.03, n_correct=2797.15, ppl=4.18, accuracy=67.433, wps=14396.9, ups=1.74, wpb=8296.1, bsz=305.7, num_updates=32700, lr=7.82062e-05, gnorm=0.563, clip=0, loss_scale=16, train_wall=57, gb_free=17.4, wall=24239
2023-08-12 19:06:15 | INFO | train_inner | epoch 023:    388 / 1474 loss=2.034, trans_loss=4.831, nll_loss=2.049, w2v_ctc_loss=0.713, task_loss=0, contrastive_loss=0.072, total=4115.99, n_correct=2783.38, ppl=4.14, accuracy=67.624, wps=14358.5, ups=1.74, wpb=8232, bsz=294.1, num_updates=32800, lr=7.80869e-05, gnorm=0.565, clip=0, loss_scale=16, train_wall=57, gb_free=16, wall=24296
2023-08-12 19:07:13 | INFO | train_inner | epoch 023:    488 / 1474 loss=2.051, trans_loss=4.84, nll_loss=2.062, w2v_ctc_loss=0.716, task_loss=0, contrastive_loss=0.126, total=4156.5, n_correct=2804.98, ppl=4.18, accuracy=67.484, wps=14327.7, ups=1.72, wpb=8313, bsz=312.2, num_updates=32900, lr=7.79681e-05, gnorm=0.564, clip=0, loss_scale=16, train_wall=58, gb_free=17.3, wall=24354
2023-08-12 19:08:10 | INFO | train_inner | epoch 023:    588 / 1474 loss=2.037, trans_loss=4.833, nll_loss=2.052, w2v_ctc_loss=0.713, task_loss=0, contrastive_loss=0.075, total=4174.84, n_correct=2825.98, ppl=4.15, accuracy=67.691, wps=14643.1, ups=1.75, wpb=8349.7, bsz=316.3, num_updates=33000, lr=7.78499e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=57, gb_free=17.3, wall=24411
2023-08-12 19:09:07 | INFO | train_inner | epoch 023:    688 / 1474 loss=2.051, trans_loss=4.843, nll_loss=2.066, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.116, total=4139.68, n_correct=2792.24, ppl=4.19, accuracy=67.451, wps=14509.6, ups=1.75, wpb=8279.4, bsz=302.2, num_updates=33100, lr=7.77322e-05, gnorm=0.597, clip=0, loss_scale=16, train_wall=57, gb_free=16.7, wall=24468
2023-08-12 19:10:05 | INFO | train_inner | epoch 023:    788 / 1474 loss=2.048, trans_loss=4.844, nll_loss=2.067, w2v_ctc_loss=0.723, task_loss=0, contrastive_loss=0.093, total=4147.97, n_correct=2796.58, ppl=4.19, accuracy=67.42, wps=14345.4, ups=1.73, wpb=8295.9, bsz=305.8, num_updates=33200, lr=7.76151e-05, gnorm=0.559, clip=0, loss_scale=16, train_wall=57, gb_free=17, wall=24526
2023-08-12 19:11:02 | INFO | train_inner | epoch 023:    888 / 1474 loss=2.058, trans_loss=4.837, nll_loss=2.058, w2v_ctc_loss=0.715, task_loss=0, contrastive_loss=0.174, total=4182.69, n_correct=2825.97, ppl=4.16, accuracy=67.563, wps=14514.8, ups=1.74, wpb=8365.4, bsz=325, num_updates=33300, lr=7.74984e-05, gnorm=0.56, clip=0, loss_scale=16, train_wall=57, gb_free=16.9, wall=24584
2023-08-12 19:12:00 | INFO | train_inner | epoch 023:    988 / 1474 loss=2.086, trans_loss=4.844, nll_loss=2.067, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.333, total=4165.01, n_correct=2799.13, ppl=4.19, accuracy=67.206, wps=14437.5, ups=1.73, wpb=8330, bsz=309.6, num_updates=33400, lr=7.73823e-05, gnorm=0.58, clip=0, loss_scale=16, train_wall=57, gb_free=17.7, wall=24641
2023-08-12 19:12:58 | INFO | train_inner | epoch 023:   1088 / 1474 loss=2.046, trans_loss=4.846, nll_loss=2.069, w2v_ctc_loss=0.723, task_loss=0, contrastive_loss=0.084, total=4092.37, n_correct=2758.42, ppl=4.2, accuracy=67.404, wps=14232.7, ups=1.74, wpb=8184.7, bsz=290.8, num_updates=33500, lr=7.72667e-05, gnorm=0.574, clip=0, loss_scale=16, train_wall=57, gb_free=17.4, wall=24699
2023-08-12 19:13:55 | INFO | train_inner | epoch 023:   1188 / 1474 loss=2.043, trans_loss=4.846, nll_loss=2.071, w2v_ctc_loss=0.718, task_loss=0, contrastive_loss=0.075, total=4164.9, n_correct=2805.14, ppl=4.2, accuracy=67.352, wps=14444.2, ups=1.73, wpb=8329.8, bsz=309.2, num_updates=33600, lr=7.71517e-05, gnorm=0.562, clip=0, loss_scale=16, train_wall=57, gb_free=13.9, wall=24757
2023-08-12 19:14:53 | INFO | train_inner | epoch 023:   1288 / 1474 loss=2.038, trans_loss=4.839, nll_loss=2.061, w2v_ctc_loss=0.709, task_loss=0, contrastive_loss=0.085, total=4136.96, n_correct=2795.39, ppl=4.17, accuracy=67.571, wps=14319, ups=1.73, wpb=8273.9, bsz=309.8, num_updates=33700, lr=7.70371e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=57, gb_free=16.7, wall=24814
2023-08-12 19:15:51 | INFO | train_inner | epoch 023:   1388 / 1474 loss=2.062, trans_loss=4.854, nll_loss=2.08, w2v_ctc_loss=0.721, task_loss=0, contrastive_loss=0.143, total=4142.84, n_correct=2789.09, ppl=4.23, accuracy=67.323, wps=14366.8, ups=1.73, wpb=8285.7, bsz=304.8, num_updates=33800, lr=7.69231e-05, gnorm=0.571, clip=0, loss_scale=32, train_wall=57, gb_free=16.2, wall=24872
2023-08-12 19:16:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-08-12 19:16:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 19:17:03 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 4.151 | trans_loss 5.166 | nll_loss 2.425 | w2v_ctc_loss 1.399 | task_loss 0 | contrastive_loss 0.288 | total 4003.4 | n_correct 2666.9 | ppl 5.37 | accuracy 66.616 | uer 17.862 | wer 19.653 | raw_wer 19.653 | bleu 22.09 | wps 2212.4 | wpb 4003.4 | bsz 141.8 | num_updates 33885 | best_bleu 22.32
2023-08-12 19:17:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33885 updates
2023-08-12 19:17:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0906.pt
2023-08-12 19:17:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0906.pt
2023-08-12 19:17:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0906.pt (epoch 23 @ 33885 updates, score 22.09) (writing took 20.235264776274562 seconds)
2023-08-12 19:17:24 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-08-12 19:17:24 | INFO | train | epoch 023 | loss 2.05 | trans_loss 4.841 | nll_loss 2.063 | w2v_ctc_loss 0.717 | task_loss 0 | contrastive_loss 0.121 | total 4136.71 | n_correct 2790.88 | ppl 4.18 | accuracy 67.466 | wps 13536.9 | ups 1.64 | wpb 8273.4 | bsz 305 | num_updates 33885 | lr 7.68265e-05 | gnorm 0.567 | clip 0 | loss_scale 16 | train_wall 841 | gb_free 13.9 | wall 24965
2023-08-12 19:17:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 19:17:24 | INFO | fairseq.trainer | begin training epoch 24
2023-08-12 19:17:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 19:17:40 | INFO | train_inner | epoch 024:     15 / 1474 loss=2.063, trans_loss=4.85, nll_loss=2.075, w2v_ctc_loss=0.714, task_loss=0, contrastive_loss=0.174, total=4082.7, n_correct=2746.86, ppl=4.21, accuracy=67.28, wps=7506.2, ups=0.92, wpb=8165.4, bsz=301.9, num_updates=33900, lr=7.68095e-05, gnorm=0.577, clip=0, loss_scale=16, train_wall=58, gb_free=17, wall=24981
2023-08-12 19:18:38 | INFO | train_inner | epoch 024:    115 / 1474 loss=2.046, trans_loss=4.821, nll_loss=2.036, w2v_ctc_loss=0.702, task_loss=0, contrastive_loss=0.177, total=4147.74, n_correct=2816.22, ppl=4.1, accuracy=67.898, wps=14306.5, ups=1.72, wpb=8295.5, bsz=317.7, num_updates=34000, lr=7.66965e-05, gnorm=0.57, clip=0, loss_scale=16, train_wall=58, gb_free=15.9, wall=25039
2023-08-12 19:18:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 19:19:00 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.174 | trans_loss 5.171 | nll_loss 2.43 | w2v_ctc_loss 1.463 | task_loss 0 | contrastive_loss 0.293 | total 4003.4 | n_correct 2657.8 | ppl 5.39 | accuracy 66.389 | uer 19.268 | wer 21.606 | raw_wer 21.606 | bleu 21.9 | wps 2364.5 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 22.32
2023-08-12 19:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-08-12 19:19:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_24_34000.pt
2023-08-12 19:19:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_24_34000.pt
2023-08-12 19:19:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 21.9) (writing took 23.067544331774116 seconds)
2023-08-12 19:20:22 | INFO | train_inner | epoch 024:    215 / 1474 loss=2.069, trans_loss=4.827, nll_loss=2.045, w2v_ctc_loss=0.695, task_loss=0, contrastive_loss=0.291, total=4244.96, n_correct=2876.11, ppl=4.13, accuracy=67.754, wps=8158.4, ups=0.96, wpb=8489.9, bsz=340, num_updates=34100, lr=7.6584e-05, gnorm=0.559, clip=0, loss_scale=16, train_wall=58, gb_free=15.7, wall=25143
2023-08-12 19:21:19 | INFO | train_inner | epoch 024:    315 / 1474 loss=2.027, trans_loss=4.823, nll_loss=2.04, w2v_ctc_loss=0.7, task_loss=0, contrastive_loss=0.073, total=4144.64, n_correct=2813.19, ppl=4.11, accuracy=67.875, wps=14553.2, ups=1.76, wpb=8289.3, bsz=309.2, num_updates=34200, lr=7.64719e-05, gnorm=0.56, clip=0, loss_scale=16, train_wall=57, gb_free=16.7, wall=25200
2023-08-12 19:22:16 | INFO | train_inner | epoch 024:    415 / 1474 loss=2.067, trans_loss=4.829, nll_loss=2.048, w2v_ctc_loss=0.721, task_loss=0, contrastive_loss=0.216, total=4152.59, n_correct=2802.73, ppl=4.13, accuracy=67.494, wps=14358.7, ups=1.73, wpb=8305.2, bsz=297.1, num_updates=34300, lr=7.63604e-05, gnorm=0.608, clip=0, loss_scale=16, train_wall=57, gb_free=11.3, wall=25258
2023-08-12 19:23:14 | INFO | train_inner | epoch 024:    515 / 1474 loss=2.042, trans_loss=4.825, nll_loss=2.042, w2v_ctc_loss=0.708, task_loss=0, contrastive_loss=0.142, total=4135.54, n_correct=2803.53, ppl=4.12, accuracy=67.791, wps=14298.3, ups=1.73, wpb=8271.1, bsz=300.8, num_updates=34400, lr=7.62493e-05, gnorm=0.57, clip=0, loss_scale=16, train_wall=57, gb_free=16.5, wall=25316
2023-08-12 19:24:11 | INFO | train_inner | epoch 024:    615 / 1474 loss=2.032, trans_loss=4.824, nll_loss=2.042, w2v_ctc_loss=0.696, task_loss=0, contrastive_loss=0.101, total=4163.63, n_correct=2824.53, ppl=4.12, accuracy=67.838, wps=14546.9, ups=1.75, wpb=8327.3, bsz=309.2, num_updates=34500, lr=7.61387e-05, gnorm=0.56, clip=0, loss_scale=16, train_wall=57, gb_free=16.7, wall=25373
2023-08-12 19:25:09 | INFO | train_inner | epoch 024:    715 / 1474 loss=2.046, trans_loss=4.837, nll_loss=2.058, w2v_ctc_loss=0.712, task_loss=0, contrastive_loss=0.121, total=4100.27, n_correct=2768.07, ppl=4.16, accuracy=67.509, wps=14333, ups=1.75, wpb=8200.5, bsz=295.4, num_updates=34600, lr=7.60286e-05, gnorm=0.576, clip=0, loss_scale=16, train_wall=57, gb_free=15.8, wall=25430
2023-08-12 19:26:06 | INFO | train_inner | epoch 024:    815 / 1474 loss=2.039, trans_loss=4.838, nll_loss=2.059, w2v_ctc_loss=0.709, task_loss=0, contrastive_loss=0.091, total=4129.03, n_correct=2794.09, ppl=4.17, accuracy=67.669, wps=14341.6, ups=1.74, wpb=8258.1, bsz=307.8, num_updates=34700, lr=7.5919e-05, gnorm=0.568, clip=0, loss_scale=16, train_wall=57, gb_free=17.6, wall=25488
2023-08-12 19:27:04 | INFO | train_inner | epoch 024:    915 / 1474 loss=2.04, trans_loss=4.838, nll_loss=2.057, w2v_ctc_loss=0.723, task_loss=0, contrastive_loss=0.068, total=4035.8, n_correct=2720.56, ppl=4.16, accuracy=67.411, wps=14034.6, ups=1.74, wpb=8071.6, bsz=278.8, num_updates=34800, lr=7.58098e-05, gnorm=0.566, clip=0, loss_scale=16, train_wall=57, gb_free=12.6, wall=25545
2023-08-12 19:28:02 | INFO | train_inner | epoch 024:   1015 / 1474 loss=2.033, trans_loss=4.836, nll_loss=2.056, w2v_ctc_loss=0.705, task_loss=0, contrastive_loss=0.072, total=4124.2, n_correct=2788.74, ppl=4.16, accuracy=67.619, wps=14287.1, ups=1.73, wpb=8248.4, bsz=295.7, num_updates=34900, lr=7.57011e-05, gnorm=0.563, clip=0, loss_scale=16, train_wall=57, gb_free=13.5, wall=25603
2023-08-12 19:28:59 | INFO | train_inner | epoch 024:   1115 / 1474 loss=2.038, trans_loss=4.821, nll_loss=2.037, w2v_ctc_loss=0.709, task_loss=0, contrastive_loss=0.114, total=4133.96, n_correct=2803.44, ppl=4.1, accuracy=67.815, wps=14505.3, ups=1.75, wpb=8267.9, bsz=310.6, num_updates=35000, lr=7.55929e-05, gnorm=0.565, clip=0, loss_scale=16, train_wall=57, gb_free=16.5, wall=25660
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-12 19:29:50 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-12 19:29:57 | INFO | train_inner | epoch 024:   1216 / 1474 loss=2.037, trans_loss=4.835, nll_loss=2.056, w2v_ctc_loss=0.706, task_loss=0, contrastive_loss=0.082, total=4141.66, n_correct=2801.09, ppl=4.16, accuracy=67.632, wps=14281.8, ups=1.72, wpb=8283.3, bsz=306.8, num_updates=35100, lr=7.54851e-05, gnorm=0.618, clip=0, loss_scale=8, train_wall=58, gb_free=16.7, wall=25718
2023-08-12 19:30:55 | INFO | train_inner | epoch 024:   1316 / 1474 loss=2.039, trans_loss=4.836, nll_loss=2.057, w2v_ctc_loss=0.718, task_loss=0, contrastive_loss=0.077, total=4114.24, n_correct=2781.66, ppl=4.16, accuracy=67.611, wps=14191.9, ups=1.72, wpb=8228.5, bsz=296, num_updates=35200, lr=7.53778e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=58, gb_free=16.5, wall=25776
2023-08-12 19:31:52 | INFO | train_inner | epoch 024:   1416 / 1474 loss=2.036, trans_loss=4.836, nll_loss=2.058, w2v_ctc_loss=0.717, task_loss=0, contrastive_loss=0.072, total=4089.93, n_correct=2766.37, ppl=4.16, accuracy=67.639, wps=14346.6, ups=1.75, wpb=8179.9, bsz=291.2, num_updates=35300, lr=7.5271e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=57, gb_free=16.8, wall=25833
2023-08-12 19:32:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
2023-08-12 19:32:48 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 4.132 | trans_loss 5.164 | nll_loss 2.426 | w2v_ctc_loss 1.347 | task_loss 0 | contrastive_loss 0.284 | total 4003.4 | n_correct 2664.1 | ppl 5.37 | accuracy 66.546 | uer 17.84 | wer 19.809 | raw_wer 19.809 | bleu 22.01 | wps 2135.5 | wpb 4003.4 | bsz 141.8 | num_updates 35358 | best_bleu 22.32
2023-08-12 19:32:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35358 updates
2023-08-12 19:32:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0105.pt
2023-08-12 19:32:52 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0105.pt
2023-08-12 19:33:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0105.pt (epoch 24 @ 35358 updates, score 22.01) (writing took 39.334762901067734 seconds)
2023-08-12 19:33:31 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-08-12 19:33:31 | INFO | train | epoch 024 | loss 2.042 | trans_loss 4.831 | nll_loss 2.049 | w2v_ctc_loss 0.708 | task_loss 0 | contrastive_loss 0.126 | total 4137.9 | n_correct 2800.98 | ppl 4.14 | accuracy 67.691 | wps 12603.7 | ups 1.52 | wpb 8275.8 | bsz 305.4 | num_updates 35358 | lr 7.52092e-05 | gnorm 0.573 | clip 0 | loss_scale 8 | train_wall 841 | gb_free 16.2 | wall 25932
2023-08-12 19:33:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 19:33:31 | INFO | fairseq.trainer | begin training epoch 25
2023-08-12 19:33:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 19:34:02 | INFO | train_inner | epoch 025:     42 / 1474 loss=2.029, trans_loss=4.824, nll_loss=2.041, w2v_ctc_loss=0.704, task_loss=0, contrastive_loss=0.081, total=4161.84, n_correct=2828.46, ppl=4.12, accuracy=67.962, wps=6393.3, ups=0.77, wpb=8323.7, bsz=309.3, num_updates=35400, lr=7.51646e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=25963
2023-08-12 19:34:59 | INFO | train_inner | epoch 025:    142 / 1474 loss=2.017, trans_loss=4.808, nll_loss=2.02, w2v_ctc_loss=0.69, task_loss=0, contrastive_loss=0.081, total=4141.46, n_correct=2826.13, ppl=4.05, accuracy=68.24, wps=14372.9, ups=1.74, wpb=8282.9, bsz=311.5, num_updates=35500, lr=7.50587e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=26021
2023-08-12 19:35:57 | INFO | train_inner | epoch 025:    242 / 1474 loss=2.023, trans_loss=4.813, nll_loss=2.027, w2v_ctc_loss=0.697, task_loss=0, contrastive_loss=0.083, total=4122.7, n_correct=2802.36, ppl=4.07, accuracy=67.974, wps=14278.9, ups=1.73, wpb=8245.4, bsz=302.9, num_updates=35600, lr=7.49532e-05, gnorm=0.582, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=26078
2023-08-12 19:36:55 | INFO | train_inner | epoch 025:    342 / 1474 loss=2.029, trans_loss=4.815, nll_loss=2.028, w2v_ctc_loss=0.696, task_loss=0, contrastive_loss=0.113, total=4132.6, n_correct=2809, ppl=4.08, accuracy=67.972, wps=14350.8, ups=1.74, wpb=8265.2, bsz=294.3, num_updates=35700, lr=7.48481e-05, gnorm=0.576, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=26136
2023-08-12 19:37:52 | INFO | train_inner | epoch 025:    442 / 1474 loss=2.057, trans_loss=4.82, nll_loss=2.035, w2v_ctc_loss=0.718, task_loss=0, contrastive_loss=0.199, total=4169.31, n_correct=2824.37, ppl=4.1, accuracy=67.742, wps=14529.5, ups=1.74, wpb=8338.6, bsz=296.4, num_updates=35800, lr=7.47435e-05, gnorm=0.598, clip=0, loss_scale=8, train_wall=57, gb_free=15.3, wall=26193
2023-08-12 19:38:50 | INFO | train_inner | epoch 025:    542 / 1474 loss=2.033, trans_loss=4.826, nll_loss=2.043, w2v_ctc_loss=0.705, task_loss=0, contrastive_loss=0.084, total=4157.91, n_correct=2822.28, ppl=4.12, accuracy=67.877, wps=14446.3, ups=1.74, wpb=8315.8, bsz=314.5, num_updates=35900, lr=7.46393e-05, gnorm=0.58, clip=0, loss_scale=8, train_wall=57, gb_free=15.6, wall=26251
2023-08-12 19:39:47 | INFO | train_inner | epoch 025:    642 / 1474 loss=2.035, trans_loss=4.811, nll_loss=2.024, w2v_ctc_loss=0.699, task_loss=0, contrastive_loss=0.152, total=4158.52, n_correct=2827.98, ppl=4.07, accuracy=68.004, wps=14422.8, ups=1.73, wpb=8317, bsz=309.9, num_updates=36000, lr=7.45356e-05, gnorm=0.582, clip=0, loss_scale=8, train_wall=57, gb_free=16.5, wall=26309
2023-08-12 19:39:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 19:40:10 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.147 | trans_loss 5.167 | nll_loss 2.427 | w2v_ctc_loss 1.391 | task_loss 0 | contrastive_loss 0.288 | total 4003.4 | n_correct 2666.8 | ppl 5.38 | accuracy 66.613 | uer 18.43 | wer 20.454 | raw_wer 20.454 | bleu 22.12 | wps 2268.8 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 22.32
2023-08-12 19:40:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-08-12 19:40:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_25_36000.pt
2023-08-12 19:40:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_25_36000.pt
2023-08-12 19:40:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 22.12) (writing took 34.30444498360157 seconds)
2023-08-12 19:41:43 | INFO | train_inner | epoch 025:    742 / 1474 loss=2.041, trans_loss=4.816, nll_loss=2.031, w2v_ctc_loss=0.704, task_loss=0, contrastive_loss=0.145, total=4138.4, n_correct=2812.46, ppl=4.09, accuracy=67.96, wps=7167.1, ups=0.87, wpb=8276.8, bsz=304.5, num_updates=36100, lr=7.44323e-05, gnorm=0.567, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=26424
2023-08-12 19:42:40 | INFO | train_inner | epoch 025:    842 / 1474 loss=2.027, trans_loss=4.818, nll_loss=2.033, w2v_ctc_loss=0.693, task_loss=0, contrastive_loss=0.092, total=4171.94, n_correct=2836.49, ppl=4.09, accuracy=67.99, wps=14658.5, ups=1.76, wpb=8343.9, bsz=324, num_updates=36200, lr=7.43294e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=56, gb_free=15.7, wall=26481
2023-08-12 19:43:37 | INFO | train_inner | epoch 025:    942 / 1474 loss=2.046, trans_loss=4.826, nll_loss=2.044, w2v_ctc_loss=0.709, task_loss=0, contrastive_loss=0.151, total=4148.78, n_correct=2812.76, ppl=4.12, accuracy=67.797, wps=14398.5, ups=1.74, wpb=8297.6, bsz=314.3, num_updates=36300, lr=7.4227e-05, gnorm=0.576, clip=0, loss_scale=8, train_wall=57, gb_free=12.8, wall=26539
2023-08-12 19:44:35 | INFO | train_inner | epoch 025:   1042 / 1474 loss=2.061, trans_loss=4.83, nll_loss=2.049, w2v_ctc_loss=0.701, task_loss=0, contrastive_loss=0.26, total=4176.78, n_correct=2824.74, ppl=4.14, accuracy=67.63, wps=14420.2, ups=1.73, wpb=8353.6, bsz=310.2, num_updates=36400, lr=7.41249e-05, gnorm=0.638, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=26597
2023-08-12 19:45:32 | INFO | train_inner | epoch 025:   1142 / 1474 loss=2.025, trans_loss=4.823, nll_loss=2.039, w2v_ctc_loss=0.703, task_loss=0, contrastive_loss=0.068, total=4046.83, n_correct=2745.09, ppl=4.11, accuracy=67.833, wps=14163.6, ups=1.75, wpb=8093.7, bsz=287, num_updates=36500, lr=7.40233e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=57, gb_free=15.3, wall=26654
2023-08-12 19:46:29 | INFO | train_inner | epoch 025:   1242 / 1474 loss=2.031, trans_loss=4.831, nll_loss=2.051, w2v_ctc_loss=0.704, task_loss=0, contrastive_loss=0.076, total=4088.37, n_correct=2770.07, ppl=4.14, accuracy=67.755, wps=14422.4, ups=1.76, wpb=8176.7, bsz=295.1, num_updates=36600, lr=7.39221e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=56, gb_free=16.2, wall=26710
2023-08-12 19:47:27 | INFO | train_inner | epoch 025:   1342 / 1474 loss=2.046, trans_loss=4.823, nll_loss=2.039, w2v_ctc_loss=0.708, task_loss=0, contrastive_loss=0.172, total=4161.94, n_correct=2824.58, ppl=4.11, accuracy=67.867, wps=14416.1, ups=1.73, wpb=8323.9, bsz=307.5, num_updates=36700, lr=7.38213e-05, gnorm=0.634, clip=0, loss_scale=8, train_wall=57, gb_free=13, wall=26768
2023-08-12 19:48:25 | INFO | train_inner | epoch 025:   1442 / 1474 loss=2.048, trans_loss=4.835, nll_loss=2.056, w2v_ctc_loss=0.706, task_loss=0, contrastive_loss=0.136, total=4121.76, n_correct=2783.48, ppl=4.16, accuracy=67.531, wps=14215.1, ups=1.72, wpb=8243.5, bsz=308.1, num_updates=36800, lr=7.3721e-05, gnorm=0.577, clip=0, loss_scale=8, train_wall=58, gb_free=16.9, wall=26826
2023-08-12 19:48:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 19:49:05 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 4.152 | trans_loss 5.168 | nll_loss 2.427 | w2v_ctc_loss 1.399 | task_loss 0 | contrastive_loss 0.286 | total 4003.4 | n_correct 2663.3 | ppl 5.38 | accuracy 66.526 | uer 17.962 | wer 19.906 | raw_wer 19.906 | bleu 22.07 | wps 2458.3 | wpb 4003.4 | bsz 141.8 | num_updates 36832 | best_bleu 22.32
2023-08-12 19:49:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36832 updates
2023-08-12 19:49:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0702.pt
2023-08-12 19:49:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0702.pt
2023-08-12 19:49:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0702.pt (epoch 25 @ 36832 updates, score 22.07) (writing took 21.918917782604694 seconds)
2023-08-12 19:49:27 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-08-12 19:49:27 | INFO | train | epoch 025 | loss 2.036 | trans_loss 4.821 | nll_loss 2.037 | w2v_ctc_loss 0.702 | task_loss 0 | contrastive_loss 0.127 | total 4138.65 | n_correct 2808.77 | ppl 4.1 | accuracy 67.867 | wps 12758.5 | ups 1.54 | wpb 8277.3 | bsz 305.7 | num_updates 36832 | lr 7.36889e-05 | gnorm 0.581 | clip 0 | loss_scale 8 | train_wall 841 | gb_free 14.5 | wall 26888
2023-08-12 19:49:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 19:49:27 | INFO | fairseq.trainer | begin training epoch 26
2023-08-12 19:49:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 19:49:36 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 19:50:14 | INFO | train_inner | epoch 026:     69 / 1474 loss=2.02, trans_loss=4.805, nll_loss=2.016, w2v_ctc_loss=0.691, task_loss=0, contrastive_loss=0.091, total=4156.4, n_correct=2829.94, ppl=4.05, accuracy=68.086, wps=7636.2, ups=0.92, wpb=8312.8, bsz=312, num_updates=36900, lr=7.3621e-05, gnorm=0.582, clip=0, loss_scale=4, train_wall=57, gb_free=12.3, wall=26935
2023-08-12 19:51:11 | INFO | train_inner | epoch 026:    169 / 1474 loss=2.048, trans_loss=4.805, nll_loss=2.017, w2v_ctc_loss=0.677, task_loss=0, contrastive_loss=0.277, total=4269.48, n_correct=2918.81, ppl=4.05, accuracy=68.365, wps=14805.4, ups=1.73, wpb=8539, bsz=338.4, num_updates=37000, lr=7.35215e-05, gnorm=0.573, clip=0, loss_scale=4, train_wall=57, gb_free=17.2, wall=26993
2023-08-12 19:52:09 | INFO | train_inner | epoch 026:    269 / 1474 loss=2.039, trans_loss=4.807, nll_loss=2.019, w2v_ctc_loss=0.705, task_loss=0, contrastive_loss=0.165, total=4124.11, n_correct=2808.52, ppl=4.05, accuracy=68.1, wps=14295.1, ups=1.73, wpb=8248.2, bsz=307.4, num_updates=37100, lr=7.34223e-05, gnorm=0.627, clip=0, loss_scale=4, train_wall=57, gb_free=15.2, wall=27050
2023-08-12 19:53:07 | INFO | train_inner | epoch 026:    369 / 1474 loss=2.027, trans_loss=4.807, nll_loss=2.019, w2v_ctc_loss=0.693, task_loss=0, contrastive_loss=0.12, total=4165.17, n_correct=2840.12, ppl=4.05, accuracy=68.187, wps=14471.2, ups=1.74, wpb=8330.3, bsz=314.8, num_updates=37200, lr=7.33236e-05, gnorm=0.578, clip=0, loss_scale=4, train_wall=57, gb_free=16.2, wall=27108
2023-08-12 19:54:04 | INFO | train_inner | epoch 026:    469 / 1474 loss=2.042, trans_loss=4.803, nll_loss=2.014, w2v_ctc_loss=0.713, task_loss=0, contrastive_loss=0.173, total=4159.48, n_correct=2835.34, ppl=4.04, accuracy=68.166, wps=14493.8, ups=1.74, wpb=8319, bsz=313, num_updates=37300, lr=7.32252e-05, gnorm=0.813, clip=1, loss_scale=4, train_wall=57, gb_free=13.4, wall=27165
2023-08-12 19:55:02 | INFO | train_inner | epoch 026:    569 / 1474 loss=2.034, trans_loss=4.813, nll_loss=2.027, w2v_ctc_loss=0.719, task_loss=0, contrastive_loss=0.089, total=4165.04, n_correct=2829.85, ppl=4.07, accuracy=67.943, wps=14485.5, ups=1.74, wpb=8330.1, bsz=305.4, num_updates=37400, lr=7.31272e-05, gnorm=0.626, clip=0, loss_scale=4, train_wall=57, gb_free=15.8, wall=27223
2023-08-12 19:55:59 | INFO | train_inner | epoch 026:    669 / 1474 loss=2.019, trans_loss=4.811, nll_loss=2.023, w2v_ctc_loss=0.695, task_loss=0, contrastive_loss=0.072, total=4129.35, n_correct=2809.44, ppl=4.06, accuracy=68.036, wps=14300.9, ups=1.73, wpb=8258.7, bsz=296.8, num_updates=37500, lr=7.30297e-05, gnorm=0.561, clip=0, loss_scale=4, train_wall=57, gb_free=17.5, wall=27281
2023-08-12 19:56:57 | INFO | train_inner | epoch 026:    769 / 1474 loss=2.044, trans_loss=4.816, nll_loss=2.03, w2v_ctc_loss=0.697, task_loss=0, contrastive_loss=0.188, total=4094.41, n_correct=2776.16, ppl=4.09, accuracy=67.804, wps=14259.6, ups=1.74, wpb=8188.8, bsz=299.5, num_updates=37600, lr=7.29325e-05, gnorm=0.566, clip=0, loss_scale=4, train_wall=57, gb_free=16.4, wall=27338
2023-08-12 19:57:54 | INFO | train_inner | epoch 026:    869 / 1474 loss=2.027, trans_loss=4.812, nll_loss=2.025, w2v_ctc_loss=0.704, task_loss=0, contrastive_loss=0.087, total=4174.8, n_correct=2840.3, ppl=4.07, accuracy=68.034, wps=14544, ups=1.74, wpb=8349.6, bsz=306.1, num_updates=37700, lr=7.28357e-05, gnorm=0.604, clip=0, loss_scale=4, train_wall=57, gb_free=17.2, wall=27396
2023-08-12 19:58:52 | INFO | train_inner | epoch 026:    969 / 1474 loss=2.03, trans_loss=4.816, nll_loss=2.03, w2v_ctc_loss=0.686, task_loss=0, contrastive_loss=0.139, total=4143.83, n_correct=2815.13, ppl=4.08, accuracy=67.935, wps=14440.1, ups=1.74, wpb=8287.7, bsz=302.3, num_updates=37800, lr=7.27393e-05, gnorm=0.575, clip=0, loss_scale=4, train_wall=57, gb_free=17.6, wall=27453
2023-08-12 19:59:49 | INFO | train_inner | epoch 026:   1069 / 1474 loss=2.022, trans_loss=4.815, nll_loss=2.028, w2v_ctc_loss=0.701, task_loss=0, contrastive_loss=0.073, total=4115.98, n_correct=2799.2, ppl=4.08, accuracy=68.008, wps=14253.7, ups=1.73, wpb=8232, bsz=293.3, num_updates=37900, lr=7.26433e-05, gnorm=0.627, clip=0, loss_scale=4, train_wall=57, gb_free=15.4, wall=27511
2023-08-12 20:00:47 | INFO | train_inner | epoch 026:   1169 / 1474 loss=2.04, trans_loss=4.826, nll_loss=2.044, w2v_ctc_loss=0.712, task_loss=0, contrastive_loss=0.114, total=4120.78, n_correct=2791.84, ppl=4.12, accuracy=67.75, wps=14232.7, ups=1.73, wpb=8241.6, bsz=299.5, num_updates=38000, lr=7.25476e-05, gnorm=0.659, clip=0, loss_scale=4, train_wall=57, gb_free=16.4, wall=27569
2023-08-12 20:00:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 20:01:10 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.163 | trans_loss 5.175 | nll_loss 2.439 | w2v_ctc_loss 1.419 | task_loss 0 | contrastive_loss 0.291 | total 4003.4 | n_correct 2660.7 | ppl 5.42 | accuracy 66.461 | uer 18.902 | wer 21.136 | raw_wer 21.136 | bleu 21.99 | wps 2288 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 22.32
2023-08-12 20:01:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-08-12 20:01:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_26_38000.pt
2023-08-12 20:01:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_26_38000.pt
2023-08-12 20:01:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 21.99) (writing took 40.460295701399446 seconds)
2023-08-12 20:02:52 | INFO | train_inner | epoch 026:   1269 / 1474 loss=2.031, trans_loss=4.829, nll_loss=2.046, w2v_ctc_loss=0.711, task_loss=0, contrastive_loss=0.075, total=4002.48, n_correct=2710.9, ppl=4.13, accuracy=67.731, wps=6445.5, ups=0.81, wpb=8005, bsz=280, num_updates=38100, lr=7.24524e-05, gnorm=0.572, clip=0, loss_scale=4, train_wall=57, gb_free=16.2, wall=27693
2023-08-12 20:03:49 | INFO | train_inner | epoch 026:   1369 / 1474 loss=2.025, trans_loss=4.819, nll_loss=2.035, w2v_ctc_loss=0.692, task_loss=0, contrastive_loss=0.087, total=4156.47, n_correct=2827.14, ppl=4.1, accuracy=68.018, wps=14415.9, ups=1.73, wpb=8312.9, bsz=312.4, num_updates=38200, lr=7.23575e-05, gnorm=0.605, clip=0, loss_scale=4, train_wall=57, gb_free=17.3, wall=27750
2023-08-12 20:04:47 | INFO | train_inner | epoch 026:   1469 / 1474 loss=2.017, trans_loss=4.812, nll_loss=2.026, w2v_ctc_loss=0.686, task_loss=0, contrastive_loss=0.08, total=4144.2, n_correct=2827.25, ppl=4.07, accuracy=68.222, wps=14349.2, ups=1.73, wpb=8288.4, bsz=312.6, num_updates=38300, lr=7.22629e-05, gnorm=0.554, clip=0, loss_scale=4, train_wall=57, gb_free=16.5, wall=27808
2023-08-12 20:04:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 20:05:12 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 4.162 | trans_loss 5.162 | nll_loss 2.42 | w2v_ctc_loss 1.451 | task_loss 0 | contrastive_loss 0.285 | total 4003.4 | n_correct 2667.2 | ppl 5.35 | accuracy 66.623 | uer 18.955 | wer 21.289 | raw_wer 21.289 | bleu 22.08 | wps 2385.2 | wpb 4003.4 | bsz 141.8 | num_updates 38305 | best_bleu 22.32
2023-08-12 20:05:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38305 updates
2023-08-12 20:05:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0808.pt
2023-08-12 20:05:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0808.pt
2023-08-12 20:05:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.0808.pt (epoch 26 @ 38305 updates, score 22.08) (writing took 18.961854737251997 seconds)
2023-08-12 20:05:31 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-08-12 20:05:31 | INFO | train | epoch 026 | loss 2.031 | trans_loss 4.813 | nll_loss 2.026 | w2v_ctc_loss 0.699 | task_loss 0 | contrastive_loss 0.124 | total 4138.03 | n_correct 2815.3 | ppl 4.07 | accuracy 68.035 | wps 12647.9 | ups 1.53 | wpb 8276.1 | bsz 305.5 | num_updates 38305 | lr 7.22582e-05 | gnorm 0.609 | clip 0.1 | loss_scale 4 | train_wall 842 | gb_free 16 | wall 27852
2023-08-12 20:05:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 20:05:31 | INFO | fairseq.trainer | begin training epoch 27
2023-08-12 20:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 20:06:33 | INFO | train_inner | epoch 027:     95 / 1474 loss=1.998, trans_loss=4.779, nll_loss=1.982, w2v_ctc_loss=0.679, task_loss=0, contrastive_loss=0.063, total=4072.98, n_correct=2797.92, ppl=3.95, accuracy=68.695, wps=7692.9, ups=0.94, wpb=8146, bsz=284.4, num_updates=38400, lr=7.21688e-05, gnorm=0.559, clip=0, loss_scale=4, train_wall=57, gb_free=16.9, wall=27914
2023-08-12 20:07:30 | INFO | train_inner | epoch 027:    195 / 1474 loss=2.014, trans_loss=4.792, nll_loss=1.999, w2v_ctc_loss=0.689, task_loss=0, contrastive_loss=0.089, total=4189.84, n_correct=2869.28, ppl=4, accuracy=68.482, wps=14548.2, ups=1.74, wpb=8379.7, bsz=323.5, num_updates=38500, lr=7.2075e-05, gnorm=0.571, clip=0, loss_scale=4, train_wall=57, gb_free=16.8, wall=27972
2023-08-12 20:08:28 | INFO | train_inner | epoch 027:    295 / 1474 loss=2.019, trans_loss=4.802, nll_loss=2.012, w2v_ctc_loss=0.701, task_loss=0, contrastive_loss=0.074, total=4168.47, n_correct=2847.29, ppl=4.03, accuracy=68.305, wps=14592.8, ups=1.75, wpb=8336.9, bsz=306.2, num_updates=38600, lr=7.19816e-05, gnorm=0.561, clip=0, loss_scale=4, train_wall=57, gb_free=17.4, wall=28029
2023-08-12 20:09:26 | INFO | train_inner | epoch 027:    395 / 1474 loss=2.051, trans_loss=4.807, nll_loss=2.019, w2v_ctc_loss=0.696, task_loss=0, contrastive_loss=0.256, total=4082.79, n_correct=2779.71, ppl=4.05, accuracy=68.084, wps=13917.5, ups=1.7, wpb=8165.6, bsz=299.1, num_updates=38700, lr=7.18885e-05, gnorm=0.693, clip=0, loss_scale=4, train_wall=58, gb_free=16.8, wall=28088
2023-08-12 20:10:24 | INFO | train_inner | epoch 027:    495 / 1474 loss=2.041, trans_loss=4.81, nll_loss=2.023, w2v_ctc_loss=0.692, task_loss=0, contrastive_loss=0.192, total=4236.86, n_correct=2885.86, ppl=4.06, accuracy=68.113, wps=14652.4, ups=1.73, wpb=8473.7, bsz=329, num_updates=38800, lr=7.17958e-05, gnorm=0.568, clip=0, loss_scale=4, train_wall=57, gb_free=17, wall=28145
2023-08-12 20:11:21 | INFO | train_inner | epoch 027:    595 / 1474 loss=2.027, trans_loss=4.798, nll_loss=2.007, w2v_ctc_loss=0.695, task_loss=0, contrastive_loss=0.133, total=4135.74, n_correct=2826.56, ppl=4.02, accuracy=68.345, wps=14441, ups=1.75, wpb=8271.5, bsz=312, num_updates=38900, lr=7.17035e-05, gnorm=0.615, clip=0, loss_scale=8, train_wall=57, gb_free=15, wall=28203
2023-08-12 20:12:19 | INFO | train_inner | epoch 027:    695 / 1474 loss=2.026, trans_loss=4.806, nll_loss=2.018, w2v_ctc_loss=0.694, task_loss=0, contrastive_loss=0.111, total=4170.03, n_correct=2843.54, ppl=4.05, accuracy=68.19, wps=14540.3, ups=1.74, wpb=8340.1, bsz=306.8, num_updates=39000, lr=7.16115e-05, gnorm=0.585, clip=0, loss_scale=8, train_wall=57, gb_free=17.8, wall=28260
2023-08-12 20:13:16 | INFO | train_inner | epoch 027:    795 / 1474 loss=2.017, trans_loss=4.804, nll_loss=2.015, w2v_ctc_loss=0.699, task_loss=0, contrastive_loss=0.074, total=4105.12, n_correct=2800.77, ppl=4.04, accuracy=68.226, wps=14296.6, ups=1.74, wpb=8210.2, bsz=293.7, num_updates=39100, lr=7.15199e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=57, gb_free=17.5, wall=28317
2023-08-12 20:14:14 | INFO | train_inner | epoch 027:    895 / 1474 loss=2.012, trans_loss=4.808, nll_loss=2.019, w2v_ctc_loss=0.688, task_loss=0, contrastive_loss=0.067, total=4105.85, n_correct=2802.42, ppl=4.05, accuracy=68.254, wps=14269.6, ups=1.74, wpb=8211.7, bsz=293.4, num_updates=39200, lr=7.14286e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=28375
2023-08-12 20:15:12 | INFO | train_inner | epoch 027:    995 / 1474 loss=2.045, trans_loss=4.806, nll_loss=2.017, w2v_ctc_loss=0.689, task_loss=0, contrastive_loss=0.254, total=4181.76, n_correct=2850.91, ppl=4.05, accuracy=68.175, wps=14444.1, ups=1.73, wpb=8363.5, bsz=313.7, num_updates=39300, lr=7.13376e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=28433
2023-08-12 20:16:09 | INFO | train_inner | epoch 027:   1095 / 1474 loss=2.014, trans_loss=4.802, nll_loss=2.013, w2v_ctc_loss=0.688, task_loss=0, contrastive_loss=0.084, total=4159.93, n_correct=2840.76, ppl=4.04, accuracy=68.289, wps=14571.5, ups=1.75, wpb=8319.9, bsz=307.8, num_updates=39400, lr=7.1247e-05, gnorm=0.568, clip=0, loss_scale=8, train_wall=57, gb_free=15.8, wall=28490
2023-08-12 20:17:07 | INFO | train_inner | epoch 027:   1195 / 1474 loss=2.021, trans_loss=4.808, nll_loss=2.019, w2v_ctc_loss=0.698, task_loss=0, contrastive_loss=0.085, total=4096.3, n_correct=2791.28, ppl=4.05, accuracy=68.141, wps=14075.3, ups=1.72, wpb=8192.6, bsz=294.1, num_updates=39500, lr=7.11568e-05, gnorm=0.608, clip=0, loss_scale=8, train_wall=58, gb_free=12.7, wall=28548
2023-08-12 20:18:05 | INFO | train_inner | epoch 027:   1295 / 1474 loss=2.03, trans_loss=4.809, nll_loss=2.021, w2v_ctc_loss=0.693, task_loss=0, contrastive_loss=0.139, total=4065.06, n_correct=2768.2, ppl=4.06, accuracy=68.097, wps=14068.9, ups=1.73, wpb=8130.1, bsz=294.3, num_updates=39600, lr=7.10669e-05, gnorm=0.579, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=28606
2023-08-12 20:19:02 | INFO | train_inner | epoch 027:   1395 / 1474 loss=2.024, trans_loss=4.807, nll_loss=2.019, w2v_ctc_loss=0.687, task_loss=0, contrastive_loss=0.124, total=4151.71, n_correct=2835.61, ppl=4.05, accuracy=68.3, wps=14469.1, ups=1.74, wpb=8303.4, bsz=312.9, num_updates=39700, lr=7.09773e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=57, gb_free=17.8, wall=28663
2023-08-12 20:19:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 20:20:09 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 4.167 | trans_loss 5.168 | nll_loss 2.427 | w2v_ctc_loss 1.449 | task_loss 0 | contrastive_loss 0.29 | total 4003.4 | n_correct 2662.7 | ppl 5.38 | accuracy 66.511 | uer 18.61 | wer 20.771 | raw_wer 20.771 | bleu 22.19 | wps 2390.4 | wpb 4003.4 | bsz 141.8 | num_updates 39779 | best_bleu 22.32
2023-08-12 20:20:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 39779 updates
2023-08-12 20:20:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.1906.pt
2023-08-12 20:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.1906.pt
2023-08-12 20:20:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.1906.pt (epoch 27 @ 39779 updates, score 22.19) (writing took 40.194825967773795 seconds)
2023-08-12 20:20:50 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-08-12 20:20:50 | INFO | train | epoch 027 | loss 2.023 | trans_loss 4.803 | nll_loss 2.013 | w2v_ctc_loss 0.692 | task_loss 0 | contrastive_loss 0.123 | total 4138.65 | n_correct 2825.29 | ppl 4.04 | accuracy 68.266 | wps 13279 | ups 1.6 | wpb 8277.3 | bsz 305.7 | num_updates 39779 | lr 7.09068e-05 | gnorm 0.581 | clip 0 | loss_scale 8 | train_wall 842 | gb_free 17.8 | wall 28771
2023-08-12 20:20:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 20:20:50 | INFO | fairseq.trainer | begin training epoch 28
2023-08-12 20:20:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 20:21:09 | INFO | train_inner | epoch 028:     21 / 1474 loss=2.007, trans_loss=4.799, nll_loss=2.008, w2v_ctc_loss=0.679, task_loss=0, contrastive_loss=0.072, total=4120.02, n_correct=2819.8, ppl=4.02, accuracy=68.441, wps=6504.5, ups=0.79, wpb=8240, bsz=307.6, num_updates=39800, lr=7.08881e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=56, gb_free=15.7, wall=28790
2023-08-12 20:22:07 | INFO | train_inner | epoch 028:    121 / 1474 loss=1.998, trans_loss=4.778, nll_loss=1.98, w2v_ctc_loss=0.681, task_loss=0, contrastive_loss=0.068, total=4107.51, n_correct=2827.47, ppl=3.95, accuracy=68.837, wps=14174.1, ups=1.73, wpb=8215, bsz=291.1, num_updates=39900, lr=7.07992e-05, gnorm=0.577, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=28848
2023-08-12 20:23:05 | INFO | train_inner | epoch 028:    221 / 1474 loss=2.001, trans_loss=4.785, nll_loss=1.991, w2v_ctc_loss=0.674, task_loss=0, contrastive_loss=0.077, total=4193.44, n_correct=2879.88, ppl=3.97, accuracy=68.676, wps=14391.7, ups=1.72, wpb=8386.9, bsz=316.2, num_updates=40000, lr=7.07107e-05, gnorm=0.587, clip=0, loss_scale=8, train_wall=58, gb_free=17, wall=28906
2023-08-12 20:23:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 20:23:28 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.179 | trans_loss 5.171 | nll_loss 2.429 | w2v_ctc_loss 1.48 | task_loss 0 | contrastive_loss 0.294 | total 4003.4 | n_correct 2664.4 | ppl 5.39 | accuracy 66.553 | uer 18.929 | wer 20.931 | raw_wer 20.931 | bleu 21.9 | wps 2208.8 | wpb 4003.4 | bsz 141.8 | num_updates 40000 | best_bleu 22.32
2023-08-12 20:23:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 40000 updates
2023-08-12 20:23:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_28_40000.pt
2023-08-12 20:23:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_28_40000.pt
2023-08-12 20:23:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_28_40000.pt (epoch 28 @ 40000 updates, score 21.9) (writing took 19.618127822875977 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-12 20:24:46 | INFO | train_inner | epoch 028:    321 / 1474 loss=2.065, trans_loss=4.795, nll_loss=2.004, w2v_ctc_loss=0.678, task_loss=0, contrastive_loss=0.411, total=4131, n_correct=2820.23, ppl=4.01, accuracy=68.27, wps=8188.5, ups=0.99, wpb=8262, bsz=313.5, num_updates=40100, lr=7.06225e-05, gnorm=0.573, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=29007
2023-08-12 20:25:43 | INFO | train_inner | epoch 028:    421 / 1474 loss=2.007, trans_loss=4.788, nll_loss=1.994, w2v_ctc_loss=0.692, task_loss=0, contrastive_loss=0.066, total=4096.12, n_correct=2808.57, ppl=3.98, accuracy=68.567, wps=14301.5, ups=1.75, wpb=8192.2, bsz=296.6, num_updates=40200, lr=7.05346e-05, gnorm=0.564, clip=0, loss_scale=8, train_wall=57, gb_free=17.6, wall=29065
2023-08-12 20:26:41 | INFO | train_inner | epoch 028:    521 / 1474 loss=2.008, trans_loss=4.792, nll_loss=1.999, w2v_ctc_loss=0.681, task_loss=0, contrastive_loss=0.083, total=4109.15, n_correct=2813.45, ppl=4, accuracy=68.468, wps=14205.9, ups=1.73, wpb=8218.3, bsz=298.6, num_updates=40300, lr=7.0447e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=57, gb_free=15.1, wall=29122
2023-08-12 20:27:39 | INFO | train_inner | epoch 028:    621 / 1474 loss=2.007, trans_loss=4.797, nll_loss=2.006, w2v_ctc_loss=0.685, task_loss=0, contrastive_loss=0.069, total=4178.56, n_correct=2862.27, ppl=4.02, accuracy=68.499, wps=14549.2, ups=1.74, wpb=8357.1, bsz=304.4, num_updates=40400, lr=7.03598e-05, gnorm=0.56, clip=0, loss_scale=8, train_wall=57, gb_free=16.1, wall=29180
2023-08-12 20:28:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-08-12 20:28:37 | INFO | train_inner | epoch 028:    722 / 1474 loss=2.015, trans_loss=4.8, nll_loss=2.01, w2v_ctc_loss=0.682, task_loss=0, contrastive_loss=0.1, total=4162.02, n_correct=2848.47, ppl=4.03, accuracy=68.44, wps=14345.4, ups=1.72, wpb=8324, bsz=319.8, num_updates=40500, lr=7.02728e-05, gnorm=0.577, clip=0, loss_scale=4, train_wall=58, gb_free=16.7, wall=29238
2023-08-12 20:29:34 | INFO | train_inner | epoch 028:    822 / 1474 loss=2.002, trans_loss=4.791, nll_loss=1.998, w2v_ctc_loss=0.678, task_loss=0, contrastive_loss=0.068, total=4084.54, n_correct=2801.44, ppl=3.99, accuracy=68.586, wps=14296.7, ups=1.75, wpb=8169.1, bsz=303.9, num_updates=40600, lr=7.01862e-05, gnorm=0.563, clip=0, loss_scale=4, train_wall=57, gb_free=16.8, wall=29295
2023-08-12 20:30:31 | INFO | train_inner | epoch 028:    922 / 1474 loss=2.024, trans_loss=4.799, nll_loss=2.008, w2v_ctc_loss=0.689, task_loss=0, contrastive_loss=0.132, total=4123.73, n_correct=2815.19, ppl=4.02, accuracy=68.268, wps=14296.6, ups=1.73, wpb=8247.5, bsz=301.6, num_updates=40700, lr=7.01e-05, gnorm=0.572, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=29353
2023-08-12 20:31:30 | INFO | train_inner | epoch 028:   1022 / 1474 loss=2.035, trans_loss=4.8, nll_loss=2.009, w2v_ctc_loss=0.691, task_loss=0, contrastive_loss=0.182, total=4178.85, n_correct=2853.74, ppl=4.02, accuracy=68.29, wps=14339, ups=1.72, wpb=8357.7, bsz=309.9, num_updates=40800, lr=7.0014e-05, gnorm=0.574, clip=0, loss_scale=4, train_wall=58, gb_free=16.9, wall=29411
2023-08-12 20:32:27 | INFO | train_inner | epoch 028:   1122 / 1474 loss=2.01, trans_loss=4.79, nll_loss=1.998, w2v_ctc_loss=0.684, task_loss=0, contrastive_loss=0.088, total=4213.43, n_correct=2889.44, ppl=3.99, accuracy=68.577, wps=14807.6, ups=1.76, wpb=8426.9, bsz=320.6, num_updates=40900, lr=6.99284e-05, gnorm=0.564, clip=0, loss_scale=4, train_wall=56, gb_free=17.8, wall=29468
2023-08-12 20:33:24 | INFO | train_inner | epoch 028:   1222 / 1474 loss=2, trans_loss=4.795, nll_loss=2.003, w2v_ctc_loss=0.667, task_loss=0, contrastive_loss=0.074, total=4113.96, n_correct=2822.27, ppl=4.01, accuracy=68.602, wps=14366, ups=1.75, wpb=8227.9, bsz=307.1, num_updates=41000, lr=6.9843e-05, gnorm=0.566, clip=0, loss_scale=4, train_wall=57, gb_free=12.9, wall=29525
2023-08-12 20:34:22 | INFO | train_inner | epoch 028:   1322 / 1474 loss=2.015, trans_loss=4.799, nll_loss=2.008, w2v_ctc_loss=0.693, task_loss=0, contrastive_loss=0.089, total=4070, n_correct=2779.57, ppl=4.02, accuracy=68.294, wps=14104, ups=1.73, wpb=8140, bsz=281.8, num_updates=41100, lr=6.9758e-05, gnorm=0.583, clip=0, loss_scale=4, train_wall=57, gb_free=17.2, wall=29583
2023-08-12 20:35:19 | INFO | train_inner | epoch 028:   1422 / 1474 loss=2.015, trans_loss=4.796, nll_loss=2.004, w2v_ctc_loss=0.683, task_loss=0, contrastive_loss=0.112, total=4154.59, n_correct=2837.5, ppl=4.01, accuracy=68.298, wps=14379.6, ups=1.73, wpb=8309.2, bsz=300.4, num_updates=41200, lr=6.96733e-05, gnorm=0.556, clip=0, loss_scale=4, train_wall=57, gb_free=16.2, wall=29641
2023-08-12 20:35:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
2023-08-12 20:36:12 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 4.154 | trans_loss 5.161 | nll_loss 2.419 | w2v_ctc_loss 1.428 | task_loss 0 | contrastive_loss 0.291 | total 4003.4 | n_correct 2675.7 | ppl 5.35 | accuracy 66.836 | uer 18.74 | wer 20.764 | raw_wer 20.764 | bleu 22.35 | wps 2185.9 | wpb 4003.4 | bsz 141.8 | num_updates 41252 | best_bleu 22.35
2023-08-12 20:36:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 41252 updates
2023-08-12 20:36:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 20:36:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 20:36:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 28 @ 41252 updates, score 22.35) (writing took 28.776634816080332 seconds)
2023-08-12 20:36:42 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-08-12 20:36:42 | INFO | train | epoch 028 | loss 2.014 | trans_loss 4.793 | nll_loss 2 | w2v_ctc_loss 0.683 | task_loss 0 | contrastive_loss 0.115 | total 4137.42 | n_correct 2833.8 | ppl 4 | accuracy 68.492 | wps 12805.9 | ups 1.55 | wpb 8274.8 | bsz 305.2 | num_updates 41252 | lr 6.96294e-05 | gnorm 0.569 | clip 0 | loss_scale 4 | train_wall 842 | gb_free 16.5 | wall 29723
2023-08-12 20:36:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 20:36:42 | INFO | fairseq.trainer | begin training epoch 29
2023-08-12 20:36:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 20:37:17 | INFO | train_inner | epoch 029:     48 / 1474 loss=2.006, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.682, task_loss=0, contrastive_loss=0.085, total=4163.69, n_correct=2860.46, ppl=3.97, accuracy=68.7, wps=7100.9, ups=0.85, wpb=8327.4, bsz=315, num_updates=41300, lr=6.95889e-05, gnorm=0.573, clip=0, loss_scale=4, train_wall=57, gb_free=16.9, wall=29758
2023-08-12 20:38:14 | INFO | train_inner | epoch 029:    148 / 1474 loss=2.009, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.683, task_loss=0, contrastive_loss=0.103, total=4123.71, n_correct=2833.63, ppl=3.97, accuracy=68.716, wps=14335.3, ups=1.74, wpb=8247.4, bsz=307.9, num_updates=41400, lr=6.95048e-05, gnorm=0.574, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=29816
2023-08-12 20:39:12 | INFO | train_inner | epoch 029:    248 / 1474 loss=2.017, trans_loss=4.776, nll_loss=1.979, w2v_ctc_loss=0.666, task_loss=0, contrastive_loss=0.186, total=4192.44, n_correct=2886.64, ppl=3.94, accuracy=68.853, wps=14446.2, ups=1.72, wpb=8384.9, bsz=328.2, num_updates=41500, lr=6.9421e-05, gnorm=0.588, clip=0, loss_scale=4, train_wall=58, gb_free=16.4, wall=29874
2023-08-12 20:40:10 | INFO | train_inner | epoch 029:    348 / 1474 loss=2.012, trans_loss=4.796, nll_loss=2.004, w2v_ctc_loss=0.696, task_loss=0, contrastive_loss=0.07, total=4095.08, n_correct=2803.38, ppl=4.01, accuracy=68.457, wps=14165.5, ups=1.73, wpb=8190.2, bsz=291.2, num_updates=41600, lr=6.93375e-05, gnorm=0.562, clip=0, loss_scale=4, train_wall=57, gb_free=17.1, wall=29931
2023-08-12 20:41:07 | INFO | train_inner | epoch 029:    448 / 1474 loss=1.99, trans_loss=4.767, nll_loss=1.966, w2v_ctc_loss=0.668, task_loss=0, contrastive_loss=0.066, total=4155.97, n_correct=2869.78, ppl=3.91, accuracy=69.052, wps=14575, ups=1.75, wpb=8311.9, bsz=307.7, num_updates=41700, lr=6.92543e-05, gnorm=0.559, clip=0, loss_scale=4, train_wall=57, gb_free=14.9, wall=29988
2023-08-12 20:42:05 | INFO | train_inner | epoch 029:    548 / 1474 loss=2.024, trans_loss=4.793, nll_loss=2.001, w2v_ctc_loss=0.684, task_loss=0, contrastive_loss=0.161, total=4159.23, n_correct=2846.4, ppl=4, accuracy=68.436, wps=14411.4, ups=1.73, wpb=8318.5, bsz=295.2, num_updates=41800, lr=6.91714e-05, gnorm=0.563, clip=0, loss_scale=4, train_wall=57, gb_free=16.8, wall=30046
2023-08-12 20:43:04 | INFO | train_inner | epoch 029:    648 / 1474 loss=2.023, trans_loss=4.78, nll_loss=1.984, w2v_ctc_loss=0.674, task_loss=0, contrastive_loss=0.228, total=4139.5, n_correct=2845.39, ppl=3.96, accuracy=68.738, wps=14103.4, ups=1.7, wpb=8279, bsz=318.8, num_updates=41900, lr=6.90889e-05, gnorm=0.568, clip=0, loss_scale=4, train_wall=58, gb_free=16.3, wall=30105
2023-08-12 20:44:02 | INFO | train_inner | epoch 029:    748 / 1474 loss=2.015, trans_loss=4.782, nll_loss=1.986, w2v_ctc_loss=0.677, task_loss=0, contrastive_loss=0.147, total=4241.03, n_correct=2913.14, ppl=3.96, accuracy=68.689, wps=14583.1, ups=1.72, wpb=8482.1, bsz=328.7, num_updates=42000, lr=6.90066e-05, gnorm=0.608, clip=0, loss_scale=4, train_wall=58, gb_free=15.8, wall=30163
2023-08-12 20:44:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 20:44:24 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.164 | trans_loss 5.173 | nll_loss 2.435 | w2v_ctc_loss 1.426 | task_loss 0 | contrastive_loss 0.289 | total 4003.4 | n_correct 2668 | ppl 5.41 | accuracy 66.643 | uer 18.69 | wer 20.95 | raw_wer 20.95 | bleu 21.83 | wps 2321.1 | wpb 4003.4 | bsz 141.8 | num_updates 42000 | best_bleu 22.35
2023-08-12 20:44:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42000 updates
2023-08-12 20:44:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_29_42000.pt
2023-08-12 20:44:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_29_42000.pt
2023-08-12 20:44:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_29_42000.pt (epoch 29 @ 42000 updates, score 21.83) (writing took 18.001007789745927 seconds)
2023-08-12 20:45:39 | INFO | train_inner | epoch 029:    848 / 1474 loss=2.001, trans_loss=4.794, nll_loss=2.002, w2v_ctc_loss=0.674, task_loss=0, contrastive_loss=0.066, total=4031.53, n_correct=2759.75, ppl=4, accuracy=68.454, wps=8249.6, ups=1.02, wpb=8063.1, bsz=282.9, num_updates=42100, lr=6.89246e-05, gnorm=0.563, clip=0, loss_scale=4, train_wall=56, gb_free=16.4, wall=30261
2023-08-12 20:46:37 | INFO | train_inner | epoch 029:    948 / 1474 loss=2.005, trans_loss=4.79, nll_loss=1.997, w2v_ctc_loss=0.685, task_loss=0, contrastive_loss=0.073, total=4082.22, n_correct=2798.47, ppl=3.99, accuracy=68.553, wps=14256, ups=1.75, wpb=8164.4, bsz=293.3, num_updates=42200, lr=6.88428e-05, gnorm=0.565, clip=0, loss_scale=4, train_wall=57, gb_free=16.3, wall=30318
2023-08-12 20:47:35 | INFO | train_inner | epoch 029:   1048 / 1474 loss=2.009, trans_loss=4.781, nll_loss=1.986, w2v_ctc_loss=0.672, task_loss=0, contrastive_loss=0.148, total=4134.44, n_correct=2842.29, ppl=3.96, accuracy=68.747, wps=14264.3, ups=1.73, wpb=8268.9, bsz=307.7, num_updates=42300, lr=6.87614e-05, gnorm=0.573, clip=0, loss_scale=4, train_wall=57, gb_free=17.1, wall=30376
2023-08-12 20:48:32 | INFO | train_inner | epoch 029:   1148 / 1474 loss=2.007, trans_loss=4.797, nll_loss=2.005, w2v_ctc_loss=0.69, task_loss=0, contrastive_loss=0.064, total=4080.4, n_correct=2791.9, ppl=4.01, accuracy=68.422, wps=14204.2, ups=1.74, wpb=8160.8, bsz=285.2, num_updates=42400, lr=6.86803e-05, gnorm=0.566, clip=0, loss_scale=4, train_wall=57, gb_free=12.5, wall=30433
2023-08-12 20:49:30 | INFO | train_inner | epoch 029:   1248 / 1474 loss=2.008, trans_loss=4.797, nll_loss=2.006, w2v_ctc_loss=0.686, task_loss=0, contrastive_loss=0.069, total=4158.63, n_correct=2844.75, ppl=4.02, accuracy=68.406, wps=14499.2, ups=1.74, wpb=8317.3, bsz=301.6, num_updates=42500, lr=6.85994e-05, gnorm=0.57, clip=0, loss_scale=4, train_wall=57, gb_free=16.7, wall=30491
2023-08-12 20:50:27 | INFO | train_inner | epoch 029:   1348 / 1474 loss=2.011, trans_loss=4.783, nll_loss=1.988, w2v_ctc_loss=0.674, task_loss=0, contrastive_loss=0.13, total=4159.66, n_correct=2857.85, ppl=3.97, accuracy=68.704, wps=14366.2, ups=1.73, wpb=8319.3, bsz=308.6, num_updates=42600, lr=6.85189e-05, gnorm=0.558, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=30549
2023-08-12 20:51:25 | INFO | train_inner | epoch 029:   1448 / 1474 loss=2.015, trans_loss=4.782, nll_loss=1.987, w2v_ctc_loss=0.675, task_loss=0, contrastive_loss=0.162, total=4169.01, n_correct=2862.58, ppl=3.96, accuracy=68.663, wps=14384.8, ups=1.73, wpb=8338, bsz=314.2, num_updates=42700, lr=6.84386e-05, gnorm=0.556, clip=0, loss_scale=8, train_wall=57, gb_free=17.8, wall=30607
2023-08-12 20:51:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 20:52:03 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 4.149 | trans_loss 5.168 | nll_loss 2.427 | w2v_ctc_loss 1.398 | task_loss 0 | contrastive_loss 0.286 | total 4003.4 | n_correct 2673.2 | ppl 5.38 | accuracy 66.773 | uer 18.377 | wer 20.518 | raw_wer 20.518 | bleu 22.26 | wps 2328 | wpb 4003.4 | bsz 141.8 | num_updates 42726 | best_bleu 22.35
2023-08-12 20:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 42726 updates
2023-08-12 20:52:03 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.2600.pt
2023-08-12 20:52:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.2600.pt
2023-08-12 20:52:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.2600.pt (epoch 29 @ 42726 updates, score 22.26) (writing took 19.276009971275926 seconds)
2023-08-12 20:52:22 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-08-12 20:52:22 | INFO | train | epoch 029 | loss 2.01 | trans_loss 4.786 | nll_loss 1.991 | w2v_ctc_loss 0.679 | task_loss 0 | contrastive_loss 0.119 | total 4138.65 | n_correct 2840.76 | ppl 3.98 | accuracy 68.64 | wps 12968.7 | ups 1.57 | wpb 8277.3 | bsz 305.7 | num_updates 42726 | lr 6.84178e-05 | gnorm 0.57 | clip 0 | loss_scale 8 | train_wall 844 | gb_free 16.1 | wall 30664
2023-08-12 20:52:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 20:52:23 | INFO | fairseq.trainer | begin training epoch 30
2023-08-12 20:52:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 20:53:13 | INFO | train_inner | epoch 030:     74 / 1474 loss=2.009, trans_loss=4.772, nll_loss=1.973, w2v_ctc_loss=0.664, task_loss=0, contrastive_loss=0.178, total=4172.13, n_correct=2877.77, ppl=3.93, accuracy=68.976, wps=7746.4, ups=0.93, wpb=8344.3, bsz=317.1, num_updates=42800, lr=6.83586e-05, gnorm=0.55, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=30714
2023-08-12 20:54:10 | INFO | train_inner | epoch 030:    174 / 1474 loss=1.997, trans_loss=4.76, nll_loss=1.957, w2v_ctc_loss=0.671, task_loss=0, contrastive_loss=0.109, total=4211.56, n_correct=2910.98, ppl=3.88, accuracy=69.119, wps=14714.4, ups=1.75, wpb=8423.1, bsz=320.5, num_updates=42900, lr=6.82789e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=30772
2023-08-12 20:55:08 | INFO | train_inner | epoch 030:    274 / 1474 loss=1.996, trans_loss=4.778, nll_loss=1.98, w2v_ctc_loss=0.677, task_loss=0, contrastive_loss=0.064, total=4115.07, n_correct=2833.04, ppl=3.95, accuracy=68.845, wps=14316.7, ups=1.74, wpb=8230.1, bsz=293.2, num_updates=43000, lr=6.81994e-05, gnorm=0.558, clip=0, loss_scale=8, train_wall=57, gb_free=15.8, wall=30829
2023-08-12 20:56:06 | INFO | train_inner | epoch 030:    374 / 1474 loss=1.986, trans_loss=4.765, nll_loss=1.964, w2v_ctc_loss=0.663, task_loss=0, contrastive_loss=0.067, total=4184.54, n_correct=2894.6, ppl=3.9, accuracy=69.174, wps=14464.2, ups=1.73, wpb=8369.1, bsz=309.3, num_updates=43100, lr=6.81203e-05, gnorm=0.546, clip=0, loss_scale=8, train_wall=57, gb_free=17.3, wall=30887
2023-08-12 20:57:03 | INFO | train_inner | epoch 030:    474 / 1474 loss=2.002, trans_loss=4.774, nll_loss=1.976, w2v_ctc_loss=0.664, task_loss=0, contrastive_loss=0.127, total=4121.08, n_correct=2837.95, ppl=3.93, accuracy=68.864, wps=14460.1, ups=1.75, wpb=8242.2, bsz=311.1, num_updates=43200, lr=6.80414e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=57, gb_free=14.9, wall=30944
2023-08-12 20:58:00 | INFO | train_inner | epoch 030:    574 / 1474 loss=1.996, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.671, task_loss=0, contrastive_loss=0.09, total=4162.58, n_correct=2870.65, ppl=3.93, accuracy=68.963, wps=14549.6, ups=1.75, wpb=8325.2, bsz=312.1, num_updates=43300, lr=6.79628e-05, gnorm=0.559, clip=0, loss_scale=8, train_wall=57, gb_free=15.4, wall=31001
2023-08-12 20:58:58 | INFO | train_inner | epoch 030:    674 / 1474 loss=2.003, trans_loss=4.775, nll_loss=1.977, w2v_ctc_loss=0.675, task_loss=0, contrastive_loss=0.104, total=4192, n_correct=2883.57, ppl=3.94, accuracy=68.787, wps=14364.5, ups=1.71, wpb=8384, bsz=315.3, num_updates=43400, lr=6.78844e-05, gnorm=0.559, clip=0, loss_scale=8, train_wall=58, gb_free=15.8, wall=31060
2023-08-12 20:59:56 | INFO | train_inner | epoch 030:    774 / 1474 loss=2.025, trans_loss=4.784, nll_loss=1.988, w2v_ctc_loss=0.684, task_loss=0, contrastive_loss=0.185, total=4103.26, n_correct=2814.02, ppl=3.97, accuracy=68.58, wps=14158.4, ups=1.73, wpb=8206.5, bsz=302.6, num_updates=43500, lr=6.78064e-05, gnorm=0.575, clip=0, loss_scale=8, train_wall=58, gb_free=16.3, wall=31118
2023-08-12 21:00:54 | INFO | train_inner | epoch 030:    874 / 1474 loss=1.999, trans_loss=4.779, nll_loss=1.982, w2v_ctc_loss=0.675, task_loss=0, contrastive_loss=0.08, total=4111.51, n_correct=2830.6, ppl=3.95, accuracy=68.846, wps=14289.5, ups=1.74, wpb=8223, bsz=297.8, num_updates=43600, lr=6.77285e-05, gnorm=0.568, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=31175
2023-08-12 21:01:51 | INFO | train_inner | epoch 030:    974 / 1474 loss=2.002, trans_loss=4.784, nll_loss=1.989, w2v_ctc_loss=0.679, task_loss=0, contrastive_loss=0.078, total=4125.95, n_correct=2833.45, ppl=3.97, accuracy=68.674, wps=14356, ups=1.74, wpb=8251.9, bsz=298.7, num_updates=43700, lr=6.7651e-05, gnorm=0.563, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=31233
2023-08-12 21:02:50 | INFO | train_inner | epoch 030:   1074 / 1474 loss=2.01, trans_loss=4.782, nll_loss=1.985, w2v_ctc_loss=0.674, task_loss=0, contrastive_loss=0.159, total=4096.17, n_correct=2811.19, ppl=3.96, accuracy=68.63, wps=14068.3, ups=1.72, wpb=8192.3, bsz=281.5, num_updates=43800, lr=6.75737e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=58, gb_free=16.2, wall=31291
2023-08-12 21:03:48 | INFO | train_inner | epoch 030:   1174 / 1474 loss=2.003, trans_loss=4.776, nll_loss=1.979, w2v_ctc_loss=0.661, task_loss=0, contrastive_loss=0.137, total=4168.92, n_correct=2873.67, ppl=3.94, accuracy=68.931, wps=14377.9, ups=1.72, wpb=8337.8, bsz=314.8, num_updates=43900, lr=6.74967e-05, gnorm=0.559, clip=0, loss_scale=8, train_wall=58, gb_free=17.6, wall=31349
2023-08-12 21:04:45 | INFO | train_inner | epoch 030:   1274 / 1474 loss=2, trans_loss=4.783, nll_loss=1.988, w2v_ctc_loss=0.681, task_loss=0, contrastive_loss=0.072, total=4038.68, n_correct=2771.74, ppl=3.97, accuracy=68.63, wps=14060.6, ups=1.74, wpb=8077.4, bsz=284.1, num_updates=44000, lr=6.742e-05, gnorm=0.61, clip=0, loss_scale=8, train_wall=57, gb_free=15, wall=31406
2023-08-12 21:04:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 21:05:08 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.145 | trans_loss 5.165 | nll_loss 2.423 | w2v_ctc_loss 1.389 | task_loss 0 | contrastive_loss 0.288 | total 4003.4 | n_correct 2671.7 | ppl 5.36 | accuracy 66.736 | uer 18.297 | wer 20.335 | raw_wer 20.335 | bleu 21.91 | wps 2269.2 | wpb 4003.4 | bsz 141.8 | num_updates 44000 | best_bleu 22.35
2023-08-12 21:05:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44000 updates
2023-08-12 21:05:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_30_44000.pt
2023-08-12 21:05:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_30_44000.pt
2023-08-12 21:05:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_30_44000.pt (epoch 30 @ 44000 updates, score 21.91) (writing took 18.996084917336702 seconds)
2023-08-12 21:06:24 | INFO | train_inner | epoch 030:   1374 / 1474 loss=1.992, trans_loss=4.775, nll_loss=1.978, w2v_ctc_loss=0.663, task_loss=0, contrastive_loss=0.078, total=4167.64, n_correct=2869.77, ppl=3.94, accuracy=68.858, wps=8384, ups=1.01, wpb=8335.3, bsz=321.9, num_updates=44100, lr=6.73435e-05, gnorm=0.569, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=31506
2023-08-12 21:07:22 | INFO | train_inner | epoch 030:   1474 / 1474 loss=2.018, trans_loss=4.782, nll_loss=1.987, w2v_ctc_loss=0.662, task_loss=0, contrastive_loss=0.231, total=4117.91, n_correct=2829.25, ppl=3.96, accuracy=68.706, wps=14274.6, ups=1.73, wpb=8235.8, bsz=312.2, num_updates=44200, lr=6.72673e-05, gnorm=0.587, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=31563
2023-08-12 21:07:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 21:07:44 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 4.144 | trans_loss 5.167 | nll_loss 2.429 | w2v_ctc_loss 1.381 | task_loss 0 | contrastive_loss 0.29 | total 4003.4 | n_correct 2666.8 | ppl 5.38 | accuracy 66.613 | uer 18.607 | wer 20.89 | raw_wer 20.89 | bleu 21.96 | wps 2348.8 | wpb 4003.4 | bsz 141.8 | num_updates 44200 | best_bleu 22.35
2023-08-12 21:07:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 44200 updates
2023-08-12 21:07:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_last.pt
2023-08-12 21:07:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_last.pt
2023-08-12 21:07:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_last.pt (epoch 30 @ 44200 updates, score 21.96) (writing took 15.099310535937548 seconds)
2023-08-12 21:07:59 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-08-12 21:07:59 | INFO | train | epoch 030 | loss 2.003 | trans_loss 4.776 | nll_loss 1.978 | w2v_ctc_loss 0.671 | task_loss 0 | contrastive_loss 0.118 | total 4138.65 | n_correct 2849.15 | ppl 3.94 | accuracy 68.842 | wps 13023.4 | ups 1.57 | wpb 8277.3 | bsz 305.7 | num_updates 44200 | lr 6.72673e-05 | gnorm 0.566 | clip 0 | loss_scale 8 | train_wall 843 | gb_free 17.2 | wall 31601
2023-08-12 21:07:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 21:08:00 | INFO | fairseq.trainer | begin training epoch 31
2023-08-12 21:08:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 21:09:05 | INFO | train_inner | epoch 031:    100 / 1474 loss=1.988, trans_loss=4.764, nll_loss=1.962, w2v_ctc_loss=0.67, task_loss=0, contrastive_loss=0.073, total=4085.38, n_correct=2825.16, ppl=3.9, accuracy=69.153, wps=7969.2, ups=0.98, wpb=8170.8, bsz=293.4, num_updates=44300, lr=6.71913e-05, gnorm=0.558, clip=0, loss_scale=8, train_wall=57, gb_free=15, wall=31666
2023-08-12 21:10:02 | INFO | train_inner | epoch 031:    200 / 1474 loss=1.991, trans_loss=4.767, nll_loss=1.967, w2v_ctc_loss=0.665, task_loss=0, contrastive_loss=0.084, total=4139.51, n_correct=2857.78, ppl=3.91, accuracy=69.037, wps=14476.1, ups=1.75, wpb=8279, bsz=299.4, num_updates=44400, lr=6.71156e-05, gnorm=0.567, clip=0, loss_scale=8, train_wall=57, gb_free=12.2, wall=31723
2023-08-12 21:11:00 | INFO | train_inner | epoch 031:    300 / 1474 loss=1.999, trans_loss=4.762, nll_loss=1.959, w2v_ctc_loss=0.671, task_loss=0, contrastive_loss=0.132, total=4148.01, n_correct=2864.93, ppl=3.89, accuracy=69.068, wps=14343.2, ups=1.73, wpb=8296, bsz=301.1, num_updates=44500, lr=6.70402e-05, gnorm=0.598, clip=0, loss_scale=8, train_wall=57, gb_free=13.6, wall=31781
2023-08-12 21:11:57 | INFO | train_inner | epoch 031:    400 / 1474 loss=1.988, trans_loss=4.772, nll_loss=1.973, w2v_ctc_loss=0.663, task_loss=0, contrastive_loss=0.072, total=4095.42, n_correct=2823.39, ppl=3.93, accuracy=68.94, wps=14217.3, ups=1.74, wpb=8190.8, bsz=286.3, num_updates=44600, lr=6.6965e-05, gnorm=0.582, clip=0, loss_scale=16, train_wall=57, gb_free=13.6, wall=31839
2023-08-12 21:12:55 | INFO | train_inner | epoch 031:    500 / 1474 loss=1.992, trans_loss=4.766, nll_loss=1.965, w2v_ctc_loss=0.674, task_loss=0, contrastive_loss=0.076, total=4115.61, n_correct=2839.98, ppl=3.9, accuracy=69.005, wps=14234.7, ups=1.73, wpb=8231.2, bsz=300.6, num_updates=44700, lr=6.689e-05, gnorm=0.569, clip=0, loss_scale=16, train_wall=57, gb_free=16.4, wall=31896
2023-08-12 21:13:52 | INFO | train_inner | epoch 031:    600 / 1474 loss=1.987, trans_loss=4.767, nll_loss=1.966, w2v_ctc_loss=0.665, task_loss=0, contrastive_loss=0.066, total=4075.9, n_correct=2812.72, ppl=3.91, accuracy=69.009, wps=14254, ups=1.75, wpb=8151.8, bsz=293.3, num_updates=44800, lr=6.68153e-05, gnorm=0.563, clip=0, loss_scale=16, train_wall=57, gb_free=12.2, wall=31954
2023-08-12 21:14:50 | INFO | train_inner | epoch 031:    700 / 1474 loss=1.982, trans_loss=4.761, nll_loss=1.959, w2v_ctc_loss=0.656, task_loss=0, contrastive_loss=0.068, total=4208.99, n_correct=2912.25, ppl=3.89, accuracy=69.191, wps=14689.7, ups=1.75, wpb=8418, bsz=315.1, num_updates=44900, lr=6.67409e-05, gnorm=0.56, clip=0, loss_scale=16, train_wall=57, gb_free=17.8, wall=32011
2023-08-12 21:15:48 | INFO | train_inner | epoch 031:    800 / 1474 loss=2.004, trans_loss=4.774, nll_loss=1.975, w2v_ctc_loss=0.664, task_loss=0, contrastive_loss=0.137, total=4104.19, n_correct=2821.82, ppl=3.93, accuracy=68.755, wps=14130.5, ups=1.72, wpb=8208.4, bsz=297.4, num_updates=45000, lr=6.66667e-05, gnorm=0.571, clip=0, loss_scale=16, train_wall=58, gb_free=15.4, wall=32069
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:0')
2023-08-12 21:16:45 | INFO | train_inner | epoch 031:    900 / 1474 loss=1.989, trans_loss=4.762, nll_loss=1.959, w2v_ctc_loss=0.669, task_loss=0, contrastive_loss=0.081, total=4099.13, n_correct=2831.76, ppl=3.89, accuracy=69.082, wps=14232, ups=1.74, wpb=8198.3, bsz=294.3, num_updates=45100, lr=6.65927e-05, gnorm=0.564, clip=0, loss_scale=16, train_wall=57, gb_free=15.3, wall=32127
2023-08-12 21:17:43 | INFO | train_inner | epoch 031:   1000 / 1474 loss=2.005, trans_loss=4.773, nll_loss=1.975, w2v_ctc_loss=0.66, task_loss=0, contrastive_loss=0.166, total=4186.81, n_correct=2889.61, ppl=3.93, accuracy=69.017, wps=14535.8, ups=1.74, wpb=8373.6, bsz=320.9, num_updates=45200, lr=6.6519e-05, gnorm=0.563, clip=0, loss_scale=16, train_wall=57, gb_free=13.5, wall=32184
2023-08-12 21:18:40 | INFO | train_inner | epoch 031:   1100 / 1474 loss=1.997, trans_loss=4.769, nll_loss=1.97, w2v_ctc_loss=0.662, task_loss=0, contrastive_loss=0.111, total=4149.25, n_correct=2863.23, ppl=3.92, accuracy=69.006, wps=14479.9, ups=1.74, wpb=8298.5, bsz=315.2, num_updates=45300, lr=6.64455e-05, gnorm=0.581, clip=0, loss_scale=16, train_wall=57, gb_free=16, wall=32242
2023-08-12 21:19:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-12 21:19:38 | INFO | train_inner | epoch 031:   1201 / 1474 loss=1.99, trans_loss=4.771, nll_loss=1.972, w2v_ctc_loss=0.668, task_loss=0, contrastive_loss=0.072, total=4156.49, n_correct=2868.65, ppl=3.92, accuracy=69.016, wps=14322.7, ups=1.72, wpb=8313, bsz=310.7, num_updates=45400, lr=6.63723e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=58, gb_free=17.3, wall=32300
2023-08-12 21:20:36 | INFO | train_inner | epoch 031:   1301 / 1474 loss=1.994, trans_loss=4.777, nll_loss=1.98, w2v_ctc_loss=0.667, task_loss=0, contrastive_loss=0.074, total=4226.19, n_correct=2911.2, ppl=3.94, accuracy=68.885, wps=14603.5, ups=1.73, wpb=8452.4, bsz=325.1, num_updates=45500, lr=6.62994e-05, gnorm=0.562, clip=0, loss_scale=8, train_wall=57, gb_free=17, wall=32358
2023-08-12 21:21:34 | INFO | train_inner | epoch 031:   1401 / 1474 loss=2.034, trans_loss=4.774, nll_loss=1.977, w2v_ctc_loss=0.67, task_loss=0, contrastive_loss=0.28, total=4192.11, n_correct=2882.21, ppl=3.94, accuracy=68.753, wps=14533.9, ups=1.73, wpb=8384.2, bsz=327, num_updates=45600, lr=6.62266e-05, gnorm=0.594, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=32415
2023-08-12 21:22:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2472, device='cuda:5')
2023-08-12 21:22:37 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 4.151 | trans_loss 5.162 | nll_loss 2.418 | w2v_ctc_loss 1.418 | task_loss 0 | contrastive_loss 0.286 | total 4003.4 | n_correct 2673.3 | ppl 5.34 | accuracy 66.776 | uer 18.597 | wer 20.778 | raw_wer 20.778 | bleu 22.36 | wps 2444.1 | wpb 4003.4 | bsz 141.8 | num_updates 45673 | best_bleu 22.36
2023-08-12 21:22:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 45673 updates
2023-08-12 21:22:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 21:22:52 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 21:23:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 31 @ 45673 updates, score 22.36) (writing took 29.648715248331428 seconds)
2023-08-12 21:23:07 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-08-12 21:23:07 | INFO | train | epoch 031 | loss 1.996 | trans_loss 4.769 | nll_loss 1.969 | w2v_ctc_loss 0.666 | task_loss 0 | contrastive_loss 0.106 | total 4136.44 | n_correct 2853.59 | ppl 3.91 | accuracy 68.987 | wps 13421.2 | ups 1.62 | wpb 8272.9 | bsz 304.9 | num_updates 45673 | lr 6.61737e-05 | gnorm 0.574 | clip 0 | loss_scale 8 | train_wall 841 | gb_free 12.4 | wall 32509
2023-08-12 21:23:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 21:23:07 | INFO | fairseq.trainer | begin training epoch 32
2023-08-12 21:23:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 21:23:31 | INFO | train_inner | epoch 032:     27 / 1474 loss=1.987, trans_loss=4.766, nll_loss=1.966, w2v_ctc_loss=0.668, task_loss=0, contrastive_loss=0.065, total=4051.41, n_correct=2796.59, ppl=3.91, accuracy=69.028, wps=6916.8, ups=0.85, wpb=8102.8, bsz=291.6, num_updates=45700, lr=6.61541e-05, gnorm=0.607, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=32532
2023-08-12 21:24:29 | INFO | train_inner | epoch 032:    127 / 1474 loss=1.971, trans_loss=4.744, nll_loss=1.937, w2v_ctc_loss=0.645, task_loss=0, contrastive_loss=0.072, total=4208.32, n_correct=2926.78, ppl=3.83, accuracy=69.547, wps=14478.3, ups=1.72, wpb=8416.6, bsz=319.1, num_updates=45800, lr=6.60819e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=58, gb_free=12, wall=32591
2023-08-12 21:25:27 | INFO | train_inner | epoch 032:    227 / 1474 loss=1.987, trans_loss=4.762, nll_loss=1.96, w2v_ctc_loss=0.663, task_loss=0, contrastive_loss=0.082, total=4157.86, n_correct=2877.57, ppl=3.89, accuracy=69.208, wps=14383.8, ups=1.73, wpb=8315.7, bsz=320.3, num_updates=45900, lr=6.60098e-05, gnorm=0.573, clip=0, loss_scale=8, train_wall=57, gb_free=14.6, wall=32648
2023-08-12 21:26:24 | INFO | train_inner | epoch 032:    327 / 1474 loss=1.971, trans_loss=4.743, nll_loss=1.936, w2v_ctc_loss=0.643, task_loss=0, contrastive_loss=0.075, total=4179.17, n_correct=2909.64, ppl=3.83, accuracy=69.622, wps=14636.4, ups=1.75, wpb=8358.3, bsz=313.4, num_updates=46000, lr=6.5938e-05, gnorm=0.574, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=32705
2023-08-12 21:26:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 21:26:46 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.217 | trans_loss 5.166 | nll_loss 2.424 | w2v_ctc_loss 1.624 | task_loss 0 | contrastive_loss 0.294 | total 4003.4 | n_correct 2670.1 | ppl 5.37 | accuracy 66.696 | uer 19.332 | wer 21.543 | raw_wer 21.543 | bleu 22.07 | wps 2319 | wpb 4003.4 | bsz 141.8 | num_updates 46000 | best_bleu 22.36
2023-08-12 21:26:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 46000 updates
2023-08-12 21:26:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_32_46000.pt
2023-08-12 21:26:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_32_46000.pt
2023-08-12 21:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_32_46000.pt (epoch 32 @ 46000 updates, score 22.07) (writing took 20.229279525578022 seconds)
2023-08-12 21:28:05 | INFO | train_inner | epoch 032:    427 / 1474 loss=1.977, trans_loss=4.75, nll_loss=1.944, w2v_ctc_loss=0.651, task_loss=0, contrastive_loss=0.074, total=4179.5, n_correct=2902.26, ppl=3.85, accuracy=69.44, wps=8303.8, ups=0.99, wpb=8359, bsz=312.2, num_updates=46100, lr=6.58665e-05, gnorm=0.556, clip=0, loss_scale=8, train_wall=57, gb_free=17.8, wall=32806
2023-08-12 21:29:03 | INFO | train_inner | epoch 032:    527 / 1474 loss=2.006, trans_loss=4.765, nll_loss=1.964, w2v_ctc_loss=0.672, task_loss=0, contrastive_loss=0.153, total=4188.83, n_correct=2891.88, ppl=3.9, accuracy=69.038, wps=14341.2, ups=1.71, wpb=8377.7, bsz=313.9, num_updates=46200, lr=6.57952e-05, gnorm=0.578, clip=0, loss_scale=8, train_wall=58, gb_free=13.1, wall=32865
2023-08-12 21:30:02 | INFO | train_inner | epoch 032:    627 / 1474 loss=1.988, trans_loss=4.765, nll_loss=1.963, w2v_ctc_loss=0.665, task_loss=0, contrastive_loss=0.08, total=4133.19, n_correct=2851.36, ppl=3.9, accuracy=68.987, wps=14110, ups=1.71, wpb=8266.4, bsz=298.7, num_updates=46300, lr=6.57241e-05, gnorm=0.569, clip=0, loss_scale=8, train_wall=58, gb_free=17.3, wall=32923
2023-08-12 21:30:59 | INFO | train_inner | epoch 032:    727 / 1474 loss=1.983, trans_loss=4.761, nll_loss=1.959, w2v_ctc_loss=0.663, task_loss=0, contrastive_loss=0.065, total=4162.1, n_correct=2881.73, ppl=3.89, accuracy=69.237, wps=14439.6, ups=1.73, wpb=8324.2, bsz=304.3, num_updates=46400, lr=6.56532e-05, gnorm=0.555, clip=0, loss_scale=8, train_wall=57, gb_free=15.9, wall=32981
2023-08-12 21:31:57 | INFO | train_inner | epoch 032:    827 / 1474 loss=1.975, trans_loss=4.757, nll_loss=1.954, w2v_ctc_loss=0.651, task_loss=0, contrastive_loss=0.063, total=4107.86, n_correct=2846.07, ppl=3.87, accuracy=69.284, wps=14280.8, ups=1.74, wpb=8215.7, bsz=292.6, num_updates=46500, lr=6.55826e-05, gnorm=0.553, clip=0, loss_scale=8, train_wall=57, gb_free=15.8, wall=33038
2023-08-12 21:32:55 | INFO | train_inner | epoch 032:    927 / 1474 loss=1.977, trans_loss=4.759, nll_loss=1.956, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.061, total=4146.9, n_correct=2872.93, ppl=3.88, accuracy=69.279, wps=14370.9, ups=1.73, wpb=8293.8, bsz=300.4, num_updates=46600, lr=6.55122e-05, gnorm=0.557, clip=0, loss_scale=8, train_wall=57, gb_free=16.9, wall=33096
2023-08-12 21:33:52 | INFO | train_inner | epoch 032:   1027 / 1474 loss=2.004, trans_loss=4.769, nll_loss=1.969, w2v_ctc_loss=0.669, task_loss=0, contrastive_loss=0.159, total=4112.45, n_correct=2834.1, ppl=3.91, accuracy=68.915, wps=14257.1, ups=1.73, wpb=8224.9, bsz=304.3, num_updates=46700, lr=6.5442e-05, gnorm=0.57, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=33154
2023-08-12 21:34:50 | INFO | train_inner | epoch 032:   1127 / 1474 loss=1.991, trans_loss=4.767, nll_loss=1.965, w2v_ctc_loss=0.665, task_loss=0, contrastive_loss=0.099, total=4015.2, n_correct=2768.73, ppl=3.9, accuracy=68.956, wps=13942.6, ups=1.74, wpb=8030.4, bsz=269.7, num_updates=46800, lr=6.5372e-05, gnorm=0.576, clip=0, loss_scale=8, train_wall=57, gb_free=14.6, wall=33211
2023-08-12 21:35:48 | INFO | train_inner | epoch 032:   1227 / 1474 loss=2.016, trans_loss=4.772, nll_loss=1.974, w2v_ctc_loss=0.665, task_loss=0, contrastive_loss=0.204, total=4158.99, n_correct=2866.57, ppl=3.93, accuracy=68.925, wps=14292.6, ups=1.72, wpb=8318, bsz=312.1, num_updates=46900, lr=6.53023e-05, gnorm=0.584, clip=0, loss_scale=8, train_wall=58, gb_free=16.1, wall=33270
2023-08-12 21:36:45 | INFO | train_inner | epoch 032:   1327 / 1474 loss=1.986, trans_loss=4.764, nll_loss=1.962, w2v_ctc_loss=0.668, task_loss=0, contrastive_loss=0.063, total=4079.56, n_correct=2816.37, ppl=3.9, accuracy=69.036, wps=14253.8, ups=1.75, wpb=8159.1, bsz=297.9, num_updates=47000, lr=6.52328e-05, gnorm=0.565, clip=0, loss_scale=8, train_wall=57, gb_free=17.2, wall=33327
2023-08-12 21:37:44 | INFO | train_inner | epoch 032:   1427 / 1474 loss=2.026, trans_loss=4.768, nll_loss=1.967, w2v_ctc_loss=0.671, task_loss=0, contrastive_loss=0.3, total=4107.37, n_correct=2831.21, ppl=3.91, accuracy=68.93, wps=14117.9, ups=1.72, wpb=8214.7, bsz=304.2, num_updates=47100, lr=6.51635e-05, gnorm=0.576, clip=0, loss_scale=8, train_wall=58, gb_free=17.4, wall=33385
2023-08-12 21:38:11 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 21:38:33 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 4.145 | trans_loss 5.161 | nll_loss 2.418 | w2v_ctc_loss 1.396 | task_loss 0 | contrastive_loss 0.293 | total 4003.4 | n_correct 2677 | ppl 5.35 | accuracy 66.868 | uer 17.941 | wer 19.955 | raw_wer 19.955 | bleu 22.19 | wps 2381.7 | wpb 4003.4 | bsz 141.8 | num_updates 47147 | best_bleu 22.36
2023-08-12 21:38:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 47147 updates
2023-08-12 21:38:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.1900.pt
2023-08-12 21:38:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.1900.pt
2023-08-12 21:38:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint.best_bleu_22.1900.pt (epoch 32 @ 47147 updates, score 22.19) (writing took 19.65099393390119 seconds)
2023-08-12 21:38:53 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-08-12 21:38:53 | INFO | train | epoch 032 | loss 1.991 | trans_loss 4.76 | nll_loss 1.958 | w2v_ctc_loss 0.66 | task_loss 0 | contrastive_loss 0.116 | total 4138.65 | n_correct 2862.97 | ppl 3.89 | accuracy 69.177 | wps 12905.3 | ups 1.56 | wpb 8277.3 | bsz 305.7 | num_updates 47147 | lr 6.5131e-05 | gnorm 0.57 | clip 0 | loss_scale 8 | train_wall 846 | gb_free 16.5 | wall 33454
2023-08-12 21:38:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 21:38:53 | INFO | fairseq.trainer | begin training epoch 33
2023-08-12 21:38:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 21:39:32 | INFO | train_inner | epoch 033:     53 / 1474 loss=2, trans_loss=4.758, nll_loss=1.955, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.166, total=4146.91, n_correct=2873.56, ppl=3.88, accuracy=69.294, wps=7676.1, ups=0.93, wpb=8293.8, bsz=319.7, num_updates=47200, lr=6.50945e-05, gnorm=0.601, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=33493
2023-08-12 21:40:29 | INFO | train_inner | epoch 033:    153 / 1474 loss=1.965, trans_loss=4.743, nll_loss=1.935, w2v_ctc_loss=0.64, task_loss=0, contrastive_loss=0.055, total=4073.36, n_correct=2831.71, ppl=3.82, accuracy=69.518, wps=14114.1, ups=1.73, wpb=8146.7, bsz=285.1, num_updates=47300, lr=6.50256e-05, gnorm=0.566, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=33551
2023-08-12 21:41:27 | INFO | train_inner | epoch 033:    253 / 1474 loss=2.008, trans_loss=4.745, nll_loss=1.939, w2v_ctc_loss=0.647, task_loss=0, contrastive_loss=0.228, total=4283.64, n_correct=2976.87, ppl=3.83, accuracy=69.494, wps=14848.1, ups=1.73, wpb=8567.3, bsz=347.6, num_updates=47400, lr=6.4957e-05, gnorm=0.569, clip=0, loss_scale=8, train_wall=57, gb_free=16.4, wall=33608
2023-08-12 21:42:25 | INFO | train_inner | epoch 033:    353 / 1474 loss=1.981, trans_loss=4.753, nll_loss=1.948, w2v_ctc_loss=0.656, task_loss=0, contrastive_loss=0.081, total=4131.27, n_correct=2860.73, ppl=3.86, accuracy=69.246, wps=14261.6, ups=1.73, wpb=8262.5, bsz=302.3, num_updates=47500, lr=6.48886e-05, gnorm=0.565, clip=0, loss_scale=16, train_wall=58, gb_free=16.2, wall=33666
2023-08-12 21:43:22 | INFO | train_inner | epoch 033:    453 / 1474 loss=1.964, trans_loss=4.739, nll_loss=1.93, w2v_ctc_loss=0.641, task_loss=0, contrastive_loss=0.061, total=4135.1, n_correct=2886.79, ppl=3.81, accuracy=69.812, wps=14510.1, ups=1.75, wpb=8270.2, bsz=309.7, num_updates=47600, lr=6.48204e-05, gnorm=0.555, clip=0, loss_scale=16, train_wall=57, gb_free=15.7, wall=33723
2023-08-12 21:44:21 | INFO | train_inner | epoch 033:    553 / 1474 loss=1.979, trans_loss=4.753, nll_loss=1.947, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.082, total=4132.78, n_correct=2863.66, ppl=3.86, accuracy=69.291, wps=14062.3, ups=1.7, wpb=8265.6, bsz=294.2, num_updates=47700, lr=6.47524e-05, gnorm=0.563, clip=0, loss_scale=16, train_wall=58, gb_free=17.8, wall=33782
2023-08-12 21:45:18 | INFO | train_inner | epoch 033:    653 / 1474 loss=1.994, trans_loss=4.769, nll_loss=1.968, w2v_ctc_loss=0.657, task_loss=0, contrastive_loss=0.118, total=4156.26, n_correct=2870.97, ppl=3.91, accuracy=69.076, wps=14466.6, ups=1.74, wpb=8312.5, bsz=300.7, num_updates=47800, lr=6.46846e-05, gnorm=0.575, clip=0, loss_scale=16, train_wall=57, gb_free=16.2, wall=33840
2023-08-12 21:46:16 | INFO | train_inner | epoch 033:    753 / 1474 loss=1.988, trans_loss=4.762, nll_loss=1.959, w2v_ctc_loss=0.676, task_loss=0, contrastive_loss=0.062, total=4074.99, n_correct=2816.72, ppl=3.89, accuracy=69.122, wps=14145.3, ups=1.74, wpb=8150, bsz=288.2, num_updates=47900, lr=6.46171e-05, gnorm=0.566, clip=0, loss_scale=16, train_wall=57, gb_free=16.3, wall=33897
2023-08-12 21:47:13 | INFO | train_inner | epoch 033:    853 / 1474 loss=1.981, trans_loss=4.75, nll_loss=1.945, w2v_ctc_loss=0.646, task_loss=0, contrastive_loss=0.132, total=4127.6, n_correct=2869.05, ppl=3.85, accuracy=69.509, wps=14426, ups=1.75, wpb=8255.2, bsz=315.3, num_updates=48000, lr=6.45497e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=57, gb_free=16.2, wall=33954
2023-08-12 21:47:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 21:47:35 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.168 | trans_loss 5.17 | nll_loss 2.427 | w2v_ctc_loss 1.45 | task_loss 0 | contrastive_loss 0.293 | total 4003.4 | n_correct 2669.6 | ppl 5.38 | accuracy 66.683 | uer 18.254 | wer 20.286 | raw_wer 20.286 | bleu 22.21 | wps 2309.6 | wpb 4003.4 | bsz 141.8 | num_updates 48000 | best_bleu 22.36
2023-08-12 21:47:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48000 updates
2023-08-12 21:47:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_33_48000.pt
2023-08-12 21:47:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_33_48000.pt
2023-08-12 21:48:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_33_48000.pt (epoch 33 @ 48000 updates, score 22.21) (writing took 38.67017052322626 seconds)
2023-08-12 21:49:13 | INFO | train_inner | epoch 033:    953 / 1474 loss=1.983, trans_loss=4.755, nll_loss=1.951, w2v_ctc_loss=0.663, task_loss=0, contrastive_loss=0.073, total=4157.37, n_correct=2881.63, ppl=3.87, accuracy=69.314, wps=6940.1, ups=0.83, wpb=8314.7, bsz=310.1, num_updates=48100, lr=6.44826e-05, gnorm=0.568, clip=0, loss_scale=16, train_wall=58, gb_free=17.5, wall=34074
2023-08-12 21:50:11 | INFO | train_inner | epoch 033:   1053 / 1474 loss=1.999, trans_loss=4.753, nll_loss=1.948, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.178, total=4134.8, n_correct=2862.71, ppl=3.86, accuracy=69.235, wps=14332.3, ups=1.73, wpb=8269.6, bsz=306, num_updates=48200, lr=6.44157e-05, gnorm=0.562, clip=0, loss_scale=16, train_wall=57, gb_free=15.8, wall=34132
2023-08-12 21:50:35 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-08-12 21:51:10 | INFO | train_inner | epoch 033:   1154 / 1474 loss=2, trans_loss=4.765, nll_loss=1.964, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.167, total=4175.46, n_correct=2884.12, ppl=3.9, accuracy=69.073, wps=14178.8, ups=1.7, wpb=8350.9, bsz=308.8, num_updates=48300, lr=6.43489e-05, gnorm=0.574, clip=0, loss_scale=8, train_wall=58, gb_free=16.5, wall=34191
2023-08-12 21:52:07 | INFO | train_inner | epoch 033:   1254 / 1474 loss=1.98, trans_loss=4.755, nll_loss=1.951, w2v_ctc_loss=0.66, task_loss=0, contrastive_loss=0.067, total=4115.15, n_correct=2851.34, ppl=3.87, accuracy=69.289, wps=14254.6, ups=1.73, wpb=8230.3, bsz=294.4, num_updates=48400, lr=6.42824e-05, gnorm=0.59, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=34249
2023-08-12 21:53:06 | INFO | train_inner | epoch 033:   1354 / 1474 loss=1.982, trans_loss=4.754, nll_loss=1.95, w2v_ctc_loss=0.658, task_loss=0, contrastive_loss=0.085, total=4121.6, n_correct=2858.41, ppl=3.86, accuracy=69.352, wps=14134.9, ups=1.71, wpb=8243.2, bsz=311.7, num_updates=48500, lr=6.42161e-05, gnorm=0.574, clip=0, loss_scale=8, train_wall=58, gb_free=15.6, wall=34307
2023-08-12 21:54:04 | INFO | train_inner | epoch 033:   1454 / 1474 loss=2.001, trans_loss=4.754, nll_loss=1.951, w2v_ctc_loss=0.648, task_loss=0, contrastive_loss=0.233, total=4131.62, n_correct=2860.15, ppl=3.87, accuracy=69.226, wps=14268, ups=1.73, wpb=8263.2, bsz=309.9, num_updates=48600, lr=6.415e-05, gnorm=0.61, clip=0, loss_scale=8, train_wall=57, gb_free=16.6, wall=34365
2023-08-12 21:54:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 21:54:37 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 4.161 | trans_loss 5.166 | nll_loss 2.422 | w2v_ctc_loss 1.442 | task_loss 0 | contrastive_loss 0.291 | total 4003.4 | n_correct 2670.8 | ppl 5.36 | accuracy 66.713 | uer 17.819 | wer 19.79 | raw_wer 19.79 | bleu 22.38 | wps 2431 | wpb 4003.4 | bsz 141.8 | num_updates 48620 | best_bleu 22.38
2023-08-12 21:54:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 48620 updates
2023-08-12 21:54:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 21:54:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt
2023-08-12 21:55:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_best.pt (epoch 33 @ 48620 updates, score 22.38) (writing took 35.17950987443328 seconds)
2023-08-12 21:55:13 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-08-12 21:55:13 | INFO | train | epoch 033 | loss 1.985 | trans_loss 4.753 | nll_loss 1.949 | w2v_ctc_loss 0.654 | task_loss 0 | contrastive_loss 0.115 | total 4138.29 | n_correct 2869.09 | ppl 3.86 | accuracy 69.33 | wps 12439.6 | ups 1.5 | wpb 8276.6 | bsz 305.6 | num_updates 48620 | lr 6.41368e-05 | gnorm 0.57 | clip 0 | loss_scale 8 | train_wall 846 | gb_free 17.9 | wall 34434
2023-08-12 21:55:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-08-12 21:55:13 | INFO | fairseq.trainer | begin training epoch 34
2023-08-12 21:55:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-08-12 21:56:06 | INFO | train_inner | epoch 034:     80 / 1474 loss=1.971, trans_loss=4.741, nll_loss=1.932, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.068, total=4123.05, n_correct=2867.99, ppl=3.82, accuracy=69.56, wps=6719.4, ups=0.81, wpb=8246.1, bsz=300.6, num_updates=48700, lr=6.40841e-05, gnorm=0.582, clip=0, loss_scale=8, train_wall=57, gb_free=16.2, wall=34488
2023-08-12 21:57:04 | INFO | train_inner | epoch 034:    180 / 1474 loss=1.966, trans_loss=4.733, nll_loss=1.923, w2v_ctc_loss=0.645, task_loss=0, contrastive_loss=0.069, total=4066.35, n_correct=2839.38, ppl=3.79, accuracy=69.826, wps=14190.8, ups=1.74, wpb=8132.7, bsz=295.2, num_updates=48800, lr=6.40184e-05, gnorm=0.556, clip=0, loss_scale=8, train_wall=57, gb_free=16.7, wall=34545
2023-08-12 21:58:02 | INFO | train_inner | epoch 034:    280 / 1474 loss=2.021, trans_loss=4.755, nll_loss=1.951, w2v_ctc_loss=0.654, task_loss=0, contrastive_loss=0.284, total=4247.33, n_correct=2939.51, ppl=3.87, accuracy=69.208, wps=14575.1, ups=1.72, wpb=8494.7, bsz=329.5, num_updates=48900, lr=6.39529e-05, gnorm=0.649, clip=0, loss_scale=8, train_wall=58, gb_free=16.4, wall=34603
2023-08-12 21:59:00 | INFO | train_inner | epoch 034:    380 / 1474 loss=1.98, trans_loss=4.732, nll_loss=1.922, w2v_ctc_loss=0.635, task_loss=0, contrastive_loss=0.168, total=4152.22, n_correct=2897.62, ppl=3.79, accuracy=69.785, wps=14286.5, ups=1.72, wpb=8304.4, bsz=316.1, num_updates=49000, lr=6.38877e-05, gnorm=0.587, clip=0, loss_scale=8, train_wall=58, gb_free=16, wall=34661
2023-08-12 21:59:57 | INFO | train_inner | epoch 034:    480 / 1474 loss=1.971, trans_loss=4.744, nll_loss=1.935, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.063, total=4080.7, n_correct=2835.09, ppl=3.82, accuracy=69.476, wps=14243.2, ups=1.75, wpb=8161.4, bsz=286.7, num_updates=49100, lr=6.38226e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=57, gb_free=15.7, wall=34719
2023-08-12 22:00:54 | INFO | train_inner | epoch 034:    580 / 1474 loss=1.966, trans_loss=4.735, nll_loss=1.925, w2v_ctc_loss=0.643, task_loss=0, contrastive_loss=0.065, total=4126.98, n_correct=2878.56, ppl=3.8, accuracy=69.75, wps=14446.6, ups=1.75, wpb=8254, bsz=300.1, num_updates=49200, lr=6.37577e-05, gnorm=0.578, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=34776
2023-08-12 22:01:52 | INFO | train_inner | epoch 034:    680 / 1474 loss=1.967, trans_loss=4.743, nll_loss=1.936, w2v_ctc_loss=0.645, task_loss=0, contrastive_loss=0.059, total=4110.23, n_correct=2856.69, ppl=3.83, accuracy=69.502, wps=14292.8, ups=1.74, wpb=8220.5, bsz=297.1, num_updates=49300, lr=6.3693e-05, gnorm=0.572, clip=0, loss_scale=8, train_wall=57, gb_free=16.3, wall=34833
2023-08-12 22:02:50 | INFO | train_inner | epoch 034:    780 / 1474 loss=1.988, trans_loss=4.763, nll_loss=1.961, w2v_ctc_loss=0.646, task_loss=0, contrastive_loss=0.13, total=4087.05, n_correct=2828.25, ppl=3.89, accuracy=69.2, wps=13993.3, ups=1.71, wpb=8174.1, bsz=297.4, num_updates=49400, lr=6.36285e-05, gnorm=0.62, clip=0, loss_scale=8, train_wall=58, gb_free=16.2, wall=34892
2023-08-12 22:03:48 | INFO | train_inner | epoch 034:    880 / 1474 loss=1.979, trans_loss=4.752, nll_loss=1.946, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.084, total=4088.94, n_correct=2839.22, ppl=3.85, accuracy=69.437, wps=14181.5, ups=1.73, wpb=8177.9, bsz=294.2, num_updates=49500, lr=6.35642e-05, gnorm=0.568, clip=0, loss_scale=8, train_wall=57, gb_free=14.8, wall=34949
2023-08-12 22:04:46 | INFO | train_inner | epoch 034:    980 / 1474 loss=1.978, trans_loss=4.749, nll_loss=1.943, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.08, total=4175.9, n_correct=2897.59, ppl=3.85, accuracy=69.388, wps=14479.4, ups=1.73, wpb=8351.8, bsz=312.7, num_updates=49600, lr=6.35001e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=57, gb_free=14, wall=35007
2023-08-12 22:05:43 | INFO | train_inner | epoch 034:   1080 / 1474 loss=1.974, trans_loss=4.748, nll_loss=1.941, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.065, total=4152.17, n_correct=2886.26, ppl=3.84, accuracy=69.512, wps=14422, ups=1.74, wpb=8304.3, bsz=309, num_updates=49700, lr=6.34361e-05, gnorm=0.558, clip=0, loss_scale=8, train_wall=57, gb_free=14.6, wall=35065
2023-08-12 22:06:41 | INFO | train_inner | epoch 034:   1180 / 1474 loss=1.976, trans_loss=4.75, nll_loss=1.945, w2v_ctc_loss=0.652, task_loss=0, contrastive_loss=0.076, total=4101.68, n_correct=2848.08, ppl=3.85, accuracy=69.437, wps=14107, ups=1.72, wpb=8203.4, bsz=298, num_updates=49800, lr=6.33724e-05, gnorm=0.568, clip=0, loss_scale=8, train_wall=58, gb_free=16.3, wall=35123
2023-08-12 22:07:39 | INFO | train_inner | epoch 034:   1280 / 1474 loss=1.973, trans_loss=4.746, nll_loss=1.94, w2v_ctc_loss=0.655, task_loss=0, contrastive_loss=0.062, total=4146.01, n_correct=2876.72, ppl=3.84, accuracy=69.385, wps=14437.7, ups=1.74, wpb=8292, bsz=300.6, num_updates=49900, lr=6.33089e-05, gnorm=0.561, clip=0, loss_scale=8, train_wall=57, gb_free=17.3, wall=35180
2023-08-12 22:08:37 | INFO | train_inner | epoch 034:   1380 / 1474 loss=1.992, trans_loss=4.755, nll_loss=1.951, w2v_ctc_loss=0.659, task_loss=0, contrastive_loss=0.125, total=4197.99, n_correct=2910.25, ppl=3.87, accuracy=69.325, wps=14500.4, ups=1.73, wpb=8396, bsz=321.1, num_updates=50000, lr=6.32456e-05, gnorm=0.574, clip=0, loss_scale=8, train_wall=57, gb_free=17.3, wall=35238
2023-08-12 22:08:37 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-08-12 22:08:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-08-12 22:09:00 | INFO | dev_st | epoch 034 | valid on 'dev_st' subset | loss 4.149 | trans_loss 5.16 | nll_loss 2.415 | w2v_ctc_loss 1.42 | task_loss 0 | contrastive_loss 0.283 | total 4003.4 | n_correct 2675.2 | ppl 5.33 | accuracy 66.823 | uer 18.467 | wer 20.689 | raw_wer 20.689 | bleu 22.06 | wps 2229.3 | wpb 4003.4 | bsz 141.8 | num_updates 50000 | best_bleu 22.38
2023-08-12 22:09:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 50000 updates
2023-08-12 22:09:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_34_50000.pt
2023-08-12 22:09:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_34_50000.pt
2023-08-12 22:09:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_shrink_v1_merge_large_0812_doubleCL_alpha2.5_mt0.5/checkpoint_34_50000.pt (epoch 34 @ 50000 updates, score 22.06) (writing took 21.92443390004337 seconds)
2023-08-12 22:09:22 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)
2023-08-12 22:09:22 | INFO | train | epoch 034 | loss 1.979 | trans_loss 4.746 | nll_loss 1.939 | w2v_ctc_loss 0.65 | task_loss 0 | contrastive_loss 0.102 | total 4133.03 | n_correct 2871.94 | ppl 3.84 | accuracy 69.488 | wps 13432.4 | ups 1.62 | wpb 8266.1 | bsz 304.3 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.578 | clip 0 | loss_scale 8 | train_wall 791 | gb_free 17.3 | wall 35283
2023-08-12 22:09:22 | INFO | fairseq_cli.train | done training in 35233.0 seconds
Exception in thread Thread-3:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-6:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
Exception in thread Thread-7:
Traceback (most recent call last):
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/threading.py", line 932, in _bootstrap_inner
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    self.run()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/site-packages/tensorboardX/event_file_writer.py", line 202, in run
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    data = self._queue.get(True, queue_wait_duration)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/queues.py", line 111, in get
    raise EOFError
EOFError
    raise EOFError
EOFError
    res = self._recv_bytes()
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 414, in _recv_bytes
    buf = self._recv(4)
  File "/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
/home/zhangyh/miniconda3/envs/st-at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1472 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
