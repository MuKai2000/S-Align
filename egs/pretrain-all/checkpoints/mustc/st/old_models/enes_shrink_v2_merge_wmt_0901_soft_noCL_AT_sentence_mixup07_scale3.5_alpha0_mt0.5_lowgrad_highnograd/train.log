2023-09-01 11:39:35 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:13686
2023-09-01 11:39:35 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:13686
2023-09-01 11:39:35 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:13686
2023-09-01 11:39:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-01 11:39:35 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:13686
2023-09-01 11:39:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-01 11:39:35 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:13686
2023-09-01 11:39:35 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-01 11:39:35 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:13686
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:13686
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:13686
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-01 11:39:36 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-01 11:39:36 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-01 11:39:40 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:13686', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-01 11:39:40 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-01 11:39:40 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-01 11:39:40 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-01 11:39:40 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-01 11:39:40 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline
2023-09-01 11:39:44 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-01 11:39:44 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_enes_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-01 11:39:44 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-01 11:39:45 | INFO | root | load pretrained hubert
2023-09-01 11:39:53 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline
2023-09-01 11:39:57 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt
2023-09-01 11:40:03 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt
2023-09-01 11:40:03 | INFO | root | share the sematic adapter and textual encoder
2023-09-01 11:40:03 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-01 11:40:03 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-01 11:40:03 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-01 11:40:03 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-01 11:40:03 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-01 11:40:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-01 11:40:03 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-01 11:40:03 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-01 11:40:03 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-01 11:40:03 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1312, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-01 11:40:19 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-01 11:40:19 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-01 11:40:19 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-01 11:40:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-01 11:40:19 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-01 11:40:19 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-01 11:40:19 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-01 11:40:19 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-01 11:40:19 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-01 11:40:19 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-01 11:40:19 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-01 11:40:19 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-01 11:40:19 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-01 11:40:19 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-01 11:40:19 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-01 11:40:19 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_last.pt
2023-09-01 11:40:19 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_last.pt
2023-09-01 11:40:19 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-01 11:40:19 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-01 11:40:19 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-01 11:40:19 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-01 11:40:21 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=260049, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-01 11:40:23 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=260049, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-01 11:41:12 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-01 11:41:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 11:41:12 | INFO | fairseq.trainer | begin training epoch 1
2023-09-01 11:41:12 | INFO | fairseq_cli.train | Start iterating over samples
True False
tensor([[  33,   27,  298,   34,   44,    6,  660,  136, 6745,    0,   23,   14,
           92,   81,  269,   22,    0,  148,  763,   18,   33,  634,   27,  101,
           70, 6978,   47, 2813,  997,   70,   22,   27,   47,  232,  161,  997,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  20,   14,   68,   91,    0,   24,  119,   64,    0,   20,   14,   92,
         1577,    8,  562, 2289,  132,  142, 4481,  454,   19,   84, 5939,    0,
           38,    0,   57,  177,   55,   23,   59, 1395,  485,   96, 1183,  886,
         1204,    0,  132,  232, 3165,  906, 5918,    0,   87,   14,    6,    9,
          634, 2122,  440,    0,    2],
        [   9,  393,  254,    6,  333,  148,   23,  286,   12, 3792,    8,  805,
         4013,  108,   63, 3791, 1201,  327,    8, 1901,   63, 3791, 2411,   27,
          236,   91, 6479,    0,  936, 4884,  447,  647,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  23,  658,    9, 2950, 4928,   47,    9,  888, 8140,  758,   15,  276,
           58,   97, 6655,  173,   30,   14,   28, 5691,  115, 1055,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [1059,   24,   14,   44,  401,  231,    0,   20,   14,   68, 3678,  152,
            0,  741,   14,    6,  315,    0,   87,   89,    0,   20,   48, 3039,
         1564,    9, 2205,   11,    9, 3130,   48, 1900,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [2317, 3878, 2819,  117, 1579,   47,   88,   67,   33,    0, 4555,    0,
            0,   11,    0,    0, 3400,  424,  240,   12,  762,  200,    0,   24,
         1737,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  23,   59,  219,   51,   97, 6926,   11, 5880,   97, 2550,    6,   11,
           23,  195, 1323,   97, 1277, 1363,  105,   67, 2290,  132,  139,    6,
          134,  715,   12,   79,   33,  260,  857, 3751,   47,   97, 8285,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  11,   70,   46, 1681,  115,    0,   46,  195, 1681,  115,  327,    9,
         2430,    6,    0,  134, 4929,    6,  195, 2989,    9, 3307,    6,   19,
          118,    0,   11,  191,    9, 5557,    6, 1267,  721,   24,  174,   14,
           28, 1373,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  11,   22,   27, 1182,   18, 1477,   18,   23,  151,   65,  158,   49,
           11,   33,   27,    9, 3866, 1019,   49,  236,   18,   27,   73, 4947,
         1053,  112,   97, 3760,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  11,  138, 1083,   27, 3100, 7478,  329,    0,   95,   24,   14,   44,
         8826,  215, 2613,   12,    0,    0,   90,   25,  683, 2269,   61,    0,
         3029,   15,  788,    0,  102, 5998, 3070,  346,   12,  138,  425, 5885,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  23,  119,   18,    8, 1899, 1373,   12,   33,   27,   73,  101,    9,
         6309, 1607,    0,   58,  400, 1209, 1411,   18, 6309,   67,    8, 1380,
          132,   14,    6,  131,   12, 2707,   18,  869,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  11,   95,   24,   14,   92, 2782,   19, 4461,   19,    9,  493,  664,
          200,    0,    9,  819, 3045,   49,   24,  195,   59, 1326,  701,   18,
         9006,   88,  549,  677, 1271,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 104, 1137, 1708, 1468, 2716,  107,   15,    0,  320,  374,    0, 2019,
            6,   24, 2234,    0,    0,    0, 2716,  107,   15,  320,  374, 4423,
            0, 2136,    0, 2716,  107,    0,   15,    0,  320,  374, 3513,    6,
            0,    0,    0, 2716,  107,   15,  320,  374, 2342,    0,    2,    1,
            1,    1,    1,    1,    1],
        [ 308,  107,   15,    9, 4315,    0, 3024,  884,  328, 2942,    9, 2760,
            0, 3279,    6, 4469,   60, 1351, 1776,    6,   11, 1115,  129, 4855,
            6,    0,  855, 5953,  648,    6,    0,  191,   24,  158,  118,  451,
          377,   14,    6,    0,   14,   20, 1118,   12, 1148,    0,  855, 8414,
            0,    2,    1,    1,    1],
        [ 310,   14,    6,    0,    0,    0,   73, 3824,   18,  171,   15,    0,
            9, 4338,    0, 3932,   15,  406, 3085,   51,    0,    0,   54, 3932,
          973,   70, 4515,    0, 7694,   14,    6, 1516,    0, 2825, 6335,  333,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   0,   74,   27, 1801,   18,   24,   65,    0, 2803,   62,    0,   65,
           20,   79,   85,  122,   85,   63,  754,  183,  124, 3513,    0,  421,
           24,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:0') tensor([38, 53, 34, 24, 34, 27, 37, 40, 30, 39, 34, 32, 47, 50, 38, 27],
       device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8257, 1.0000, 1.0000, 1.0000,
        0.8760, 1.0000, 1.0000, 0.7622, 1.0000, 0.7617, 0.9165],
       device='cuda:0', dtype=torch.float16)
2023-09-01 11:41:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
True False
tensor([[ 103,   20, 5299,    9,  240,    0, 1248,  640,   50,   11,    0,   84,
         3981,    0,   20,    0,  241,    0,   41, 4664,    0,   65,   14,   28,
           20,    0,    0,   79,  236,    0,  342,   65,   14,   28,   20,    0,
            0,  453,  236,   18,  173,  941,   47,    8,    0,  540,  169,   19,
           97, 1088,    3,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  46,   14,   44,   64,   20,   14,   68, 5825,  152,    9,   41,  496,
          309,   28, 2392, 2078,  242,   11,    9,   41,  496,  838, 1590,    6,
         2078,    3,    9, 1720,  838, 1590,  654,  195,  888,   92,  408, 3031,
           19,    9,  373,    0,  103, 2882, 2392,   27, 3865, 5471, 1082,  520,
            0,   11,    9, 1720,  838, 1590,  654,  195,  752,   11, 2151,  264,
            9, 4056,    6,   15,  169,    0,   11, 1249,    9,    8,   83, 4367,
          576,  462,   15,  905, 4317,   96, 7808,    9, 2882, 2392,  914,   38,
         2335,    0,    2],
        [  11, 1055,    0, 1084, 1383,   20, 2382,    0,   32,  943,  121, 2348,
          346,    0,   27,   73, 1077,   64,    8, 2341,   27,   93,  191,   46,
          308,   96,    0,  121,  145,   23,  158, 1115,   12,  869,   72,    9,
          161,   14,    6, 2341, 6261,    6,  103,   22,   14,    6,   90, 4609,
          275,   46,   65, 5848,  105,  658,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  20, 1054,  610,   62, 5154, 5122,   49, 6078, 2327,    0,  681,   14,
            6, 1263, 4548,    6,   49,   11,   20,  529,  118,  118,   53,  973,
           70,    0,   24,  119,    0,   64,   22,   14,    6,   91,   12,   69,
           19,    9,  716, 1813,    0,   93,  380, 3052,  885,   14,    6,  373,
            0,   11,   46, 2404,   50, 8823,   53,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 104, 1796,  204,   73, 8143,  124, 2408,  115,    9, 2586,    0,  106,
           12,   79,   38,  145,   59, 2145,  626,  723,   12,  741,   46,  126,
          221, 3249,   11,  741,   46,  126,  308,   12,  691,   72,    9,  715,
           11, 3636,   15, 2586,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [5381,  327,    9, 7299,  530,   79,   73,  119,   64,   14,    6, 7808,
           22,    0,   58,   64,   23,   79,  119,   27,   18,    0,   67,    9,
            7,   83, 1430,   60, 2287,   15,   69,   53,    0,    9, 4044,   15,
          233, 8851, 6701,   11, 8545, 2975, 1457,   18,   23,  129,  105,   62,
           47,  882,   27,  131,  115,   19, 3157,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  58,   74,   14,    6,   85,  275,   18,  678,   14,   28,  158,  246,
         2071,    0,  121,   27,   18, 1277,   15, 5077,    6,    6,  136, 4882,
          986,   81,   15,  230, 1606,   15, 7964,    6,  695,  757, 1139,   67,
            9, 1199, 3399,   15, 3577,   11, 4151,   11, 7603,    6,   18,   54,
         2955,   19,   33, 1199,  420,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  74,  145,   69,    8, 7764, 1019,   95,   24,  142, 5691, 8121,   60,
            0,   58, 1020,    9, 4874,   15,    8,  499, 6498, 1390, 2195,   12,
         1383,    0,    9, 6712,   12, 8121, 7784, 7432,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:4') tensor([52, 87, 56, 57, 42, 57, 55, 34], device='cuda:4') tensor([0.7808, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
True False
True False
tensor([[ 104,   54,   73,  101, 2975,  125, 3333,    0,   70,   24,  508,   59,
         1085, 2908,   51,   19,  277, 2227, 2083,   51,   19,   32,   34, 1813,
           49, 4143,  260,   74,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  46, 2587,   49,   24,   65,  139,   62,    9,  948,  227,   85,  191,
           22, 2345,   12,  101, 1542,  313,   62, 1135,   15,  555,  206,    0,
          939,    0,  424,  939,    0,  424,  939,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  19,    9, 1610, 5791,    6,    0,    9, 5109, 9307, 3369,  775, 2091,
           12, 6890,  124,    8,  877, 5018, 4991, 8116,   19,  963,   28,   28,
            6, 4123,  362,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [1014,  418,    0,   11,  245,    0,    9,  234,   72,  593,    0, 7656,
            6,    0,   46,   14,   44,  692,   47,  593,  103,   46,  218, 2343,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  38,   22,   14,    6,    8,  365, 4677,   47,   73,  283,   47,    9,
         7680,    6,   11,    9,  410, 7510,    6,   11,    9, 5560,   20, 2767,
         2196,    0,  205, 1737,  103,   74,   14,    6,   32,  882,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  67,  138, 1282, 1909, 4794,    0,   24,  610, 1454,   63,  785,   63,
          785,    0,    0,    0,    0,   11,   24, 6329,   47,    8, 5459, 1141,
           51,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  58,  250,   15,   81,    0,   64,   46,  199,   27,   46,  199,   24,
            0,   46,  199, 1383,   12,  119,   18,  134, 3287,   85,    6,  316,
         1133, 1172,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [   0,    0,    0,   23, 2878,   19,   90, 2781,  700, 9383,    0,    0,
         2539,  392,    0,  497,    0,    6,    0,    0, 1042,    0,  394,   19,
           97, 3373,  953,   60,  634,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [1525,    0,   38,   95,   24,  241,    8,    0,  165,    0,   33,  763,
           18,   24,   54,    8, 1194, 1025,  108,   11,   24, 1025,  823,  103,
          788,   27, 1336,   61,   19,    0,    8, 1194, 7480,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 266,   75,    9, 1374,    0,  437,   50,   64,   24,  137,    0,   41,
          655,   89,   14,  150,  529,   50,   18,  788,    0,   11,   23,   14,
          150,  218,    9,  869,   15,    9, 2705,  511,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  11,   70,    9,  373, 1155,    0,  112,    9, 2516, 4365,    0,   46,
         5717,    0,    9, 6895,    0,   11,  165,  700,  881,   62,   75, 1275,
            0, 1458,    0,  103,    9,  277,  539,   51,  199,    6,   22,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  38,    0,   96,    8, 7503,   14,    6,  643,   15, 1407,    0, 3188,
           27,  197,    8, 2165,   14,    6,  196,   15, 4604, 1607,    0, 1281,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  20,  119,   23, 1045,   24,   12,   79,   33,  460,   62,  961, 1635,
         2960,    0,   11,   24,   14,   92,  126,  914,    8, 3363, 1319, 1794,
          856,   33,  275,  533,    0,   38,  421,   24,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  38,  751, 3215,   19, 3760,    0,   11,   19,   19,  422,  925,  431,
         3760,    0,   47,   50,   27,  914, 8073,   60,    0,  914, 3175,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 103,   24, 1542,  313,    0,   88,   19, 5589,    6,    0,    0,    9,
           85,    6,    0,    0,   19,    9, 4793,    0,    0,  266, 4082,  239,
            9,   85,    6,    0,    0,   19,    9,   47,   99,  135,   34, 2561,
            0,    0,   49,   58,    0,   73,   19,    9,  161,   15,  620,  140,
          350,  349,    0,    2],
        [  23,  245,   59,  594,   92, 3616,  179, 4392,    0,  132,   27,    9,
         1391,  108, 1185,   15,  557,   83,   51,    0,   87,   14,    6,    9,
           85,  132,  232, 7146,   61,   33, 4555,   11, 5029,    9, 8065,   69,
          150,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1]], device='cuda:3') tensor([30, 33, 29, 27, 36, 27, 28, 31, 35, 34, 37, 26, 34, 25, 52, 40],
       device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8257, 1.0000, 0.7617, 0.9497,
        1.0000, 0.9092, 1.0000, 1.0000, 1.0000, 0.7266, 1.0000],
       device='cuda:3', dtype=torch.float16)
True False
True False
tensor([[  22,   65,   59,   81,    9, 2221, 2361,    6,   15,  188,   77, 1752,
          106,   22,   27,  435,   15,    9,  354, 1287,    0,   11,  511, 1055,
            0,   74,   27,   32, 3684,   12, 4514,    0,   32, 5334,    6,   93,
         3656,   14,    6,  525,   53,    0,   11,   32, 3340,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  22,   27,   38,  778,    0,   24,   14,   44,  701, 3062,   60,  138,
         3684,  107,    0,  733,   12,  308,  115,   67,   18,  747,   63, 8595,
           63, 5775, 1411,  234,    0,   58,   20, 1882,   18,    9,  325,   28,
           63,  418,  296,  430,   48,  131,  231,    0,  106,   20,   48,  538,
           12, 2100,   11, 6308,   84, 2071,   47, 1929,  239,   20,  126,  490,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 231,    0,  419,  313,  179,    6,   48,  227,   11,   87,   48, 1172,
            0, 1081, 1626,   44,   83,  299,    0, 1997, 1240, 1823, 3069,  125,
         1094,  934, 1081, 2497,  125, 1094,  934,  241,    0,   41,  655, 5077,
           30,  397,   14,    6,   13, 1240,   11,  805, 1240,   86,   14,  135,
          159, 1240,   54, 3981,    6, 6558,    9, 3021,    3,   57,  361,   55,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  20,  158,   12,  928,   84, 2344,   12,   84,   70,    6,  633, 1092,
            0,   11,   20,   14,   68,  692,  115,   75,  543,    0,   11,   20,
           14,   68,  131,    0,   41, 6018,   80,    0, 2590,   80,    0, 2590,
           80,    3,  148,  763,    0,   41,  216,  592,   84, 2344,    3,   11,
           87,  137,    6,   20,   14,   68, 1167,    0,   41,  216,  592,   84,
         1159,    3,   38,   23,  126,   33,  282, 3715, 2043, 6458,    0,   57,
          177,   55,   58,   22,   14,    6, 2539,   22,    0,    2],
        [  23, 1621,    9, 4898,   15,    9, 5180, 5823,   15,    9,   19,    6,
          223, 1707,    0,    8, 2038, 5789,   15,    9, 4794,    6,   11, 9535,
            6,   18,    0, 1301,   75,    9,    0, 1790, 8556,    0,  121,   46,
           14,   44, 6990,    0,  121,   12, 6559,  118,    0,   64,  942,    6,
          118,   11,   64, 1547,   19,  134,  196,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,  152,   20, 1381,  643,  107,    0,   15,  588,    0,   18,   24,
           54,   73, 3262,   12,  335,   33,   70,   90, 1480, 4064,   12,  453,
           88,   14,    6, 3521,    6,  490,   90, 4287,    0,   11,   23, 3662,
            7,  237,  296, 4877,   61,  118, 8492,   11, 1765,  118,  320, 2116,
           12,  453,  264,   12, 1964,   46,  488,  232,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9, 4349,  422, 2027, 6541,  108,    0,   12, 2021,   35,   51,  135,
         1869,  313,    0, 4718,    8, 1269, 4875,  191,    9, 2369, 5757,    0,
          148,   48,  435,  107,   15, 3176,    6,   11,  525,    6,   15,  814,
         1869,  135,    6,    0, 3163,  115,  189,    9,  502, 5757,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 722,  905,  112,  905,   67, 5287, 2624, 1138,  212,  237,  962,  223,
            6,   49, 2141,    0, 3421,    0, 4057,    6,   49,   20, 1733, 3268,
           73,  283,   12,   90, 1575, 9597, 1864,    0,   58,   12,    8,  558,
         9421,  686,  863, 1864,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([47, 62, 61, 82, 57, 58, 48, 42], device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.9668, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
True False
True False
tensor([[  97,  232,  648,   48,   62, 5152,   51, 5589,   19,    9, 1903,  452,
          432,   15,   97,  853,    0, 1030,   67,    7,   44, 1376,   28,    6,
            0, 1221,  370,  377, 1823,    6,   11, 3162,   88,    0,    9,  277,
          341,  494,   15,  497,   34,  885,   81,  233,    0,   62,    9, 2205,
            0,   19,    9,   81, 2515,    0,   19,    9, 3079, 1813,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  38,   20,  837,   84,  502, 3889, 1191,   12,  553,    8, 3774,    0,
          433,   41,    6,   89,  838, 5178,    3,   22,   48, 2582,   12,   69,
            8, 1498,   11,  788, 2640,   18,  435,   22, 4588,   12,  658,    8,
         2158, 2379,    0,   11,   47,    9, 1106,  719,  200,    0,   22,   14,
            6,  221,    8, 6480,   47, 3575, 5790,    6,   11, 3983,    6,  722,
           62,    9, 3074,   19, 7267,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  46,   59, 1790,   89,   63,    6, 2561,   60, 1170,    6,   91,   41,
            6,  842,  454,   89,   68,  220, 3893,  242,   93,   41, 1774,  496,
         1632, 5123,    6,    3,   20,   14,   68,   73,  131,   12,  194,  189,
            9, 3310,    6,   15,  104,  607,  102,    0,    0,   58,    9, 1899,
          643,   27,    0,   33,    0,   95,  414,   15,  118, 1941,   61,   33,
         4853,  105, 3678,   63,  339, 1881, 2064,   15,    9, 8894, 1725,    0,
          165,   23,  470,  139,  248,    0, 5400,  356, 1621,   75,    9, 1980,
          362,   83, 1564,   67,    9, 8894,  628,  775,    0,    2],
        [  38,   20, 1399,   12, 2100,   62,  751,  609,   11,    0,    8,  322,
          105,  571,    6,   12,  577,    0,  218,    9,  250, 3503, 3059,   19,
          686, 5574,    0,    0,   11,   18, 3059,   27,    9,    0, 3370,  299,
           15,    0, 1153,    0,  103, 1115,  173,  221, 4815,   61,    0,    0,
            0,    0,    0, 1153,   46, 5495,    8, 1586,   12,  686, 5574,   11,
          470,    0,   69,    7,  216,  225,   61,    0,    0,    0,   93, 1153,
            0,   46,  174,    0,    0,   14,   28, 5495,    8, 1586,    0,   12,
          686, 5574,   11,  470,   69, 6445,    0,    2,    1,    1],
        [  11,    9,  308, 1240,  881, 1564,    0, 6016,   67,    8,  318,   15,
            9,  354, 2336,  191,   97, 7482,    6,   54,    9, 2483,  452,   49,
         3345,    0, 2708,    0, 3836,    0, 1714,  476,   49,  283,  112, 2078,
           60,  118,  347, 4803, 1168,   15,    8,   51,   44,  322, 1430,    0,
           23,  158,   16,  696,  125,  731,   30,    6,   11,    9,   82,  970,
         1976,   15,  184,  177, 1543,   97, 1906,    6,  189, 3602,    6,    0,
         5496,   60,    8, 5612,   11, 7745,  643,   15, 1407,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 250,   15,    9, 2289, 6373,   61,    9, 5285,   15,   18,    0,   11,
          241,    0,   41,   77,   44,   24, 3162,    0,  132,   27,  131,   12,
         2612,    8, 3623,   15,  282, 2289,    0, 5474,   61,  115,   49,  242,
           57,  177,   55,   41, 9758,   12,  308, 1136,    8, 6486,   11, 3326,
           67,  134, 3543,  273,   53,   19, 2096,  307,  496,  513,  237, 5218,
            6,    3,  106,   18,   14,    6,   64,   46,  433,  118,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,    8, 2723,   48, 1621,    0,   90,    0, 8416, 4167,  475,    0,
           48, 1276,   19,  270,  430,   88,   12,    0, 3362,    9,    0,    0,
         2723,    0,    0,   11,   74,   48,    0,    0,   32,    0, 1111,    0,
           41, 2331,    0,    0,   54,   23,    0,  131,   12,   79,    3,   41,
          127,  174,   14,   28,    0,  119,    3,    0,   19,    8,  562,    0,
         1090,    0,    0, 1617,    6,   15, 1592,    0,   15,    0,   88,   74,
           49, 7015, 8821,    6,    0,  132,  142, 2503,    9, 3297,    6, 3893,
           15, 2553, 8529,    6,    0,   41,  179, 6110,    0,    2]],
       device='cuda:6') tensor([60, 67, 94, 92, 83, 72, 94], device='cuda:6') tensor([1.0000, 1.0000, 0.9668, 0.7808, 1.0000, 1.0000, 0.7266],
       device='cuda:6', dtype=torch.float16)
True False
True False
tensor([[  11,   18,  763,   18,   23,  286,    8, 2534,  105,    0, 2534,  105,
          122, 7708, 1053,   15,   88,   12,  942,  230, 3385,    0,   12,   73,
          139, 2733,   70, 7057,   11,   39,  408,  105,   11, 7509,   11, 3220,
            0,   12,  139,  118,   70,  205,   18,   46,   65,  668,   30, 2961,
           11, 1267,  327,   11, 8777,    0,   11,   38, 4187,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  93,  255,  103, 9153,    0, 1650, 1130, 1129,    0,   91,    8,  358,
          644,   28, 1994,  622,    0,   19, 1637,  377,  352,  731,  295,    0,
         1054,   23,   59,   88, 1030,    0,   19,    9,  354, 1410,   67,    9,
          354, 1814,   15,  788,    0,    0,   11, 1006,   15,  118,  195,   69,
           90, 1126,  804,   72, 7143,   11, 1006,   15,  118,  195, 5821,   22,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 233,    3,   15,  118,    0, 3146, 2773,    6,    0,  584,  235, 2208,
          150,  422,    6,    0, 4564,    6,   11, 6443,    8,   51, 1645,  454,
           96, 1135, 7980,  973,   70, 2415,   89,  183,    0,  351,  154,    0,
           69,   34,  592, 2799,   11, 9220,    0,  567,   62, 3502,   12,  577,
           97, 1521,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,  165,   23,  605,   22,    0,   19,    8, 2052,    0, 2484,   65,
           89,  307,  521,    9, 4633,   15, 4574,    0,   23,  605,    9,  232,
         1214, 3050, 2808, 1679,  277, 1810,    6,  587, 3122,    0,  227, 6558,
           81,    9, 8124,  454,    0,  152,   22,   27,    0,   46,   14,   44,
          101,   72,    8,   50,  411,  540,    0,   22,   14,    6,  433,    8,
          277, 1810,    6,  587, 3122,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 287,   88,   78, 1818,   18,  101,   70,   23,   14,   92, 2145,   96,
         9571,   19,   85, 1330,  536,   12, 2624,  127,  150,  333,    6,   15,
         1330, 1652,    0,   85, 6423,   12,  287, 6876,    0,   23,   59,   12,
          194,   12,  287,  502, 6069,    6,   96,   85,  502, 6069,    0, 1711,
          104,  287,  502, 6069,    6, 6293,   60,   90, 8887, 6569,   15, 6948,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  58,   74,   14,    6,    8, 4589,  105,   19,    6, 7164,  352, 5003,
            6, 3871,   49,    9,  570,    0, 2423,    0,   18,   88,   67, 5862,
         9537,   54,  315,  106,   46,   54,  152,   12, 2469,  141,  236, 3220,
          136,    0,   93,   46,   54, 6929,  105,   90,  496, 1376,   11,  701,
         6331,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,   64,   23,   14,   44,  733,   12,    0,   79,  102,   27,   12,
         1491,  107,   95,   23,   65,  197,  335,   33,  634,   12, 1758,    9,
         1011, 2682,    6,   15,  276,   19,    0,    8, 3250, 6569,   15, 3932,
            0,   65,   23,  197, 3351, 3976,    9, 4452, 2203,    0,   19,    8,
         2699,   38,   18,   23,   65,  197,  194,  424,   22,   19,    8, 2592,
          196,   11, 2707, 2884,   18,  508, 2122,  456,  107, 1458,    9, 1287,
           18,    0,   23,  199,   12,    0,    2],
        [9287,   81,    9, 3663,   24,  126,   12,    7,  125,  160,   89,    0,
          121,   12, 2821,   80,  284,  974,    0,  121,   12, 8678,    0,  121,
           12, 5802, 2566,    0,  121,   12, 7333,   99, 4545,   19, 3074,    0,
           81,  230, 3663,   18,   16,  329,   51,   24,   12,  878, 5508,   62,
         3502,   11,   12, 7741, 7582,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([59, 62, 52, 67, 62, 51, 79, 55], device='cuda:0') tensor([1.0000, 0.9258, 1.0000, 1.0000, 1.0000, 1.0000, 0.9517, 1.0000],
       device='cuda:0', dtype=torch.float16)
2023-09-01 11:41:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
tensor([[   9, 3037,   27, 1863,   30,  112,  392, 1991, 6065,    6,   18, 2612,
          129,  440,  996,   12,  864,  189,  414, 3335,   11,  245,  864,  327,
          271,  425,    8, 1126,    6,   75,    9,  354,  169,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  12,  445,  107,    0,  310,   14,    6,  553,  112,  692,   75,  236,
           62,  148,  799,   11,  573,  151,   79,  419, 1068,  108,   11, 6732,
          430,  171, 7042, 4368, 1402,    6,   62,    9,  354, 1779,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  20,   14,   68,  131,   12,  460,    8,  282,  525,   19,    8, 3706,
           72,  121,   23,   65,  335,   33,  326,   15,  788,   12, 1025,  236,
           18,   14,    6,  131,   12,  577,  141,  137,   72,    9,  161,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  46,  488,    0,   41, 1810,    0,   23,   14,  150,  752, 7507, 1340,
           28,    6,    0,  240,   63,  341,  328,    6,   15,    8,  647,    0,
           18,   14,    6,   73,  131,   12,   79,  988, 2065,   12,  543,    3,
           38,   46,  316,   18,    0,    2,    1,    1,    1,    1,    1,    1,
            1],
        [  23, 1910,   12, 9696, 4592,    6,   70, 7610,  105,  897, 1359,    6,
            0, 1215,   63, 7868,  127,   92,  573,    0, 4601,   51,  315,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  11,    9, 1700,   18,   32,   85,   27, 3738,   12,   50,  218,  141,
          199,   12, 1921,  169,   67, 3172,   18, 2621,   12,  869,   72,  141,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  38,    8, 5358,    0,   96, 2034,   65,  151,  943,  106,   22,   65,
         8960,   97, 4725,   47, 1215,   63,   44,  398,  154,   83,  299,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  11,  165, 1807,   18,    0,   23,   65, 1491,  107,  121,    9,  524,
          470, 2384,   19,    9,  801,    0,   38,   64, 9595,   22,  470, 2638,
            0,  121, 2093,   22,  470, 2824,  345,   93, 3028,  115,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 103,  139, 1581,   50,  160,   51,  431, 6769,    0,  266,   32, 2924,
          239,  594,  418,   30,   11, 1286,    6,  377,    8, 2425,   30, 8393,
            6,   93,   12,    9,  908,  817,  133,   28,   15, 2460,  587,  571,
         1309,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  22,   48,   18,  748,   15,  897,   89,  635,   72,   64, 2758,    6,
           18, 2781,   50,   12,  301,   84, 1127,   80,  190,   28,  521,   47,
            8, 1794,   11, 2697,  521,   12,    9,  543,  136,   77,   89,  223,
         6731,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  11,   23,   14,   92,  245, 3271,  474, 2426,   22,   38,   18,    9,
         3925, 6367, 4943, 1388,   62,  414, 4194, 3201,   67,    8, 2344,    0,
           91,   33,   20,  376,   51,    0,   38,  310,   14,    6,  529,   33,
            8,  752,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  85,  493,  665,   27,   18, 2482,   23,    0,   91,  493, 4131,    0,
           23, 1702,    9, 5060,   12,  800,    8,   32, 2409,  431, 1872, 2357,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  23,  286,   12, 1921,  169,  102,   12,  151, 1795,   97,  227,   12,
         1323,   33,  283,   67,   88,   18,   23,  199,   12, 1323,   33,   67,
            0,  106,  857,   33,    0,   23, 1876,   65,   14,   28,   59,    8,
         1363, 1441,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  46,  137,   18, 2586,    0,   70, 1085,  347,    9, 1475, 7127,  823,
         8363,    6,   90,   19,   30,  974,   11, 2934, 4705,   47, 2586,   19,
           81, 1886,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  58,   95,   24,  266,   75,    9, 1758,    0,   22,   65,  266,   91,
          250,   15,    9, 2072, 3202, 4708,  173,  221, 5819,   61,  727,   96,
            9, 7011, 4633,    0,   11,  250,   15,   22,   27,  102,   49,   22,
         1301,   91,   22,   14,    6, 2149,    0,   22,   14,    6, 1900,    0,
            2],
        [  11, 1153,   46,  825, 2684,   34,    6,   93,    9,  569, 5951, 1204,
         2044,  225,  868,   93,   69,   89,  307, 2909,    0,   46,  195,   73,
            7, 1845,  134, 4930,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:4') tensor([35, 36, 37, 42, 26, 26, 25, 36, 39, 40, 40, 26, 40, 28, 49, 30],
       device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9541, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:4', dtype=torch.float16)
True False
tensor([[  27, 4067,   48, 3538, 3437,  327, 8310,  850,   47, 2842,  115,    8,
         1900, 4096,  216,  225,    0,   11,  219,  145, 2014,   67,  300, 1870,
          233,  300, 1330, 2420, 1197,  352,   11,  219,  145, 2701,    0,   11,
          219,  145, 2701,    0,   11,  165,    0,  219,  145,  864,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  85,   15,    9, 5787,  205,   72,    9, 2976, 7969, 6252,    0,   11,
            0,   15,  588,    0,    9, 1231,  122, 6851,   49,   11,   22,   14,
            6,  131,   12, 2621,   91,    8,  117, 4330,  643,    0,   58,   20,
          137,   22,   27,   90,  531,   85,   12,  137,   72,   49,   27,   18,
            9,   88,  132,   54, 2186,  430,  104, 4031,  129, 1427,    6,   15,
         2785,  320,  373,    0,  132,   54, 5161, 4031, 1814,    6,   15,  169,
         8532,   60,    0, 3248,   60,    0, 5518,   60,   62,    9, 8533,   15,
            9, 1231,    0,   54,  401,   38, 8865,   47, 1363,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 102,    0,   64,   14,    6,   16, 3384,    6, 1800,  346,   19, 1044,
          104, 1014, 2770, 1411, 5435,    6,   49,  197,    0,   85,   15,  118,
           27,   84, 1163,   14,    6, 4038,  299,    0,   11,   22,   27,    8,
         3538, 3142, 1167,    0,  106,  219,  241,   22,   12,   50,    0,   41,
          383,   92,    8,  387,   96, 1014, 2770, 1411,    0,   24,   54, 5163,
           12,  543,   47,  276,    3,   11,   22,   48,    8,  786, 1411,   49,
          174,   14,   28,  158, 2955,   19,  206,   88,   14,    6, 1199,    0,
           93,   24,   14,   44,  131,   12,  158, 5150,    0, 1281,    0,   20,
          137,   95, 1429,  151,   48, 1014, 2770, 1411,    0,  219,   14,   51,
         2567,  118,    0,    2,    1,    1],
        [ 102,    0,   20,  204,   59, 1773,   12, 5545,   67,    9,  173,  362,
          216,  135, 6597,  191,    6,  487,  350,  159,    0,   91, 2328,   15,
         3032,    6,  316,  986,    9,  161,    0,   58,    9, 2174,   48,   20,
         1230,   14,   28,  255,  151,   18, 5716,    0,  106,   20,   48,   74,
          103,  417, 3097,  126, 6445,   41,  655,    8,  938, 1069,    6,    3,
           11,  101,   33,  472,    0,   23,  269,    8,  248,    8,  938, 1069,
            6, 2019,    0,    9,   41, 1031,   15,  184, 1851,  309,    3,   11,
           23,  142,  117, 3455,    0,  106,   74,   48,   73,   85,   58,  240,
         3269,  972,  622,  222,   53,    0, 5334, 3087, 2122,   28,  494,   11,
         1217, 2122,  100,  418,    0,    2],
        [  64,   20,  682,   12,  583,   24,   48,  104, 3763,   15, 2734,  233,
          152,   62, 1135,   15,    9, 1638,  106,   64,   18, 2692,   24,    8,
          833,   15,   27,    9,  196,    0,   95,   24,   14,   44,  197,   62,
            9, 3854,    0,   24,   65,  137,   72,   33,   49,   33,   27, 1486,
            8, 1373,   19, 4590,   12, 9011,   61, 1811,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   9, 1562,  105,   12,  134, 8557,    6,   48, 1577,  112,    8, 1740,
         1028,   96,   33, 2484, 3138,    0,   11,    0,   62,    9,  206,  928,
            0,    8, 8983,  928, 1028,  107,   12,  301,  134, 2525,    6,  950,
         2744,   60,  347,  764, 2525,    6,   47,    9, 9628,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:6') tensor([ 48,  95, 112, 114,  58,  47], device='cuda:6') tensor([1., 1., 1., 1., 1., 1.], device='cuda:6', dtype=torch.float16)
True False
True False
tensor([[7889,    0,   20,  ...,    1,    1,    1],
        [ 320, 1275,    0,  ...,    1,    1,    1],
        [  38,   95,   24,  ...,    1,    1,    1],
        ...,
        [  11,   96,   18,  ...,    1,    1,    1],
        [  11,  171,   15,  ...,    1,    1,    1],
        [  13,    0,  524,  ...,    1,    1,    1]], device='cuda:1') tensor([13, 20, 15, 11, 13, 12, 22,  9,  8, 10, 20, 19, 10, 24, 14, 19, 11, 15,
        14, 23, 12, 21, 11, 15, 14, 11, 12, 14, 19, 12, 17, 17, 20, 15, 12, 22,
        21, 14, 13, 18, 23, 16, 13,  8, 24, 14, 16,  9], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.7324, 1.0000, 0.9287, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8159, 1.0000, 0.7622, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8599, 1.0000, 1.0000, 1.0000, 0.7808,
        0.8257, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000], device='cuda:1', dtype=torch.float16)
True False
tensor([[  72,    8,  472,   11,    8, 1006,  534,    0, 1373,  622,   30, 2097,
         1350,    0,  132,  245, 1765,    8,  460,  152,   75, 1456,   19, 5338,
           62, 2255,  935,    0, 2352,   50,  233,   12,  825,    9, 5109,   15,
          980,   60, 1758,    6,    0,  148,   27, 8594,   14,    6, 2374,   63,
          295, 3552, 3853,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  24,  269,   12,    0,  635,  741,   18,   27,  922,   12,   24,    0,
         3146,   64,   20,  101,  922,   24,  420,    0,   57,  177,   55,    0,
           33,   27,    9,  283,  196,   47,    0,   24,   12, 1373,  189,    9,
         2484,  905,    0,   11,   59,    8,  266,   75,   22,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [   0,   95,    0,   23,   14,   44, 3529,    0,   12,    0,   33,  619,
         3760,    0,  872,    0,   64,   23,   14,   44, 4675,    0,   75,    9,
            0,  783,  452,   78,  212,  215, 2360,    0, 1618,  227,    0,  102,
           11,   75,    9,  706,   28, 9416, 3931,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  11,   84, 1884,  145,  291,   49,   41, 4286,  377,   24,    0,   20,
          139,   24,    3,   11,  134, 1884,  145,  291,   49,   41,  259,  513,
         1240,  690,  139,    6,   50,    0,  421,   24,    3,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  75,  631,  295,    0,  915,  105,    0,   74,   14,    6,    8,  318,
           15, 2725,    7, 2896,  346,   19,    9,  937,  432,   15,    9, 6112,
            0,   58,  103,   20,  567,   12,  631,  295,  604,  200,  534,    0,
           11,   70,   90, 7038,    0,   20,  241,    0,   41, 1963,  119,   64,
            0,   23,  286,    8,   69,   44,   28,  216,    3,   11,   20,   48,
          922,    0,   41,  259,    0,    2],
        [   9, 2265,   19, 1241,   27, 1537,   19,   97, 2992,    0,   67, 5271,
          132, 2204,  593,    0, 1771,    0, 1532,   11, 2296,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  11,   46, 5202,   51,    8, 2736,   34,  132,  241,    0,   41, 5087,
         1496, 2226,    6,  141,   12,    9,  161,    3,   87,  241,    0,   41,
          402,   48,   91,    8,  700,   18,  550,   62,   19,   97,  848,   49,
           22, 2352,  141, 1059,    3,   11,   38,   18,   14,    6,   64,   22,
           14,    6,   72,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 131,   96,    8, 3841, 2038,    0,   12, 3760,    0,   12, 1852,  328,
           63, 8364, 5412,   27,  117,   94, 2417,    0,   57,  361,   55,   38,
           23,  489,  767,    0,   64,  204,   22,  266,   91,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([53, 47, 45, 34, 66, 23, 53, 35], device='cuda:1') tensor([1.0000, 0.9258, 0.7559, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
True False
True False
tensor([[   8, 9291, 9090,  635,    0,   58,  197,    0,   22,   14,    6,    8,
          282, 4055, 1258,  239,   24,  508,  137,    0, 1816,   60,   18,    9,
         2264,  141,  686, 1447,  102,   65, 2444,    8,  276, 2935,   30,   15,
          283,  664,  200,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   95,   24,  194,  233,    9,  569,  797,  200,   93,   38,   12,
            8, 9476,   61,   63,  418,   60,  923,    0,  326,   15,    8, 4575,
         6440,   67, 6292, 4943,    6,    0,  165,   24,  158,   72,    8, 2520,
          392, 7733, 7162,   75, 2991,  329,   93, 3296, 1593,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 343,    6,  394,  115,   33, 7142,   19,  148,   87, 2674,    0,   47,
         4563,  334,  997,    0,   19, 4563,  334,  896,    6,   49,   87, 2674,
         3727, 3931,   62,   85,    8, 1126,    6,   11,   87, 2674, 7711, 7607,
            0,   11,   87, 3428,   33,  275, 2729,  346,  347,  169,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,   46,   14,   44, 2955,   19,   47,   99, 7942,    0,   46,   14,
           44,  767,   72,  236,  117,    0,  117,  531,   49,   11,   18,   14,
            6, 1027,  134, 2594, 1245, 3060,    0,    3, 1955,    6,   96,  102,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2708,    6,   65, 2727,  124,   93,  716, 2076,  487,  124,    0,   46,
           65, 4970,   93, 7478,    0,   46,   65, 6687,   93, 4481,    0,   11,
            9,   88,   18, 6628,    9, 1779,    6,  233,   12,    9, 6287,  136,
           11,    9, 1549,   54, 1054,   73,  230,   19, 4104,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  57,  177,   55, 1655,    0,   88,  629,   12,  291,    0,   41,  259,
           34,  466,   14,    6, 1281,    0,   58,   95,   24, 5919,   64,   87,
          241,    0,  741,  145,   69,  141,  329,   58,   22,  145,   69, 8307,
            3,  231,    0,   20,  459,   14,   28,   59,   18,   19, 1083,    0,
           38,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1677,  328,  296,   99, 1986,   61,   22,  189,   41, 5087, 1423,   27,
          138, 1423,    3,   38,    0,  628,  237, 1014,   89, 2463,    0,   91,
           81, 1307, 4248,  265, 1069,    6,    0,   87, 3711,  127,   61,   50,
          160,  273,   53,    0,   87, 2576,   61,  118,    0,   87, 7502,  118,
           67,  248,  184,  105,   34,  431,    6,  148,  142, 2440, 2588,   28,
          105,  134,  425,   37,  278,   83,  299,   15, 5401, 1064,    0,    2],
        [  11,  626,  723,   12,   64,   24,  653,  137,    0, 8529,    6,   54,
          196,  866,   72, 1790,    0,   11,    8,  318,  122,   72, 4705,    0,
         4705,   47, 2071,    0, 4705,   12,  611, 1991,    0, 4705,   12,  611,
          531,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([41, 47, 48, 38, 47, 51, 72, 39], device='cuda:2') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:2', dtype=torch.float16)
True False
tensor([[  33,  129,   83,  284, 1359,  123, 1075,   27,   90, 4593,   12,   68,
          237,   18,   27, 5401,  105, 7130,   11,   25, 1197,  281,  492,  346,
            0,   11,   24,   81,   54,    9,  232,   88,   12,  139,   22,   19,
         1592,   15,  200,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  11,   64,   20,  137,   23,   14,   92,  704,    0,   27,  335, 2540,
            6,    0,   19,  414, 1464,    0,   70, 1397,   46,   54,    9, 1277,
          536,    0,    9,   32,   34,   68,    0,    9,  196,  205,   54, 3135,
           12,   69,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [1964,   22,   27,   24,   54, 8072,   12,  982,    0,   22,   27,   73,
          793,  439,   61,   62,    9, 6628,   15,  138,   12,   30, 1229,    0,
           73,  255, 1631,   34, 1581,   19,  171,  966,    6,   83, 1291, 4222,
           15,  138, 2096,  154,  179,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  19,  436,    0,   20,  102,  610,    9, 7761, 9055,   41, 1831,  136,
            3,  106,   22,   14,    6, 2288, 2456,   61,   50,  347,  189,    8,
          248,  196,   15,  356,    0,  189,  248, 5123,    6,   11,  189,    8,
          248, 1618,   15, 5140,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  11,   33,   48,  326,   15,    8, 5243,   12,   50,    0,   58,   23,
          605,   18,   70,  540,   70,   24, 1074,    9,  766,  205,   19,  138,
          897,    0,  148,   24,  316,  112, 3036,  118,  345,    0,   18,   24,
          204,   79,    8,  961,  315, 1191, 5126,   60,   64,   12,   79,   11,
          191,   12,  394,  138, 2698,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  20,  611,   38, 2483,  105,   18,   24, 1937,  834,    8,  783, 2610,
          108,   12,  260,    9, 1423,   11, 5996,  233,   12, 1681,    8,   87,
           83,  216,   44,   15, 7938,    0,  797,    3,  787,    0,  101,   12,
         2465,    8, 2975,  125,   11,   23,   61,   22,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  81,   15,   84, 1217, 1356,  142, 5029,   67,    9,  354, 2210,    0,
            9,  460,    0, 1807,   12,  141,  103,   23, 1390,  764,  832,   12,
           69, 4153,   30,   47,    8, 1061,  360, 3094,   12,   69,  190,   68,
         1001,   61,   12,    9, 2023,    0,  103,   88,  435,   97,   50, 2463,
          225,  277,   89,  259, 2112,   68,  804,   67,  236,   12,   69, 1624,
           61,    0,    2],
        [   9,  493,  275,   19,    9,  161,   24,  690,  199,   12,  139,   27,
          138, 3879,    0,   41,  127,   14,   92,  269, 1356,  132,  197,  500,
          335,  134,  425, 2357, 7781, 3172,  106,   74,   14,    6,    9, 1586,
           18,   22,  508, 6293,  134, 3879,   62,    9, 2607,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:2') tensor([41, 40, 43, 42, 55, 46, 63, 47], device='cuda:2') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:2', dtype=torch.float16)
True False
True False
tensor([[  23,  126,    8, 1448, 1453, 1814,   15,  882,   19,    9, 6747,   61,
          496,    6,   12,  218,   33, 2675,    0,   38,   23,  142,  733,   12,
         6771,   18,  107,  112, 8767,   97,   19,  216,  592,   12, 1006,    9,
         3914,  744,   23,  470,   59,  221, 5274,    0,    2,    1,    1,    1],
        [  11,   20,  922,   24,   18,   19,    9,  232,  562,  200,   15,  733,
           12,   79,   33,    9,  764,   63,  657,  677,  333,   61,  196,    0,
          131,  107,   12,  555,  712,    0,   23,  921, 5459,   72,    3,   88,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   18, 3715,    0,   18, 2226, 4708,  327,    9,  161,    0,   18,
          558, 2111,  102, 3814,    6,    8, 2351, 1101,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 102,    8,  318,   15,   24,   54,  921, 2194,   67,    7,  237,  861,
           89,  132,    0,  264,   19, 5413,    0, 6795,   51,  121,   87,   48,
          538,   12, 7217,  122,  239, 2292,    3, 1526,   15,  648,  827,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11, 8572,  108, 4813,    6,   54,    9,   85,    6,  132, 1863,    9,
         6348,   11,    9, 6992,   99,   11,    9, 1235,   11,    9, 4405,  358,
          970,   28,    0,   11,   16,  329,  451,  592,   12, 3113, 2328,   15,
         5917, 5676, 8572,  454,    0,    2,    1,    1,    1,    1,    1,    1],
        [  64,   14,    6, 1537,   27,   23,   14,   44, 1486,   33,  634,   49,
           22,   14,    6, 3865,  122, 9187,    0,  122,  793,  341,   28,   49,
           11,   23,   14,   44, 5190,   22,  264, 3681, 1452,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  85,   15,    9, 1160,    0,  103,   24,  158,  578,  246, 1259, 2269,
            0,   22,   14,    6,  778,   12,  953,   81,    9, 4719,  736,    6,
            0,   38,   24,   59,   12, 4252,  402, 1109,    0,   11,   18,   14,
            6,  191,    9, 4747, 1070, 1277,  577,    6,   24,    0,    2,    1],
        [  85,   15,  118,  197,    0, 2878,  430,  105,  832,    0,  605,   50,
          103,   20, 2766,   61,  189,    8,    7,  252,   75,    8, 5519,   19,
          248, 1761,   62,  719,    6, 1614,   77,   44,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 231,  424,  287,  200,  722,   19, 3341,   11, 1485,    6,    0,  719,
          200,  534,    0,   20,  605, 1148,  722,   62,    9, 1303, 3695,   15,
          406, 9229,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   20,   14,   68,  131,   12,  460,    0,   72, 4535,   18, 6307,
          197,  751,   33,  591,  901,    0,    0,    0,  232,    0,   15,   81,
           12,  658,  121,   22, 5863,    6,   12,    9,   29,    6,   15, 3107,
           70,   23, 1110,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  23,  126,  221, 2381,   61,   19,    9,    0, 1275,  112, 4575,  277,
          539,   34,   44,  133,    6,    0,    0,  132,  174,   14,   28, 2621,
           12, 2545, 1886,   47,   64,   46,   54,  106,   46,   14,   92,    0,
            0,  500, 1085,  118,   19,    9,   65, 1776,   89,  490,    0,    2],
        [  46,   81,   59,    8, 1630,  800,    0,  973,   70,   46,   14,   44,
           81,  117,  540,   49,   46,   14,   44,  748,   15,  107,  983, 1343,
          105,  540, 5299,   12,  206, 4351,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  20,  283, 1333,   33,    9,  206,  373,    0,    9, 4259,  133,   15,
         8742,  459,   14,   28, 2250,    9, 8742,    0,   87,  197, 4718,    9,
           12,  210, 1019,    0,   58,  165,    0,    9, 4259,  133,   15,   12,
          210, 1019,  145,   69,    8, 8243, 1170,    0,   57,  177,   55,    2],
        [  11,    9, 6594,  241,    0,   41, 4664,   32, 1561,    3,   11,   38,
           20,  435,    8, 5162,   67,    8, 1561,    0,   11,   20,  837, 5639,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 264,   19, 7720,    0, 1273,   48, 2690, 2430, 4169,    0,  790,   11,
         5271,    8, 3791,    0,   67,   11,  857, 6200,    0,   15,  588,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 102,    0,   20,  174,   14,   28,  199,   24,   12,  158,   50, 1172,
           49,   20, 2472,  658,    9, 6289,   15, 5191,    0, 2423,  103,   22,
          881,   12,    9,   88,   18,   23,  593,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([45, 38, 22, 37, 42, 35, 47, 34, 28, 41, 48, 33, 48, 26, 25, 33],
       device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8760, 0.9092, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
True False
tensor([[  23,  218, 2477,  ...,    1,    1,    1],
        [ 152,   14,    6,  ...,    1,    1,    1],
        [  23,  645,   19,  ...,    1,    1,    1],
        ...,
        [  23,  199,   88,  ...,    1,    1,    1],
        [  18, 1891,  795,  ...,    1,    1,    1],
        [ 171,   20,  255,  ...,    1,    1,    1]], device='cuda:5') tensor([10, 13, 10, 20, 20, 14, 16, 10, 16,  9, 17, 12, 10, 12, 11, 16, 13, 12,
        18, 18, 17, 12, 13, 21, 11, 13,  8, 12, 11, 10,  8, 18,  9, 11, 16, 11,
        10, 13, 15, 13, 18, 13, 17,  9, 15, 12, 15, 14, 17, 12, 18, 10, 11, 14,
         8, 10], device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 0.7500, 0.9639, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.7012, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9395, 1.0000, 1.0000, 1.0000, 1.0000, 0.9209, 1.0000, 1.0000,
        1.0000, 1.0000, 0.8965, 0.8105, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9541, 0.9697, 1.0000, 1.0000, 0.8945, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7271, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000], device='cuda:5', dtype=torch.float16)
True False
tensor([[  11,  152,   14,  ...,    1,    1,    1],
        [  38,   20, 2391,  ...,    1,    1,    1],
        [   9, 2278,  260,  ...,    1,    1,    1],
        ...,
        [   9,  729,  275,  ...,    1,    1,    1],
        [  19,    9,   65,  ...,    1,    1,    1],
        [  38,   12,  158,  ...,    1,    1,    1]], device='cuda:5') tensor([41, 45, 43, 22, 41, 47, 21, 34, 40, 28, 28, 33, 63, 30, 47, 35],
       device='cuda:5') tensor([1.0000, 0.9199, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.7324, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
True False
True False
tensor([[ 103,  738, 1954, 3890, 2607,   60, 2499,   61,   96, 5416,    6,   11,
         1367,    6,   12, 2551,   19,    9, 6233,    6,   11,  855, 7533,    6,
            0, 1217, 7112,    6,   91, 1221,    0,   21,  127,  705,  183, 1540,
            6,   19, 4203,  284,  350,   44,   11, 1221,    0, 2871,  127,  328,
          726, 2409,  225,  904,   19,  248,   93,  154,  223,    6,  142,   75,
            9,   47,   99, 6635,   15,  878,   60,  863, 8286,  333,   12, 1088,
         5851,    6,   19, 2877, 1217, 3840,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  23, 1621, 4085, 8395,    6,    0, 3849,  430,  549,  402,    6,    0,
          165,   23,  489, 6515,  107,    0,  184,  841, 1686,  185,    6,    0,
           12, 1708,   19,    9,  497,    0,    6,    0,    0,    0,    0,   11,
           23, 2989,   61,    0,   23, 3724,   61,    0, 1153,    0,   46,  142,
         2744,   60,   47,   97, 7560,    6,    0,   11, 1153,   46,    0,  142,
         1306,   60,   62,    9,  788,   46,  605,   62,  499,  898,    0,    0,
           11,   46,  142,    0,    0, 5260,  299,    0,   48, 1537,  347,    0,
          499,    0,  898,    0,   47, 7191, 6309,   61, 7560,    6,    0,    2],
        [   9,   79,   28,  987,    0,   20,  309, 3038,  433, 5820,  212,  150,
            0,   48,    9,  117,  493, 4780, 1451, 1447,   12,  194,  686,    9,
          569, 2967,  490, 1061,    6,  168, 3431, 8704,   51,    0,   11,   20,
         2407,   61, 1066, 1242,   15, 1199, 4576,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,    0,    0,   23,  218, 2577,  833,   15,   18, 1638,  101,   70,
           46,  218, 2577,  833,   15,    0,   64,    0,   46,   14,   44,  401,
          106,  843,    9,    8,   34,   83,   15,    9,    0,  354, 1901,   62,
            0,    9,  354,    0, 3103,  225,    6,    0,    0,    0, 3738,   12,
            9,  121,  133,    6,    0,   15,    9,  354,    0, 6469,    6,    0,
           46, 1390,    0,    0,  132,   46,   54,   11,   23,    0, 1390,    0,
          132,   23,   54,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  58,    0,   24,  119,    0,   20,  563, 2074,  105,   96, 1701,    0,
           11,   20,  682,   12,  553,  112, 2339,   24,   18, 1020,  705,  159,
          362, 2183, 1372,  922,   50,   18,   23,  195,   69, 1027,    9, 2116,
           12,  308,  152,   12, 1456,   19, 4574,    0, 2074,  105,    0,   20,
           48,  117, 2998,   51,   11,    0, 7477,    0,  129, 2094,   92,   51,
          106,    0,   24,  119,    0,   20,  563,    8,  406, 2756, 8678,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 102,   23,   14,   92,  629,   33, 1124,   12,  752,   12,  658,   73,
          101,  121,    8, 1151,  684, 2587,    6,    0,   11, 6401,  949,  134,
         3663,   11, 9159,    0,   58,  245,  752,   12,  658,    9, 4854,    6,
           15,   20,  297,   77,  331,  426,    0,   11,    9, 4854,    6,   15,
         1402,    6,   93, 7557,    6,   18,  508, 4482,    9, 7129,  744,   15,
            8, 1185,    0,   93,   90, 3006,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  58,  174,   14,   28, 3109,  727,  138, 1461,    6, 1055,    0,  106,
            9, 2278,   20,  309,   89,   27,   18,    9,  634,   18,  173,  561,
           51,  973,   25,    6,  352,  216,  225,  329,    0, 9523, 3657,    6,
           62,    9,   37,  353,   27,    9,  354,  634,   18,  173, 2352,   33,
         2181,   12,   97, 2071,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  119,    0,   97, 6111,  145, 2621,   12,   69, 1200,  105, 9230,
            0,  106,   74,   14,    6,   90, 4031,  777,   15, 2594, 1646, 1184,
         1886,    0,   95,   24,   65,  695,  183,  124,   22,  112,  692,   75,
            9,  777,   15,    9, 1861,   11,    9,  777,   15,   81,  397,   53,
           11,   38, 4187,    0,   11,    8,  264,   63, 3526,   63,  655,   63,
          179,   92,  160,  456, 3192,  299,  195,  437,   24,   74,   54,   72,
          374,   12,    9,    3,  328, 1184, 1886,    0, 2594, 1646,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([81, 96, 45, 77, 73, 68, 54, 84], device='cuda:0') tensor([1.0000, 0.8599, 1.0000, 0.7324, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
True False
tensor([[  84, 1123,   27,   18,   23,   59,    9, 3299,  777,   15, 4436,   19,
            9, 3265, 7336,    0,   11,   20,  137,   18,   14,    6,    9, 3790,
           28, 6821,   47,   97, 4630, 6084, 9159,    0,   11,   64,   27,   22,
           18,   23,   79,   18,   32,  206, 1466,  589,    0,   11,  148,   20,
          691,   48, 1419,   12, 2612,  141,   12, 2166,   18, 1320,    0, 3299,
          777,   15, 4436,   19,    9, 7336,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,    0,   20, 4528,   18,  250, 6270,    6, 3968,    0,   73,  106,
            9, 2388,  184, 4350, 2879,    0, 1014, 5377,   65,   14,   28,  158,
           22,   12,  260,   49,   95,   24,  266,   75,  250, 1199, 1111,    6,
            0,   46,  195,  197, 5640,   95, 1807,    9, 2116,   12,  942,   64,
           46,  291,   46,   14,   44,  131,   12,  942,   49,   11, 2292,  429,
           15,  230, 3408,   93,  122,  195, 3968,    0,  106,    9, 5712,   60,
           27, 1172,   49,   73,   81,    9,   16,   77, 3664, 2520,    6,  195,
           69,   19,  561,  103,   46,   14,   44, 1804,    0,    2],
        [   0,   11,   38,   20, 1053,  107,    0,   12,   79,    9,  283,    0,
         6573,  275,   18, 1115,  145,   79,    0,   19,    0,    0,  973,    8,
            0, 6849,    0,   20,    0,    0, 1558,   28,    0,   84, 1191,   11,
            0,    0, 2091,   12,  942,   33,    8, 2549, 5018, 2372,    0,    0,
            8, 6557,   61,    0,    0,  129,  328, 6110,   28,    0,    0, 4869,
          276,  148,    0,   23,  433,    0,   41, 5509,  123,    3,    0,  148,
          763,   41,  655,   26,    6, 1063,  242,   93,   41,  655, 1255,   15,
            9,  943,    3,    2,    1,    1,    1,    1,    1,    1],
        [  33,   27, 1200,    0,   19,    9, 5720,   15,   81,   33,    0,  201,
         1368,  307,    0,    9,  250, 2068,   83, 3281,    0, 7912,    0, 6539,
          524,  296,  108,   15,   81,    0,  241,    0,   41, 1963,   65,  335,
          414, 1461,   24,  199,   62,   97, 2332,    3,   20,  593,    9, 4790,
           51,  897, 1359,    0, 6695,    6, 2697,    0, 4576, 1363, 3282,    6,
          233,   11,  201, 1368,  307,  977,    6,  115,   22,    6, 2332,   49,
           32,    0,  151,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  95,   24,   59,    8,  117,  315, 1277,   15,    8, 1309,  148,    0,
           27,  629,    0,   95,   24,   59,    8,  117,  315, 1277,   15,    8,
         2525,    0,  121,  231,   46,   54, 1771,   51,    0,    0,   11,  104,
            0,   54, 4167, 1070, 2525,    6,    0,   46,   54,    0,    0,  231,
         1771,   51,   19,  287, 3390,    0,   95,   24, 3885,   33, 1505, 9417,
          189, 4082,  716,    6,  736,    6,  191,    8, 4082,  716,    6,  736,
          197, 1323,    6,    0, 1425, 4390,    6,    0,  165,   74,   14,    6,
            8,    0, 2208,   15, 2661,    0,    2,    1,    1,    1],
        [  11,   20,  241,    0,   41,  865,   34,  133,    6,    0,  121,  287,
         9573, 2289,   79,   24,  137,    9, 5446,  686,  137,  158, 7169,  320,
          472,    3,   11,    9, 2289,  142,    8,  390,  125,  154,   83,  571,
          103,   20,  241,    9, 5446,  686,  137,   18,  797,  107,   15,  320,
          766, 9573, 2289,  158, 7169,   19,    9,  472,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 103,    0,   97,    0, 7832,    0, 3165,    0,   12, 4537,  313,  179,
            0,    0,    0,    0,   23,   65,  194,  264,   12,   18,  609,    0,
           11,  139,  121,   23,   65,  218,  122, 3954,    0, 3879,    0,  640,
            0,    0,  260,    0,   11,  795,    0,    0,   79,   24,  119,   64,
         4662, 1282,  284, 1229,    0,    0,   19,   24,    0,   93,   64,  878,
            6,  107,  138,   16,  108,  496,  571, 1215,    0,   64, 7260,    6,
         3045,   24,   12,    0,   69, 8158,   93,   64, 2214,  205,   54,  131,
           12,  878,   24,  107,   15,   18, 2214,  377,    0,    2],
        [  38,   23,  394,  271, 3036,    6,    0, 3258,    6,    0,  271, 4012,
            6,    0, 4983, 1063,    6,    0,  189,    8, 1505, 7964,   15, 1592,
           15, 2723,    6,    0,   11,  165,  629,  171,  908, 1309,  953,   60,
           12, 2612,   24,   12,  197,   59,    8, 2034,   67,  543,    0,   11,
           22,   48,  326,   15, 2096,   80,   80, 3278,    0,  106,   87,  145,
          291,  104,  205,   18,  101, 1494,   61,   91,   46,  151, 4426,   24,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([68, 94, 88, 77, 91, 58, 94, 74], device='cuda:2') tensor([1.0000, 1.0000, 0.7266, 1.0000, 0.9121, 1.0000, 0.7617, 1.0000],
       device='cuda:2', dtype=torch.float16)
True False
tensor([[  24,    6, 4877,  631, 2711, 2613,    0,   19,  206, 1265,    0,   23,
           14,   44, 2581,   60, 3876,   67, 3876,    0, 1530,  239, 2581,   60,
         3876,   67,    9, 3307,    6,   18,   23,   14,   44,  751, 9362,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  72,    8,  472,  534,    0,   20, 4772,    9,  232, 2862,    0,   11,
          236, 1632,  422,   51,  810,  296,  105, 3175,  941,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  41,  273,   51,    0,   24,  139,  543,    0,   62, 1300, 1665,    0,
          796,   83,   60, 7652, 1859,  397, 1457,   19,    8, 7583,   49, 1133,
           58, 6824,    0,  814,  474, 2426,    6,    0,   62,   49,  419, 1252,
           19,    9,  596,    0, 4536,   51,   12, 4458,   33,    0, 7434,    0,
         8240,    0,    2],
        [  22,   14,    6,  221,  719,  200,    0,   11,   20,   14,   92,  530,
          500,  221,   70,  231,   70,   20,   48,    9, 3706,  490,   20, 3998,
          648,   96,   84, 1328,  349, 2999,   14,    6, 2510,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  11,   87,   48,   19,  978,  235,  159,  346,   19,    0,   20,  137,
            0, 5720, 8146, 2857,   47,   47, 2066, 4490, 7107,    6,  112,    0,
           20,  137,    0, 5614, 9028, 2327,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [1592,   15, 2102,   54, 4706,  106,    9,  647,   63, 1359, 2884,   54,
           38, 3967,    0,   11,   19,  171, 3974,    6,    0,  255,  230,  174,
           14,   28,  260,   11,   24,   59, 3187, 2291,  814,  237,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  11,   33,   48, 1085,  112,    8, 2659,   19, 1980,    0,   77,    0,
         3280, 7403,  844,  859,   28,  132,  241,    0,   41,  873,  402,    8,
         3706,    0,   24,  642,   23,   65, 4469,    8, 1471,  827,  857,  197,
          751,  414,  827,    3,   38,   23,  316, 1458,   18,    0,    2,    1,
            1,    1,    1],
        [8288,    0,   20,  126,   11, 2171,   12,   59,   90, 2472, 2051, 1380,
           15, 4021,   11, 9155,    6,   19,   84,  425, 1380,    0,   11,   18,
           14,    6,    9,  283,  196,   18,   33,  260,   65,  151, 4802,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  11,  950,   20,   48, 8072,   12, 1491,  107,  121,   11,  342,  387,
          284,    6, 9191,    0,   93, 1370, 4611,  215,  390,   51,    6,    0,
          218, 1494,    0,   20,  489,   12,  137,   72,  134, 1775, 2037, 1031,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  20,  642,    0,   95,   24,  266,   75,    9, 1066,  936, 1107, 3831,
         7194,   49,    9, 1659, 1475, 3136, 2119,    0, 1182,  240,  200,   49,
          320, 5179,  126,  221, 1898,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   9,  535,    0, 1950, 1030, 1410,    0, 1950, 6970,   11,    9, 1046,
          135,    6,   18,   79,   88, 3949,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   27,   85,   15,    9,  117,  562, 1699,   53,   19,    9,  161,
          191,   74,   54,  502,  832, 5592,    6,   19,   22,    6, 1465,  422,
           18,   23,   65,  752,   12,   79,  236, 3461, 3411,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  23,  245, 1326,   18, 5795, 1039, 4578, 1997,  456,   53,   48, 2802,
          346,   47, 1262,  671,   60,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   0,   11,    9,  206,  275,    0,   72,  104, 1652,   27,   18,    0,
            0,   22,  283, 1846,    8,  783,  777,   15,    0,    0,  151,   25,
           44,  210,  307,  329,    0,   88,    0,  722,  533,   12,  707,  118,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [   0,  152,   54,   84,  240, 2846,  497,    0,    6,    0,    0, 4247,
           53,   15,    9,  493,    0,  728,  200,    0,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33, 5596,    6,  189,  233,  955, 1008,    3,   88, 4706, 3531,  105,
          555,  472,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:1') tensor([37, 23, 51, 35, 32, 36, 47, 37, 39, 31, 20, 35, 19, 38, 22, 16],
       device='cuda:1') tensor([1.0000, 1.0000, 0.8613, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.7207, 0.7812, 1.0000],
       device='cuda:1', dtype=torch.float16)
True False
tensor([[ 102,    0,   33, 2129,  204, 5848,  105,  668,   99,  271, 3271,   80,
           99,  140,  890,    0,   58,   87, 1338,   24,   59,   12, 3088,    8,
         2257,   38,   22,  678,   14,   28,  825,   19,  683,   83,  346,    0,
           11,   24,   59,   12,  869,   47,  138, 7312,  112, 4165,  677,   60,
         4868,    8,  373,    0,    2,    1,    1],
        [  38,    0,   24,  119,    0,  112,    0,    0,  356,    0,  538,   12,
         2220,    0,    8, 4303,   11,    0, 8313,   22,  189,   90, 4081,    0,
            0,    9, 1801,    0,   95,   24,  195,    0, 2203,    9, 6363,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,  104, 6159,   54,  151,  531,    0,   11, 2423,  152,    0,   33,
           56,  216, 1892,    0,  191,  104, 3136,   63, 2076,  297,   60, 5376,
           19,    9,  367, 1473,  455, 2114,  127,   54, 1011, 1423,   75,    9,
         4993,   15,    9, 1423,   11,    9,  410,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   70,   23,  553, 4150, 1287,    0,   23,   14,   44, 6847, 1867,
          107,   15,    9, 1135,  374, 9185, 3385,    0, 3146,    9, 1064,   18,
           24,  335,   12, 2581,  937,  362,  296,  284,    6,    0,  148,   27,
            9,  777,   85,  823,   63,  181,  150,   60, 2825,    0, 1930,  127,
          159,    0,    2,    1,    1,    1,    1],
        [  22,   14,    6,    8,  561,  191, 2875,   65, 1858,  533,   12, 3838,
         3199,  169,   67,  134,  681,    0,  287,   15,  132,   68,   54,   74,
           47,  129,    6,  125, 1453, 1858,    6,    0,  171,   67, 5333,   61,
         2381,    6,  233,    8, 2935,   30,   15,  287,  200,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 152,   27,    9, 3822,    0,  906,  972,  225,  124,  150,  709,  520,
            8,    0,  127,    0,   27,    8,  151,  778, 1938,    0,  906,  972,
          225,  124,  150,  709,  520,    8,    0,  127,    0,   18,   27, 2149,
         6307,    6,  171, 4590,  136, 1938,   62, 1135,   15,   18,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [ 647,  105,    0,    0,  158,   19, 2521,   67,  906,  171, 1494,   49,
            0,  707, 1494,    0,    9, 1740,   27,    9, 4358,   23,   81,  795,
            0,    0,   11, 1055,  121,  287,   15,  141,   54, 5459,   19,  751,
            0,   97, 1740,    0,  158, 5459,    0, 1025,   12,  277,   60,    0,
         1025,   12,  795,   90, 4358,    0,    2],
        [3191,    0,  204,   24, 2769,  138, 1465,  107,   18,  196,    0,   74,
           23,  194,    0, 1281,    0,   11,  636,   72,  152,    0,   72,  664,
         2495,    0,   27,    9, 3194, 2662,   15,    8,  502,  721, 5487,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:3') tensor([53, 37, 45, 51, 47, 48, 55, 37], device='cuda:3') tensor([1.0000, 0.8154, 1.0000, 1.0000, 1.0000, 1.0000, 0.9258, 1.0000],
       device='cuda:3', dtype=torch.float16)
True False
tensor([[ 497,   68,   68,   21,   63,  383,  734,    0,   70,  295,   11, 1282,
           89,  448,    0,   11,  287,    0,  573,  986,    0,    9, 4646,  161,
            0,  583,   18,   22,   27, 1184,   12, 7741, 6976,    6,   12, 1241,
            0,  148,   46,  119,   27,    0,    9,  823,  763,   12,    8,    0,
          511,  794,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 236,  941,   19,    9, 2054,    6,   18,   27, 1190, 4144, 1504,   12,
           70,   41,  655, 1801, 1650,    3,  191, 3164,    0, 2733,  142,  856,
          122, 3617,  122, 2093,  239, 2290,  126,  221, 7037,   47,    0,   96,
            8,  800, 3003,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 9386,    0,   32, 2497,   77,   25,   92,  360,   61,   22,    6,
          248, 3575,   62,   38,  349,   83,   60, 7431,    6,   19,    9,   37,
          353,    0,   11,   74,   27,    8,  675,  284,  299,   12, 4792,   12,
          218,    8, 3074,   63, 7696,   20, 4296,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  58,  102,   85,  819,  275,    0,   95,   22,   27,    9, 1078,    0,
         6675,   11, 7386,   49,   11,   22,   27,   49,   18,   23,   54,  102,
           39,  313,   61,  533,   19,    8,  196,   18,  173,  500,  221,  914,
            9,  354,  490,    0,  165,   22,   14,    6,  245,    9, 1078,   18,
           23, 1323,    8,    7,    6,  284, 2112,   67,  555,  206,    0,    2],
        [  58,   64,   22,  589,   79,   27,   12,  218,   11, 2279,   99,  783,
         5153,   18,   24,   65,  137,   15,   91, 7867,    6,    0,   11,  104,
           54,    9,  721, 1850, 3812,  379,    0,   11,  103,    9, 2516,   27,
         2322,    9, 5153,  101, 6693,  727,   11,   38,   32,  700,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 106,   64, 1059,  589,   47,  141,   27,   22, 6095,    6,  141,  107,
           15,    9, 4094,  108,   18, 2151,    6,  141,   11, 7628,    6,  141,
           96,    9, 1683,    0,   11, 1118,    0,   41, 1963,   65, 2372,   11,
          137, 1331,  223,  455,   92,  105,  580,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 301,    0,   47, 3974,    0,  103, 1217,  848,  943, 5559,  454,  194,
          107,   12, 5559,   25, 5486, 6158,    6,   15, 1217, 3943,  112, 2923,
            0,   11,    9, 2862,   18, 2849,   96,    9, 5559,  266,   91,   46,
          204,   59,  221, 1773,  728,  200,  534,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11, 4156,   22,    6, 4330, 4142,  112,    9, 4593, 6749,    6,   11,
           22,    6,  672, 1110,   89,   63,  758,   11, 2722,   80,   51, 1311,
            0,   22,   27,   73,  236, 2582,   12, 6063,   12,  138,  591,   11,
           78,  313,  107,  138,  488,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:3') tensor([52, 41, 45, 60, 48, 45, 45, 44], device='cuda:3') tensor([0.9121, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
True False
tensor([[  70, 3159,   93,   19, 2687,    0, 5458,    6,   93, 3771, 3074,    6,
            0,   74,   54, 3616,  454, 3181,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  232, 3373,   48,   47, 1273,   12, 2151,  115,  104,  719,   63,
         7335, 6674, 3258,    6,   18, 8503,   61,  107,   41, 4909,  115,  122,
            3,    9, 1170,   15,    9, 1026,    0,    2,    1,    1,    1,    1],
        [  11, 1055,    0,   95,   74,   14,    6,   85,  275,   18,    0,  320,
            0,    0,  406,  356,   65, 2620,    0,   62,    0,   20,  137,   22,
           14,    6,   18, 8320,  470,  537,    0,    2,    1,    1,    1,    1],
        [  58,   95,   23,  137,   72,   22,    0,   23,   54,  197, 2482, 6567,
         1677,  452,    6,   62,   33, 1201,    0,    9,  406, 1464,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  20,   48, 7695, 2182,   19, 4593, 8036,    0,   58,   20,  174,   14,
           28,  137,   18,   23,  470, 2519,    9, 1659, 1595,   12, 1025,    8,
         2812,   91, 4593, 8036,    0,    2,    1,    1,    1,    1,    1,    1],
        [ 117, 1610,   19,   84,   10,    6,   28,    0,   20,  245,  550,   12,
          139,    9, 4804,  654, 5096,   87,  136,  108, 3369,   15, 1518,   75,
          271, 8139,  345,   19, 4804,    0,    2,    1,    1,    1,    1,    1],
        [  11,    9,  234,   27,    0, 1168,   15,    8, 3298, 2083,    0,   22,
           14,    6,  101, 1336,   81,    9,  169,   62,  138, 2607,   62,    9,
         2464,    0,   11,   24,   65,   79, 2744,   53,  151, 4674,    0,    2],
        [  33, 2997,  640, 6085,    6,   11,  205,   91, 7235,   27,  131,   12,
           69, 5787,   11,  255, 7220,   60,    0,   11,   23,   14,   92, 1130,
         1085,   22,  553,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [  11,   20, 3150,  158,   22,   49,   74,   54, 3481, 2346, 1586,    6,
           11,   15,  588,    0,   18, 2850,   47, 6798, 4178, 1436,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,  103,   22,  881,   12, 4495,   11,  134, 4501,    6,    0,   33,
         3662,   27,    9,  227,  326,   15, 1012,   60,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 102,    0,   85,  275,  148,   27,  117, 4589,   11, 5787,   19,  436,
           27,    9,  436,   18,   81,   33,   65,   69, 4277,   51,    0,  101,
          112,    9, 3980,  136,    0, 5410,   11,    9,  923,    0,    2,    1],
        [ 171, 1617, 3587, 1088,  863, 3647, 1586,   61,  134,  425,  848,   12,
          577, 5272,  345,   33, 1699,   11, 1249,   22,   19,   22,    6, 2820,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  85,   15,  118,   27,  943,    0,  106,    9, 2213,   15, 2296,   27,
            8, 1075,   15,  788,  953,   60,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11, 8413,   11,  493,    0,   23,  286,   12, 1323, 4543,  640, 3714,
            0, 1199,   11, 2340, 1441,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   20,  126, 5788,   67,  300, 6669,  106,  219,   14,    6,    8,
         2683, 3143, 4814, 3326,   75,  540, 6250,  152,    0,   11,  219,   14,
            6,  401, 2472, 4143,    0,    2,    1,    1,    1,    1,    1,    1],
        [  20,  137,   22,   27, 2472,    9, 1078,   18, 3260, 3399,   15, 1485,
         2213,   54,   73,  722,    9,  196,   23,  145,   91,  118,   12,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:2') tensor([20, 32, 32, 24, 30, 31, 36, 29, 24, 22, 35, 27, 20, 19, 30, 25],
       device='cuda:2') tensor([1.0000, 1.0000, 0.8662, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9253, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:2', dtype=torch.float16)
True False
tensor([[  46,  812,   97, 1538, 1461,    6,   11,   23,  316,    9, 4012,   67,
            9, 2129,  322,  125,  685, 1683,   19,   18, 1725,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  33,   27,   96,  764, 3142, 6231,    0,  266,   75,   18,    0,  227,
            0,   91,    3,  851,  954,  200,  534,    0,   22,   14,    6, 1130,
           74,    0,   19,    9, 1263,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  11,   38,   12,   50,    0,   33, 1269, 1477,    0,   33, 1269, 5663,
           27,  236,   18,   20,  199,   12, 4309,  347, 3553,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  33,   27,  233,  955,  850,    3, 3258,    6,   15, 2594, 2684,    0,
           22,   14,    6,    9, 3299, 6895,  690,  435,  112, 1886,   15,    8,
         6990, 1771,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  20,  550,   74,  106,    9, 7011,  682,   12,  553, 7273,   60,  118,
           11, 5190,  118,  189,  738, 5151, 5955,  620,  160,   92,    6,    0,
         4351,   11, 2123,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  20,   14,   92, 4528,  122,   11,  122,   88, 1486, 5735, 1962,    6,
          104, 1090,   12,  752,   12,  977,  115,    8,  672, 1136,  134,  848,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 171,   15,   24,  653,   73,   69, 4825,   18,  171,   15,    9,  161,
           14,    6, 8234, 2551,  400, 1197,  633,   67,  171,   15,    9,  161,
           14,    6,  250,  231,   63, 9453, 5208,   77,   99, 4901, 2644,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 103,   20,   48,  282,    0,   84, 1163,  629,   12,  291,    0,   41,
          655,  593,   24,   59,   47,  138,  681,   27,   91,   32,  206, 1700,
           19,    9,  161,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  22,  165, 1390,  977, 2640,    0,   11,   22, 3488, 2899,   12, 2353,
          418,  181,    0,    9, 3504,  857, 2408,  727,  134, 5415,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 421,   24,    0,   57,  361,   55,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  22,   14,    6,    8,  672,   18,   14,    6,  122, 8645,   19,  148,
           20, 1336, 4658,    6,   15,    9,  570,    0, 1935,    6,   15,  206,
         1248,    0, 2343,  431, 2728,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  33,   48, 3982,  346,  106,    9, 1407,   48,  578,  315,    0,   11,
            9,  693,  241,    0,   41,  259,    0,   23,  101,  199,   33,  977,
            3,    9, 5109, 2022,  152,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  20, 2044,  370,  135,   12,  300,  580,    0,   70,   20,  126,  103,
           20,   48,    8, 1185,    0,   11,   20,  310,  194,    0,   11,  165,
           20, 4634,   51,   18,   20,  145,  610,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  95,   81, 1155,  231,    0,  569,  472,   20,   14,  150,   69,  264,
            0,   11,   20,   14,  150,   59,    8,  318,  122,   12,  583,   24,
            0,   11,   95,   81,  678,   14,   28,  194,  231,    0,   20,   14,
          150,   69,   19, 6486,    0,   91,   23,   30, 1351,  188,   99,    0,
            2],
        [ 106,   33,   27,   73,    0,   33,   27,   73,    0,   11,   33,   27,
           73,   64,    8,  722, 1163, 1301,   91,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  20,   59,    9,  315, 8312,   15,  356,    8, 1492, 5790,   63,  225,
           63,   44, 1724, 2262,    0,    0,   75,    9, 1600,    0,   15, 1195,
         8535,  937,    6,   75, 7501, 2541,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:0') tensor([23, 31, 23, 28, 29, 26, 38, 29, 24,  7, 31, 31, 33, 49, 21, 32],
       device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9453,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9033],
       device='cuda:0', dtype=torch.float16)
True False
tensor([[ 102,    0,  342,   27,   18,  570,   11,   18, 2078,   38, 3866,    0,
          342,   79,   23,  701, 2612, 1452,   12, 1279,  264,   62, 1859,  360,
         1887,  125,  362, 1552, 8247,  426,    6,   18, 4707, 2117,   12, 8190,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  11,   87,  500,  914, 4908,   61,   96,   64,   87, 1050,    0,    8,
          325, 1453,    0, 6021,    6,   28, 1839,  853,    0, 3818,  746,   61,
           62,   90,  107,   83, 1994, 3528,   90, 6466,  705, 1404,   30, 2632,
            0,  457, 2624, 1138,  212,  237,  962,    0,   11,  765, 4133,  126,
           32,  234,  132,   48, 5163,   47,   64,   48, 2968,   90, 9004,  726,
          757,   15, 3742,    0,   58,   87, 1733, 1077,   15,   85, 1091,  275,
            0,   33, 5154, 1804,   12,   69, 4843,   61,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  11,   20, 3287,   22,    0,    0,    9, 8889,   15,    9, 8123,   49,
            0,    8, 2556,  240,   63,   12,  719,   63,    0, 1962, 1140,   49,
           11, 1055,  555,   15,  118, 1084,  922,    8,  570,    0,   91,   46,
          126,   38,    0,  246, 2850,    0,   11,   81,   24,  126,   12,   79,
           48, 2803,    8,   50,  160, 1240,    0,    0,   57, 1927,  768,   55,
            0, 8123,    6, 1053,    8, 4236,   47,   50,  160,  273,   53,   11,
         3949,  307,  744,   12,  795,   62, 1135,   15,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  23,    0,  489,   67,  719,    0,   11,   23,   14,   92,  102,  269,
         1298,  936,   62,    0,   11,    9, 2199,   15,   18,   48,   18,   88,
          142,  165,    0,  131,  189, 5306,    6,    0,    0, 2503,   47,    0,
            8,  984,   12,   51, 1573, 1101, 5721,    0,    0,   11,    9, 2199,
           15,   18,   48,    0,  171, 4990,  115, 2006,    9, 1814,   15,  844,
         4092,    6,   46,  269,    0,   15, 1363, 3661, 4775,    0,   11,  165,
           46,    0,  550,   62,   12,   50,  757, 4775,    0,   11, 3848,  104,
           54,  151,    0,  151,  783, 1373,    6,    0,   18,    0, 6464,  984,
         1485,    0, 7276,   27, 2419,    0,   12,  795,  107,   19,    8,  777,
           15, 1033,    0,    0,   11,   23,  102,   59, 4990,  401, 1832,   53,
           99,   11,   46,   14,   92,  115, 2006,    0,  134,  844, 4092,    6,
           11, 4220, 8294, 6695,    6,    0,    0,    0,   46,   14,   44,  401,
         1106,  744,   11, 4837,   11,  205,   18,   46,  145,   59,  500,  704,
          490,    0,    2],
        [ 129, 2713,   68,   60,  593,   70,    0,  236,   20,  158,   12,  707,
           67,    0, 1115,   20, 9693,    0, 1530,  239,  236,   18,  101, 1129,
           12,   50,  857,   84,  551,   93,   37,    6,  520,    0,   27, 4970,
           60,    0,    0,   22,   14,    6,  530,  778,    0,  593,  530,  611,
            6, 3150, 4359, 1101,   60,   11, 2305,  677,   60,    0,  171, 1090,
            0,   11,  103,   20,  611,  151, 3492,  346,    0,   20,   59,   12,
         3518, 1148,    0,   84, 1191,   19,   33, 2628,   27,   12,  460,   12,
           84, 3517,   72,   64,   20,  199,   12,  218,  533,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:7') tensor([ 38,  82,  82, 159,  95], device='cuda:7') tensor([1.0000, 1.0000, 0.9258, 0.8965, 0.9541], device='cuda:7',
       dtype=torch.float16)
True False
tensor([[  38,  152,    0,  ..., 4708,    0,    2],
        [   9,  647, 1553,  ...,    1,    1,    1],
        [  58,    9,   88,  ...,    1,    1,    1],
        ...,
        [  38,   70,    9,  ...,    1,    1,    1],
        [ 647,  105,    0,  ...,    1,    1,    1],
        [   9,  406, 4842,  ...,    1,    1,    1]], device='cuda:7') tensor([150,  71,  66,  99,  81,  81,  69], device='cuda:7') tensor([0.7617, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
True False
tensor([[1133,  151,    0,  ...,    1,    1,    1],
        [  58,  102,   22,  ...,    1,    1,    1],
        [  23,  204,  101,  ...,    1,    1,    1],
        ...,
        [  22,   14,    6,  ...,    1,    1,    1],
        [  11,   18,   14,  ...,    1,    1,    1],
        [ 101, 2014,    0,  ...,    1,    1,    1]], device='cuda:7') tensor([ 9,  9, 10, 11,  7,  8, 11, 11,  9, 12, 11,  9, 14, 11, 11, 14, 10,  8,
         9,  8, 13, 18, 13, 11,  9, 11, 10,  7, 11, 10,  9, 12, 16, 10, 10, 10,
        10,  9,  9, 11, 12,  7, 11, 11,  9,  8,  8, 12, 14, 17, 21, 10,  9, 14,
         7, 12, 10,  8,  9, 12, 11,  9,  8, 11, 15,  8,  8, 12,  7, 18,  9, 11,
        12,  9,  9, 12,  9, 12, 12, 15,  8, 12,  9, 12,  9,  9, 11,  7],
       device='cuda:7') tensor([0.7446, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9961, 1.0000, 1.0000, 0.8003, 0.9263, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 0.9512, 1.0000, 1.0000, 1.0000, 0.9204, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 0.8525,
        1.0000, 1.0000, 1.0000, 1.0000, 0.8716, 0.8296, 1.0000, 0.7476, 0.7451,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8477,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8662, 1.0000,
        1.0000, 1.0000, 0.8276, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:7', dtype=torch.float16)
True False
tensor([[  11,   23, 2481,    8, 3530,  533,   18, 1779,    6,   12, 8504,  296,
          105, 1320, 1926,    0,   58,   18, 3488,  414, 5901, 5824,   93, 3483,
           12, 5442,  345,  189,    8,  117, 1328,  757,    0,  695, 3202,  557,
           89,  252, 1540,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 424,   81,    0,   20, 1816, 1148,    8, 1217, 1288,   67,    8, 1743,
         1888, 1530,  239,    8, 1743, 1288,   67,    8, 1217, 1163, 4947,   47,
          499, 2742,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  20,   65,  878,  648,    9,  899,  586,    0,  844,  723,   22,  115,
           19,    9, 1476,   11,  500,  310,   24, 3824,   24,   14,   44,    8,
          387,    0,   41, 1515,  573,    0, 6537,   27,    0,   79,   22,   81,
            0,   79,   22, 6093,   11,  500,  310,  118,  139,   24, 7967,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 555,   85,   65,   16,  216,  360,  115,   12,    8, 4032,   15, 8048,
          299,    0,  148,  763,   18, 1701,    0, 5591,  236, 2203,    0,   27,
            8,   83, 4367, 1800,  430,    8,  286,   47, 9632,   15, 8048,  299,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 255,    8, 2524, 2269,   61, 2372,   65,   79,   18,   47,   24,    0,
           38,  106,   15, 2342,    0,   20,   14,   68,  152,  420,    0, 1879,
            0, 1030,  580,   67,    8, 3636,   11,    8, 9477,    0,  250,   15,
            9,  169,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [8673,  115,    8,  628,  576, 1335,   91,    8,   39,   77,  398,   15,
         6054,    0,  928,   63,  978,   92,  555,   12, 2878,    9,  206,    0,
         1487,    9, 2305,    6,   28,    6,   62,    0,   11,   64,   23,   14,
           44,  401,    0,   22,   27, 1084, 4947,  928,  295,  243,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  38,   33,  188,   77,  677,  173,  171, 4590,  136, 5576,    6,    0,
           24,   65, 1267,   22,   62,    0,    0,   11,  102,    0,   22,   14,
            6,    0,   91, 2339,    0,  138, 2880,   12, 2697, 3296,    0,   95,
           24,   59,  973,    8, 2880,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 103, 6161,   94, 3189,   28,   28, 1547,  115,  189,    8,  502,  721,
         5487,    0,   87, 2040,    6,   90,    8,   34, 2770,    0,  148,  549,
          360,    6,    8, 1561,   60, 1140,    0,  148, 1547,  233,    8, 7262,
           19,    9, 3535,    0,   11,  165,   24, 8709,  115,    8,  861,  456,
          148,  173,  221, 5127,  135,   61,  189,    9, 3535,  112,    9, 1140,
            0,    2]], device='cuda:6') tensor([41, 28, 49, 38, 40, 48, 43, 62], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9199, 1.0000],
       device='cuda:6', dtype=torch.float16)
True False
tensor([[4368,  943,    6, 3181,   19,    9,  161,    0,   58,   20,  199,   12,
         2100,   62, 4505,   11,   62, 1102,   19,  910,    0,  106,   22,   27,
          191,   20,  119,    0,   11,  106,   22,   27,  191,   84, 1255,   27,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  20,  126,  101, 1085,    9, 2019,   41, 2428, 1246,  242,   49,    0,
           57,  177,   55,   67, 4359,  307,  322,    0,    0,   11,    0,   20,
           14,   68,   91,    0,    0,    0,   24,  119,   64,    0,   20,   14,
           92,    0,  269,   12,   79,   85,   72, 6161, 2603,    0,    2,    1],
        [   8,  169,  103, 3515,  862,   61,   12, 2372,  502,   72,  334, 1606,
           15,  794,    6,    0,  287,   15,  118, 6485, 1564, 2534,  105,  209,
          487,  252,  768,  654, 3695,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  12, 4843,   18,   87,   14,    6,    0,   41,  259,   28,  151, 4505,
           30,    3, 1397,    0,    0,    7,   30,  744,  271,    0,    0, 1004,
           19,   13,  353,    6,    0,    9, 5191,    6,   87,    0, 2666,   51,
         2193,  115,    0,    0,  271, 2628,   67,  933,   11, 1356,    0,    2],
        [ 231, 1020,   85, 1118,   18,    0,   85, 1118,    0, 1655,    0, 1525,
            0,   18,  763,   18,   64,  943,    6,   19,    9,  232,  561,   27,
           73, 2169,   58, 2169, 7945, 7236,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1168,   15, 5162, 1115,   14,    6,  902,    0,   91, 5162,    9, 1931,
            0,   20,  643,   75,  118,   67,    9, 2559,   11,   20, 2220,  345,
            9,  334,   73,   53,   20, 1474,    0,   11,  165,   20,  707, 1494,
         8161,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  38,   64,   20,  199,   12,   79,  102,   27, 6062,   24,   12,    8,
         1464,  433,    9, 9228,    0,   33,   27, 2249,   30, 4261,    0,   87,
           14,    6,    8, 9228,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  33,   27,   90,   25,   63,   44,  215, 4133,   61,   49,   33,   27,
           73,    8, 6501,   49,   25,   63,   44,  215, 4133,   61, 3997,   15,
         8594, 1455,    0,    9, 4527,   18,   24,   59,    0,   67,   81,    9,
         3298, 2083,    6,  977,    0,    2,    1,    1,    1,    1,    1,    1]],
       device='cuda:6') tensor([38, 47, 31, 48, 32, 40, 30, 42], device='cuda:6') tensor([1.0000, 0.8521, 1.0000, 0.8589, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:6', dtype=torch.float16)
True False
tensor([[ 152,   14,    6,    0,    0,   64,    0,   20,    0,    0,  199,   12,
         2651,    0,  103,   24,    0,  194,    0,  648,    0,  420,    0,   93,
           12,    0,    0,    9, 2510, 6136,    0, 2246,    0, 6052,    8,  869,
           34,    0,  421,  118,    0, 2574,    0,  543,    0,   93,  300,    8,
            0,  525,   15,  577,    0,  636,  255, 5458,   70,    8,  869,   34,
         1833,   47,    8, 1299, 1526,    8, 1794,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  11,  103,   84, 3543, 3493,    9,  570,    0,   87, 1118,    0,   41,
          655, 2963, 4261,  452,  275,   48,    0,   20,   14,   51,  500, 1085,
          414,   15,   18,  490,    0, 3769, 6054,    0,   57,  177,   55,   20,
          459,   14,   28,  119,   64,   48,   50,  757,    0,   64,   48,  600,
          496,  768,  654,    0,   20,   48, 5274,   86,  252,   92,    6,   67,
         2439,    0,   57,  177,   55,   20,  101, 1596,   14,   28,  691,  121,
          287,  205,   24,   65,  158,  152,    3,    2],
        [ 102,    0, 2183,  677, 7194,   60,    0,  197, 3658,  115, 5489,    9,
         3771,   11, 1486,    8, 1191,   67,   82, 1395, 3282, 1503,    0,   57,
          177,   55,   38,  271, 1700,   27,   18,   87,   14,    6,    0,   75,
           82, 1395, 3282, 1503,    0, 2726,  106,   87,  611,    6,   91,   87,
           65,  197,  335,  898,   12, 4316,    9, 4445,  640, 3257,   11, 3024,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  38,   19,    9, 2744, 2230,   20, 2491,   51,   19,   41,  350, 1752,
           14,    6,  373,    3,   11,   20,   48, 5716,   11,    7, 2645,   61,
           12,  139,    8, 2885,   15, 5554,    6,   11, 5554,    6,   15, 1163,
           14,    6,  373, 3004,    6,   18, 2674,  115,   62,    9, 7737, 3774,
            0,  287,   15,  118,  101,   19,    9, 1106, 2443, 1526,  112,  375,
           83, 1022, 1856,  475,    6,  101,   91, 1148,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  23, 4348, 1138, 1109,   62, 2169,    0,   23, 4761, 4545,    0,   24,
          286,   12, 4250,  138, 2354,   38, 2483,  105,   38,   18, 1273,   65,
         5596,   22,  189,  134,  425, 3151,  106, 1309,   27,   90, 2777, 2483,
           19, 2403,  352,  333, 1204,   11,  556, 2403,  352,  333, 1204,  499,
         3604,  108,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 762,    0,   81,   15,  148,   27,   12,  291,    0,   46,  435,    8,
         2019,  107,   15,    9, 1263,    0,   41,  987,   68,  370,  333,    3,
           11,   22, 2411, 1504, 3191, 2547, 2598, 1496,  179,    0,  589,  236,
         2621, 2362,   72,   33,   12,   24,    0,  236, 5527,    0,  236,  521,
            0,  236, 1172,   67,   33, 1159,    0,  137,   72,   22,    0, 1440,
            0,    9, 1123,   27,    0,   20,  126,    8, 2041, 6134,    0,   64,
            0,    2,    1,    1,    1,    1,    1,    1],
        [  11,   24,   59,    9, 3265, 7336, 7166,    9,  457,  184, 1818,   77,
         2004,   15,   97, 4010,   67,    9,  375, 9110,   15, 2330,   18,   54,
            0,   19,  436,    0,    9, 2785,    6,   15,   97, 4010,   11,   18,
           23, 5849, 1628,  250, 2071,   12,    0,   70,   23,  470,    0,  106,
           18,   14,    6,  151,    9, 1492,   18,   27, 3713,   60,   19,   97,
         4010,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   19,  436,    0,   33,   27,   85,  747, 3624, 6436,    6,    0,
          148,   27,    9,  777,   15, 3624, 6436,    6,   18,   54,  629,   62,
         1313, 1359, 5358,    6,   19,    9, 1317, 1173,  320, 1137, 1526,    0,
           23,  335,  719,  747, 6436,    6,    8,  373,   62, 1313, 1359, 5358,
            6,    0,   11, 2255,  105, 3464,   15,  118,   54,  129,  352,   61,
           93, 8677,   51,    0,   46,  101,  174,   14,   28,   79,   18,   19,
           18, 2476,    0,    2,    1,    1,    1,    1]], device='cuda:3') tensor([69, 80, 62, 70, 52, 74, 63, 76], device='cuda:3') tensor([0.7085, 1.0000, 0.9399, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
True False
tensor([[8218,   63, 1739,  349, 1526,    8,  373,    0,   33,  275,   48, 2447,
            0, 2224,  105, 2447, 2718, 3192, 1436,    0,   41, 2969,   89,  328,
           60,  115,   12,   33,  643,   27,   48,  346,  169,    3,   41, 2331,
           14,    6,    9,  335,    0,  315, 1275,    3,   41,  295, 1144,  551,
          521,    0,    9, 4576,   67,   22,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  58,    9,  122,   18,   20,  137,   72,   22,    0,    9,  122,   18,
           20,  137,   18,    8,  562,  205,   18,   23,   59, 1333, 1564,    9,
          196,   54,  915, 3601,    6,   47,  121,   23,  145,  129, 1869,   44,
           95,   23,   23,  682,   12,  335,    9, 1451,   12,   59,    8, 3250,
           34,  161,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  38,   20,   65,  394,  171, 3695,   62,    0,   22,  678,   14,   28,
          577,    0,   20,   65, 2729,  124,    9, 3695,    0,   11,   12,    9,
         7759,   24,  691,   20,  459,   14,   28, 8963,    9, 3695,    0,  148,
           20,  459,   14,   28,    0,   20,   14,   92, 6276,   12,   24,   18,
          138, 1884,  142,    7,  235, 3016,   60,   24,    0,    2,    1,    1,
            1,    1,    1,    1,    1],
        [  46, 3198,  134, 4575,    0,   46, 1025,  121,   12, 2545,    9,  882,
           46,  195,  445,   19, 1462,    0,   11,   46,   54,  538,   12,  499,
         1109,   67, 1472,   96,    9,  354, 1464,    0,   57,  177,   55,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 104,   50,  108, 1825,   28,    6,   54, 4831,  430,   90, 5097,  991,
           83,  571,   18,   27, 1592,   15, 1955,    6,  764,    0,   11,   46,
           14,   44, 1986,   60,   22,   12,    8, 2442, 1586,    0,   19,   33,
         1078,    8, 2377, 1413,  112, 1886,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  11,  112,  722,  533,   67,    8, 8577,   61, 8391,    0,  347,    9,
           88,    0,  686,   11, 3692, 2347,    0,   23,   65,  707,    8,  558,
          406, 2756, 2294,   11,   81,  825,  558, 9190,   19,  406, 2756,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9, 1699, 1230,   14,   28,    7,   30, 1229,    0,    0,   22,   48,
            0, 2624,  127, 1825,    0,    0,    0,    0,   11,   22, 2522, 6564,
           49,  129,  254,  683,    0,  345,    9, 4633,    0,    0,    8,    0,
          502, 4556, 5110, 1661,    0, 1600,    0,    0, 2335, 1390,    9, 3963,
         1679,  108,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  11,    0,    0,   19,    0,    0,   18,    0, 8891,    0,  801,   15,
         7306,   15,  232,  700,    0,   70,    9, 1901, 2015,    6,   12,    0,
          899,  655,    9,    0, 6747, 1776,   53,   15,    9,    0, 1370,  370,
         1411,  105,    0, 1269, 4262,    0, 3164,  741,   46,   59, 1333,   19,
            0,    9, 4609,   27,    8, 1068,  331, 2613,   19, 1370,  370, 1411,
          620,  160,  723,    0,    2]], device='cuda:4') tensor([56, 52, 58, 36, 44, 37, 52, 65], device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7622, 0.7617],
       device='cuda:4', dtype=torch.float16)
True False
tensor([[  20,  145,  291,    9,  823,  275,   18,   23,  204,   79,  420,   27,
           47,    9,  573,   19,    9, 1777, 4879,   91,   33,   85,    0,   11,
          191,   24,  308,   96,    0, 2024,   11,   38, 4187,    0,  132,   59,
         4142,   11,  132,   59, 3430,   12, 2025,   11,   12, 1306,    0,  286,
           12,  301,    9, 4543,   62, 9730,   12,   69,  122, 2519, 1319,   19,
         3657,   60,   90,  537,   12, 6056, 5260,  299, 1437, 2289,   11,  573,
           81,  233,    9,  161,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  38,   20,  982,   61,   18,   20, 3400,  139, 1307,  135,    6,   19,
           17,  370,  487,   89,    0,   38,  424, 6196, 2867,    6,   19,    8,
          777,   15, 1446,    0,  191,   20,  982,  500, 1787, 1307,  135,    6,
           93, 3764,    6,    0,   20, 1894,   61,  719,  206, 2644,    6,  246,
         1221, 1258,  239,   17,  370,  487,   89,    0,   67,   33,   85,    0,
          688,  294,   21, 4518, 1953,    0,  356,    9, 3461, 1221,  744,   28,
          561,   62,  935,    0,   70, 6021,   70, 2939,    0,   11, 1200,  105,
            0,  101,    8,  797,   63,  754,  183,  124, 5703,   96,    9,  783,
           98, 1411, 3561,  191,   20,   48, 2138,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  11,   18,   27, 2051,   95,   24,   14,   44,    0,    0,    0,   19,
            8, 2286,    0,   11,    0,   74,   14,    6,    0,    8,    0, 4072,
            0,   57,  177,   55,    0,    0,    0,    0,    0,   58,    9,  535,
            0,   27,   64, 1129,  103,    0,    9, 4072,  881,  648,  320, 1275,
            0,   11,   33,  536,   27, 2676,  346,  233,   11,  233,    0,   11,
          233,  580,    0,   11,    0,   22, 1155,   96,  356, 1986,  127,   92,
            0,   93,  276,   63,  383, 1252,    0,    0,    0,    0,   12,  807,
          566,  125, 1070,    0,   93,  863,   63,  168,  295, 2066,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  22, 2025,    6,   38,   21,   80, 2588,   28,  105,   12,    9,  535,
            0,   41,   14,  127,   14,   68, 3036,    8, 1263,   62, 3220,    0,
           14,   20, 1941,    0,   11,   20,   14,   68, 1045,    0,  855,   44,
          136, 3220,    0,   14,  112,  855,   44,  136, 3220,    0,   14,   88,
          642, 8070,    6,    0, 1977, 3207,  295,  339, 3283, 2843, 1306,    6,
            0,   11,  972, 9527,  715,    6,    0,  855,  259,    0,   14,   20,
         1123,    0,  855,  586, 2076, 1657, 4055,    6,    0,   73,  365, 3220,
            0,   14,  855,   44,  136, 3220,    0,   14,   19,  206, 1265,    0,
         4144,    6,   12,    9, 3220,   18,   27,   73,  365,    0,  950,    9,
         3220,   18,   27,  365,    0,   18,   65,  197,   69,  704,    0,   27,
           73,  365, 3220,    3,    2],
        [  38,    9,  569, 1373,   47,  141,   19,   33,  953,   27,  102,   12,
          445,    8,  196,   12,  310,   81,   15,   24,  942,  205,   91,   33,
            0,   11,   38,    9,  196,   18,   23,   14,   44, 2078,   60,   18,
           27,  112, 4190, 1388, 4499,    6,   12,   88,  191,   23, 1941,  121,
           46,   65,  335,  104, 1606,   15, 2321,    0,   11,  165,  245,  722,
           12,  158,    9, 2321,   11,    9, 3394,   11, 4595,    6,  107,  189,
            9,  365,  161,   19,    8, 6569,   15, 1033,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([ 78, 105,  96, 125,  82], device='cuda:4') tensor([0.9985, 1.0000, 0.7476, 1.0000, 1.0000], device='cuda:4',
       dtype=torch.float16)
True False
tensor([[  11,   18, 1140,   74,   27,    8, 6435,   96,    9,  354, 2738,    0,
         2137,   62,    9,  354, 3327,    6,    0,   18, 3261,    6,   18, 2286,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9, 1012,  342,   23,  199,   12,  394,  118,   19,    8,  524,   38,
            9,  524, 1863,    6,    0,   27,  106,   22,   14,    6,   73, 4049,
           91,    8,  406,    0,   22,   14,    6,   73,  767,   91,  141,    0,
            2,    1],
        [ 961, 4630,    0,   11,   64,   18, 3249,   50,   48,   18,  741,   23,
           79,   62,    9, 2118,   15,   97,  935,  195,   69, 3093,   61,   12,
          141,   12, 4074,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 101,    0,   47,    8, 3706,    0,  872,  103,   46,  394,   81,  134,
         4810,    0, 1768,    8, 2210,   70, 1777,   70,  928,  873,  677,   60,
            0,    0,   67,   38, 1572,    0,    2,    1,    1,    1,    1,    1,
            1,    1],
        [  23,  470,   69,  115,   12,    8, 3587,   19,  392, 7167,   75,    9,
          250,    0,   58,   84, 1744, 5373, 3783,  284,    6,  284,   19, 4221,
           28,  154, 1118,   87,  137,    6,    8,  766,  200,    0,    2,    1,
            1,    1],
        [ 103,   24, 1496,  986, 4262,    6,    0,   22, 7192,    6,  115,   19,
            9,  424,  295,  328,   15,  138, 3775, 1348, 1457,  733,   12, 2363,
            7,  237, 2314,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  11,  432,   15,  342,   33, 1064,   27,  531,   11,   81,    9,  205,
           18,   23,   79,   54,  531,    0,   27,   18,  104,   54,    9,  607,
           23,  286,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  58,  636,   22,  101,  763,   24,   14,   44,  122, 7866,   12,   64,
          250,   88,   65,   14,   28,  139,   93,  611,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  11,   64,   22, 5895,   50,   12,   79,   48, 6969, 1148,   11,   84,
           88,   11,  741,   23,  691,   19,    0,  106,   23,   14,   44,   73,
         6439,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19,  436,    0,  101, 1027,    8, 1516,   51, 2970, 1093,   93, 5527,
         1714, 1306,    6,    0,  104,  205,   54,  245,  205,   18,  529,  141,
            8,  318,   15, 7340,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  97, 4347,  260,   27,    8,  432,    0,    8, 1505,  432,    0,   15,
            9, 4943,   15,   33, 2294,    0,   11,   22,   27, 6364,   47,    9,
         4943,    6,   15,   97, 2875,    0,    2,    1,    1,    1,    1,    1,
            1,    1],
        [  38,   23, 2712,   47,    8,  562, 1665,    0,   11,   87,  241,    0,
           41, 4664,  174,   14,   28,   24,  437,  118,   64,   24,  691,    3,
           20,  593,   18,    0,   58,   23,  174,   14,   28,   59,    9,  169,
            0,    2],
        [  38,    0,  287,   15,  141,   54,  102, 4944,    8,  326,    0,   15,
            8,  390,  281,  105,  125,  181, 1282,  284, 1229,    0,    0,  856,
            0, 1424,   68,  237,   96,    0,  578,    0,  246, 6173,   52,   30,
            0,    2],
        [  18,  653, 1494,  417, 1572, 3976,   60,    0,   58,   20,  197,  445,
           22,  556,  731, 1774, 2217,    0,  106,   22,  763,   74,   54,   38,
          287,  314, 1433, 2570, 7130,    6,   12,  260,   62,    0,    2,    1,
            1,    1],
        [  22,    0, 2345,   18, 3491,    0,    0,    0,    0,    8, 4605,  571,
         4425,   65,   25, 4092,   97, 2086,    0,   12, 2117,    8,  276,    0,
           67,  562,  108, 8047,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1],
        [  19, 2438,  694,  936,    0,  387,  383,  903,  383,  550,   62,  963,
          133,  135,  296,   68, 1031,   12,   50,   83,  281,    0,   11,   87,
         2125,   61,   67,    8,  129,   28,  225,  776,   15, 1592,    0,    2,
            1,    1]], device='cuda:6') tensor([26, 37, 29, 31, 35, 29, 28, 22, 27, 30, 31, 38, 38, 35, 31, 36],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 0.8477, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.8101, 1.0000, 0.7544, 1.0000],
       device='cuda:6', dtype=torch.float16)
True False
tensor([[  57,  177,   55,   58, 1168,    0,   20, 3060,   61,    8,  620, 3479,
         3281, 1553,   15, 8828,   18,   24,  204,  283, 2545,   96,    9,  264,
           15,   84,  897,    0,   11,   22, 1390, 2195,   12,   50,   18, 4152,
           60, 2659,    6,  459,   14,   28,  976,   44, 2724, 1068,   89,    0,
         8449,    0,  417,  329,   51, 3633,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  38,   24,   65,  553,   12,  139,  121, 3648, 1070,   11,   19,    6,
          125,  331,   60, 5954,    6,   15,  906, 1312,   19, 1462,   18,   23,
           65,  335,   12,  942, 4725,  327,  248, 1214,   63, 5005, 8622,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  11,    9, 1779,    0,   12, 4489, 3725, 1891,  453,    0,   23,   54,
          131,   12,  286,   12, 4514, 5472,    6,   49,   11, 1440,    0,   18,
           14,    6, 5472,    6,   67,    8,  814,   49, 5472,    6,   15,   12,
           30,    6,   15, 2511, 7734,   96,    9, 4340,   19,    9, 3651, 4314,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [2268,   60,    0,   20,  119,   20,  291, 2268,   60,   11,    8,  318,
           15,   24,  137, 1790,  430,    0,    8,  318,   15,   24,  137,   72,
            9,  188,  418,   51, 5006,   18,   24,  139,   49, 5701,   73,  138,
          790, 6515,   12, 1429, 1079,   49,   93,  733,   12, 5596,    9,    8,
         2319,   44, 2425, 1436,   39,  133,    0, 1980,  295,   80,    0, 1738,
         1945,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  20,  126,  101,  837, 1906, 2205,    0, 2257,   84, 3684,   12,  266,
           91,  688, 1583,  736,   50,  734,    0, 1807,  727,  250,  741,   18,
           20,  425,   61,    0,   11, 6567,   67,   81,    9, 6364,    6,   49,
          171, 7193,    0,    8,  562, 6704,    0,   11,    0,   15,  588,    0,
            8, 2583,  768,   49,  106,   20,   48,  131,   12, 2567,    9,  161,
            0,   11,   20,  488,   20,  145,  101,  553,   67,    9, 1575, 4212,
            0,    2],
        [  20, 1338,   96, 1124,   60,    9, 7609,   15, 3553,   47,  122,  239,
            8, 4032,   18,  103,   23,  795,    8, 1736,   49,   11,   33,   27,
           19,    9, 2950, 7354,   49,   23, 8676, 4678, 3709,   67,  122, 4577,
            0,  122, 3370,  299,    0,  122, 7086,    0,   11,   23,   14,   44,
          122, 2398,   12, 2166,  107,   12, 1472,   47,  577,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [   9, 1012,   47,   33,  173,   12,   79,   67,    9, 6992,   80,   28,
          587,  140, 2360, 3918,  963,   28, 5902, 1204, 3918,    8, 2291,  179,
          136,    8, 1126,    6,    0,    9,  591,   14,    6,   11,  923,   14,
            6, 3363, 2774,  536,   18, 7277,    6,   97, 2714,   63,  422,   63,
          398, 2645, 2774,    0,  121,  589,   22,  260,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  24, 5683,   18,    0,  240,   63,  485,  154,  796,  273,  352,    0,
            0,   24,   59,    8,  728,    0,  429, 2208,   15,  356,    0, 3519,
            0,   11,  115,   12,   72, 1867,    0,  826,  727,   49,    0,    0,
          102,    0,   20,   14,   68,  917,    0,   72, 3519, 6067,  105,   49,
         3437,  640,    8,  374,   11,    0,  458,  429, 2208,   15,  856, 3519,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1]], device='cuda:3') tensor([57, 37, 50, 63, 74, 59, 58, 62], device='cuda:3') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8311],
       device='cuda:3', dtype=torch.float16)
True False
tensor([[4150, 3272,    6,    9, 5098,   15, 2556,  901,   11, 3841,   12,    9,
         2782, 1004,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9, 6384,   75, 1997,  622,  622, 6854,  671,  195, 2015,   33,  472,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,  301,  141,  347,  230,    0,   11, 6062,  141,   12,  506,   17,
           99, 1225,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  20, 2437,    8, 1263,   72, 1867, 2278,   88,   81,  233,   33,  712,
          401,  499, 3847,  260,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 231,    0,   85, 2967, 1021,    0,   20,  126,    8,  318,   15,  609,
            0,   11,   20,   48,  538,   12,   79,  506, 6216,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  62,   33,  916,  373,    0,   74,  142, 1158, 3420,   13, 1082,  107,
           19, 4563, 2197,    6,   15,    7,  987, 5177,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,   50, 2753,  216,    0,   19,   18,  833,    0,  197,   27, 6798,
           63,  179, 2831,   83,   60,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 573,   19,  300, 1088,   59,  221,  190, 1022,    6,   61, 5678,   62,
          134,  196,   12,  671,   11,   62,  134,  196,   12,  260,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [ 209,  362,    0,   24,  567,   12,   13,   12, 2043,  189, 8558,    0,
          232,   19,  686,  476,    0,  165,   19, 3889,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11, 1142,    0,   22,   27,    8,  991,  513,   80,    0, 3364,  530,
         5152,  125,    0,    9, 2812,    0, 1501,  496, 1752,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,  104, 3521,    6,  943,   73,  101,   19, 2773, 1362, 4287,    6,
            0,   46,  943,   19,  984,   11, 1412, 4287,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,  112, 6024,    0,   23,  126,    9,  232,  665,   15, 2733,   18,
           65,  139,  511,  239,   88,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  22,   14,    6,    8, 1579,  275,   49,   22,   14,    6,    8, 1347,
         3609,    0,  148,   27,  117,    0,    0, 4658,   61,   11,    8,    0,
         4780, 1595,   67,   32, 3727,    0,    2],
        [  74,   14,    6,  245,    8, 4006, 7080,   51,   47,  672,    0,  148,
         5701,  195, 5763,   19,    9,  569, 1617,  200,    0,  433,  817,  383,
            0,    2,    1,    1,    1,    1,    1],
        [  57,  361,   55,   73, 2581,    9,  161,   94,  297, 5660, 3038,    0,
          106,  619, 1332,   14,   28, 6178,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9, 1262,  241,  490,    9, 7258, 1891,  460,    6,   18,   46,  142,
          131,   12,   79,  236,   91,  664,  248, 1871, 3333,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   95,   20,  139,  236, 4957,    0,   20,   14,   68,  131,   12,
           69, 8368,    0, 3084,    0,   41,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  95,   24,  953,    9,  354,  240, 2330,   19,    8,  334,  196,    0,
           24,   65,  197,  158, 3926,   15,    9, 6329,  108, 1263, 2268,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  20, 2091,  580,    0,    0,   41,   77,  150,  230, 1336, 4826,  134,
         4120,    3,   20,  488,   33, 1494,   61,  380, 1229,  105, 9164,    0,
            2,    1,    1,    1,    1,    1,    1],
        [  20,  137, 3942,   11,  993,  681, 2621,   12,   59,  122,   15,    8,
           13,   30,  411,   30,   15, 5140,  239,    8, 3854, 2645,   15, 5140,
            0,    2,    1,    1,    1,    1,    1],
        [  11,   24,  283,   59,   12, 2960,    0,  191,   23,   59,  700,    0,
           74,   27,    8, 2850,  196,   12, 3975,  609,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  18,   14,    6,    9, 1618,   15,  325, 2380,  476,   23,   59,  102,
            0,   11,   22,   14,    6, 4602,    0,   84, 2880,   27,   19,  436,
            8, 3065, 1945,  124,    0,    2,    1],
        [ 102,   84,  570,  173,    8,  318,   12,   79,   67, 2372,   60,    0,
         3848,   20,   14,   68,    0, 2087,   12,  218,   84, 4930,  308, 1220,
            0,    2,    1,    1,    1,    1,    1],
        [  58,   85, 1012,   27,  106,   22, 2238,  949,    8,  453,    0,   96,
          490,   12,  424,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:1') tensor([16, 14, 16, 18, 23, 22, 19, 24, 22, 23, 23, 19, 31, 26, 20, 23, 19, 25,
        25, 26, 22, 30, 26, 17], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8589, 1.0000, 1.0000, 0.8521, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.9512, 1.0000, 1.0000, 1.0000, 0.9751, 1.0000], device='cuda:1',
       dtype=torch.float16)
True False
tensor([[  58,   20, 1059,  ...,    1,    1,    1],
        [  11,    9,  354,  ...,    1,    1,    1],
        [  38,   20,  834,  ...,    1,    1,    1],
        ...,
        [  87,  394,    8,  ...,   22,    0,    2],
        [ 195,  171, 2935,  ...,    1,    1,    1],
        [  20,  550,   47,  ...,    1,    1,    1]], device='cuda:1') tensor([28, 22, 25, 15, 22, 22, 24, 23, 26, 21, 24, 34, 27, 17, 26, 19, 21, 24,
        19, 26, 32, 46, 28, 28], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 0.9067, 1.0000, 1.0000, 0.7896, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.7266, 0.8916, 1.0000], device='cuda:1',
       dtype=torch.float16)
True False
tensor([[  11,   38,    0,   33,   27,  138, 4358,    0, 1281,    0,   57,  177,
           55,    0,   58, 5821,    9, 1135,  432,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   20,  137,   85,   15,    9, 3601,    6,   15, 1156,   27,   18,
          255,    9, 2052,  452, 1942,    6,   53,   65,   69,  801,    6,   15,
         2116,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  33,   27,  103, 8416,   48,   47,  135,   61,  107,   15,    9, 9291,
         4435, 6112,   15, 1231, 1014, 1810,  454,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,   46,  435,    8, 7479, 3504,   49,   46, 3343, 5459,    9, 1169,
           12,  335,   33,  609,    0,   11,  152,   54,    9, 3102,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,   23, 1045, 1452,    0,   95,   24, 6972,   22,   81,  345,    0,
           64,   27,   75,    9, 3925,   15, 3517, 8009,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  20,  199,   24,   12,  266,   75,  138, 1744,    8,  282,  525,    0,
           54,   46,   15,    9,  354, 1872,  476,   70,   24,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  20,  563,    8, 1842, 2532,  411,    0,   57,  177,   55,   20, 2257,
         1248,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  95,   20, 1045,   24,   12, 2166,  107,   49,   24,  174,   14,   28,
           59,   12, 2288,   79,   22,    0,   58,   19, 1083,   49, 2166,  107,
           12,    9, 1135,   15,    9,   41,   99,    3,  191,  145,   24, 2166,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  58,   20,  137,   22,   14,    6,   19,  383,  408,   12,  691,   18,
           23,  195,  690,  218,    8, 1441,   18,   27, 9156,  105,   50,  296,
          215,   83,  159,  571,    0,   22,   14,    6,   90, 3576, 2372,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  47,    8, 2967,   11,    8, 1006,    0,   20, 1596,   14,   28,  137,
           72, 1423,   60,  373,  857, 4773,   60,    0,   11,   20,  530,   59,
            9, 6367,   19,   50,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1532,  454,  291, 3950,   91,   33,   54, 1804,   47,    9, 8472,    6,
            0,    9,  784,   51,  136,    6,    0,    9,    8,   34,  775, 2327,
            0,    9, 2534,    6,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  58,   20,  137,   33,   27,  197,  101,    9, 1935,   15,  121,   23,
           14,   44,  131,   12, 2581, 1103,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,  420,    0,  424,    9,  558, 1485, 1650,    0,   74,   48,    8,
          729,  248, 1053,   15, 2916,   72,  121,    9, 1412,  470,  888,   92,
          408,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 231,    0,   75, 1454,    0,  664,   19,    9,    0, 2246,    0,   67,
           84,    0,  897,  345,   62,   84, 6556,  106,   20, 2968,    0,  126,
           30,   14,   28,  277,  154, 1786,    0,   81,    0,    0,    0, 1275,
            0,    9,    0, 1461, 5824,    6,    0,   11,   22,   14,    0,    6,
            9,  248, 1761,  787,    0,    2,    1],
        [   0,   11,   24,  245,   59,   12, 2667,   62, 7922,   63,  243,  135,
           44,   99,    0,    0,    0, 6747, 1776,   53,   11,  151, 1059,   18,
           74, 1702,   14,   28,   69,  414, 8505,    0,   18, 1275,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 171,   15,    9, 4468, 1362,  549,  402,    6,   24,   14,   92,    0,
          269,   12, 1424,   34,  339,   44,    0,    0,   19,  790,    0,   75,
          216,  225,  426,    0,  298,  322, 6820,    0,    0,    0, 4829,    0,
         6800, 1818,  333,    0,    0,    0,  888,  243, 1721,   51,    0, 1063,
            0,    0,    0,    0, 3772,    0,    2]], device='cuda:7') tensor([21, 27, 21, 24, 22, 23, 15, 38, 37, 30, 30, 20, 27, 54, 36, 55],
       device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.7715, 0.8232, 0.7358],
       device='cuda:7', dtype=torch.float16)
True False
tensor([[2388,  886,    0,   32, 6373,    0,   58, 2442, 1567,   63,  873,   34,
         1864,    6, 2166,  264,   12,    8,  540, 5035,   15,  137,  454,  132,
          126, 4631,   51,   70,   12,  342,   23, 7804,   12,  631, 2237, 1109,
           97, 4434, 1437,  786,    0,  973,   70,    9, 1888,   15, 2442,  476,
            0,  182,    6, 1945,    6,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9, 1891, 2620,  426,   49,    8, 3670, 1909, 1891, 2620,  426,   49,
           27, 1804,  106,   15,    9, 2950, 2728,   18,   23,   14,   44,   62,
            8, 9595,   47,   72,    8,  719,   63,  243,  135,   44,   99,  161,
           11,   23,   59,   12,  453,  588,   12, 1858, 4704,  240, 5086,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   11,   23,   14,   44,  401,   33,  106,   38,  287,   15,    0,
            9,  997, 2637,   19,  908, 2698,   91,    0, 2123,   93, 9519,    6,
           93,    0, 5712,  806,   54,  648,   12,  171,    0,   15,    9, 8234,
           11,    0,  250,    0,  417,  390,    6,    6,   53,    6,   61,    0,
           88,   62,    9, 1201,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  23, 5029,    9,  792,    0, 1569,   67,    9, 1141,  216,   47,  233,
          955, 1242,    0,  269,    9, 1391,  454, 1775,  222,   92,   51,    0,
           11,  240, 1090,  424,   46,  142, 3135,   12,  194,  115,    0,    9,
         1141,  216, 1399,   12, 3252,    9, 1391,  454,    0, 2131,  430, 1675,
         2785,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   20, 5178,  691,   97,  648,    6,    0,   97,  260,  672,    6,
            0,   97, 1664,  145,  825,  122, 3429,   11, 4603,   11,  122,  406,
           99,   11, 1269,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  38, 2292,  429,   15,   64,   20,   79,   67,   84,  762, 1526,   15,
          393,  125,  169,  447, 1794,   27,   12,  301, 5638, 9296, 3125,    6,
           15, 1160,   91,   33,   96,   84, 8619,   11, 8409,  118,   19,    8,
          196,   18, 1532,    6, 3841, 1012,   60,   11, 2705,  535, 7474,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  47, 1955,    6,   75,   18,    0,  643,    0,    9, 4072,  126,  221,
            8,    0, 1635, 4443,   47,   81,    9, 6469,   18,   88,  142,    0,
         6079,   60,   62,    9,    0,    0, 1303, 1258,    0,   11,    9,    0,
         4946, 1262,   48,  197,    0,  536,  757, 3038,  556,  411, 2049,  430,
            0, 4072,    6,   11, 1805,   15,  206, 7337,    6,  578,    0,   91,
          400, 2355,  949,   11, 2590,  133,   92,    6,    0,    2],
        [  38,  233,    9,  493,    0, 2751,  200,    0,    0,  790,    0,  327,
            9,  161,    0, 1110,    6,    0, 1481,   11,  115,    0,   59,    0,
         3819,  134, 3408,    0,   11,    0,   20,  488,    0,   20,    0, 2619,
           95,    0,   15,  230,    0,  392,  747, 3408,    0,    0, 1153, 2290,
         1079,  173,  488,   12,  394,  115, 1163,   14,    6,  373, 3004,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([55, 49, 54, 51, 29, 49, 70, 62], device='cuda:5') tensor([1.0000, 1.0000, 0.8120, 1.0000, 1.0000, 1.0000, 0.8667, 0.7944],
       device='cuda:5', dtype=torch.float16)
True False
tensor([[  22,  812, 8596,   11,  447,    6, 2969, 1093,   12,   73, 1558,   28,
            0,   58,   20, 1333,  236,   18,   20, 1338,  490,   62,    9,   85,
          429,  373,    0,   11,   18,   27,   18,   24,   65,  283,   69,  315,
          103,   24,   54, 3461,    0, 8038, 1646, 3738,   12,  138,  425, 1740,
           11,  722,   19,    8, 4253,   30,  426,   67,   18,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  57,  361,   55,   95,   24,  151,  199,   12,  119,   95,    8, 1288,
           27,    8, 6692,   93,   73,    0,  834,  300,    0,   57,  177,   55,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 102, 6574,   49, 1485, 6574,   49,   27,  236,   23,  470,   81,   69,
         5882,   72,    0,   11,   73,  101,  106,   15,  230,   75,    9, 2464,
           15,    9,  499,  976,  108,  123,  494,   89,    0,   58,  106, 3159,
           11, 2687,   67, 1805,   15, 1485, 6574,   79, 3239,    0,   73,  101,
            9,   88,   75,    9, 2464,    0, 1383,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   20,  642,    0,    0,   20,  869, 5178,    0,   11, 3433,  105,
           70,  231,    0,   58, 5178,    0,    0,    0,   57,  177,   55,    0,
            0,    0,    0,  232,   15,   81,    0,   90, 1123,   12,    0,   33,
          635,  195, 2469,  141,  122,   72,  104,    0,  392, 4495,    0,  517,
          352,    0,    0,  935,    0,   11, 2939,    0,   73,  283,    0,    0,
           72,  121,   46,    0, 2028,   67,  134, 1410,  420,    0,   58,  121,
           46,  142, 1039,    6,   15,  200,    0,  534,    0, 1153,   46,  142,
         3480,  329,  540,  534,   93,   73,    0,    2,    1,    1],
        [  22,   14,    6,  245, 2894,  329,    0,  106,   15,    9, 1199, 1277,
           15, 8855,    0,   47, 2899,  132,   65, 1628,    9,  250,   12,  197,
         1496,  189,    9,  551,  939,   11,  291,    0,   41, 5257, 1261,  233,
           74,    0,   20,  199,   12,  277,  970,  602,  154,  104,  488,    6,
          189,  134, 4010,    3,   38,   24,   65, 3690,    0,   24,   65, 8090,
         3690,    8, 3300, 4542,   12,    9,   88,  132,   54,  250, 9667,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  535,   27,   18,    0,   95,   20,   14,   68,   33,   79,   83,
            0,   11,   20,  158,  138,  557,   28, 3784,  420,   11,   24,   14,
           92,  269,    8,  240, 3379,  127, 2662, 2555,   19,  138,  645,   34,
            0,   11,   24,  308,  264,   12,   50,   19,  392, 1242,   11,   22,
           14,    6,  392, 3379,  127, 2662,    6,    0,  316,   18, 2825,  577,
           24,   93,   73,    0,  121,   79,   20,  119,    0,  145,   22,   59,
          221,  374, 3379,  127, 2662,    6,    0,   93,  563,   20, 2408,   24,
            8, 2825,   67,   32, 4874,   11, 3804, 1593,    0,    2],
        [1472,   23, 1367, 1143,   11, 1765, 2165,   34,  431, 1261, 7308,    0,
           11, 1472,  530,   23, 1876,  241,   12,  118,    0,   41, 5486,  194,
          648,   11,  752,   12, 3824,   72,  138, 3991,    3,  122, 2482,    0,
           23,   14,   92, 2091, 7308, 2880,    6,    0, 3537,  108,  758,  129,
           28,   44,  757,    6,   49,  287,  205,  148,  653,  298,   68, 1125,
         5700,  105,  129, 2094,   92, 3363,    0,   58,  148,  174,   14,   28,
          197, 3982,  124,  713,   28,    6,   51, 5869,    6,  233,    9,  540,
         2571,    0,    2,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:5') tensor([59, 25, 57, 92, 73, 94, 87], device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 0.7896, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
True False
tensor([[4165,  259, 3560,  352,  383,  782,    0,   24, 2621,   12, 3261,    8,
         1675, 4829,   18,   27,  326,   15,   25,  946, 1506,   61,   11,    8,
         2857,  108,   15,    9, 4442,   68,    6,   15,    9, 3433, 1347,    6,
            0,   11,   18, 4968,   19, 4165,    6,    6,  397,    6,   18,   24,
         3261,    0,   12,   50,    0,   70,    8, 6588,    0,   27, 8923,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  33,   27,    8,  326,   15,  619,  635,   18,   20,   70,   90, 9469,
          422,  508,  834,   19, 2038,    0,   58,  103,   20,  834,   18,  326,
           15,    8,  635,   19, 2038,    0, 1797,  429,   15,    9, 1521,   54,
          530, 3273,  296,  237, 3664,    9,  493,  275,   20,  241,    0,  797,
          429,   54, 5164,   51,  107,   62, 4085,    0,   11,  165,   74,   14,
            6,    9, 2592,   89, 1476,   28,    6,   19,    9, 1303, 5589,  132,
          980,  483,   34,   28,    6,  107,    9, 1123,  490, 2290, 1079,  173,
          126,    8, 2208,   12,  137,   72,   22,    0,   11,   20,   70,    9,
         9469,  422,  563, 9692,  877,   28, 3281,   18, 1429,  197, 1338,    9,
         1123,    0,    2],
        [  58, 1864,    6,  420,   59,   12,  864, 2652, 3941,  299,   75,  457,
         1779,  117, 2088,   11, 1491,  107,  121,   12,  137,  533, 4045,  105,
            0, 2587, 2483, 3575, 9686,    6,    0,  707, 9720,    0, 1491,  107,
            9, 1675, 1373,    6,   11, 6949,  118,   12, 8859,    0,  106,   81,
          104,  315, 6933,    6,   11, 7382,  723,   11, 8799,  112, 1607,   54,
           73,  131,   12,   69,  832,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 148,  269,   50,  767,    0,   20,  241,    0,   27,   74,  414, 7746,
          476,   12,   33,  729,    8,   51, 1031,   18,  103,   24,   14,   44,
          906, 3462,    0,   22,   14,    6,  823,   12,  301,    9, 4635,   14,
            6, 4221,    0,   69,   19, 4104,    0,   69,   19,  551,    0,   93,
           54,   74, 3151,    6,  191,   23,   14,   44,  948,  511,  521, 1486,
            9, 8475,   14,    6, 4221,   11,   59, 1115, 1079, 1863,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [  11,    9,  647,  275,   12,   79,    0,   15,  588,    0,   27,   12,
         3055, 2924, 2522,    0,   18,  763,   12,  445,  132, 1079,  173,  221,
         8241,    0,   58,  653,   73,   69, 2522,   60,   38,  246,  227,  102,
           91, 1115,   67,    8, 4082, 1078,   15,  814,    0,  237,    0,    0,
           93, 1115,  132,   27,  101, 5016,   60,  107,   19,    9, 3840,    6,
            0,   58,   19,    9,  354, 1261,    0,   11,  165,   46,  286,   12,
           69,    0,   19,    8,  196,    0, 5590,   70,  231,    0,  916,   12,
            9, 3750, 1516,  953,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1],
        [ 230,   15,  141,   19, 1241,   59, 4826,   62,   12,    8, 1199, 1111,
           18,   23,  174,   14,   28,  869,  121,  287, 2328,   15,  993,   88,
         3968,    0,   23,   14,   44,  131,   12, 2171,   12,   79,    9,  354,
          275,   18,  459,   14,   28,  260,    0,   11, 2629,   27,  856, 3162,
           72,   22,   49,  227,    0,   49,  832,   12,  291,    0,   41,  179,
         6110,   27,  832,    3,   38,  152,   14,    6,    8, 1199, 1111,   18,
         1876,  589,   73,  218,  414,  833,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1]], device='cuda:2') tensor([ 61, 111,  67,  72,  90,  80], device='cuda:2') tensor([1., 1., 1., 1., 1., 1.], device='cuda:2', dtype=torch.float16)
True False
tensor([[1525,    0,   38,   84,  937, 3398,   27,   72, 4145,   11, 1309,    0,
         6979, 1630, 6533,    6, 2137,   62,  121,   23,  266,   91,   93,  191,
           23,  308,   96,    0, 4368,    0, 3345,    0, 2038,    0,   64, 1056,
          141,  132,   23,   54, 3393,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  46,  567,  115,   67,   90,   25,  154,   77,  938,   61,  729, 2316,
          622,  757, 6054,  433, 3796, 1471,   77,  435,  107,   15, 1286,  474,
           68, 2316,  622,  757,    0,    8, 2491,   15, 1832,   53,   99,  435,
           96, 6181,   63,  398,   61, 1654,   38,    9, 1832,   53,   99,   27,
          738,   19,   86,  220,  487,   63, 1000, 4113, 1019, 6437,    6, 1168,
           15,   86,  220,  487,   63, 1008, 4113, 1019, 6437,    6,   96, 1639,
           30,   63,  398,   61, 1654,    0,   11,    8, 2491,   15, 2073,   99,
           18,  173,  392,  787,    9, 1618,   15,  793,  105,  125,  622,   30,
         1119,    6,  239,  414, 2087, 2073,   99,   19,    9,  161,    0,    2],
        [  11,   20,  291,   18, 1142,   44,  105,    0,  432,  105,  106,   57,
          350,  313,   38,  237,   55,   20,  286,   18,    0,   57,  177,   55,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  20,   14,   92, 2081,    8,  318,   15,    9, 1279,  917,   12,    9,
          392, 2663, 4151,   18, 5345, 1475, 1675, 6874,    6,    0,  675,  418,
         1124,    0,    9, 2541,   15, 7061,   14,    6, 1872, 3964, 1124, 1600,
            0,   11,    9,  250,  531,   58,    9, 1676, 2087,   27,    9, 1475,
         1872, 4287, 3646, 1261,   18,   27,    9,  161,   14,    6,  540,  452,
            0,  250, 3242,   61, 2866,   15, 1675, 6874,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  20,  497,   34,  496,   24,    0,   58,  283,    8,  282,  525,    0,
          106,   20,  174,   14,   28,  199,   12,  158,   24,   19, 4424,    0,
           12,  101,  834,  327,   11,  139,  121, 1579,   22,  145,   69,   12,
          158, 1964, 2825,   24,  199,    0,  191,  789,   24,  199,   22,    0,
          103, 2969,   24,  199,   22,    0, 4065,   19,    9,  497,    0,    6,
            0,    0,   11,  171,   15,   24,  653,   69, 5716,   12,  119,   18,
           74,   54,  287, 1849,  454,   18, 2574,    8, 3525,  191,   95,   24,
         2404,  118,    8, 2268, 2210,    0,   46, 7986, 2707,   89,   15,    9,
         2825,   19,  664, 1665,   93,  866,    0,    2,    1,    1,    1,    1],
        [  11,  103,   24,   14,   44, 3039,  115,    9, 2981,   12, 5318,   12,
           79,  236,  148,   32,  406,  173,  690,  704,  490,    0,   11,    0,
           19,  436,    0,   32, 1561,   49,   74,   54,   32, 1561,  115,   74,
         7434,   75,  955,    0, 1000,  954, 4884,   49,  103,   24,   14,   44,
          733,   12,   79,   18,    0,   11,  165,    9, 3420,   54, 1028, 1106,
           24,    0,   22, 1930, 2600,   24,    0,   11,   24,  245, 2242,  117,
            0,  117, 3662,   18, 1462,   27,   38,  246,  122, 1777,  239,   23,
           54,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:4') tensor([ 43, 108,  25,  71, 104,  87], device='cuda:4') tensor([1., 1., 1., 1., 1., 1.], device='cuda:4', dtype=torch.float16)
True False
tensor([[  33,   27,   90,  ...,    1,    1,    1],
        [ 310,   14,    6,  ...,    1,    1,    1],
        [   9, 1455,  567,  ...,    1,    1,    1],
        ...,
        [  88,   59,    8,  ...,    1,    1,    1],
        [  11,    9,  234,  ...,    1,    1,    1],
        [  11,   95,   22,  ...,    0,    2,    1]], device='cuda:4') tensor([12, 10, 16, 10, 18, 13, 10, 13, 11, 13, 14, 16, 16,  9, 13,  7, 16,  9,
        17, 13, 12, 11, 10,  8, 15, 12, 12, 10, 16, 15, 14, 13, 12,  8, 10, 13,
        11, 14, 12, 14, 10, 13, 13, 16,  8, 16, 20, 12, 15,  7, 17, 11, 11, 15,
        10, 13,  9, 11, 14, 13, 19, 10, 10, 19], device='cuda:4') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9307, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 0.7905, 1.0000, 1.0000, 1.0000, 1.0000,
        0.7349, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8774,
        1.0000, 0.7466, 0.8140, 0.8950, 0.7529, 1.0000, 1.0000, 1.0000, 0.9844,
        1.0000, 1.0000, 1.0000, 0.9985, 1.0000, 0.8379, 1.0000, 1.0000, 1.0000,
        1.0000, 0.7002, 0.8853, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000], device='cuda:4', dtype=torch.float16)
True False
tensor([[  22,   65,   69, 7847,    6,    0,   93,   22,   65,   69, 1471,    0,
           93,   22,   65,   69, 4033,   93, 5162,   93, 3036,    0, 1964,   22,
           27,    0,    2,    1,    1,    1,    1,    1],
        [  46, 2091,  248,  882,    6,    0,  334,  648,    6,    0, 2362,  897,
            6,   11, 4853, 3420,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   23,   14,   44,  117, 5165,   15, 2408,  788,   12,   88,    0,
           38,   18,  282,  790,   91,   33,  174,   14,   28,   79, 6439,  205,
            0,    2,    1,    1,    1,    1,    1,    1],
        [  74, 1332,   14,   28,    8, 1091, 7668, 2289,   14,  671,   19,  300,
         2736,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [3259,    0,   97, 1248,   54,  246,  122,  239,   33, 5761, 1082, 2452,
          215, 1976,   15, 4794,   11, 8596,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  20, 1050,   88, 8072,   12,  158,  112,    0,   73, 4360,   64,   48,
           64,   11,   64,   48,  569,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  22,   14,    6, 5468,   19,   84, 1083,    0,   11,  165,   22,   14,
            6, 5468,   19, 2011,   15,    9,  729,  953,   18,   20,  194,  347,
            0,    2,    1,    1,    1,    1,    1,    1],
        [  20,  174,   14,   28,   59,   12, 2998,  799, 3707,    0,  283, 1654,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [ 400,  154, 6743, 1733,   18, 1220, 8016,    6,   91, 4170, 5595,  108,
           61,    9, 7836,    6,   96,  134, 1465,  845,  331,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  74, 2021,    0, 4604,    0,    9, 2551,   12,  139,  596, 1738, 1640,
          254,   30,  328,    0,   19,    8,  334,  700,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  58,   22,   14,    6,  169,   23, 1938,    9,  196,   23,  137,   72,
         3074,   11,    9, 4211,   23,   59,   19, 6251,   22,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  32,    0,   23,   59,   12,   59,    9, 1930,  360,  476,   12, 1025,
           96, 2296,   15, 1485, 1646, 1950,   88,    0,  191,  789,   46,   54,
            0,    2,    1,    1,    1,    1,    1,    1],
        [  20,  145,  878,    9, 7262,   53,  648,    0, 3926,   60, 2608,   53,
           11,  716, 1813,    6,    0, 5848,  105, 5305,  118,   19,   84, 1870,
            0,    2,    1,    1,    1,    1,    1,    1],
        [  11,  151,    0,   46,  316,   81, 3973,   15,  205,   12, 4055,    9,
          993,   88,  189,  767,   64,    8,  457, 1849,    9, 6071,   48,  131,
           12,   69,    0,    2,    1,    1,    1,    1],
        [  24,  119,    0,   95,   24,   14,   44,  101, 2667,   60,   67, 1429,
         8264,  105,    0,   24,  174,   14,   28,  151,  869,   95,   46,   14,
           44, 2667,   60,   67, 1429, 1079,    0,    2],
        [  20, 5510,  543,   47,   72,   90, 3319,   70,   87,  351, 2426,   61,
           75,   22,    0,  733,   12,  445,    8,  196,  107,   15,   33,  275,
            0,    2,    1,    1,    1,    1,    1,    1],
        [   9,  206, 2663, 5154,   15,  499, 9256,    6,   27,    9, 8346,   15,
            9, 1630,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  20,  642,    0,   46,   54,   73,   49,   46,    8,  225,   14,   28,
          695,   99,  430,   47,    9, 3892, 3609,   15, 3193, 1351,  125, 6434,
           93,  988,   91,   18,    0,    2,    1,    1],
        [  58,    8,  318,   15,  792,  173,  221, 3109,   30,  107,   75,    9,
          984, 1618,   11,   75,    9, 4946, 1618,   12,  266,   75,   19,  683,
           83,  284,  804, 3932,    0,    2,    1,    1],
        [ 102,    0,   20,   14,   51,   91,   12,  583,   24,    8, 3768, 1159,
           15,   84, 1261,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  823, 1299,    6,    0,   93,    9,  250, 2905, 1299,    6,    0,
           54,    9,   85,    6,   67,    8,  151, 1903, 2821,  757, 4708, 9286,
            0,    2,    1,    1,    1,    1,    1,    1],
        [  11, 4707,    0,  115,   19,    9, 2697,  215,  962,   34,    0,    9,
         4968, 1053,    6,   18, 2612,    9, 5664,    6,   12, 4802,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  20, 1390, 9156,  105, 2182,   19,    9, 1248,   11, 3991,   18,  218,
          118, 3514,   11, 3220,  136,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1],
        [  23,   14,  150, 1727,    9, 6498,   11,   24,   14,  150,   59, 1899,
            6,   19,  138,  928,   12,  158,  138,  933,  107,  152,  227,  727,
            0,   41,    2,    1,    1,    1,    1,    1]], device='cuda:2') tensor([27, 18, 26, 15, 20, 19, 26, 14, 24, 22, 23, 26, 26, 28, 32, 26, 17, 30,
        30, 17, 26, 24, 19, 27], device='cuda:2') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        0.8936, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000], device='cuda:2',
       dtype=torch.float16)
True False
tensor([[  93,    0,   70, 1888,  419,  270,  408,   19,  190,   34,  154,   68,
          629,   12,  291,   12, 5508,    0,   41,  371,  220,   88,  393, 2523,
            9,  194,    6, 7558,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  20,   14,   92, 1045,   18,  635,   81,  986,    9,  712,    0,   11,
         3181,   20,  834,   22,    0,   32,  943,  191,    0,   74,   14,    6,
            8, 1505,   52,  299,   15,    9, 1992,   18, 1702,   14,   28,  394,
          115,  134,  928,    0,    2],
        [  20,   14,   68,  917,   72, 5818,  513,   89,  628,   80,   28,    6,
           11,  358,  411,  125,  912,   34, 5030,   12,   89, 5507,    6,   11,
         1046,  135, 1395,   34,    6,    0,    9,  729, 2716,   17, 1823,    6,
            0,    2,    1,    1,    1],
        [  46,  241,    0,   41,  402,   14,    6,  131,   12,   69,  578, 1340,
          757, 2462,   47,   24,   12,  194,    3,   87, 5886,   61,    0,   41,
          127, 1381,  194,    3,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  38,   22, 1898,  107,   18,   84, 3032,  644,   89,   72,   90, 2407,
          216,  225,   60,  276,   15,  356,    9, 2249,   34,  295,  376, 1230,
           14,   28,  131,   12,  308, 1220,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  11,    9, 1849,   18,   46, 8035,   27,    9, 5992,   60,  712, 2692,
         3069,  296,  212,  284,  762,  429,   15,    9, 1423,   61, 2064,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  20, 2482,    0, 1326,   33,  184,  105,   34,  431,  112,  188,  307,
         1823,    0,    0,    0,    0,  400,  622,   30,    0,   41, 1657,    0,
            9,    0,   69,  150,    0,    6,   18,  530,   65, 5824,    0,    2,
            1,    1,    1,    1,    1],
        [  11,   33, 3234,   83,  723,    6, 1433, 1359, 1538,  183,  160,  181,
           27,   38, 2483,    0,   62,    8, 4249, 4928,    0,   22,   14,    6,
           72,  374,  787, 7324,  239, 7058,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  95,   24, 1323,   24,   11,  138, 3517,   14,    6, 1466, 1727,    6,
            0,   46,  195, 3794,    8, 1159,   19,  134, 1083,   72,  138, 3692,
          276,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 308,   12,  137,   15,   22,    0, 4360,  103,   24,   14,   44, 1172,
          857,    8, 3710,    0,   93,  138, 1163,    0, 2339,   24,    0,   27,
            8,  117,  778,  535,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [1543,  107,   18,   95,   24,   59, 4759,   29,    6,    0,  250,   88,
           18, 3427,  134, 4759, 3427,   22,   75,   64,   14,    6,  433,    9,
          400,  494,  154,   77,    0,    9,   19, 1310, 4259,    0,    2,    1,
            1,    1,    1,    1,    1],
        [  38,   23,  174,   14,   28,  199,   12,  119,   18,  138, 4781,  248,
         2719,   27,  511,  239, 1133,    0,   58,   18,   22,   14,    6,  511,
          239,    9,  823, 2904, 2719,   23,   59,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  33,   85,  812,   50, 1775,  222, 1126, 2796,  105,    0,   20,  145,
          291,    0, 1137,   12, 1691, 2130,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   9, 1903,  452,  777,   15, 4124, 1092, 8264,  284,   53, 7738,   19,
         6968,    0,   67,  101,  458,    3, 3519,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  20,   14,   68,  356, 1254,   28,   63,  140, 6183,  102,    0,  152,
            0,   24,  174,   14,   28,  921,  139,   22,    0,   58,    0,   20,
           79,    0,   64,   95,   20, 2191,  866,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  20,  459,   14,   28,  158,  107,    0,   58,   48,   19,    9,  596,
         1486,  393,   51,  782,    6, 1839, 3428,    6,    0, 1486, 3187, 2393,
          565,    0, 4852,   12,    9,  902,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:6') tensor([30, 41, 38, 29, 32, 25, 36, 32, 27, 30, 35, 33, 20, 21, 33, 32],
       device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7090, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9443, 1.0000],
       device='cuda:6', dtype=torch.float16)
True False
tensor([[  11, 1055,    0,   24,   14,   44,    9,  387,  132,   14,    6, 2576,
           61,  634,   73,  101, 1020,   58, 4868,    0,   11,   23,   59,   12,
          752,   11,  658,  342,   22,   27,    0,   24,   14,   92, 1807,  141,
          171, 2044,  776,    6,    0,   58,    0,  152,   14,    6,    8, 1159,
           15,   24,   70,    8, 2129,    0,   67,    8, 5826, 5509,   14,    6,
         2580,   99,    0,    2,    1,    1],
        [  23,   14,   92, 2145,   96,    9, 1638,   15, 1701,   70, 1423,   15,
         1282, 2497,   34,    6, 7632,   62, 2922,    6,   15, 1061,  360,    6,
            0,   11, 9265, 1259,   34, 1001,    6,   67,    9, 4578,  861,  456,
         4055,    0,   12,    9, 1638,   15, 1701,   70,    8, 1423,   15, 6175,
         7760,   53,    0, 1169, 2316, 1203,   51,    6,    0, 1801, 1677,  474,
            6,    0,    2,    1,    1,    1],
        [  97, 3420,   59, 3723, 5472, 1287,    0, 5472,   67,    8,   41,   28,
            3,   23,   14,   44,  917, 1872, 7672, 2287,  152,    0,   57,  177,
           55,  230, 1287, 1267, 1489,  233, 1020,  320, 1867,  200,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  11,  424,    9,  786,    0,   20, 1050,   90,   25, 2004,  580,    0,
           84,   25, 2004,   48,    8,  117,  375,  754,  520, 8042,    0,   11,
           87,  922,   50,    0,   41, 4909,    0,   74,   14,    6,    8,  535,
          148,   20,  204,   73, 1905, 1389,  200,  534,    0,   11,  148, 2629,
           65, 1905,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  23,  151, 5286,   46,  142,  131,   12,  493, 2292, 1090,   93,  766,
         1090,    0,  106,   46,   54, 1330,  715,   61,    0,   11, 2939,   27,
            8, 7383,   89, 1201,    0,   38,   23, 5803,    9, 7383,  145,  553,
            8,   83, 4367, 1800,  430,   62,    9, 2118,    0,   11,  424,    8,
          950,   23, 2200,   14,   28,   59,  832,  715,    0,   24,  119,    0,
           12, 1074,  118, 5037,    0,    2],
        [  11,  165,   46,  194,  189,  134, 3517,   61, 1004,   11,   23, 2444,
           18, 3491,   46,   14,  150,  137, 1790,   27,   72,  118,    0,   18,
           46,   14,  150,   69,  538,   12, 7712,  974,  134, 1772,    0,  134,
         4705,    6,    0,  134, 4482,    6,    0,   22,   14,    6,   25,   44,
          136, 2361,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  33, 1892,   15, 1314,  235,  160,  322,   27, 2221, 1143,  112,   22,
            6,  277, 1590,   28, 6706, 3530,    0, 6730,    6,   67, 5866,  136,
         3934,   89, 1823,    6,   11,  104,   86,   83,  216, 3823,  136, 8885,
            6,  148, 2612,   47,  122, 7806,    0,  511, 8372,  299,   11, 4590,
          136, 7165,   75, 2205, 1618,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  38,   20, 5845,   18, 6230,   59,   81,  104,  531, 3107,    6,    0,
           11,   46,   14,   92,  245,  102,    0,  101,  233,    9, 1106,  562,
          200,    0,  221, 3268,   12,    8,  729, 3661,   15,  334, 3932,    0,
         3146, 4948,   68,  295, 3871, 6519,  397, 1516,    0, 1255, 1516,    0,
         5077,   30, 1103,    0,   11,  255, 8669,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1]], device='cuda:0') tensor([64, 63, 36, 52, 66, 52, 55, 57], device='cuda:0') tensor([1., 1., 1., 1., 1., 1., 1., 1.], device='cuda:0', dtype=torch.float16)
True False
tensor([[  11,   15,  588,    0,  103,   22,  881,   12, 6084, 4317,    0,   41,
         2591,   22,   93, 3427,   22,  242, 1775, 2094,    6,    0,   38,   24,
          199,   12, 1858, 1416,  105,  184, 6930,  346,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  20,   14,   68,  917,   12,   24,    0, 1191, 1914,  475,    6,   11,
         4045, 1314,  487,  225,   60, 4439,    6,    0,   11, 3647,    0,   11,
         1323, 6592,  454,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 106,   96,    8, 6135, 3003,    0, 1485, 7763,  299,   69,  496,   28,
            6, 3846, 7763,  299,    0,  148,   69,  496,   28,    6,  499, 7763,
          299,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [   9, 2943,   27,   18,    8, 3891,  587,  243,   11, 5259,  204, 5763,
          533,    0,   67,    9,  675, 1433,    6,   25, 1304,   34, 1094,   96,
            9, 1370, 4366,   61, 3171,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  22,   14,    6,    8,  596, 7091,  299,  536,    0,   11,    8, 1899,
         2635,   15,   22,   27, 2137,   62,    9,  634,   12, 6591, 3340, 5754,
           62,    9,  672, 4315,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  38,  287,  973,   18,   22,   14,    6,  102,  101,    8,  648,   47,
         2516,  132,  151,   91, 6437,  431,    0, 6850, 3409,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 358,    0,   32,    0, 2169,   50,    0,   22,   14,    6, 5169,    6,
            6,    0,   57,  177,   55,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  38,   23, 1399,   12,  453,   97, 5225,   63,  627,  353,   34, 3893,
           63,   67,   63,  243, 2645, 1319,   63,  390, 1144,    6, 4293,   11,
         1267,   22,  189,    9, 5225,   63,  627,  353, 3893,   63,   67,   63,
           77,   63,  243, 2645, 1319,   63,  140,  418,    6, 5902, 4293,    0,
            2],
        [1137, 3587, 6518,    6,  555,  671, 1794, 1717,   89,   60,   47,    9,
         3551,   15,  356, 6518,   15,    9, 1794,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  38,   23,  682,   12,  752,   12,  445,    8,  196,   12,  218,    8,
         7324,  980,  296,  473,   28,  124,   38,   18,   23,  204, 5972,   67,
         4533, 1259, 3720,  136,   19,    9, 1347,    6,   19, 7829,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  95,   24,  301, 1789, 1001,    6,    0,   24,  119,    0,    9,  729,
          977,   63, 7278, 1864,   27,    8, 4143, 3974,   15, 4940,   63, 1469,
          235, 1350, 2206, 2436,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  87,  241, 3351,    6,   59, 1158,  334, 1373,    6,    0,   11,   95,
           85,  684,  589,   81, 1158, 1373,    6,    0, 4184,   27,  117, 1903,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [ 139,  132,  335,    6,  688,  127, 2076, 2393,    0,    0,   54,   23,
          131,   12,   69,  538,   12,  445,  414,    0,    0,  688,  127, 2076,
         2393,    0,    0,    0, 8185,  993,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  87, 1569,    0, 4604,   88,   12,  445, 2603,   11,   12,  752,   12,
          878, 2698,  189, 8072, 3840,    6,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  20, 1390,   90, 3020, 6022,   47,   85,   15,    9, 6236,    6,    0,
          147,  496, 1092, 4188, 2440,  243,  296,  313,    0,   11,   19,   18,
         3171,    0,   20,  126, 1973,   12,    9, 5554, 3413, 1070, 1894,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1],
        [  84, 2846, 7992,   99,   27,  640, 1965,   11, 2631, 2495,    0,  191,
           20,   65,  139,    9,  161,  392,   63, 4547,  105,    0,   58,  245,
           75,    8,  406, 1779,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1]], device='cuda:0') tensor([34, 29, 27, 31, 30, 23, 18, 49, 21, 36, 30, 26, 32, 20, 38, 30],
       device='cuda:0') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 0.7905, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
True False
tensor([[   9, 5361,    6,   54, 5704,   60,    0, 4661, 5035, 6199,    0, 6348,
            0, 2198, 2436,    0,  741,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  62,   69, 7215,   15,    9,   69,   53,    0,  421,   24,    0,   57,
          361,   55, 3191, 7498,    0,  421,   24,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   20,  241,    0,   41, 1810,    0,   58,  342,  459,   14,   28,
           87, 5016,  107,   67,    9,  206, 2102,    3, 6160, 6893,  376,  328,
           49,   22, 2025,    6,   12, 4073, 1444,  476,   11,  245, 4270,   15,
         6354,    0,    2,    1,    1,    1,    1,    1,    1],
        [   9,  497,  968, 7143,   96,    9, 1901, 4481,    6,    9, 2243,   15,
          104, 3949, 1319, 9521,    6,   11,    7,  586,  216, 2049,  949,    9,
          596,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 102,    0,   84,  232,  643,   61,   11,   50, 1573,  329, 1004,   67,
         4829,   19, 8642, 7738,  103,   20,   48, 1932,  200,  764,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  85,   15,  104, 1492,    6,   19,  916,    0,  433,   41,  259,   34,
          328,  712,    3,   48,  197,  326,   15,    8, 2230, 2510, 5861,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [1274, 3417,   89, 5149, 3723,    0,   18,   14,    6,    8,  502,  864,
            0,   21,   99,  938, 5149, 2716,    0,  308,   62,    0, 1543,  107,
            0,  194,  648,   11,   79,    9, 3841,    0,    9, 2716,   12, 1435,
           27,    8, 2826,  453,    0,    2,    1,    1,    1],
        [  20,   65, 3261,   22,    0,  132,   77,    0,   19,   84,  897,    0,
           24,  119,    0,  132,   77,    0, 1525,    0,   20,   65,   79,  245,
            0,   84, 6615,    0,    0, 1655,    0,   22, 2692,   50,  236,   12,
           79,    0,    0,  236,   12,  260, 2715,    0,    2],
        [   9,  499,   63,   99, 2227, 4029, 1070, 7805,    6, 2801,    0,    9,
         1624,   15,  230,  499,   63,   99, 2227, 4029, 1070, 7805,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  684, 8854,   60,    9,   86,   83, 8854,   27, 5498,    8, 5826,
          806,  620,  160,   92,   12,  218, 1077,   46,  174,   14,   28,  158,
         5206,  346,    0,   11,   38,   62,    0,   58,   46,   14,   44,  500,
          629,   91,   18,    0,    2,    1,    1,    1,    1],
        [  11,  165,   46,   47,   99,   83,  160,  181,    0,   11,  165,   46,
         7864,    9, 2357,    6,   47,  356,    9, 1024, 1821,  132,    0, 1765,
          118,    9, 3564,   28,  135, 1031,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  38, 1148,   11, 4483,  206, 3456, 2361, 1066,   63,   12,  458,   63,
         1855,   63, 1432,    6, 1351, 3452,   62,    8, 2475,   12,  888,   30,
           19, 1352, 1737,  353,   67, 3124,  681,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   38,  171, 2673, 2335,    0,   24,   14,  150,   69,  538,   12,
          795,   11,  942,   11, 8823,   67, 5576,    6,   19,   33, 6609,  248,
          196,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  24,  199,   12,  119,  122,   72,   33,    0,  194,   11,  139,   33,
         1492,    0,   41,  100,   60,  169,    0,  401,  584,  376,    6,    6,
         2393,    3,   24,  195, 1474,   72,   22,    0,   11,   24,  195,  593,
           22,    0,    2,    1,    1,    1,    1,    1,    1],
        [  22,   14,    6, 1518,    6,   11, 5626, 1552, 5648, 5225,  233, 3220,
          136, 9627,  723,    0,   41, 5257,   14,    6,  915,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   0,   24,   65, 2960,   18,   33, 2710,   51,   27,  906, 3678, 8291,
          426,    6,   12, 1074,    9, 4726, 3879,   51,    0,  121,    0,  316,
           23,  800,    9, 3453,    6,   12,   79,   33,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([19, 21, 39, 27, 24, 25, 42, 45, 25, 41, 33, 33, 27, 39, 23, 34],
       device='cuda:7') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9180, 1.0000,
        1.0000, 0.9766, 1.0000, 1.0000, 1.0000, 1.0000, 0.9380],
       device='cuda:7', dtype=torch.float16)
True False
tensor([[  95,   22,   14,    6,    0,    0,    0, 7313,    0,   18,    0,  763,
           18,    9, 3891,    0,  107,  152,   54, 1700,    9,  877,  270,   28,
          462,  136,    0,    0, 4317,   15,  943,   18,   23,   79,   73,  139,
            0,    2,    1,    1,    1,    1,    1],
        [  11,   85,  255,   60,    0,   20,   48, 2209,  569,   12,    8,  117,
         2309, 7966,  466,   19, 2249,  464,  133,    0,   11,   87, 1045,   64,
           20,  488,  145,   69,  315,   19, 4958,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   20,  682,   12,  583,   18,   20,   65, 7277,   84,  853,    0,
          751,  634,    0,   96,  152,    0,   96,  540, 6250,    0,   38,   20,
          269,  152,  493, 1275,   11,   20,  119,  741,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  58,   49, 5016,   62,    0,  106,    9, 4444,   12,    9,  647,   11,
         1594, 1443,  151,   54,  117, 3225,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   9, 2251,  275,   27,    0,   64,   23,   14,   44,  401,   27,   23,
           14,   44, 2614,    9, 1083,   15,   98,  160, 5678,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  85,   15,    9,  250, 3175,  205,   72,   33, 1587,   48,    9, 2034,
           18,  941, 8492,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  33,   27,  191,    9, 1660,   11, 3399,   18,   20,   14,   92, 1336,
           61,  420,  195, 5701,  795,    8, 1899, 2402, 2715,  104,  117,  531,
         2274,    6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  87,   14,    6,    8,  387,    0,    8,  147, 1404,   28,    0,   90,
         3412,  147, 1404,   28,  692,   75,   33, 6995,  147, 1404,   28,    0,
         1167,    0,   41, 4909,   75,   50,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  12,   69,  538,   12,  453,  844, 1063,    6,   11, 3602, 6236,    6,
          189, 5557,    6,    0,   22,   14,    6, 4143,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   22,   14,    6,   73, 6067,  223,   99,  804,    0,   58,   22,
         1129,    0,   22,  589, 1052,    0,   20,  611,   22,    0,    8, 1930,
         2993, 1457,  264,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  33,  941,  106,    9, 2949,   15, 3853,   11, 1579,   89,  142, 2576,
           61,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  23,   14,   44,  917,   27,   77, 1469,   70, 4824,  968,    0,  805,
          379,  201,  408,    0, 1738,    0,  135,    0,  231,    6,    0, 6140,
           93, 1810,    0, 8361,   87,  225,  154,  225,    0, 2207,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [2916,   18,  291,    0,   33,   27,  121,   24, 6590, 2118,    6,    0,
           33,   27,   64,  551,   27,   81,   72,    0,   33,   27,  121,   24,
          583,   24,   14,   44,    8, 4490,   15,  138,  184, 4169,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   20,  137,   33,  234,   15, 5715,    0,   70,  356,   75,    9,
         1255,   15,  457, 2441,    0,   27,  117, 1775, 1376,  329,    0,   24,
           81,  119,   22,    0,   24,  119,    8,  524,  103,   24,   14,   92,
         1085,   22,    0,   67, 5715,    0,    2],
        [  11,    9,  898, 1118,   22,   14,    6,  151, 2251,   95,   23,  204,
          194,  107,   11,   69,    8, 1277,    0,   93,    8,  265, 1069,   93,
            8,    0,  277, 1831,    6, 3080,   91, 1631,  307,  353,   93, 2975,
            6, 2021,    0,    2,    1,    1,    1],
        [  11,   19,  436,    0,   24,    0,  204,   79,   33,  414,  148,  196,
           49, 3088,   33,  275,   67,   81, 1606,   15,  793,  105, 3823,    6,
           11, 1850, 3812,  379,   11, 3795,  118,  115,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:3') tensor([38, 33, 34, 20, 23, 17, 28, 32, 22, 29, 15, 36, 36, 43, 40, 34],
       device='cuda:3') tensor([0.7710, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9487, 0.9697],
       device='cuda:3', dtype=torch.float16)
True False
tensor([[ 103,   24,   14,   44,   19,   22,    0,   24,   14,   44,   91,    8,
         4746,   19,    8,  765, 3282,    0,   24,   65,   14,   28,  255,  139,
          233,    9, 1135,    0,    2,    1,    1],
        [  19, 1286,  212,  127,   33,  472,    0,   23,   14,   92,  101,  126,
            9,  232,   81,   63, 6018, 1225,   14,    6,    0,  648,  135, 2770,
           30, 2014,   63,  842,  583,    0,    2],
        [  11, 1221,    0, 2761,  431,    6, 4699, 2381,   61,   11, 2707,   61,
            9, 7653,   11,   22,   48,    8,  594, 4034, 1094, 2661,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [  24,  119,   22, 1230,   14,   28,   90, 1069,   18,  435,   50, 1249,
           84, 1255,  668,  150,    9,  190,   68, 1001, 5658,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   20,  137,   18,    9,  232, 1373,   12,  161, 2995,   27,   47,
           88,   12, 2382,  555,  206,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,  152,   14,    6,    9,  535,   18,   20,   14,   92,  221, 2182,
           19,   19,    9,  493, 4032,   93,   38,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  32, 4811,    7,   13,  675, 1382,    0,   38,    9,  387,   62,    9,
          227,    0,   47,  543,    0,   87,   14,    6, 3039,  327,    9,  923,
            0,    2,    1,    1,    1,    1,    1],
        [  23,   59, 7785,    0,   23,   54,    9, 2812,    6,    0,   15,   97,
          425,  848,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 530,    0,   19,   84, 3964,    0,   23,   59,  240, 4211,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 102, 5208,  852,   48,    8, 1827,    0,   38,   87, 1183,  886,  327,
           67, 3421,    8,  318,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  20,  834,   88, 1190,    0,   11,   46,  291,    0,   41,  127,   91,
            9,    8, 1614,   77,  571, 2738,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 102,   64,   14,    6,    9, 1365,  133, 2628,    0,   95,  988,    0,
          640,  104, 1859,  179,   80, 1019,  456,    6,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   47,  171,  169,    0,   87,   48, 6746,   33,   48,    9, 1012,
           23,  269, 1564,   38,  231,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 102,   33,   27,    8,  499, 5861,    0,   11, 4464,    6,   54,  117,
         5882,   72,   33,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  23, 4220,  105,    0,   95,  690,    0,  460,   72,    9,  375,  181,
         2532,  422,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [6161,   30,    0,   27,    9, 4134,  328,  429, 2462,    0,    0,   18,
           87,  173,    0, 2124,   63,  839,   99,  571,   82,    6,    0,    2,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   20,  241,    0,   41,  127,   65,   14,   28,  301,   24,   74,
            0,   58,   20,   65,  301,   24,  189, 4249, 1128,  758,  189, 3386,
           63,  135,    0,    2,    1,    1,    1],
        [ 132,  425,    6,    0,  525,  278,  225,    0,  530,    8,  318,   15,
           24,    0,    0, 1281,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  18,   14,    6,   84, 3509,    0,   87,  127,  273,    0,  132,  459,
           14,   28,  301,   22,   70,  231,   70,   20,  316,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1],
        [2388,   68,    0,    9,  800,   15,    9, 7061, 5870,  173, 3573, 1682,
           63,  225,   67,   90, 1659, 2997,   63,  181,   83,  299,   15,    9,
          853,    0,    2,    1,    1,    1,    1],
        [  38,  948,    0,   20,   14,   92,  283, 2712,   72,    9,  196,  609,
           27, 4250,   51,    0,    0,   58,    9,  196,   22,   14,    6, 7315,
          943,    6,  101,   70,  246,    0,    2],
        [ 486,  855,  281, 2591,   20,   14,   68,   72,   12, 4442,  125, 1429,
           14,    6,   70,    6,  486,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,    0,  982,    9, 8823,   60, 3147,   23,  316,   75,    9, 1935,
            0,    9,  326,   15,  282,  902,    0,    0,    9, 8161,   24,  316,
            0,    2,    1,    1,    1,    1,    1],
        [  20,  197,  126, 5475,    9, 7042,  375,  384, 1094,   15,    8, 3555,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  23,  204, 2587,   22,   47,    8, 8801,   15,    9, 3157,   11,    8,
         8801,   15,    9,  169,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  64,   20,  752,   12, 8499,  102,   27,    9, 3693, 7668,  476,   15,
          205,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [   0,   64,   23, 5691,    0,   20,  119,  102,    0,   27,    9, 2464,
           15,    9,  410,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  23,   59,   32,  738,   63,  398,  474,   83,  215,  181, 1639,   30,
          277,   89,   34,  842,    0,   32, 1627, 4113,    6,    0,   32,  953,
           61,  882,    6,    0,    2,    1,    1],
        [  38,   64,   23,   59,   12,  197,   79,   27,  942,  104, 4468, 1362,
         1412, 4151,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [1594,  275,   62,   84, 2766, 5660,    0, 6826,    6,   67, 1744, 2094,
            6,   18,   65, 7056,    9, 1441,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  33,   27, 3061,  968,  602,   28,   28,   17,  566,  968,    0,   11,
           87,   14,    6,   96,  497,   28,  768, 1235,   77,  243,  677,    0,
         1701,    0,    2,    1,    1,    1,    1],
        [8722,  136, 9338,   60, 5182,    6, 1607,    0,   19,    8, 8344,   15,
         2213, 1978,   19,  406, 2233,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:5') tensor([29, 31, 24, 23, 19, 21, 26, 16, 12, 18, 20, 22, 19, 17, 16, 24, 28, 18,
        23, 27, 31, 18, 26, 14, 18, 15, 17, 29, 16, 20, 27, 19],
       device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8975, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8394, 1.0000, 0.8652,
        1.0000, 1.0000, 0.9819, 1.0000, 0.9487, 1.0000, 1.0000, 1.0000, 0.9043,
        1.0000, 1.0000, 1.0000, 1.0000, 0.9458], device='cuda:5',
       dtype=torch.float16)
True False
tensor([[  22,   27,    9, 1380,    0,   11,   22,   14,    6,  245,    9, 1088,
           18,  335,    6,   33,  634,   19, 1033,   18,   23,  204,   73,   47,
           53, 2650,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  70,   87,  864,    6,  271, 4513,    6, 1978,  345,    0,   22,  763,
           18,   87,   14,    6, 4513, 3386,  103,   22,   14,    6,   81,    9,
          196,  152,   75,    9,  837,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  58,    9,  250,  531, 1012,   27,   18,    8,  536,   15, 2555, 7678,
         4155, 2927,   53,   97,  425, 3430,   19,   81, 3973,   15, 1033,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  95,   24,  137,   15,   22,    0,  681, 2419,  671,   33,  472,  195,
           69,  129,  284, 1657,   19,  458, 1008,  894,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 358,    0,   38, 3019,    0,    9, 1083,   15,  138, 2441,   27,  961,
         1083,   63,  513, 5319, 1094,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [   9,  122,  787,   24,   14,   44, 5792,   12,  335,  138,  365, 4145,
            0, 3019,   19, 9137,  136, 2011,    0,    9,  122, 2398,   18, 4145,
           27,   12,  158, 1370, 1119,  179,   11,  716,  789,  346,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  20, 1050, 4021,   11, 1356, 1737,    0,   84, 1521,    0, 1367, 2102,
            0, 1737,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,   24,  653,  834,    0,   65,   23,  335,   33,  245,   12, 5496,
         2597,  571,  654,    6,   14, 4434,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  22,  173,   85, 4365, 5637,    0,   22,  173,   90, 6503,   62,    9,
          537,    0,   38,   22, 2033,  277,   41, 1119,    3,   11,   18, 2033,
          277,  342,   46,  610,   22, 2131,    6,   63, 1000,   63,  622,  565,
         4179,   63,  785,   63, 1119,    0,    2,    1,    1],
        [ 152,   14,    6, 2624,   99,  917,   72,   42,  600, 1971,    0,   57,
         4870,   55, 2624,   99, 7126,    0,   42,  600, 1971,   48, 1413,   19,
            9, 1414,   15,    8, 1505,    0, 3170, 4677,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [2716, 1019,   63,  181,  938,  429,   15,    9,  161,   14,    6,  596,
           27, 1062,   28, 5754,    0,  121,   72,   23,  335,   18,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 231,    0,   95,   23,  151,  137,   72,   22,    0,   23,  139,   18,
            9, 1199, 1277,   15,    9, 1231,  420,  151, 1332,   14,   28,  695,
          757, 1139,   67, 6798,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 2197,   85,    0,   23, 7740,   22,    0,   19, 2197,  240,    0,
           23, 2803,  596,   11, 1062,   28,   12,   22,    0, 3795,   22,  533,
            0,   11,   23,  707,  236,   18,   23,  610,   41, 1890,   89,    3,
           22,   14,    6,   91,  557,  140,   89,    0,    2],
        [  97, 3031,  110,    0, 3560,  379,    0,   48,   90,  283, 1185,    0,
            8, 1269,    0, 2785,   61, 2369,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  11,  165,  139,  121,   22,  417,  685,    6,   53,   33,  206, 1261,
            0,  132,   54, 6461,    6,    0,   67,    9, 3527,   15,   90, 8246,
          105, 2379,   51,   19, 2076,   30,   83,  299,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  22, 2574,    6, 4733,  426,   11, 3727, 1955, 4828,   12,    9,  250,
           41,  370,    6, 2497,  150,   61,  242,   19, 7810, 8013,    6,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:0') tensor([28, 31, 25, 22, 19, 36, 16, 20, 43, 34, 24, 30, 45, 20, 34, 25],
       device='cuda:0') tensor([1.0000, 1.0000, 0.9805, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:0', dtype=torch.float16)
2023-09-01 11:42:25 | INFO | train_inner | epoch 001:    102 / 1826 loss=17.956, trans_loss=5.726, nll_loss=4.569, w2v_ctc_loss=23.232, task_loss=7.847, contrastive_loss=0, total=3940, n_correct=63.34, ppl=23.74, accuracy=1.608, wps=19893.8, ups=1.68, wpb=11837.1, bsz=400.3, num_updates=100, lr=4.098e-06, gnorm=3.668, clip=0, loss_scale=32, train_wall=65, gb_free=19, wall=125
2023-09-01 11:43:24 | INFO | train_inner | epoch 001:    202 / 1826 loss=13.002, trans_loss=5.749, nll_loss=4.623, w2v_ctc_loss=15.595, task_loss=6.444, contrastive_loss=0, total=4000.63, n_correct=62.39, ppl=24.64, accuracy=1.56, wps=20402.6, ups=1.69, wpb=12045.3, bsz=454.7, num_updates=200, lr=8.096e-06, gnorm=8.42, clip=33, loss_scale=32, train_wall=58, gb_free=19.5, wall=184
2023-09-01 11:44:23 | INFO | train_inner | epoch 001:    302 / 1826 loss=7.23, trans_loss=5.744, nll_loss=4.639, w2v_ctc_loss=6.712, task_loss=6.096, contrastive_loss=0, total=3981.48, n_correct=56.04, ppl=24.92, accuracy=1.408, wps=20145.8, ups=1.68, wpb=11963.3, bsz=445.6, num_updates=300, lr=1.2094e-05, gnorm=1.325, clip=0, loss_scale=32, train_wall=59, gb_free=18.5, wall=244
2023-09-01 11:44:53 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-01 11:45:22 | INFO | train_inner | epoch 001:    403 / 1826 loss=6.807, trans_loss=5.705, nll_loss=4.623, w2v_ctc_loss=6.092, task_loss=5.691, contrastive_loss=0, total=4007.21, n_correct=62.97, ppl=24.64, accuracy=1.571, wps=20420.7, ups=1.69, wpb=12059.5, bsz=431.7, num_updates=400, lr=1.6092e-05, gnorm=0.619, clip=0, loss_scale=16, train_wall=58, gb_free=19.6, wall=303
2023-09-01 11:46:21 | INFO | train_inner | epoch 001:    503 / 1826 loss=6.512, trans_loss=5.571, nll_loss=4.461, w2v_ctc_loss=5.778, task_loss=5.108, contrastive_loss=0, total=3994.6, n_correct=83.75, ppl=22.03, accuracy=2.097, wps=20336.4, ups=1.69, wpb=12013, bsz=442.9, num_updates=500, lr=2.009e-05, gnorm=0.43, clip=0, loss_scale=16, train_wall=58, gb_free=19, wall=362
2023-09-01 11:47:20 | INFO | train_inner | epoch 001:    603 / 1826 loss=6.352, trans_loss=5.537, nll_loss=4.425, w2v_ctc_loss=5.568, task_loss=5.199, contrastive_loss=0, total=3946.86, n_correct=97.37, ppl=21.48, accuracy=2.467, wps=20282.2, ups=1.71, wpb=11872, bsz=424.9, num_updates=600, lr=2.4088e-05, gnorm=0.439, clip=0, loss_scale=16, train_wall=58, gb_free=18.5, wall=420
2023-09-01 11:48:18 | INFO | train_inner | epoch 001:    703 / 1826 loss=6.045, trans_loss=5.523, nll_loss=4.404, w2v_ctc_loss=5.113, task_loss=4.831, contrastive_loss=0, total=3953.79, n_correct=96.7, ppl=21.18, accuracy=2.446, wps=20248.4, ups=1.7, wpb=11877.7, bsz=431.2, num_updates=700, lr=2.8086e-05, gnorm=0.578, clip=0, loss_scale=16, train_wall=58, gb_free=18.7, wall=479
2023-09-01 11:49:18 | INFO | train_inner | epoch 001:    803 / 1826 loss=5.791, trans_loss=5.519, nll_loss=4.404, w2v_ctc_loss=4.724, task_loss=4.877, contrastive_loss=0, total=3973.13, n_correct=89.64, ppl=21.18, accuracy=2.256, wps=20172.8, ups=1.69, wpb=11951.9, bsz=428.7, num_updates=800, lr=3.2084e-05, gnorm=0.701, clip=0, loss_scale=16, train_wall=59, gb_free=18.6, wall=538
2023-09-01 11:50:16 | INFO | train_inner | epoch 001:    903 / 1826 loss=5.584, trans_loss=5.528, nll_loss=4.415, w2v_ctc_loss=4.396, task_loss=4.668, contrastive_loss=0, total=3998.85, n_correct=100.95, ppl=21.34, accuracy=2.524, wps=20650, ups=1.72, wpb=12033.4, bsz=439.4, num_updates=900, lr=3.6082e-05, gnorm=0.888, clip=0, loss_scale=16, train_wall=58, gb_free=19, wall=597
2023-09-01 11:51:14 | INFO | train_inner | epoch 001:   1003 / 1826 loss=5.455, trans_loss=5.548, nll_loss=4.431, w2v_ctc_loss=4.176, task_loss=4.478, contrastive_loss=0, total=3988.93, n_correct=101.67, ppl=21.58, accuracy=2.549, wps=20584.4, ups=1.72, wpb=12000.5, bsz=449.7, num_updates=1000, lr=4.008e-05, gnorm=0.907, clip=0, loss_scale=16, train_wall=58, gb_free=18.7, wall=655
2023-09-01 11:52:13 | INFO | train_inner | epoch 001:   1103 / 1826 loss=5.372, trans_loss=5.565, nll_loss=4.447, w2v_ctc_loss=4.027, task_loss=5.086, contrastive_loss=0, total=3947.12, n_correct=109.84, ppl=21.81, accuracy=2.783, wps=20206, ups=1.7, wpb=11874.1, bsz=420.8, num_updates=1100, lr=4.4078e-05, gnorm=1.082, clip=0, loss_scale=16, train_wall=58, gb_free=18.6, wall=714
2023-09-01 11:53:11 | INFO | train_inner | epoch 001:   1203 / 1826 loss=5.283, trans_loss=5.571, nll_loss=4.449, w2v_ctc_loss=3.882, task_loss=5.186, contrastive_loss=0, total=3914.85, n_correct=126.34, ppl=21.85, accuracy=3.227, wps=20313.1, ups=1.73, wpb=11775.7, bsz=401.2, num_updates=1200, lr=4.8076e-05, gnorm=1.091, clip=0, loss_scale=16, train_wall=57, gb_free=18.5, wall=772
2023-09-01 11:54:10 | INFO | train_inner | epoch 001:   1303 / 1826 loss=5.195, trans_loss=5.568, nll_loss=4.446, w2v_ctc_loss=3.75, task_loss=5.018, contrastive_loss=0, total=3925.76, n_correct=130.86, ppl=21.79, accuracy=3.333, wps=19927.4, ups=1.69, wpb=11815, bsz=424.1, num_updates=1300, lr=5.2074e-05, gnorm=1.05, clip=0, loss_scale=16, train_wall=59, gb_free=19.2, wall=831
2023-09-01 11:55:08 | INFO | train_inner | epoch 001:   1403 / 1826 loss=5.133, trans_loss=5.573, nll_loss=4.45, w2v_ctc_loss=3.648, task_loss=5.026, contrastive_loss=0, total=3918.07, n_correct=134.77, ppl=21.85, accuracy=3.44, wps=20269.8, ups=1.72, wpb=11779.6, bsz=410.4, num_updates=1400, lr=5.6072e-05, gnorm=1.069, clip=0, loss_scale=16, train_wall=58, gb_free=18.7, wall=889
2023-09-01 11:56:07 | INFO | train_inner | epoch 001:   1503 / 1826 loss=5.055, trans_loss=5.578, nll_loss=4.454, w2v_ctc_loss=3.525, task_loss=4.941, contrastive_loss=0, total=3995.02, n_correct=136.37, ppl=21.92, accuracy=3.413, wps=20477.8, ups=1.71, wpb=12005.7, bsz=425.9, num_updates=1500, lr=6.007e-05, gnorm=1.114, clip=0, loss_scale=16, train_wall=58, gb_free=19.6, wall=948
2023-09-01 11:57:05 | INFO | train_inner | epoch 001:   1603 / 1826 loss=5.003, trans_loss=5.564, nll_loss=4.442, w2v_ctc_loss=3.454, task_loss=5.072, contrastive_loss=0, total=3877.45, n_correct=133.07, ppl=21.73, accuracy=3.432, wps=20080.3, ups=1.72, wpb=11667.5, bsz=409.1, num_updates=1600, lr=6.4068e-05, gnorm=1.099, clip=0, loss_scale=16, train_wall=58, gb_free=19.6, wall=1006
2023-09-01 11:58:03 | INFO | train_inner | epoch 001:   1703 / 1826 loss=4.938, trans_loss=5.566, nll_loss=4.445, w2v_ctc_loss=3.356, task_loss=4.775, contrastive_loss=0, total=3947.06, n_correct=136.14, ppl=21.78, accuracy=3.449, wps=20411.8, ups=1.72, wpb=11879.5, bsz=425.7, num_updates=1700, lr=6.8066e-05, gnorm=1.134, clip=0, loss_scale=16, train_wall=58, gb_free=18.6, wall=1064
2023-09-01 11:59:02 | INFO | train_inner | epoch 001:   1803 / 1826 loss=4.889, trans_loss=5.569, nll_loss=4.448, w2v_ctc_loss=3.278, task_loss=4.755, contrastive_loss=0, total=3948.71, n_correct=136.3, ppl=21.83, accuracy=3.452, wps=20176.2, ups=1.7, wpb=11885.2, bsz=430.7, num_updates=1800, lr=7.2064e-05, gnorm=1.058, clip=0, loss_scale=16, train_wall=58, gb_free=18.8, wall=1123
2023-09-01 11:59:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([[  74,   27,    8, 6977,    0,    0,  640,  773,   83,   44,  260,   11,
            0,  315,  260,    0,   11,   70,    0, 2899,  132,   14,    6,    0,
            0, 1569,   70,   90, 3630,   93,    8, 1914,  475,    0,  119,    6,
            0,   22,   14,    6,    0,    8, 6977,   24,   14,   44, 5678, 8072,
           12,  158,   62, 1135,   15,    0,    2],
        [  11,   20,  605,   18,   97, 7967,   27, 4195,   86,  696, 1128,    0,
           58,   22,   14,    6,   97, 2516,   11, 1318, 4333,  220,   18,   14,
            6, 5163,   47,  138, 4816,    6,    0,  138, 8573,    0,  138, 4145,
           11,   38,  246, 2652,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  95,   24,  266,   75, 2374,  898, 6819,   49,   19,   33, 1078,    0,
           19,    9, 1135,  374, 6553,   15,    9, 1231,   49,  122,  239, 5973,
          429,   15,    9, 1498, 1114,  108, 2012,   27,   62, 6845, 1498, 2644,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  38,   20,   59,   33, 6629,  333,  640,   64,   23,   14,   44,  906,
           88,   79,   11,    9, 2812,   18,   14,    6,  748,   15, 2224, 5958,
           11,    9, 2812,   18,    0,   19,    8,  833,    0,   88,  508, 2638,
           67,  134,  425, 2734,   11, 1711,  255,   69, 3754,  377,   61,  189,
          401,   18,    0,    2,    1,    1,    1],
        [  11,   23,  691,   18,  104, 2774,    6,   19, 3030,   63,  422, 1335,
         2336,   54,   19,  602, 1918,   93,  825, 1425,  986, 2387,  454,  106,
           15,    9, 2949,   37,   92,   89,   61,  112,    9, 6290,    0,   11,
           73,  112, 1265,   93, 1494,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  20,  137,   33,   27,   37,   83,  483,  455,   92, 8347,    0,   95,
          690,   22,  142, 1804,    0,   18, 1273,   14,    6, 3158,   54,  393,
         1869, 1504,   12,   69,  101,    8,  282,  525, 6175,    0, 1281,    0,
           38,   18,   48, 1135, 6628, 6597,  824,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  20,  286,   12,  437,   24,   33,    0,  152,    0,  151,    0,   11,
            0,    0, 3659, 1646,    0,    0,   20,  941,   12,    0,  255, 2250,
            8,    0,    0,  248, 3326, 1075,  148,   20,  459,   14,   28, 2242,
            0,   11,    0,   22, 1390,    8,  796,  496,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  18,  763,   18,    9, 7965, 2739,  181,   27, 2186,  430,   90, 5975,
         3768,   18,   14,    6,  888, 1845,   60,   67,    9, 2996,  309,   99,
          804, 6442, 3878,   19,    9,  591,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1]], device='cuda:2') tensor([55, 42, 39, 52, 43, 45, 46, 32], device='cuda:2') tensor([0.8232, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.7808, 1.0000],
       device='cuda:2', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[  33,  729, 2265, 2674,  141,  121, 8307,  973,    8, 7887,   48,   11,
          121,  457,   11, 1200,    9, 8821,  387,    0,    9, 8821, 1288,    0,
          121,  619,   11, 1200,  104,   88,   54,  103, 2969,   46,   59,    8,
         2372,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 102,    9,  232,  665,   20,   14,   68,  131,   12,  583,   24,   27,
            8, 2705,   67, 2760,  225,  775,   14,    6, 1516,    0,   11,   33,
           13, 1240,  173, 2760,  225,  775,   14,    6, 1516,    0,   11,  219,
          173,  104, 2526, 1406,   19,  300,  591,    0,   11,   20,   14,   68,
          131,   12,  583,   24,   64,  219,   14,    6,   91,  103,    9, 2526,
         1406,   54, 1898,  521,   11,  219,  173,  300, 2760,  225,  775,   14,
            6, 5869,    6,    0,   11,  165,   23,   14,   44,  131,   12, 1267,
           22,   62,    0,    2],
        [ 102,    0,  103,   88,  291,   74,  195,   69,  248, 2603,   19,    9,
          794,    0,   18, 1886,   65,   79,  511,  239,    8,  127,    0,   18,
         1886,   65,   79,  511,  239, 1660,    0,   46, 3400,  137,   72,  738,
           63,    6, 2497,  150, 2603,    0,   91, 1801, 5255, 6177, 2255,  161,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  58, 1290,   46,   59,    9,  354,  957, 6819,  447,  684,    0,   46,
          470,   30,   14,   28,  529, 6789,   12, 1472,   49,   64,   12,   79,
           11,   64,   73,   12,   79,    0,   57,  361,   55,  152,   23,   65,
          158,  122, 1913,  957,   81,  233,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  38,   23, 5203,    0,   95,   74,   54, 1446, 1564,   97, 4233,   34,
         3136,   18,   54, 5206,  346,   67, 1859,  328,  136,  974,    6,    0,
           11,   95,   74,   54, 2516,   18,   54,  538,   12,  645,   19,  104,
         2336,    0,  165, 1711,    0, 1711,  104, 2516,  204,   59, 5097,   12,
         2043,  345, 1859,  328,  136,  974,    6,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  22,   27,  701,    8, 2443,   63,  674,  349,  596, 4661,  106,  596,
            0, 1547, 2866,  183,  346,    0,  117, 1054,    0,  103,   24, 2908,
           22,   49,    8,    0, 1185,  419, 1457,  271,   93,    0,  300,  928,
          189,   22,    0,    0,  236, 1279,    6,    0,  189,   22,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [  19,    9, 8530,    0,   33,   27, 2423,  531,    0,  106,   73,  283,
           79, 8530, 5560, 6946,    6,   59,   12, 1474,  555,  206,    0,   58,
           46,  245,   59,   12, 2387,   47,  889,   53,   19,    9, 1410,   18,
          508, 2238,  124, 6362, 2159, 4314,   93,  977,  596,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1],
        [ 102,    0,  872,    8, 1789,  191,  171,   15,  230, 3522,   54, 6910,
         6362,   11,  171,   54,  700,    0,   85,   27,   90,   90,  270,  133,
           62,    8, 1637, 4034,   89, 1997,  135,   11,    9,  206,   85,   27,
            8,  713,   60,   63,  390,  886, 3037,   62,    8, 6708,   15, 3761,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1]], device='cuda:3') tensor([39, 88, 51, 44, 57, 48, 47, 50], device='cuda:3') tensor([1.0000, 0.9985, 1.0000, 1.0000, 1.0000, 0.8291, 1.0000, 1.0000],
       device='cuda:3', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[  47,  665,    0,   95, 2106,   15,  141,  199,    6,   12,  194, 1300,
          826,   90, 3319,    0, 1044,   15,  141,  195,  286,   90,   70, 1473,
         1070, 3201,  433,    8,  524,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [ 102,   18, 2345,  326,   15,  903,   30,  168,  408,    0,   58,  137,
           72,  121,   19,  365,  276,    0,   95,   23,   14,   44, 1027,    8,
         2034,   11,   23,  199,   12,  453,    9, 5860,    0,   74,   54, 1033,
           15,  401,   22,  877,  235, 6694,    0,   24,  174,   14,   28,  101,
         8741,  227,  189,   22,    0,    2],
        [   9,  206,  373,    0,   20,   48,  917,   12,    9, 1127,   80,   15,
            8, 5332, 2726, 4378, 4405,    0,   11,   70,   27, 1054,    9, 1078,
            0,   23,  269, 3216,    9, 5860,   15, 1313,  237,   30,  237,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  46,  218,  115,    8, 1091,    0, 4608,  209,  273,  384,  235,  966,
         2515,   60,    9,  354, 2317, 3950,   11,   81,  435,   96,    9,  354,
         4075,   15, 7357,    0, 2526,   30,    6,    0,  375,  868,    6,    0,
         2710, 3122,    6,    0, 1328,  183,  660,  468,    0,   18,  218,  115,
           24,   11,   50,    0,    2,    1],
        [ 102,    0,  342,    0,  145,   19,   28,  397,    0,    0,  310,   50,
         1921,    8,  318,   15,    0,  169,   11,  792,    0,    0,  233,    9,
            0,  493,    0,  374,  200,    0,  733,   12,    0,  658,    9, 1772,
           15, 8040,    6,    0,   11,  553,  767,   72,  104, 1606,   15, 1684,
          136, 3604,  454,    0,    2,    1],
        [  11,  103,   23,  977,   97, 1255,    6,   12,    9, 4828,    0,    9,
         4828, 7355,  141,   12,   79,  236,    0,   11,   18,   27,    9, 1594,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  87,  241,    0,   41, 1810,    0,   58,   20,   65,   14,   28,   79,
           22, 2322,    3,   87, 1765,   50,    8, 2885,   15,  506,  797,  799,
           18,   20,  550,   49, 1932,  122,  799,   49,  320, 1091, 2246,   20,
          550,   11, 2381,   61,  118,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  87,   14,    6,  101,    8,  705,  225,   28,   15,    8,  387,   49,
          955,  936,   63, 1855,   63, 1432, 1140,  466,  132, 2521,   61,    9,
         1172, 4790,   11,  126, 1044,   15,  271, 1465,    6, 3563,   28,   38,
         1024,  105,   46,  126,   12,   69, 9391,  346,   75,    9, 6615,    0,
            2,    1,    1,    1,    1,    1],
        [  11,   23,   59,   81,   23,  286,    0,  832,  957,   96,    9, 1901,
          881,   12,    9,  935,  320, 3319,   12, 4661,    9, 1695,  161,   14,
            6,  957, 1772,   47,   90, 1659,  472,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  20, 1390, 2182,   19,    9,  196,   46,  142, 8925,   12, 2488, 1489,
            0,   33,  107, 1885, 5663,   15, 1788,   83,  576,  225,  476,    0,
            9, 3456,   15,    9, 9483,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  75,    9, 1935,   15,    9, 7301, 4167,  299,    0,  103, 2773, 7301,
            0,  102, 2773, 7301,    0,   48, 2447,   47, 2773,    0,   87,  435,
           22,  117, 2195,   18,   87,   48,   73, 2182,   19,  888,  938,   60,
         5007,  105,   19,    9,  161,    0,    2,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  11, 6464,   19,  341,    6,  476,   15, 9136,    6,   11, 7323,    6,
          195, 6702,  105, 1559, 6054,  212,    6,  592,   28,    6,   15,    9,
          161,    0, 7808, 3637, 1282, 4036,    6,   11, 1485, 6950,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [1245,  158, 3271, 7626, 1985,  327,  935,   14,    6, 2118,   81,    9,
          169,    0,   11,   85,   15,    9,  502,  275,   18, 1129,   27, 1245,
           96,  738, 2981,    6, 1547,  209,  222,   51,   61,   11, 3917,   51,
           11, 8125,   61,   19,    9,  410,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  74,   65,   69,   32, 7302, 5337,   12,   13,   89,   60,  345, 7241,
           15, 5542,   93,  447,  235, 3016,   60,    9,  896,   15,    8,  861,
          181,   95,   22,  678,   14,   28, 3285,    9,  196,   24,   14,   44,
          131,   12,  864, 1021,   19,  276,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  11,    9,  643,   72,  899, 4526,  654, 3059, 2738,   27,   22, 2692,
           24,    9, 5653,   15,    9, 4348, 1138,  136,  196,   12, 7195,  138,
         4252, 2296,   67,  138, 2867,   89, 2728,   12, 4277,  248, 4470,    6,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1],
        [  58,   24,  119,   18,  138,  833,    6,   54,   73, 2577,    0,   11,
         3966,   74,   14,    6,  171,  380,  296, 2277,   15,  191,   22,   14,
            6,  131,   12, 1423, 3334,  112,   18, 3764,   15,  721,    0, 2488,
           60, 2287,  640, 4805,    0,  894,   11,  636, 4805,    0,  785,    0,
            2,    1,    1,    1,    1,    1]], device='cuda:5') tensor([31, 54, 37, 53, 53, 26, 43, 49, 33, 31, 43, 36, 44, 44, 38, 49],
       device='cuda:5') tensor([1.0000, 1.0000, 1.0000, 1.0000, 0.8018, 1.0000, 1.0000, 1.0000, 1.0000,
        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000],
       device='cuda:5', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[  11,   95,   20,   14,   68, 3318,  152,   19, 1303,   15,   24,  420,
            0,   22,   14,    6,  106,   20,   86,  962,   84,  276,   12,    9,
          277,  339,  237, 5950,  758,   11, 7622,   15,   85,  869,   34,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  97,  848, 5167, 1119,   92,  327,   97, 4941,    0, 2592, 1775,  252,
         1093,    6,   11,  122,    0,   11,   20,  174,   14,   28,  137,   33,
          195,  310,  115,  414,  169, 2335,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19,  436,    0,    0,  283, 1137,  429,   15,    0,   70,  383, 1851,
            6, 1894,   61,   12,    9,    0, 2923,    0,  537,   67,    9,    0,
           70,  383,  360, 1092, 5161,    8,    0, 1091,  373,   19, 2857,    0,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [  22,   14,    6,  326,   15,   91,   19, 1906, 2205,  824,    0,   95,
         2899, 1050,   18,    0,    9,  675,  307, 4733, 2650,  834,    6,    9,
          502, 1906, 2205, 2357,  108, 1127,   80,    0,   41, 2331,   14,    6,
          138,  777,    0,    2,    1,    1,    1,    1,    1,    1],
        [  97, 1255,    6,    0, 4919,   96,    0,    9, 5475,    0, 2708,   15,
            0, 8990,    0,   11,   23, 6618,  122,  239,    9, 5475, 2708,   15,
         8446,   63, 1311,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  58,   95,   20,  194,   67,    0,    9, 3195,    0,   11,   20, 2169,
            9, 3195,   11,   20,  864,   67,    9, 3195,    0,   20,    0,  194,
           12,    0,    9,    0,    0,  569,  561,    0,   11,   22, 1129, 5881,
         1646,   11, 5847, 1646,   11, 2174, 6694,    0,    2,    1],
        [  11,    0,   70,   20,   48,  733,   12,    0,    0,  583,    0,   24,
            0,   70,  948,   70,   20,   14,   68, 5882,    0, 2743,  589, 4601,
            6,  106,   22, 4514,    6,    0,    9,    0, 1262,   96,   22,    6,
         3943,    0,    2,    1,    1,    1,    1,    1,    1,    1],
        [1290, 5638, 2482,    0,  790, 1569,    0,   62,   97, 2610,    6, 8865,
            0,   58,  245,   19, 2520,  744,    0,   98,  150,    6,    0, 2430,
            6,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  19, 1953, 5661,    0,   47, 3974,    0,   20,  204, 1863, 1106,   33,
         7938,   98,  150,   11,  139,   33,  387, 1351,  633,   60,  104,  766,
           63,  125, 2561,  705,  313,    6,   15, 7938, 3681,  271, 7016,  264,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1],
        [   8,    0, 3059,   91,  191,    0,   79,   20,  194,   12,  671,    0,
            0,    0,   64,  470,   20, 2663,   19,    0,    0,    0,    0,  132,
           79,   20, 3521,   47,    0,   79,   20,  301,   33, 1191,    0,   93,
           18,   85,    0,    2,    1,    1,    1,    1,    1,    1],
        [ 777,   85,   27,   18,   22, 1846,    8,  540,  169,   12,  977,    9,
          549, 1457,  264,  115,   49,   24,  119,    0,   72, 2443,   12, 8045,
         1526,   95,   74,   14,    6,   32, 2697, 1136,   15,   22,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [ 240,  429,    0, 1332,   14,   28,    0,  458,  429,    0,   58,   19,
            9,  524, 1199,    0,    0,  148,   27, 2824, 1996,    0,   18,   14,
            6,    0, 1505,    0,   18,   14,    6,    8, 2225, 2227,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  38,   24,  470,   69,  538,   12,  158,   96,    0,  291,    0, 3024,
         5487,    0,   12,   13,  565,   19, 1137, 1665,   49,  762,    0, 1137,
         1665,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  18,   14,    6,  342,  765,   68,  350, 4093,    6,  445,  233, 1797,
          429,   15, 2699,    6,   19, 4113, 1019, 5434,    6,    0,   58,   70,
          562,   70, 1095,  429,   19, 3416, 7625, 5434,    6,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1],
        [  32,    0,   20,   14,   68,   73, 3137,   24,  414, 2330,    0,  106,
         1217, 3138,    6,  174,   14,   28,    0, 1487, 1768,  577, 1319,  549,
          360,    6,    0,   15,   19,  377,    0,   11,  672,   27,   73,    0,
         3794,   61,    0, 3137,   24,    9, 5651,    6,    0,    2],
        [  11,   22,   14,    6, 1537, 3181,    0, 3152, 4464,    6,   11, 6539,
            6,    0, 1957,  468,  571,    6,   11,  691,   34,    6,    0,    9,
         2637,   11,    9, 1950,    0, 3257,   11, 3024,    8, 3791,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1]],
       device='cuda:7') tensor([37, 32, 38, 40, 29, 45, 39, 27, 38, 40, 36, 36, 27, 35, 46, 36],
       device='cuda:7') tensor([1.0000, 1.0000, 0.7729, 1.0000, 0.8232, 0.8745, 0.8237, 1.0000, 1.0000,
        0.7310, 1.0000, 0.8584, 0.9717, 1.0000, 0.9209, 1.0000],
       device='cuda:7', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[1084,   81, 3408,   15,   33,  326, 1279, 2804, 1182,    8, 4032,  106,
          578,  287,   88, 3238,  107,   15,    9, 1374,    0,   93, 6729,   47,
            9, 1124, 1221,  744,  115,    0,   93,    9, 5381,  158, 8315,   61,
            0,   93,   46, 1737,    0,   11, 2629,  864,    6,    9, 3037, 2924,
          345,    9, 1725,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  11,   19, 1886,    0,   23,  610,   22,    9,   41,  513, 1240, 7528,
            3,   24,   65,  139,   33,  250, 3662,  103,   24,  301,  727, 1115,
           14,    6, 2234,   11,   24, 5936,  118,  189,    8, 1046,   30, 2961,
            0, 2052, 8159,    0,   47,    8, 1299,   15, 1242,    0,   57,  177,
           55,   88,  197, 5458,   47,   33,    0,   11,   46, 3400,  308,  107,
          326,   15,  796, 1252,   72,  134, 7124,  169,   19,    9, 3138,    0,
            2,    1,    1,    1,    1,    1,    1],
        [2096,  398,  664,   27,   73,  131,   12,   79,  988,   12,   24,  233,
           74,    0,   11,    9,  596,   27,   38, 5827,   19,  230,   13,  592,
            6,   18,    9, 9027,  173,   32, 5470,   12, 6291,    0,  151,    0,
           11,   38,   46,   54, 2813,  134,  425, 1901,    6,   83,   44,  179,
            0,   11,   33,   27,    9,  721,  896,   24,  139,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 103,   46,  142, 4012,   61,  424,    9, 1374,    0,   81,    9, 8266,
          241,   18,   46, 6137,  105, 5286,   18,    9, 1554,   11, 2636, 1343,
          426, 1374, 4162,   51,    8, 2539,   89, 2950, 3636,  148,  145,   59,
           16, 1598, 4429,    6,   47,  901,   70, 8526,   12,    9,  801, 1204,
         2124,  657, 1433, 3487,   68, 1515,   28, 5895,   12,    9, 8266,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [ 355,  731,   30, 1338,  282,   72, 1959,    6,    0,   57,  177,   55,
           87,   48,   70,  868, 1343,   61,   12,  119,   18, 2289,  158, 7314,
         2963,  297,    6,   11,   23,  184,  831,   61,  320, 2967,    0,   57,
          177,   55,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [7456,    0,   95,   24, 1026, 1833,   49, 1027,  704,    9, 1124,   11,
         1027, 2091,   12,  158, 8285,   12,  553,    9, 3813,    6,    0,   95,
           24, 1026, 1833,  107,   19,  169,    0,  121,  287,  200,  490, 1429,
         1547,  189,    8, 1367,   11,   33, 7308,   27, 2904,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  20,  199,   12,  583,   24,    9, 1580,   15,   33, 2343,    0,  106,
           33, 2343,   27, 5714,  804,   12,   64,   23,   79,   12,    9, 1455,
           41,  987,  376,    6,    6,  333,  242,   19,   97, 1477,   49,   23,
         2660,   22,  115,   11,   23,  419,  754, 1343,   22,    6, 7988,    6,
           11,   22,    6, 2023,   60,   19,  276,    0,  148,   27, 4393,   89,
            0,    2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1],
        [  58,  152,   14,    6,    9,  275,    0,   64, 2019,   79,   24, 7196,
           46,  137,   46,   14,   44,   19,    0,  227,    0,  231,    0,    0,
           46, 2472,  137,   18,   46,   14,   44,    9,  315, 1821,   19,    9,
         4710, 1209,   99,    8,  390,  281,  105,  125,  181, 2019,    0,  227,
            0,   11,   24,   14,   51,  511,  691,   18,   46,  137,   18,   46,
           14,   44, 3615,   51,  963,   28,   28,   11,   18,   23,    0,   23,
           54,    9, 4710, 1209,   53,    0,    2]], device='cuda:6') tensor([53, 73, 59, 61, 39, 47, 62, 79], device='cuda:6') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.9907],
       device='cuda:6', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[  11,   64,  151,  943,   61,   12,   50,   49,   22, 1230,   14,   28,
          283,  573,   19,    9, 7912,  161,    0,  255, 1397,   20,  316, 1474,
           96,    8,  318,   15,  118,    0,   11,   22,  316, 1559,    8,  318,
           15,  118,    0,   22,   48,  245,   88,   15,   81,  334, 6849,    6,
            0,    2,    1,    1,    1],
        [  58,    9,  205,   23,   59,   19, 1630, 1053,    0,    0,   50,    0,
         2804,   96,  250,   15,    9,    0,   88,   19,   84, 1088,    0,    0,
            0,    0,   11,   20,  563,   19,  640,  104,  240,  161,    6,   67,
          832,   15,   84, 1255,   12, 2714,   47, 3847,    0,   19,    9,  206,
            0,    2,    1,    1,    1],
        [ 190, 2114,  127,  105,   47,  141,    0, 1084, 1383,  132,   14,    6,
         2574,   61,    8, 5477, 2838,    6,   12, 3106,    0,   64,   79,   24,
          158,   95,   24, 2073,   90,   20,  135,   32, 2506, 5477,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  97, 3420,    0,    0,  195,   69,  443,   89,  141,    0,   11,   19,
           18,  953,    0,   23,   14,  150,    0,    0, 3427,    0,  551,    0,
            0,   11,   18,   14,    6, 7402,    0,   73,  101, 7402,   49,   18,
           14,    6,    0,    0,  844, 2832,  179,   60,    0,    0,   22,   14,
            6, 8923,    0,    0,    2],
        [ 315, 2306,  852,  398,    0,   73,   12,   33, 4940,    0,   57,  177,
           55,   58,   20,  145, 2651,   18, 2436,  857, 5258,  299,   27,    8,
         3573, 3340,   15,  169,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 171, 5381,   59,   69, 3046,   12,  834,  104, 1443,   72, 1933, 1727,
            6,   75, 3750, 2644,    6,    0,   58,   20,  691,    9,  169,  173,
          308,   12,  129, 1645,  402,   33, 9223,   70,    8,  729,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  70, 6461,    6,   49,  101,  137,  264,   12,  138, 9573,  200,   49,
           23,   14,   44,  131,  347,    8, 7282,   11, 1054,  417, 1035, 1125,
          299,  974,  453,   19,   97, 5905, 2970, 1093,    0,    2,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [ 310,   14,    6,  301,   90,  665,    0,   64,    0,   65,   23,   79,
           67,  101,   85,  684,   14,    6,  609,    0,   64,   65,   23,   79,
           67,   18, 1360,   14,    6,    0,  609,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:4') tensor([50, 50, 36, 53, 30, 36, 34, 33], device='cuda:4') tensor([1.0000, 0.7974, 1.0000, 0.7729, 1.0000, 1.0000, 1.0000, 0.9229],
       device='cuda:4', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([[  33,   27,   38,  531,    0,   32,   85,  204, 4631, 1437,   18,    0,
           58,   23,  286,   12,  245,  137,   72,  121,   12,  800,  499,  898,
         3991,   18, 7526, 2340,  476,   11, 5360,  488, 1319,  758,    0,    2,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  57,  361,   55,   58, 7970,    0,    9,  232,    0, 4423,   14,    6,
          221,  327,   47, 1797,  200,    0,   57,  177,   55,   22,  653,   69,
         4423,   14,    6,   73,   38,  531,    0,   58,   22,   14,    6,  326,
           15, 5869,   77,  571,    0, 1332,   14,   28,   22,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [   8, 4660,   60,   91,   33,   85, 2574,    6,  973,    8,  805,  565,
          216, 5177,  191,   81,    9,  248, 1064,   27, 6293,   61,   11,   81,
            9, 8701,   27,  629,   70, 1840,    6,   12, 6293,   81,  134,  248,
          648, 1973,   80, 2414,    0,    2,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  20,  269,  741,   47,   84,  648, 3398,   96,  230,  240,    0, 2640,
            6,    0,   57,  177,   55,   11,    0,   64,   20,  126,  605,   48,
           90, 4899,   18, 2885,   61,    8, 7964,   15,  233, 1481,    3,  334,
            0, 4351,    6,   18,   54,  605,  103,   24,   59, 1476,   83,   44,
           77,  571, 1103,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  19,   85,    0,   20,    0,  629, 6347,  972,  987, 2486,  454,   12,
          800,  569,   63, 6352,  462,  672, 4169,    0,   11,   19,    9,  206,
            0,   20,   48,    8,    0,  609, 4600,  692,   47, 2850,  277, 1945,
         7735,   34,    6,   15, 7866, 1871, 2573,    0,    2,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  20, 2381,   61,    9,    0, 2644,  287,  787,    0,   11,    0, 1273,
            0,    0,  326,   15,   49,   24,   65,  139,   33,   96,  240, 5970,
          727,    0,   22,   14,    6,    0,   90, 3722,    0, 1600,   15,    9,
         2205,    0,   11,    9,  729, 5095,    0,   27,   12, 4644,    9,  686,
            0,   12, 4644,    0, 3511,   70,  432,   15,    9,    0,  686, 4500,
           15,    9,  853,    0,    2],
        [  22,   14,    6,    8, 5523,   15, 4259,   68, 7626,    6,   11,    8,
         1053,   15, 5574,  194, 7735,    6,   18,   59,  221, 1788,  377,   61,
          233, 3769,   47,  240,  783, 3351,  674,  379,    0,  106, 8315,  333,
           27,    9, 8850,   50, 1573, 1109,   34,   14,    6, 3880, 8444,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1],
        [  84, 5360,    0,   84, 5360,   47,  356, 2124,   63,   30, 4379,  284,
          329,   19,   84, 2916,   11, 5289,   27,  134, 5428,   61, 3242,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1]], device='cuda:1') tensor([36, 47, 42, 53, 45, 65, 49, 25], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 0.9443, 0.9355, 0.8647, 1.0000, 1.0000],
       device='cuda:1', dtype=torch.float16)
True False
tensor([[  46,  286,   84, 1532,    0,  106,  250,   15,   84, 1356,   54, 5711,
            6,    0,   93, 4057,    6,    0,   93, 3633,    6,    0,   11,   24,
           14,   44,   81,  392,    0,   11,    8,  540, 6771,   15,  206, 7565,
          125, 1070, 7041,    6,    0,   11,   20,  199,   12,  158,   12,   24,
           70, 2335,   70, 1184,    0,   20,  199,   24,   12,  119,   18,   20,
           14,   68,   74,   47,   24,    0,    2,    1,    1,    1,    1,    1,
            1,    1],
        [   9,  569, 1249,   62,   84, 2675,   48,  594,  150,  210,    0, 8552,
            0,  191,   23, 1333,   18, 1314,  806,    6,  142,   73,  283, 4005,
           58, 4791,  538,   12, 3713,  115,  134, 6747, 2650,   92,    6,   11,
         6328,   19, 2707,   60, 1804,  863, 3830,   12, 3218,    9,  863,   15,
          134, 4164,  454,   11,  134, 1088,    0,    2,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  19, 9381,    0,   20,   48, 8118,   61,  112,    9, 1677,  135,  839,
          622, 1138, 4185,   12,  556,   99, 2532,   99,    8, 1320,  923,   15,
          260,  433,   41, 2591,    6,   15, 2728,    3,   22,   48,    8, 2580,
           99,   49,    8,  117,  231, 1320, 2580,   99,    0,   75,   18,    0,
            2,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  11,   38,    9,  569,  169,   24, 2382, 1115,    0,    8, 2362,   34,
            0,   85, 8557,    0,  658,   18,   24,  645,   19,    9, 2812,  127,
           92, 2174,    0,   11,   38,  589,   87,    0,  658,   22,    0,   11,
          103,   24,  658,   22,   24,  195, 3792,  236, 8626,    0,    2,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [  87,  126,  241,    0,   41, 3154,  328, 2204,    6,  832,   12, 2391,
          439,  398,   89,  320,  387,   14,    6,  286,    0,   58,   73,  320,
          387,   14,    6,  620,   44,   61,    3,   38,    9, 2210,   87,   48,
         2408,  141,   48,   24, 1381,  158,  122,   96,  866,   11,  866,   11,
          866,   38,   18,   24,   65, 1323,   22,   47,  122,   11,  122,   88,
            0,   73,  283,    9, 3768, 1955,    0,   58,    9,  794, 1955,    6,
            0,    2],
        [  11, 3191, 1183, 4473,   53, 1370,  222,  243,  189,    9, 1414,   15,
            9, 1977,  309,  135,   67,  271, 8507, 3528,  271,  897,    0,  643,
           60,   75,    9, 2023,    0,   11,   87,  241,    0,   41,  377,  408,
          397,    3,   11,  104, 1505, 2615, 1258,    6,   67,  134,  264,  376,
          313,    6,   11,  134,  923, 1465,  422,    0, 2590, 2319, 1985,   12,
            9, 2023,    0,    2,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 103,  219,   48, 2465,   60,  230, 3438,    0,   20,  174,   14,   28,
          137,  250,   88,  658,   18,    0,   75,    9,  354,  169,    0,  219,
           48,  751,    9, 2192,   15,  856,   88,  533,   12, 2465,  230, 3438,
           12,  460,   72,  121,   12, 7741,    9, 4548,  402, 1253,   30, 1262,
           19,  300,  712,    0,    2,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1],
        [ 102,    0,   20, 3461, 1059,   51,   11, 5286,   18,    0,   84, 5469,
          145,  453,  121,   23, 2666,    0, 2286,  723,    0,   96, 2195,   63,
         2532,  430,    0,   11,  300,  627,  254, 1909,    0,   12,    0,  122,
         5124, 2361,   11, 4603, 4064,    6,    0,    0, 4064,    6,   18,  142,
            0,  866, 3967,   11,  122, 6272,    0,   64,   48,   20,  767,    0,
           20,   14,  150,  308,  264,   12,   18,    0,    2,    1,    1,    1,
            1,    1]], device='cuda:1') tensor([67, 56, 49, 47, 74, 64, 53, 69], device='cuda:1') tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 0.8774],
       device='cuda:1', dtype=torch.float16)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([5.9605e-08], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 12:00:09 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 9.457 | trans_loss 12.087 | nll_loss 11.533 | w2v_ctc_loss 4.249 | task_loss 31.676 | contrastive_loss 0 | total 3505.91 | n_correct 178.091 | ppl 2963.46 | accuracy 5.08 | uer 56.095 | wer 55.731 | raw_wer 55.731 | bleu 0 | wps 817.1 | wpb 3505.9 | bsz 119.3 | num_updates 1823
2023-09-01 12:00:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1823 updates
2023-09-01 12:00:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 12:00:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 12:00:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 1 @ 1823 updates, score 0.0) (writing took 4.427725134009961 seconds)
2023-09-01 12:00:14 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-09-01 12:00:14 | INFO | train | epoch 001 | loss 6.736 | trans_loss 5.595 | nll_loss 4.476 | w2v_ctc_loss 6.098 | task_loss 5.273 | contrastive_loss 0 | total 3956.53 | n_correct 103.639 | ppl 22.26 | accuracy 2.619 | wps 19224 | ups 1.62 | wpb 11900.6 | bsz 427.4 | num_updates 1823 | lr 7.29835e-05 | gnorm 1.476 | clip 1.8 | loss_scale 16 | train_wall 1065 | gb_free 18.9 | wall 1194
2023-09-01 12:00:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 12:00:14 | INFO | fairseq.trainer | begin training epoch 2
2023-09-01 12:00:14 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 12:01:06 | INFO | train_inner | epoch 002:     77 / 1826 loss=4.854, trans_loss=5.568, nll_loss=4.447, w2v_ctc_loss=3.221, task_loss=4.903, contrastive_loss=0, total=3877.76, n_correct=132.82, ppl=21.81, accuracy=3.425, wps=9402.9, ups=0.81, wpb=11670.1, bsz=418.5, num_updates=1900, lr=7.6062e-05, gnorm=0.975, clip=0, loss_scale=16, train_wall=58, gb_free=19.4, wall=1247
2023-09-01 12:02:05 | INFO | train_inner | epoch 002:    177 / 1826 loss=4.807, trans_loss=5.567, nll_loss=4.448, w2v_ctc_loss=3.147, task_loss=4.97, contrastive_loss=0, total=3945.55, n_correct=135.77, ppl=21.83, accuracy=3.441, wps=20400.5, ups=1.72, wpb=11878.2, bsz=423.1, num_updates=2000, lr=8.006e-05, gnorm=1.027, clip=0, loss_scale=16, train_wall=58, gb_free=19.4, wall=1305
2023-09-01 12:02:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([5.9605e-08], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 12:02:58 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 9.41 | trans_loss 12.097 | nll_loss 11.547 | w2v_ctc_loss 4.07 | task_loss 31.676 | contrastive_loss 0 | total 3505.91 | n_correct 177.909 | ppl 2993.06 | accuracy 5.075 | uer 54.671 | wer 54.323 | raw_wer 54.323 | bleu 0 | wps 823.9 | wpb 3505.9 | bsz 119.3 | num_updates 2000 | best_bleu 0
2023-09-01 12:02:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-09-01 12:02:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_2_2000.pt
2023-09-01 12:03:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_2_2000.pt
2023-09-01 12:03:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 11.610584326001117 seconds)
2023-09-01 12:04:08 | INFO | train_inner | epoch 002:    277 / 1826 loss=4.758, trans_loss=5.572, nll_loss=4.452, w2v_ctc_loss=3.068, task_loss=4.998, contrastive_loss=0, total=3980.57, n_correct=141.33, ppl=21.89, accuracy=3.55, wps=9738.6, ups=0.81, wpb=11976.8, bsz=426.2, num_updates=2100, lr=8.4058e-05, gnorm=1.018, clip=0, loss_scale=16, train_wall=57, gb_free=18.6, wall=1428
2023-09-01 12:05:06 | INFO | train_inner | epoch 002:    377 / 1826 loss=4.733, trans_loss=5.577, nll_loss=4.458, w2v_ctc_loss=3.027, task_loss=4.835, contrastive_loss=0, total=3951.89, n_correct=135.19, ppl=21.98, accuracy=3.421, wps=20364.2, ups=1.71, wpb=11884.4, bsz=426.5, num_updates=2200, lr=8.8056e-05, gnorm=0.908, clip=0, loss_scale=16, train_wall=58, gb_free=19.3, wall=1487
2023-09-01 12:06:05 | INFO | train_inner | epoch 002:    477 / 1826 loss=4.668, trans_loss=5.576, nll_loss=4.459, w2v_ctc_loss=2.931, task_loss=4.458, contrastive_loss=0, total=3989.48, n_correct=136.89, ppl=21.99, accuracy=3.431, wps=20263.5, ups=1.69, wpb=11999.9, bsz=452.2, num_updates=2300, lr=9.2054e-05, gnorm=0.971, clip=0, loss_scale=16, train_wall=59, gb_free=19.4, wall=1546
2023-09-01 12:07:04 | INFO | train_inner | epoch 002:    577 / 1826 loss=4.647, trans_loss=5.576, nll_loss=4.459, w2v_ctc_loss=2.898, task_loss=4.565, contrastive_loss=0, total=4010.91, n_correct=139.88, ppl=22, accuracy=3.487, wps=20627, ups=1.71, wpb=12062.1, bsz=444.2, num_updates=2400, lr=9.6052e-05, gnorm=0.841, clip=0, loss_scale=32, train_wall=58, gb_free=19.2, wall=1604
2023-09-01 12:08:01 | INFO | train_inner | epoch 002:    677 / 1826 loss=4.631, trans_loss=5.553, nll_loss=4.435, w2v_ctc_loss=2.878, task_loss=5.169, contrastive_loss=0, total=3863.36, n_correct=138.7, ppl=21.63, accuracy=3.59, wps=20190.1, ups=1.73, wpb=11638.9, bsz=397.9, num_updates=2500, lr=0.00010005, gnorm=0.86, clip=0, loss_scale=32, train_wall=57, gb_free=19.4, wall=1662
2023-09-01 12:08:59 | INFO | train_inner | epoch 002:    777 / 1826 loss=4.597, trans_loss=5.571, nll_loss=4.455, w2v_ctc_loss=2.823, task_loss=4.74, contrastive_loss=0, total=3968.04, n_correct=136.58, ppl=21.93, accuracy=3.442, wps=20641.7, ups=1.73, wpb=11939.6, bsz=431.9, num_updates=2600, lr=0.000104048, gnorm=0.86, clip=0, loss_scale=32, train_wall=57, gb_free=19.3, wall=1720
2023-09-01 12:09:58 | INFO | train_inner | epoch 002:    877 / 1826 loss=4.582, trans_loss=5.572, nll_loss=4.456, w2v_ctc_loss=2.797, task_loss=4.654, contrastive_loss=0, total=3980.03, n_correct=133.13, ppl=21.96, accuracy=3.345, wps=20418.6, ups=1.7, wpb=11977.6, bsz=438.8, num_updates=2700, lr=0.000108046, gnorm=0.923, clip=0, loss_scale=32, train_wall=58, gb_free=18.7, wall=1778
2023-09-01 12:10:57 | INFO | train_inner | epoch 002:    977 / 1826 loss=4.567, trans_loss=5.575, nll_loss=4.46, w2v_ctc_loss=2.769, task_loss=4.913, contrastive_loss=0, total=3945.98, n_correct=136.68, ppl=22.01, accuracy=3.464, wps=20008.7, ups=1.69, wpb=11874, bsz=425.1, num_updates=2800, lr=0.000112044, gnorm=0.841, clip=0, loss_scale=32, train_wall=59, gb_free=19.3, wall=1838
2023-09-01 12:11:55 | INFO | train_inner | epoch 002:   1077 / 1826 loss=4.526, trans_loss=5.59, nll_loss=4.475, w2v_ctc_loss=2.7, task_loss=4.621, contrastive_loss=0, total=3979.79, n_correct=131.68, ppl=22.24, accuracy=3.309, wps=20571.2, ups=1.72, wpb=11956.7, bsz=440.4, num_updates=2900, lr=0.000116042, gnorm=0.791, clip=0, loss_scale=32, train_wall=57, gb_free=19.1, wall=1896
2023-09-01 12:12:53 | INFO | train_inner | epoch 002:   1177 / 1826 loss=4.535, trans_loss=5.585, nll_loss=4.471, w2v_ctc_loss=2.706, task_loss=5.307, contrastive_loss=0, total=3887.16, n_correct=133.2, ppl=22.17, accuracy=3.427, wps=20166.8, ups=1.73, wpb=11682, bsz=391.1, num_updates=3000, lr=0.00012004, gnorm=0.817, clip=0, loss_scale=32, train_wall=57, gb_free=18.4, wall=1954
2023-09-01 12:12:54 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-01 12:12:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-01 12:14:21 | INFO | train_inner | epoch 002:   1279 / 1826 loss=3.846, trans_loss=4.907, nll_loss=3.591, w2v_ctc_loss=2.367, task_loss=3.514, contrastive_loss=0, total=3929.23, n_correct=390.59, ppl=12.05, accuracy=9.941, wps=13436.9, ups=1.14, wpb=11812.3, bsz=416.9, num_updates=3100, lr=0.000124038, gnorm=1.519, clip=0, loss_scale=8, train_wall=87, gb_free=15.8, wall=2042
2023-09-01 12:15:47 | INFO | train_inner | epoch 002:   1379 / 1826 loss=3.237, trans_loss=4.263, nll_loss=2.733, w2v_ctc_loss=2.116, task_loss=3.204, contrastive_loss=0, total=4014.04, n_correct=1017.9, ppl=6.65, accuracy=25.358, wps=14070.9, ups=1.17, wpb=12070.4, bsz=449.7, num_updates=3200, lr=0.000128036, gnorm=1.25, clip=0, loss_scale=8, train_wall=85, gb_free=16.6, wall=2128
2023-09-01 12:17:14 | INFO | train_inner | epoch 002:   1479 / 1826 loss=3.086, trans_loss=4.148, nll_loss=2.586, w2v_ctc_loss=2.005, task_loss=3.194, contrastive_loss=0, total=4011, n_correct=1169.54, ppl=6, accuracy=29.158, wps=13880.2, ups=1.15, wpb=12058.4, bsz=450.7, num_updates=3300, lr=0.000132034, gnorm=1.223, clip=0, loss_scale=8, train_wall=86, gb_free=15.4, wall=2214
2023-09-01 12:18:41 | INFO | train_inner | epoch 002:   1579 / 1826 loss=3.033, trans_loss=4.114, nll_loss=2.545, w2v_ctc_loss=1.95, task_loss=3.455, contrastive_loss=0, total=4002.57, n_correct=1205.45, ppl=5.83, accuracy=30.117, wps=13896.1, ups=1.15, wpb=12046, bsz=432, num_updates=3400, lr=0.000136032, gnorm=1.059, clip=0, loss_scale=8, train_wall=86, gb_free=15.6, wall=2301
2023-09-01 12:20:06 | INFO | train_inner | epoch 002:   1679 / 1826 loss=2.965, trans_loss=4.1, nll_loss=2.526, w2v_ctc_loss=1.858, task_loss=3.367, contrastive_loss=0, total=3942.23, n_correct=1206.67, ppl=5.76, accuracy=30.609, wps=13893.9, ups=1.17, wpb=11854.9, bsz=424.3, num_updates=3500, lr=0.00014003, gnorm=0.967, clip=0, loss_scale=8, train_wall=85, gb_free=17.5, wall=2386
2023-09-01 12:21:32 | INFO | train_inner | epoch 002:   1779 / 1826 loss=2.939, trans_loss=4.093, nll_loss=2.514, w2v_ctc_loss=1.828, task_loss=3.647, contrastive_loss=0, total=3960.82, n_correct=1226.54, ppl=5.71, accuracy=30.967, wps=13768.4, ups=1.16, wpb=11899.2, bsz=411.9, num_updates=3600, lr=0.000144028, gnorm=1, clip=0, loss_scale=8, train_wall=86, gb_free=16.6, wall=2473
2023-09-01 12:22:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([1.1921e-07], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 12:22:50 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 5.845 | trans_loss 7.677 | nll_loss 5.713 | w2v_ctc_loss 2.159 | task_loss 17.66 | contrastive_loss 0 | total 3505.91 | n_correct 1077.91 | ppl 52.44 | accuracy 30.745 | uer 31.464 | wer 32.072 | raw_wer 32.072 | bleu 0.15 | wps 1235.2 | wpb 3505.9 | bsz 119.3 | num_updates 3647 | best_bleu 0.15
2023-09-01 12:22:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3647 updates
2023-09-01 12:22:50 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 12:22:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 12:23:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 2 @ 3647 updates, score 0.15) (writing took 12.54301715400652 seconds)
2023-09-01 12:23:03 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-09-01 12:23:03 | INFO | train | epoch 002 | loss 4.125 | trans_loss 5.106 | nll_loss 3.843 | w2v_ctc_loss 2.586 | task_loss 4.326 | contrastive_loss 0 | total 3956.32 | n_correct 459.047 | ppl 14.35 | accuracy 11.603 | wps 15855.4 | ups 1.33 | wpb 11900 | bsz 427.2 | num_updates 3647 | lr 0.000145907 | gnorm 0.99 | clip 0 | loss_scale 8 | train_wall 1233 | gb_free 16.3 | wall 2563
2023-09-01 12:23:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 12:23:03 | INFO | fairseq.trainer | begin training epoch 3
2023-09-01 12:23:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 12:23:56 | INFO | train_inner | epoch 003:     53 / 1826 loss=2.901, trans_loss=4.074, nll_loss=2.492, w2v_ctc_loss=1.779, task_loss=3.562, contrastive_loss=0, total=3870.82, n_correct=1208.89, ppl=5.63, accuracy=31.231, wps=8101.5, ups=0.7, wpb=11638.8, bsz=402.1, num_updates=3700, lr=0.000148026, gnorm=1.016, clip=0, loss_scale=8, train_wall=85, gb_free=14.5, wall=2617
2023-09-01 12:25:21 | INFO | train_inner | epoch 003:    153 / 1826 loss=2.844, trans_loss=4.059, nll_loss=2.476, w2v_ctc_loss=1.705, task_loss=3.219, contrastive_loss=0, total=3972.92, n_correct=1248.73, ppl=5.56, accuracy=31.431, wps=14040.9, ups=1.17, wpb=11959.8, bsz=442.8, num_updates=3800, lr=0.000152024, gnorm=0.907, clip=0, loss_scale=8, train_wall=85, gb_free=14.8, wall=2702
2023-09-01 12:26:48 | INFO | train_inner | epoch 003:    253 / 1826 loss=2.843, trans_loss=4.06, nll_loss=2.475, w2v_ctc_loss=1.706, task_loss=3.31, contrastive_loss=0, total=3987.3, n_correct=1262.41, ppl=5.56, accuracy=31.661, wps=13867.8, ups=1.16, wpb=11991.7, bsz=436.4, num_updates=3900, lr=0.000156022, gnorm=0.942, clip=0, loss_scale=8, train_wall=86, gb_free=14.5, wall=2788
2023-09-01 12:28:14 | INFO | train_inner | epoch 003:    353 / 1826 loss=2.797, trans_loss=4.059, nll_loss=2.473, w2v_ctc_loss=1.643, task_loss=3.045, contrastive_loss=0, total=4052.04, n_correct=1287.91, ppl=5.55, accuracy=31.784, wps=14165.4, ups=1.16, wpb=12182.4, bsz=465.3, num_updates=4000, lr=0.00016002, gnorm=0.978, clip=0, loss_scale=8, train_wall=85, gb_free=15.3, wall=2874
2023-09-01 12:28:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([5.9605e-08], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 12:28:54 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.787 | trans_loss 7.625 | nll_loss 5.647 | w2v_ctc_loss 2.083 | task_loss 17.487 | contrastive_loss 0 | total 3505.91 | n_correct 1103.45 | ppl 50.11 | accuracy 31.474 | uer 30.217 | wer 30.973 | raw_wer 30.973 | bleu 0.25 | wps 1108.2 | wpb 3505.9 | bsz 119.3 | num_updates 4000 | best_bleu 0.25
2023-09-01 12:28:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-09-01 12:28:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_3_4000.pt
2023-09-01 12:28:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_3_4000.pt
2023-09-01 12:29:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 0.25) (writing took 11.736299409996718 seconds)
2023-09-01 12:30:32 | INFO | train_inner | epoch 003:    453 / 1826 loss=2.803, trans_loss=4.059, nll_loss=2.474, w2v_ctc_loss=1.649, task_loss=3.458, contrastive_loss=0, total=3933.57, n_correct=1244.69, ppl=5.55, accuracy=31.643, wps=8574.5, ups=0.72, wpb=11831.1, bsz=421.1, num_updates=4100, lr=0.000164018, gnorm=0.952, clip=0, loss_scale=8, train_wall=85, gb_free=14.8, wall=3012
2023-09-01 12:31:59 | INFO | train_inner | epoch 003:    553 / 1826 loss=2.79, trans_loss=4.051, nll_loss=2.465, w2v_ctc_loss=1.64, task_loss=3.4, contrastive_loss=0, total=3955.37, n_correct=1265.02, ppl=5.52, accuracy=31.982, wps=13590.2, ups=1.14, wpb=11905.2, bsz=436.9, num_updates=4200, lr=0.000168016, gnorm=0.899, clip=0, loss_scale=8, train_wall=87, gb_free=16.3, wall=3100
2023-09-01 12:33:24 | INFO | train_inner | epoch 003:    653 / 1826 loss=2.761, trans_loss=4.044, nll_loss=2.456, w2v_ctc_loss=1.6, task_loss=3.152, contrastive_loss=0, total=4015.46, n_correct=1291.3, ppl=5.49, accuracy=32.158, wps=14185.7, ups=1.17, wpb=12085.4, bsz=446.4, num_updates=4300, lr=0.000172014, gnorm=0.899, clip=0, loss_scale=8, train_wall=85, gb_free=15.9, wall=3185
2023-09-01 12:34:51 | INFO | train_inner | epoch 003:    753 / 1826 loss=2.77, trans_loss=4.04, nll_loss=2.45, w2v_ctc_loss=1.614, task_loss=3.425, contrastive_loss=0, total=3965.92, n_correct=1278.22, ppl=5.47, accuracy=32.23, wps=13850.8, ups=1.16, wpb=11932.5, bsz=425.4, num_updates=4400, lr=0.000176012, gnorm=0.88, clip=0, loss_scale=8, train_wall=86, gb_free=11.5, wall=3271
2023-09-01 12:36:17 | INFO | train_inner | epoch 003:    853 / 1826 loss=2.751, trans_loss=4.046, nll_loss=2.456, w2v_ctc_loss=1.58, task_loss=3.599, contrastive_loss=0, total=3923.6, n_correct=1261.32, ppl=5.49, accuracy=32.147, wps=13697.3, ups=1.16, wpb=11793.5, bsz=409.8, num_updates=4500, lr=0.00018001, gnorm=0.847, clip=0, loss_scale=8, train_wall=85, gb_free=14.5, wall=3357
2023-09-01 12:37:43 | INFO | train_inner | epoch 003:    953 / 1826 loss=2.736, trans_loss=4.03, nll_loss=2.438, w2v_ctc_loss=1.565, task_loss=3.366, contrastive_loss=0, total=3981, n_correct=1286.79, ppl=5.42, accuracy=32.323, wps=13922.6, ups=1.16, wpb=11981, bsz=429.9, num_updates=4600, lr=0.000184008, gnorm=0.832, clip=0, loss_scale=8, train_wall=85, gb_free=16.2, wall=3443
2023-09-01 12:39:10 | INFO | train_inner | epoch 003:   1053 / 1826 loss=2.735, trans_loss=4.034, nll_loss=2.441, w2v_ctc_loss=1.569, task_loss=3.363, contrastive_loss=0, total=3974.74, n_correct=1291.26, ppl=5.43, accuracy=32.487, wps=13732.4, ups=1.15, wpb=11951.6, bsz=435.6, num_updates=4700, lr=0.000188006, gnorm=0.873, clip=0, loss_scale=8, train_wall=86, gb_free=15.6, wall=3530
2023-09-01 12:40:37 | INFO | train_inner | epoch 003:   1153 / 1826 loss=2.712, trans_loss=4.03, nll_loss=2.437, w2v_ctc_loss=1.532, task_loss=3.611, contrastive_loss=0, total=3906.86, n_correct=1268.7, ppl=5.42, accuracy=32.474, wps=13502.5, ups=1.15, wpb=11755.5, bsz=407, num_updates=4800, lr=0.000192004, gnorm=0.808, clip=0, loss_scale=8, train_wall=86, gb_free=12.6, wall=3617
2023-09-01 12:42:03 | INFO | train_inner | epoch 003:   1253 / 1826 loss=2.705, trans_loss=4.033, nll_loss=2.439, w2v_ctc_loss=1.527, task_loss=3.46, contrastive_loss=0, total=3999.95, n_correct=1306.84, ppl=5.42, accuracy=32.671, wps=13934, ups=1.16, wpb=12025.1, bsz=427.4, num_updates=4900, lr=0.000196002, gnorm=0.835, clip=0, loss_scale=8, train_wall=86, gb_free=14.7, wall=3704
2023-09-01 12:43:29 | INFO | train_inner | epoch 003:   1353 / 1826 loss=2.702, trans_loss=4.03, nll_loss=2.435, w2v_ctc_loss=1.519, task_loss=3.663, contrastive_loss=0, total=3932.16, n_correct=1283.93, ppl=5.41, accuracy=32.652, wps=13794.9, ups=1.17, wpb=11824.6, bsz=404.8, num_updates=5000, lr=0.0002, gnorm=0.829, clip=0, loss_scale=8, train_wall=85, gb_free=15.7, wall=3789
mt_weight tensor(0.5000)
asr_weight tensor(0.3729, device='cuda:0')
2023-09-01 12:44:54 | INFO | train_inner | epoch 003:   1453 / 1826 loss=2.681, trans_loss=4.02, nll_loss=2.424, w2v_ctc_loss=1.496, task_loss=3.452, contrastive_loss=0, total=3859.8, n_correct=1266.42, ppl=5.36, accuracy=32.811, wps=13648.3, ups=1.18, wpb=11609, bsz=414.4, num_updates=5100, lr=0.00019803, gnorm=0.471, clip=0, loss_scale=16, train_wall=84, gb_free=16.2, wall=3874
2023-09-01 12:46:20 | INFO | train_inner | epoch 003:   1553 / 1826 loss=2.697, trans_loss=4.026, nll_loss=2.432, w2v_ctc_loss=1.521, task_loss=3.603, contrastive_loss=0, total=3906.34, n_correct=1279, ppl=5.4, accuracy=32.742, wps=13644, ups=1.16, wpb=11751.4, bsz=405.8, num_updates=5200, lr=0.000196116, gnorm=0.476, clip=0, loss_scale=16, train_wall=85, gb_free=17.4, wall=3961
2023-09-01 12:47:45 | INFO | train_inner | epoch 003:   1653 / 1826 loss=2.678, trans_loss=4.019, nll_loss=2.422, w2v_ctc_loss=1.498, task_loss=3.396, contrastive_loss=0, total=3944.06, n_correct=1300.02, ppl=5.36, accuracy=32.961, wps=13924.1, ups=1.17, wpb=11862.8, bsz=430.8, num_updates=5300, lr=0.000194257, gnorm=0.477, clip=0, loss_scale=16, train_wall=85, gb_free=14.3, wall=4046
2023-09-01 12:49:11 | INFO | train_inner | epoch 003:   1753 / 1826 loss=2.657, trans_loss=4.013, nll_loss=2.413, w2v_ctc_loss=1.472, task_loss=3.275, contrastive_loss=0, total=3980.4, n_correct=1320.53, ppl=5.33, accuracy=33.176, wps=13999.5, ups=1.17, wpb=11966.7, bsz=443.8, num_updates=5400, lr=0.00019245, gnorm=0.455, clip=0, loss_scale=16, train_wall=85, gb_free=14.8, wall=4131
2023-09-01 12:50:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.3729, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.3729, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.3729, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.3729, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.3729, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.3729, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.3729, device='cuda:1')
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 12:50:54 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.57 | trans_loss 7.488 | nll_loss 5.473 | w2v_ctc_loss 1.671 | task_loss 18.514 | contrastive_loss 0 | total 3505.91 | n_correct 1149.36 | ppl 44.4 | accuracy 32.784 | uer 24.725 | wer 26.398 | raw_wer 26.398 | bleu 0.17 | wps 1092.2 | wpb 3505.9 | bsz 119.3 | num_updates 5473 | best_bleu 0.25
2023-09-01 12:50:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5473 updates
2023-09-01 12:50:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_0.1704.pt
2023-09-01 12:50:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_0.1704.pt
2023-09-01 12:51:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_0.1704.pt (epoch 3 @ 5473 updates, score 0.17) (writing took 6.926625596999656 seconds)
2023-09-01 12:51:01 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-09-01 12:51:01 | INFO | train | epoch 003 | loss 2.747 | trans_loss 4.038 | nll_loss 2.448 | w2v_ctc_loss 1.581 | task_loss 3.4 | contrastive_loss 0 | total 3956.37 | n_correct 1278.15 | ppl 5.45 | accuracy 32.306 | wps 12944.7 | ups 1.09 | wpb 11900.1 | bsz 427.2 | num_updates 5473 | lr 0.000191162 | gnorm 0.782 | clip 0 | loss_scale 16 | train_wall 1558 | gb_free 16.9 | wall 4242
2023-09-01 12:51:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 12:51:02 | INFO | fairseq.trainer | begin training epoch 4
2023-09-01 12:51:02 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 12:51:33 | INFO | train_inner | epoch 004:     27 / 1826 loss=2.672, trans_loss=4.012, nll_loss=2.412, w2v_ctc_loss=1.491, task_loss=3.379, contrastive_loss=0, total=3978.01, n_correct=1311.57, ppl=5.32, accuracy=32.971, wps=8447.7, ups=0.71, wpb=11961.2, bsz=425.9, num_updates=5500, lr=0.000190693, gnorm=0.494, clip=0, loss_scale=16, train_wall=85, gb_free=14.1, wall=4273
2023-09-01 12:52:58 | INFO | train_inner | epoch 004:    127 / 1826 loss=2.617, trans_loss=3.992, nll_loss=2.388, w2v_ctc_loss=1.421, task_loss=3.378, contrastive_loss=0, total=3982.54, n_correct=1333.52, ppl=5.24, accuracy=33.484, wps=14084.3, ups=1.17, wpb=11987, bsz=434.7, num_updates=5600, lr=0.000188982, gnorm=0.46, clip=0, loss_scale=16, train_wall=84, gb_free=16.1, wall=4359
2023-09-01 12:54:24 | INFO | train_inner | epoch 004:    227 / 1826 loss=2.607, trans_loss=3.993, nll_loss=2.387, w2v_ctc_loss=1.411, task_loss=3.303, contrastive_loss=0, total=3984.87, n_correct=1336.07, ppl=5.23, accuracy=33.529, wps=13941.9, ups=1.16, wpb=11982.5, bsz=433.5, num_updates=5700, lr=0.000187317, gnorm=0.457, clip=0, loss_scale=16, train_wall=85, gb_free=16.4, wall=4444
2023-09-01 12:55:49 | INFO | train_inner | epoch 004:    327 / 1826 loss=2.608, trans_loss=3.986, nll_loss=2.378, w2v_ctc_loss=1.416, task_loss=3.217, contrastive_loss=0, total=3975.1, n_correct=1338.11, ppl=5.2, accuracy=33.662, wps=14062.7, ups=1.18, wpb=11950.1, bsz=444.4, num_updates=5800, lr=0.000185695, gnorm=0.483, clip=0, loss_scale=16, train_wall=84, gb_free=17.1, wall=4529
2023-09-01 12:57:15 | INFO | train_inner | epoch 004:    427 / 1826 loss=2.609, trans_loss=3.98, nll_loss=2.37, w2v_ctc_loss=1.414, task_loss=3.289, contrastive_loss=0, total=3998.07, n_correct=1347.71, ppl=5.17, accuracy=33.709, wps=14044.9, ups=1.17, wpb=12019.5, bsz=438.5, num_updates=5900, lr=0.000184115, gnorm=0.456, clip=0, loss_scale=16, train_wall=85, gb_free=15.3, wall=4615
2023-09-01 12:58:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-01 12:58:42 | INFO | train_inner | epoch 004:    528 / 1826 loss=2.601, trans_loss=3.989, nll_loss=2.383, w2v_ctc_loss=1.402, task_loss=3.682, contrastive_loss=0, total=3882.44, n_correct=1300.47, ppl=5.22, accuracy=33.496, wps=13390.4, ups=1.15, wpb=11678.8, bsz=405.3, num_updates=6000, lr=0.000182574, gnorm=0.455, clip=0, loss_scale=8, train_wall=87, gb_free=16.3, wall=4702
2023-09-01 12:58:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 12:59:22 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 5.525 | trans_loss 7.448 | nll_loss 5.417 | w2v_ctc_loss 1.61 | task_loss 18.244 | contrastive_loss 0 | total 3505.91 | n_correct 1161 | ppl 42.72 | accuracy 33.116 | uer 23.99 | wer 25.539 | raw_wer 25.539 | bleu 0.4 | wps 1138.1 | wpb 3505.9 | bsz 119.3 | num_updates 6000 | best_bleu 0.4
2023-09-01 12:59:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6000 updates
2023-09-01 12:59:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_4_6000.pt
2023-09-01 12:59:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_4_6000.pt
2023-09-01 12:59:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_4_6000.pt (epoch 4 @ 6000 updates, score 0.4) (writing took 12.778318994998699 seconds)
2023-09-01 13:01:00 | INFO | train_inner | epoch 004:    628 / 1826 loss=2.599, trans_loss=3.992, nll_loss=2.384, w2v_ctc_loss=1.4, task_loss=3.29, contrastive_loss=0, total=4022.2, n_correct=1356.33, ppl=5.22, accuracy=33.721, wps=8732.4, ups=0.72, wpb=12084.8, bsz=439.9, num_updates=6100, lr=0.000181071, gnorm=0.454, clip=0, loss_scale=8, train_wall=85, gb_free=16.8, wall=4841
2023-09-01 13:02:26 | INFO | train_inner | epoch 004:    728 / 1826 loss=2.585, trans_loss=3.975, nll_loss=2.368, w2v_ctc_loss=1.387, task_loss=3.641, contrastive_loss=0, total=3910.18, n_correct=1317.19, ppl=5.16, accuracy=33.686, wps=13658.6, ups=1.16, wpb=11776.4, bsz=406.6, num_updates=6200, lr=0.000179605, gnorm=0.428, clip=0, loss_scale=8, train_wall=86, gb_free=16.2, wall=4927
2023-09-01 13:03:53 | INFO | train_inner | epoch 004:    828 / 1826 loss=2.567, trans_loss=3.968, nll_loss=2.355, w2v_ctc_loss=1.369, task_loss=3.174, contrastive_loss=0, total=4016.26, n_correct=1374.03, ppl=5.12, accuracy=34.212, wps=13980, ups=1.16, wpb=12076.8, bsz=451, num_updates=6300, lr=0.000178174, gnorm=0.429, clip=0, loss_scale=8, train_wall=86, gb_free=15.8, wall=5013
2023-09-01 13:05:19 | INFO | train_inner | epoch 004:    928 / 1826 loss=2.572, trans_loss=3.974, nll_loss=2.363, w2v_ctc_loss=1.377, task_loss=3.421, contrastive_loss=0, total=3971.1, n_correct=1354.28, ppl=5.14, accuracy=34.103, wps=13882.2, ups=1.16, wpb=11940.4, bsz=433.3, num_updates=6400, lr=0.000176777, gnorm=0.423, clip=0, loss_scale=8, train_wall=85, gb_free=16.4, wall=5099
2023-09-01 13:06:44 | INFO | train_inner | epoch 004:   1028 / 1826 loss=2.57, trans_loss=3.966, nll_loss=2.353, w2v_ctc_loss=1.371, task_loss=3.56, contrastive_loss=0, total=3913.53, n_correct=1332.05, ppl=5.11, accuracy=34.037, wps=13736.5, ups=1.17, wpb=11769.8, bsz=411.1, num_updates=6500, lr=0.000175412, gnorm=0.42, clip=0, loss_scale=8, train_wall=85, gb_free=17, wall=5185
2023-09-01 13:08:11 | INFO | train_inner | epoch 004:   1128 / 1826 loss=2.564, trans_loss=3.961, nll_loss=2.346, w2v_ctc_loss=1.367, task_loss=3.723, contrastive_loss=0, total=3917.52, n_correct=1341.67, ppl=5.08, accuracy=34.248, wps=13600.2, ups=1.15, wpb=11783.4, bsz=397.9, num_updates=6600, lr=0.000174078, gnorm=0.427, clip=0, loss_scale=8, train_wall=86, gb_free=15.2, wall=5272
2023-09-01 13:09:37 | INFO | train_inner | epoch 004:   1228 / 1826 loss=2.543, trans_loss=3.955, nll_loss=2.337, w2v_ctc_loss=1.348, task_loss=3.459, contrastive_loss=0, total=3970.57, n_correct=1377.83, ppl=5.05, accuracy=34.701, wps=13832.5, ups=1.16, wpb=11937.7, bsz=423, num_updates=6700, lr=0.000172774, gnorm=0.421, clip=0, loss_scale=8, train_wall=86, gb_free=17.3, wall=5358
2023-09-01 13:11:03 | INFO | train_inner | epoch 004:   1328 / 1826 loss=2.546, trans_loss=3.952, nll_loss=2.334, w2v_ctc_loss=1.355, task_loss=3.641, contrastive_loss=0, total=3859.77, n_correct=1344.02, ppl=5.04, accuracy=34.821, wps=13514.2, ups=1.16, wpb=11606, bsz=400.3, num_updates=6800, lr=0.000171499, gnorm=0.42, clip=0, loss_scale=8, train_wall=85, gb_free=12.3, wall=5444
2023-09-01 13:12:29 | INFO | train_inner | epoch 004:   1428 / 1826 loss=2.529, trans_loss=3.94, nll_loss=2.318, w2v_ctc_loss=1.347, task_loss=3.215, contrastive_loss=0, total=3969.93, n_correct=1405.79, ppl=4.99, accuracy=35.411, wps=13993.6, ups=1.17, wpb=11941, bsz=446.7, num_updates=6900, lr=0.000170251, gnorm=0.436, clip=0, loss_scale=8, train_wall=85, gb_free=11.2, wall=5529
2023-09-01 13:13:55 | INFO | train_inner | epoch 004:   1528 / 1826 loss=2.514, trans_loss=3.928, nll_loss=2.305, w2v_ctc_loss=1.333, task_loss=3.131, contrastive_loss=0, total=4031.91, n_correct=1444.06, ppl=4.94, accuracy=35.816, wps=14125.5, ups=1.16, wpb=12134.9, bsz=460.3, num_updates=7000, lr=0.000169031, gnorm=0.428, clip=0, loss_scale=8, train_wall=85, gb_free=16, wall=5615
2023-09-01 13:15:21 | INFO | train_inner | epoch 004:   1628 / 1826 loss=2.51, trans_loss=3.919, nll_loss=2.293, w2v_ctc_loss=1.331, task_loss=3.422, contrastive_loss=0, total=3940.83, n_correct=1418.29, ppl=4.9, accuracy=35.99, wps=13738.7, ups=1.16, wpb=11857.3, bsz=422.2, num_updates=7100, lr=0.000167836, gnorm=0.427, clip=0, loss_scale=8, train_wall=86, gb_free=16.2, wall=5701
2023-09-01 13:16:46 | INFO | train_inner | epoch 004:   1728 / 1826 loss=2.511, trans_loss=3.923, nll_loss=2.298, w2v_ctc_loss=1.336, task_loss=3.623, contrastive_loss=0, total=3878.48, n_correct=1396.42, ppl=4.92, accuracy=36.004, wps=13683.9, ups=1.17, wpb=11676.7, bsz=400.5, num_updates=7200, lr=0.000166667, gnorm=0.43, clip=0, loss_scale=8, train_wall=85, gb_free=15.9, wall=5787
2023-09-01 13:18:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 13:18:48 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 5.295 | trans_loss 7.141 | nll_loss 5.008 | w2v_ctc_loss 1.537 | task_loss 18.644 | contrastive_loss 0 | total 3505.91 | n_correct 1299.27 | ppl 32.17 | accuracy 37.06 | uer 22.844 | wer 24.518 | raw_wer 24.518 | bleu 1.93 | wps 1181.3 | wpb 3505.9 | bsz 119.3 | num_updates 7298 | best_bleu 1.93
2023-09-01 13:18:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7298 updates
2023-09-01 13:18:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 13:18:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 13:19:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 4 @ 7298 updates, score 1.93) (writing took 11.557972869981313 seconds)
2023-09-01 13:19:00 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-09-01 13:19:00 | INFO | train | epoch 004 | loss 2.565 | trans_loss 3.962 | nll_loss 2.347 | w2v_ctc_loss 1.374 | task_loss 3.412 | contrastive_loss 0 | total 3955.28 | n_correct 1364.37 | ppl 5.09 | accuracy 34.495 | wps 12933.2 | ups 1.09 | wpb 11896.9 | bsz 426.6 | num_updates 7298 | lr 0.000165544 | gnorm 0.438 | clip 0 | loss_scale 8 | train_wall 1555 | gb_free 16.8 | wall 5921
2023-09-01 13:19:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 13:19:00 | INFO | fairseq.trainer | begin training epoch 5
2023-09-01 13:19:00 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 13:19:09 | INFO | train_inner | epoch 005:      2 / 1826 loss=2.495, trans_loss=3.905, nll_loss=2.274, w2v_ctc_loss=1.326, task_loss=3.468, contrastive_loss=0, total=3928.47, n_correct=1440.6, ppl=4.84, accuracy=36.671, wps=8282.4, ups=0.7, wpb=11817.1, bsz=419, num_updates=7300, lr=0.000165521, gnorm=0.425, clip=0, loss_scale=8, train_wall=84, gb_free=15.5, wall=5929
2023-09-01 13:20:33 | INFO | train_inner | epoch 005:    102 / 1826 loss=2.437, trans_loss=3.868, nll_loss=2.223, w2v_ctc_loss=1.267, task_loss=3.281, contrastive_loss=0, total=3930.02, n_correct=1490.77, ppl=4.67, accuracy=37.933, wps=14006, ups=1.19, wpb=11814.9, bsz=438.6, num_updates=7400, lr=0.000164399, gnorm=0.442, clip=0, loss_scale=8, train_wall=84, gb_free=11.3, wall=6014
2023-09-01 13:21:59 | INFO | train_inner | epoch 005:    202 / 1826 loss=2.426, trans_loss=3.852, nll_loss=2.206, w2v_ctc_loss=1.27, task_loss=3.178, contrastive_loss=0, total=3981.02, n_correct=1537.19, ppl=4.61, accuracy=38.613, wps=14002.8, ups=1.17, wpb=11984.5, bsz=446.6, num_updates=7500, lr=0.000163299, gnorm=0.436, clip=0, loss_scale=8, train_wall=85, gb_free=17.2, wall=6099
2023-09-01 13:23:24 | INFO | train_inner | epoch 005:    302 / 1826 loss=2.421, trans_loss=3.837, nll_loss=2.183, w2v_ctc_loss=1.278, task_loss=3.405, contrastive_loss=0, total=3965.56, n_correct=1557.16, ppl=4.54, accuracy=39.267, wps=13975.6, ups=1.17, wpb=11923.3, bsz=426.6, num_updates=7600, lr=0.000162221, gnorm=0.448, clip=0, loss_scale=8, train_wall=85, gb_free=16.2, wall=6185
2023-09-01 13:24:50 | INFO | train_inner | epoch 005:    402 / 1826 loss=2.422, trans_loss=3.814, nll_loss=2.153, w2v_ctc_loss=1.305, task_loss=3.573, contrastive_loss=0, total=3934.83, n_correct=1587.71, ppl=4.45, accuracy=40.35, wps=13747.6, ups=1.16, wpb=11834.7, bsz=414.1, num_updates=7700, lr=0.000161165, gnorm=0.468, clip=0, loss_scale=8, train_wall=86, gb_free=15.6, wall=6271
2023-09-01 13:26:16 | INFO | train_inner | epoch 005:    502 / 1826 loss=2.357, trans_loss=3.757, nll_loss=2.082, w2v_ctc_loss=1.261, task_loss=3.287, contrastive_loss=0, total=3977.97, n_correct=1691.82, ppl=4.23, accuracy=42.53, wps=13931, ups=1.16, wpb=11976.4, bsz=443.8, num_updates=7800, lr=0.000160128, gnorm=0.488, clip=0, loss_scale=8, train_wall=85, gb_free=16.5, wall=6357
2023-09-01 13:27:42 | INFO | train_inner | epoch 005:    602 / 1826 loss=2.348, trans_loss=3.727, nll_loss=2.039, w2v_ctc_loss=1.284, task_loss=3.511, contrastive_loss=0, total=3964.36, n_correct=1754.32, ppl=4.11, accuracy=44.252, wps=13877.6, ups=1.16, wpb=11925, bsz=420.9, num_updates=7900, lr=0.000159111, gnorm=0.516, clip=0, loss_scale=8, train_wall=85, gb_free=16, wall=6443
2023-09-01 13:29:07 | INFO | train_inner | epoch 005:    702 / 1826 loss=2.329, trans_loss=3.683, nll_loss=1.982, w2v_ctc_loss=1.298, task_loss=3.582, contrastive_loss=0, total=3892.49, n_correct=1801, ppl=3.95, accuracy=46.269, wps=13772.9, ups=1.18, wpb=11710, bsz=409.9, num_updates=8000, lr=0.000158114, gnorm=0.541, clip=0, loss_scale=8, train_wall=84, gb_free=15.3, wall=6528
2023-09-01 13:29:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 13:29:48 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.677 | trans_loss 6.213 | nll_loss 3.785 | w2v_ctc_loss 1.568 | task_loss 18.711 | contrastive_loss 0 | total 3505.91 | n_correct 1774.27 | ppl 13.78 | accuracy 50.608 | uer 21.838 | wer 23.369 | raw_wer 23.369 | bleu 12.28 | wps 1103.6 | wpb 3505.9 | bsz 119.3 | num_updates 8000 | best_bleu 12.28
2023-09-01 13:29:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8000 updates
2023-09-01 13:29:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_5_8000.pt
2023-09-01 13:29:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_5_8000.pt
2023-09-01 13:30:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_5_8000.pt (epoch 5 @ 8000 updates, score 12.28) (writing took 11.560819498001365 seconds)
2023-09-01 13:30:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-01 13:31:28 | INFO | train_inner | epoch 005:    803 / 1826 loss=2.308, trans_loss=3.649, nll_loss=1.939, w2v_ctc_loss=1.303, task_loss=3.679, contrastive_loss=0, total=3881.69, n_correct=1851.59, ppl=3.83, accuracy=47.701, wps=8313.1, ups=0.71, wpb=11675.8, bsz=405.5, num_updates=8100, lr=0.000157135, gnorm=0.578, clip=0, loss_scale=8, train_wall=87, gb_free=15, wall=6668
2023-09-01 13:32:52 | INFO | train_inner | epoch 005:    903 / 1826 loss=2.275, trans_loss=3.61, nll_loss=1.889, w2v_ctc_loss=1.3, task_loss=3.279, contrastive_loss=0, total=4005.67, n_correct=1986.55, ppl=3.7, accuracy=49.593, wps=14216.9, ups=1.18, wpb=12056, bsz=445.5, num_updates=8200, lr=0.000156174, gnorm=0.536, clip=0, loss_scale=8, train_wall=84, gb_free=16.6, wall=6753
2023-09-01 13:34:18 | INFO | train_inner | epoch 005:   1003 / 1826 loss=2.27, trans_loss=3.589, nll_loss=1.861, w2v_ctc_loss=1.31, task_loss=3.525, contrastive_loss=0, total=3983.05, n_correct=2011.21, ppl=3.63, accuracy=50.494, wps=14033.6, ups=1.17, wpb=11986.6, bsz=422.9, num_updates=8300, lr=0.00015523, gnorm=0.549, clip=0, loss_scale=8, train_wall=85, gb_free=17.4, wall=6838
2023-09-01 13:35:43 | INFO | train_inner | epoch 005:   1103 / 1826 loss=2.239, trans_loss=3.563, nll_loss=1.827, w2v_ctc_loss=1.299, task_loss=3.246, contrastive_loss=0, total=3974.9, n_correct=2059.47, ppl=3.55, accuracy=51.812, wps=13959.6, ups=1.17, wpb=11955.5, bsz=450.4, num_updates=8400, lr=0.000154303, gnorm=0.546, clip=0, loss_scale=8, train_wall=85, gb_free=16.5, wall=6924
2023-09-01 13:37:10 | INFO | train_inner | epoch 005:   1203 / 1826 loss=2.216, trans_loss=3.541, nll_loss=1.797, w2v_ctc_loss=1.283, task_loss=3.686, contrastive_loss=0, total=3914.61, n_correct=2064.96, ppl=3.48, accuracy=52.75, wps=13581.9, ups=1.15, wpb=11768.7, bsz=406, num_updates=8500, lr=0.000153393, gnorm=0.542, clip=0, loss_scale=8, train_wall=86, gb_free=10.9, wall=7011
2023-09-01 13:37:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-01 13:38:37 | INFO | train_inner | epoch 005:   1304 / 1826 loss=2.204, trans_loss=3.511, nll_loss=1.759, w2v_ctc_loss=1.29, task_loss=3.404, contrastive_loss=0, total=3971.24, n_correct=2141.29, ppl=3.39, accuracy=53.92, wps=13740.3, ups=1.15, wpb=11944.6, bsz=425.6, num_updates=8600, lr=0.000152499, gnorm=0.535, clip=0, loss_scale=4, train_wall=86, gb_free=14.9, wall=7098
2023-09-01 13:40:02 | INFO | train_inner | epoch 005:   1404 / 1826 loss=2.19, trans_loss=3.506, nll_loss=1.751, w2v_ctc_loss=1.282, task_loss=3.367, contrastive_loss=0, total=3981.04, n_correct=2167.65, ppl=3.37, accuracy=54.449, wps=14106.1, ups=1.18, wpb=11963.1, bsz=429.3, num_updates=8700, lr=0.00015162, gnorm=0.511, clip=0, loss_scale=4, train_wall=84, gb_free=16.6, wall=7182
2023-09-01 13:41:28 | INFO | train_inner | epoch 005:   1504 / 1826 loss=2.185, trans_loss=3.484, nll_loss=1.727, w2v_ctc_loss=1.289, task_loss=3.584, contrastive_loss=0, total=3937.22, n_correct=2169.12, ppl=3.31, accuracy=55.093, wps=13827.9, ups=1.17, wpb=11847.9, bsz=414, num_updates=8800, lr=0.000150756, gnorm=0.536, clip=0, loss_scale=4, train_wall=85, gb_free=15.3, wall=7268
2023-09-01 13:42:54 | INFO | train_inner | epoch 005:   1604 / 1826 loss=2.168, trans_loss=3.47, nll_loss=1.707, w2v_ctc_loss=1.283, task_loss=3.339, contrastive_loss=0, total=4001.21, n_correct=2233.1, ppl=3.26, accuracy=55.811, wps=13989.7, ups=1.16, wpb=12033.7, bsz=430, num_updates=8900, lr=0.000149906, gnorm=0.511, clip=0, loss_scale=4, train_wall=85, gb_free=16.5, wall=7354
2023-09-01 13:44:19 | INFO | train_inner | epoch 005:   1704 / 1826 loss=2.16, trans_loss=3.471, nll_loss=1.706, w2v_ctc_loss=1.274, task_loss=3.334, contrastive_loss=0, total=3999.89, n_correct=2245.18, ppl=3.26, accuracy=56.131, wps=14112.2, ups=1.17, wpb=12016.4, bsz=431.6, num_updates=9000, lr=0.000149071, gnorm=0.501, clip=0, loss_scale=4, train_wall=84, gb_free=15.4, wall=7439
2023-09-01 13:45:45 | INFO | train_inner | epoch 005:   1804 / 1826 loss=2.138, trans_loss=3.452, nll_loss=1.683, w2v_ctc_loss=1.259, task_loss=3.294, contrastive_loss=0, total=3997.09, n_correct=2264.66, ppl=3.21, accuracy=56.658, wps=13950.1, ups=1.16, wpb=12018.6, bsz=442.5, num_updates=9100, lr=0.00014825, gnorm=0.52, clip=0, loss_scale=4, train_wall=85, gb_free=15.9, wall=7525
2023-09-01 13:46:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 13:46:44 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.165 | trans_loss 5.52 | nll_loss 2.897 | w2v_ctc_loss 1.426 | task_loss 18.652 | contrastive_loss 0 | total 3505.91 | n_correct 2145.73 | ppl 7.45 | accuracy 61.203 | uer 21.382 | wer 23.062 | raw_wer 23.062 | bleu 22.81 | wps 1148.9 | wpb 3505.9 | bsz 119.3 | num_updates 9122 | best_bleu 22.81
2023-09-01 13:46:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9122 updates
2023-09-01 13:46:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 13:46:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 13:46:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 5 @ 9122 updates, score 22.81) (writing took 13.719397315988317 seconds)
2023-09-01 13:46:58 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-09-01 13:46:58 | INFO | train | epoch 005 | loss 2.282 | trans_loss 3.63 | nll_loss 1.915 | w2v_ctc_loss 1.285 | task_loss 3.424 | contrastive_loss 0 | total 3956.26 | n_correct 1923.8 | ppl 3.77 | accuracy 48.627 | wps 12936.5 | ups 1.09 | wpb 11899.8 | bsz 426.9 | num_updates 9122 | lr 0.000148071 | gnorm 0.511 | clip 0 | loss_scale 4 | train_wall 1551 | gb_free 17 | wall 7598
2023-09-01 13:46:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 13:46:58 | INFO | fairseq.trainer | begin training epoch 6
2023-09-01 13:46:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 13:48:12 | INFO | train_inner | epoch 006:     78 / 1826 loss=2.105, trans_loss=3.426, nll_loss=1.652, w2v_ctc_loss=1.223, task_loss=3.509, contrastive_loss=0, total=3898.11, n_correct=2239.9, ppl=3.14, accuracy=57.461, wps=7988.5, ups=0.68, wpb=11726.7, bsz=413, num_updates=9200, lr=0.000147442, gnorm=0.504, clip=0, loss_scale=4, train_wall=84, gb_free=15.7, wall=7672
2023-09-01 13:49:38 | INFO | train_inner | epoch 006:    178 / 1826 loss=2.115, trans_loss=3.425, nll_loss=1.649, w2v_ctc_loss=1.242, task_loss=3.641, contrastive_loss=0, total=3953.57, n_correct=2278.91, ppl=3.14, accuracy=57.642, wps=13752.7, ups=1.16, wpb=11890.2, bsz=415, num_updates=9300, lr=0.000146647, gnorm=0.501, clip=0, loss_scale=4, train_wall=86, gb_free=17.2, wall=7759
2023-09-01 13:51:03 | INFO | train_inner | epoch 006:    278 / 1826 loss=2.109, trans_loss=3.418, nll_loss=1.642, w2v_ctc_loss=1.241, task_loss=3.597, contrastive_loss=0, total=3921.02, n_correct=2269.91, ppl=3.12, accuracy=57.891, wps=13849.5, ups=1.17, wpb=11798.2, bsz=410.9, num_updates=9400, lr=0.000145865, gnorm=0.491, clip=0, loss_scale=4, train_wall=85, gb_free=16.7, wall=7844
2023-09-01 13:52:29 | INFO | train_inner | epoch 006:    378 / 1826 loss=2.093, trans_loss=3.399, nll_loss=1.619, w2v_ctc_loss=1.228, task_loss=3.35, contrastive_loss=0, total=3977.69, n_correct=2321.27, ppl=3.07, accuracy=58.357, wps=13909.5, ups=1.16, wpb=11972.5, bsz=437.6, num_updates=9500, lr=0.000145095, gnorm=0.502, clip=0, loss_scale=4, train_wall=85, gb_free=14.4, wall=7930
2023-09-01 13:53:56 | INFO | train_inner | epoch 006:    478 / 1826 loss=2.088, trans_loss=3.396, nll_loss=1.614, w2v_ctc_loss=1.227, task_loss=3.601, contrastive_loss=0, total=3925.99, n_correct=2302.02, ppl=3.06, accuracy=58.635, wps=13716.9, ups=1.16, wpb=11811.9, bsz=412.5, num_updates=9600, lr=0.000144338, gnorm=0.487, clip=0, loss_scale=4, train_wall=85, gb_free=17.4, wall=8016
2023-09-01 13:55:22 | INFO | train_inner | epoch 006:    578 / 1826 loss=2.089, trans_loss=3.403, nll_loss=1.621, w2v_ctc_loss=1.225, task_loss=3.444, contrastive_loss=0, total=3961.06, n_correct=2324.86, ppl=3.08, accuracy=58.693, wps=13835.8, ups=1.16, wpb=11910, bsz=424.8, num_updates=9700, lr=0.000143592, gnorm=0.507, clip=0, loss_scale=4, train_wall=85, gb_free=17.5, wall=8102
2023-09-01 13:56:46 | INFO | train_inner | epoch 006:    678 / 1826 loss=2.069, trans_loss=3.383, nll_loss=1.599, w2v_ctc_loss=1.211, task_loss=3.333, contrastive_loss=0, total=3962.11, n_correct=2350.36, ppl=3.03, accuracy=59.321, wps=14052, ups=1.18, wpb=11922.3, bsz=431.9, num_updates=9800, lr=0.000142857, gnorm=0.475, clip=0, loss_scale=4, train_wall=84, gb_free=16.8, wall=8187
2023-09-01 13:58:12 | INFO | train_inner | epoch 006:    778 / 1826 loss=2.062, trans_loss=3.382, nll_loss=1.595, w2v_ctc_loss=1.207, task_loss=3.356, contrastive_loss=0, total=3994.23, n_correct=2378.08, ppl=3.02, accuracy=59.538, wps=14009, ups=1.17, wpb=12010.9, bsz=435.9, num_updates=9900, lr=0.000142134, gnorm=0.482, clip=0, loss_scale=4, train_wall=85, gb_free=15, wall=8273
2023-09-01 13:59:37 | INFO | train_inner | epoch 006:    878 / 1826 loss=2.059, trans_loss=3.377, nll_loss=1.591, w2v_ctc_loss=1.206, task_loss=3.351, contrastive_loss=0, total=3963.99, n_correct=2359.4, ppl=3.01, accuracy=59.521, wps=14013, ups=1.17, wpb=11928, bsz=434.2, num_updates=10000, lr=0.000141421, gnorm=0.479, clip=0, loss_scale=4, train_wall=84, gb_free=17.1, wall=8358
2023-09-01 13:59:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 14:00:16 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.069 | trans_loss 5.362 | nll_loss 2.698 | w2v_ctc_loss 1.461 | task_loss 18.901 | contrastive_loss 0 | total 3505.91 | n_correct 2230 | ppl 6.49 | accuracy 63.607 | uer 20.178 | wer 21.831 | raw_wer 21.831 | bleu 25.13 | wps 1182.3 | wpb 3505.9 | bsz 119.3 | num_updates 10000 | best_bleu 25.13
2023-09-01 14:00:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10000 updates
2023-09-01 14:00:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_6_10000.pt
2023-09-01 14:00:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_6_10000.pt
2023-09-01 14:00:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_6_10000.pt (epoch 6 @ 10000 updates, score 25.13) (writing took 15.07812958699651 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:0')
2023-09-01 14:01:57 | INFO | train_inner | epoch 006:    978 / 1826 loss=2.056, trans_loss=3.377, nll_loss=1.587, w2v_ctc_loss=1.204, task_loss=3.211, contrastive_loss=0, total=4004.17, n_correct=2396.48, ppl=3, accuracy=59.85, wps=8618.7, ups=0.72, wpb=12032, bsz=444.8, num_updates=10100, lr=0.00014072, gnorm=0.442, clip=0, loss_scale=4, train_wall=85, gb_free=16.3, wall=8497
2023-09-01 14:03:22 | INFO | train_inner | epoch 006:   1078 / 1826 loss=2.057, trans_loss=3.362, nll_loss=1.571, w2v_ctc_loss=1.213, task_loss=3.39, contrastive_loss=0, total=3952.95, n_correct=2378.21, ppl=2.97, accuracy=60.163, wps=14042.3, ups=1.18, wpb=11893.5, bsz=422.9, num_updates=10200, lr=0.000140028, gnorm=0.427, clip=0, loss_scale=4, train_wall=84, gb_free=16, wall=8582
2023-09-01 14:04:46 | INFO | train_inner | epoch 006:   1178 / 1826 loss=2.054, trans_loss=3.365, nll_loss=1.575, w2v_ctc_loss=1.209, task_loss=3.466, contrastive_loss=0, total=3919.76, n_correct=2355.54, ppl=2.98, accuracy=60.094, wps=13930.2, ups=1.18, wpb=11794.6, bsz=423.3, num_updates=10300, lr=0.000139347, gnorm=0.437, clip=0, loss_scale=4, train_wall=84, gb_free=15.3, wall=8667
2023-09-01 14:06:11 | INFO | train_inner | epoch 006:   1278 / 1826 loss=2.045, trans_loss=3.351, nll_loss=1.558, w2v_ctc_loss=1.209, task_loss=3.253, contrastive_loss=0, total=3957.79, n_correct=2391.17, ppl=2.94, accuracy=60.417, wps=14038.5, ups=1.18, wpb=11907.5, bsz=441.4, num_updates=10400, lr=0.000138675, gnorm=0.434, clip=0, loss_scale=4, train_wall=84, gb_free=17.3, wall=8752
2023-09-01 14:07:37 | INFO | train_inner | epoch 006:   1378 / 1826 loss=2.056, trans_loss=3.36, nll_loss=1.567, w2v_ctc_loss=1.215, task_loss=3.655, contrastive_loss=0, total=3937.9, n_correct=2374.25, ppl=2.96, accuracy=60.292, wps=13757.5, ups=1.16, wpb=11838.1, bsz=409.6, num_updates=10500, lr=0.000138013, gnorm=0.44, clip=0, loss_scale=4, train_wall=85, gb_free=16.3, wall=8838
2023-09-01 14:09:04 | INFO | train_inner | epoch 006:   1478 / 1826 loss=2.045, trans_loss=3.36, nll_loss=1.565, w2v_ctc_loss=1.203, task_loss=3.464, contrastive_loss=0, total=3946.83, n_correct=2385.98, ppl=2.96, accuracy=60.453, wps=13688.6, ups=1.15, wpb=11859.6, bsz=426.1, num_updates=10600, lr=0.000137361, gnorm=0.443, clip=0, loss_scale=8, train_wall=86, gb_free=16, wall=8924
2023-09-01 14:10:30 | INFO | train_inner | epoch 006:   1578 / 1826 loss=2.041, trans_loss=3.36, nll_loss=1.567, w2v_ctc_loss=1.2, task_loss=3.54, contrastive_loss=0, total=3944.52, n_correct=2387.47, ppl=2.96, accuracy=60.526, wps=13831.2, ups=1.17, wpb=11859.3, bsz=421.2, num_updates=10700, lr=0.000136717, gnorm=0.422, clip=0, loss_scale=8, train_wall=85, gb_free=15.4, wall=9010
2023-09-01 14:11:55 | INFO | train_inner | epoch 006:   1678 / 1826 loss=2.02, trans_loss=3.343, nll_loss=1.547, w2v_ctc_loss=1.182, task_loss=3.299, contrastive_loss=0, total=3973.29, n_correct=2428.64, ppl=2.92, accuracy=61.124, wps=14035.7, ups=1.17, wpb=11948.7, bsz=433.7, num_updates=10800, lr=0.000136083, gnorm=0.432, clip=0, loss_scale=8, train_wall=84, gb_free=17.1, wall=9095
2023-09-01 14:13:19 | INFO | train_inner | epoch 006:   1778 / 1826 loss=2.042, trans_loss=3.35, nll_loss=1.559, w2v_ctc_loss=1.211, task_loss=3.352, contrastive_loss=0, total=3956.46, n_correct=2407.13, ppl=2.95, accuracy=60.84, wps=14072.5, ups=1.18, wpb=11909.1, bsz=429.1, num_updates=10900, lr=0.000135457, gnorm=0.425, clip=0, loss_scale=8, train_wall=84, gb_free=15.4, wall=9180
2023-09-01 14:14:01 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:4')
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 14:14:39 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.02 | trans_loss 5.281 | nll_loss 2.598 | w2v_ctc_loss 1.481 | task_loss 18.882 | contrastive_loss 0 | total 3505.91 | n_correct 2271.82 | ppl 6.05 | accuracy 64.8 | uer 20.771 | wer 22.562 | raw_wer 22.562 | bleu 26.45 | wps 1202.7 | wpb 3505.9 | bsz 119.3 | num_updates 10948 | best_bleu 26.45
2023-09-01 14:14:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10948 updates
2023-09-01 14:14:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 14:14:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 14:14:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 6 @ 10948 updates, score 26.45) (writing took 11.839557789004175 seconds)
2023-09-01 14:14:51 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-09-01 14:14:51 | INFO | train | epoch 006 | loss 2.065 | trans_loss 3.378 | nll_loss 1.591 | w2v_ctc_loss 1.213 | task_loss 3.419 | contrastive_loss 0 | total 3956.37 | n_correct 2356.85 | ppl 3.01 | accuracy 59.571 | wps 12986.5 | ups 1.09 | wpb 11900.1 | bsz 427.2 | num_updates 10948 | lr 0.00013516 | gnorm 0.461 | clip 0 | loss_scale 8 | train_wall 1548 | gb_free 16.5 | wall 9272
2023-09-01 14:14:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 14:14:51 | INFO | fairseq.trainer | begin training epoch 7
2023-09-01 14:14:51 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 14:15:43 | INFO | train_inner | epoch 007:     52 / 1826 loss=2.02, trans_loss=3.33, nll_loss=1.532, w2v_ctc_loss=1.188, task_loss=3.343, contrastive_loss=0, total=3931.13, n_correct=2416.84, ppl=2.89, accuracy=61.48, wps=8221.5, ups=0.69, wpb=11831.4, bsz=427.9, num_updates=11000, lr=0.00013484, gnorm=0.43, clip=0, loss_scale=8, train_wall=85, gb_free=16.2, wall=9324
2023-09-01 14:17:09 | INFO | train_inner | epoch 007:    152 / 1826 loss=1.993, trans_loss=3.322, nll_loss=1.52, w2v_ctc_loss=1.157, task_loss=3.365, contrastive_loss=0, total=3953.01, n_correct=2441.41, ppl=2.87, accuracy=61.761, wps=13832.2, ups=1.16, wpb=11890.5, bsz=431.4, num_updates=11100, lr=0.000134231, gnorm=0.429, clip=0, loss_scale=8, train_wall=85, gb_free=12.7, wall=9410
2023-09-01 14:18:34 | INFO | train_inner | epoch 007:    252 / 1826 loss=2.01, trans_loss=3.323, nll_loss=1.52, w2v_ctc_loss=1.179, task_loss=3.747, contrastive_loss=0, total=3898.38, n_correct=2410.86, ppl=2.87, accuracy=61.843, wps=13753.5, ups=1.17, wpb=11720.9, bsz=401.6, num_updates=11200, lr=0.000133631, gnorm=0.428, clip=0, loss_scale=8, train_wall=85, gb_free=15.5, wall=9495
2023-09-01 14:19:59 | INFO | train_inner | epoch 007:    352 / 1826 loss=1.988, trans_loss=3.323, nll_loss=1.523, w2v_ctc_loss=1.152, task_loss=3.058, contrastive_loss=0, total=4034.29, n_correct=2497.28, ppl=2.87, accuracy=61.901, wps=14301.3, ups=1.18, wpb=12139.1, bsz=466.5, num_updates=11300, lr=0.000133038, gnorm=0.413, clip=0, loss_scale=8, train_wall=84, gb_free=16.8, wall=9580
2023-09-01 14:21:24 | INFO | train_inner | epoch 007:    452 / 1826 loss=1.988, trans_loss=3.316, nll_loss=1.513, w2v_ctc_loss=1.157, task_loss=3.212, contrastive_loss=0, total=3989.92, n_correct=2477.45, ppl=2.86, accuracy=62.093, wps=14210.5, ups=1.18, wpb=12006.9, bsz=444.5, num_updates=11400, lr=0.000132453, gnorm=0.421, clip=0, loss_scale=8, train_wall=84, gb_free=15.1, wall=9664
2023-09-01 14:22:49 | INFO | train_inner | epoch 007:    552 / 1826 loss=1.997, trans_loss=3.313, nll_loss=1.508, w2v_ctc_loss=1.17, task_loss=3.31, contrastive_loss=0, total=3952.57, n_correct=2459.57, ppl=2.84, accuracy=62.227, wps=13923.6, ups=1.17, wpb=11886.2, bsz=433.2, num_updates=11500, lr=0.000131876, gnorm=0.422, clip=0, loss_scale=8, train_wall=85, gb_free=16.7, wall=9750
2023-09-01 14:24:16 | INFO | train_inner | epoch 007:    652 / 1826 loss=1.981, trans_loss=3.312, nll_loss=1.508, w2v_ctc_loss=1.148, task_loss=3.473, contrastive_loss=0, total=3942.35, n_correct=2455.08, ppl=2.84, accuracy=62.275, wps=13700.8, ups=1.16, wpb=11857.5, bsz=423.7, num_updates=11600, lr=0.000131306, gnorm=0.417, clip=0, loss_scale=8, train_wall=86, gb_free=16.8, wall=9836
2023-09-01 14:25:41 | INFO | train_inner | epoch 007:    752 / 1826 loss=1.978, trans_loss=3.314, nll_loss=1.51, w2v_ctc_loss=1.142, task_loss=3.417, contrastive_loss=0, total=3949.88, n_correct=2462.49, ppl=2.85, accuracy=62.343, wps=13976.6, ups=1.18, wpb=11876.3, bsz=431.9, num_updates=11700, lr=0.000130744, gnorm=0.421, clip=0, loss_scale=8, train_wall=84, gb_free=15.5, wall=9921
2023-09-01 14:27:07 | INFO | train_inner | epoch 007:    852 / 1826 loss=1.987, trans_loss=3.311, nll_loss=1.506, w2v_ctc_loss=1.159, task_loss=3.666, contrastive_loss=0, total=3930.38, n_correct=2447.68, ppl=2.84, accuracy=62.276, wps=13748.8, ups=1.16, wpb=11820.2, bsz=404.7, num_updates=11800, lr=0.000130189, gnorm=0.412, clip=0, loss_scale=8, train_wall=85, gb_free=11.2, wall=10007
2023-09-01 14:28:32 | INFO | train_inner | epoch 007:    952 / 1826 loss=1.979, trans_loss=3.306, nll_loss=1.5, w2v_ctc_loss=1.151, task_loss=3.248, contrastive_loss=0, total=4048.82, n_correct=2531.58, ppl=2.83, accuracy=62.526, wps=14218.5, ups=1.17, wpb=12177.2, bsz=443.4, num_updates=11900, lr=0.000129641, gnorm=0.405, clip=0, loss_scale=8, train_wall=85, gb_free=15.8, wall=10093
2023-09-01 14:29:58 | INFO | train_inner | epoch 007:   1052 / 1826 loss=1.971, trans_loss=3.298, nll_loss=1.49, w2v_ctc_loss=1.145, task_loss=3.403, contrastive_loss=0, total=3928.57, n_correct=2464.79, ppl=2.81, accuracy=62.74, wps=13824.5, ups=1.17, wpb=11816.1, bsz=424.5, num_updates=12000, lr=0.000129099, gnorm=0.415, clip=0, loss_scale=8, train_wall=85, gb_free=13.8, wall=10178
2023-09-01 14:29:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 14:30:37 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 3.957 | trans_loss 5.219 | nll_loss 2.525 | w2v_ctc_loss 1.411 | task_loss 18.943 | contrastive_loss 0 | total 3505.91 | n_correct 2311.27 | ppl 5.75 | accuracy 65.925 | uer 19.77 | wer 21.628 | raw_wer 21.628 | bleu 27.32 | wps 1194.2 | wpb 3505.9 | bsz 119.3 | num_updates 12000 | best_bleu 27.32
2023-09-01 14:30:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12000 updates
2023-09-01 14:30:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_7_12000.pt
2023-09-01 14:30:39 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_7_12000.pt
2023-09-01 14:30:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_7_12000.pt (epoch 7 @ 12000 updates, score 27.32) (writing took 12.624869728984777 seconds)
2023-09-01 14:32:16 | INFO | train_inner | epoch 007:   1152 / 1826 loss=2.002, trans_loss=3.31, nll_loss=1.506, w2v_ctc_loss=1.182, task_loss=3.696, contrastive_loss=0, total=3932.31, n_correct=2454.26, ppl=2.84, accuracy=62.413, wps=8556.2, ups=0.72, wpb=11834.7, bsz=408.4, num_updates=12100, lr=0.000128565, gnorm=0.446, clip=0, loss_scale=8, train_wall=86, gb_free=15.5, wall=10317
2023-09-01 14:33:42 | INFO | train_inner | epoch 007:   1252 / 1826 loss=1.973, trans_loss=3.304, nll_loss=1.499, w2v_ctc_loss=1.144, task_loss=3.407, contrastive_loss=0, total=3998.13, n_correct=2502.74, ppl=2.83, accuracy=62.598, wps=13997.7, ups=1.16, wpb=12030.8, bsz=433.4, num_updates=12200, lr=0.000128037, gnorm=0.414, clip=0, loss_scale=8, train_wall=85, gb_free=15.8, wall=10403
2023-09-01 14:35:07 | INFO | train_inner | epoch 007:   1352 / 1826 loss=1.979, trans_loss=3.3, nll_loss=1.494, w2v_ctc_loss=1.156, task_loss=3.578, contrastive_loss=0, total=3909.99, n_correct=2452.17, ppl=2.82, accuracy=62.716, wps=13834.8, ups=1.18, wpb=11765, bsz=411.2, num_updates=12300, lr=0.000127515, gnorm=0.414, clip=0, loss_scale=8, train_wall=84, gb_free=15.7, wall=10488
2023-09-01 14:36:33 | INFO | train_inner | epoch 007:   1452 / 1826 loss=1.98, trans_loss=3.307, nll_loss=1.5, w2v_ctc_loss=1.155, task_loss=3.307, contrastive_loss=0, total=3964.82, n_correct=2482.74, ppl=2.83, accuracy=62.619, wps=13939.3, ups=1.17, wpb=11915.6, bsz=435.9, num_updates=12400, lr=0.000127, gnorm=0.417, clip=0, loss_scale=8, train_wall=85, gb_free=17.5, wall=10573
2023-09-01 14:37:58 | INFO | train_inner | epoch 007:   1552 / 1826 loss=1.974, trans_loss=3.302, nll_loss=1.495, w2v_ctc_loss=1.147, task_loss=3.593, contrastive_loss=0, total=3929.01, n_correct=2463.46, ppl=2.82, accuracy=62.699, wps=13833.2, ups=1.17, wpb=11818.4, bsz=413.4, num_updates=12500, lr=0.000126491, gnorm=0.414, clip=0, loss_scale=8, train_wall=85, gb_free=11.7, wall=10659
2023-09-01 14:39:25 | INFO | train_inner | epoch 007:   1652 / 1826 loss=1.954, trans_loss=3.3, nll_loss=1.492, w2v_ctc_loss=1.122, task_loss=3.189, contrastive_loss=0, total=3986.3, n_correct=2507.32, ppl=2.81, accuracy=62.898, wps=13831.8, ups=1.15, wpb=11985.1, bsz=449.1, num_updates=12600, lr=0.000125988, gnorm=0.417, clip=0, loss_scale=8, train_wall=86, gb_free=15.9, wall=10745
2023-09-01 14:40:50 | INFO | train_inner | epoch 007:   1752 / 1826 loss=1.973, trans_loss=3.301, nll_loss=1.494, w2v_ctc_loss=1.148, task_loss=3.423, contrastive_loss=0, total=3979.89, n_correct=2501.6, ppl=2.82, accuracy=62.856, wps=13985.1, ups=1.17, wpb=11965.7, bsz=429.8, num_updates=12700, lr=0.000125491, gnorm=0.418, clip=0, loss_scale=16, train_wall=85, gb_free=17.3, wall=10831
2023-09-01 14:41:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 14:42:31 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 3.934 | trans_loss 5.186 | nll_loss 2.481 | w2v_ctc_loss 1.413 | task_loss 18.989 | contrastive_loss 0 | total 3505.91 | n_correct 2326.82 | ppl 5.58 | accuracy 66.368 | uer 19.212 | wer 20.964 | raw_wer 20.964 | bleu 27.92 | wps 1202.3 | wpb 3505.9 | bsz 119.3 | num_updates 12774 | best_bleu 27.92
2023-09-01 14:42:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12774 updates
2023-09-01 14:42:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 14:42:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 14:42:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 7 @ 12774 updates, score 27.92) (writing took 10.420791842014296 seconds)
2023-09-01 14:42:42 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-09-01 14:42:42 | INFO | train | epoch 007 | loss 1.983 | trans_loss 3.309 | nll_loss 1.504 | w2v_ctc_loss 1.154 | task_loss 3.421 | contrastive_loss 0 | total 3956.37 | n_correct 2468.85 | ppl 2.84 | accuracy 62.402 | wps 13002.2 | ups 1.09 | wpb 11900.1 | bsz 427.2 | num_updates 12774 | lr 0.000125127 | gnorm 0.42 | clip 0 | loss_scale 16 | train_wall 1551 | gb_free 16.5 | wall 10943
2023-09-01 14:42:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 14:42:43 | INFO | fairseq.trainer | begin training epoch 8
2023-09-01 14:42:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 14:43:12 | INFO | train_inner | epoch 008:     26 / 1826 loss=1.969, trans_loss=3.289, nll_loss=1.481, w2v_ctc_loss=1.147, task_loss=3.721, contrastive_loss=0, total=3885.49, n_correct=2448.03, ppl=2.79, accuracy=63.004, wps=8232.8, ups=0.7, wpb=11693.8, bsz=401.5, num_updates=12800, lr=0.000125, gnorm=0.432, clip=0, loss_scale=16, train_wall=84, gb_free=9, wall=10973
2023-09-01 14:44:37 | INFO | train_inner | epoch 008:    126 / 1826 loss=1.945, trans_loss=3.274, nll_loss=1.461, w2v_ctc_loss=1.123, task_loss=3.507, contrastive_loss=0, total=3955.57, n_correct=2512.91, ppl=2.75, accuracy=63.528, wps=14050.6, ups=1.18, wpb=11904.6, bsz=420.5, num_updates=12900, lr=0.000124515, gnorm=0.413, clip=0, loss_scale=16, train_wall=84, gb_free=16.9, wall=11058
2023-09-01 14:46:05 | INFO | train_inner | epoch 008:    226 / 1826 loss=1.931, trans_loss=3.27, nll_loss=1.456, w2v_ctc_loss=1.106, task_loss=3.451, contrastive_loss=0, total=3921.96, n_correct=2498.57, ppl=2.74, accuracy=63.707, wps=13531.3, ups=1.15, wpb=11805, bsz=428, num_updates=13000, lr=0.000124035, gnorm=0.411, clip=0, loss_scale=16, train_wall=87, gb_free=15.8, wall=11145
2023-09-01 14:47:30 | INFO | train_inner | epoch 008:    326 / 1826 loss=1.937, trans_loss=3.275, nll_loss=1.46, w2v_ctc_loss=1.111, task_loss=3.632, contrastive_loss=0, total=3887.36, n_correct=2470.86, ppl=2.75, accuracy=63.561, wps=13749.2, ups=1.18, wpb=11693.5, bsz=406.4, num_updates=13100, lr=0.00012356, gnorm=0.405, clip=0, loss_scale=16, train_wall=84, gb_free=11.5, wall=11230
2023-09-01 14:48:56 | INFO | train_inner | epoch 008:    426 / 1826 loss=1.939, trans_loss=3.274, nll_loss=1.46, w2v_ctc_loss=1.111, task_loss=3.621, contrastive_loss=0, total=3929.14, n_correct=2500.92, ppl=2.75, accuracy=63.651, wps=13677.6, ups=1.16, wpb=11818.2, bsz=411.9, num_updates=13200, lr=0.000123091, gnorm=0.42, clip=0, loss_scale=16, train_wall=86, gb_free=15.5, wall=11317
2023-09-01 14:50:22 | INFO | train_inner | epoch 008:    526 / 1826 loss=1.943, trans_loss=3.275, nll_loss=1.462, w2v_ctc_loss=1.121, task_loss=3.499, contrastive_loss=0, total=3997.46, n_correct=2545.91, ppl=2.76, accuracy=63.688, wps=13948.3, ups=1.16, wpb=12027.4, bsz=435.5, num_updates=13300, lr=0.000122628, gnorm=0.409, clip=0, loss_scale=16, train_wall=86, gb_free=15.2, wall=11403
2023-09-01 14:51:48 | INFO | train_inner | epoch 008:    626 / 1826 loss=1.928, trans_loss=3.275, nll_loss=1.462, w2v_ctc_loss=1.104, task_loss=3.126, contrastive_loss=0, total=4026.89, n_correct=2565.65, ppl=2.75, accuracy=63.713, wps=14176.6, ups=1.17, wpb=12112.3, bsz=460.2, num_updates=13400, lr=0.000122169, gnorm=0.409, clip=0, loss_scale=16, train_wall=85, gb_free=16.4, wall=11488
2023-09-01 14:53:13 | INFO | train_inner | epoch 008:    726 / 1826 loss=1.928, trans_loss=3.277, nll_loss=1.463, w2v_ctc_loss=1.102, task_loss=3.468, contrastive_loss=0, total=3935.44, n_correct=2514.03, ppl=2.76, accuracy=63.882, wps=13893.4, ups=1.17, wpb=11831.7, bsz=421.4, num_updates=13500, lr=0.000121716, gnorm=0.403, clip=0, loss_scale=16, train_wall=85, gb_free=15.3, wall=11574
2023-09-01 14:54:39 | INFO | train_inner | epoch 008:    826 / 1826 loss=1.932, trans_loss=3.277, nll_loss=1.461, w2v_ctc_loss=1.105, task_loss=3.39, contrastive_loss=0, total=3996.46, n_correct=2553.17, ppl=2.75, accuracy=63.886, wps=13993.3, ups=1.17, wpb=12007.6, bsz=432.5, num_updates=13600, lr=0.000121268, gnorm=0.422, clip=0, loss_scale=16, train_wall=85, gb_free=16.5, wall=11659
2023-09-01 14:56:04 | INFO | train_inner | epoch 008:    926 / 1826 loss=1.938, trans_loss=3.269, nll_loss=1.453, w2v_ctc_loss=1.117, task_loss=3.594, contrastive_loss=0, total=3885.41, n_correct=2483.3, ppl=2.74, accuracy=63.913, wps=13727.2, ups=1.17, wpb=11686, bsz=406.3, num_updates=13700, lr=0.000120824, gnorm=0.408, clip=0, loss_scale=16, train_wall=84, gb_free=11.4, wall=11744
2023-09-01 14:57:29 | INFO | train_inner | epoch 008:   1026 / 1826 loss=1.942, trans_loss=3.267, nll_loss=1.451, w2v_ctc_loss=1.126, task_loss=3.582, contrastive_loss=0, total=3950.23, n_correct=2527.32, ppl=2.73, accuracy=63.979, wps=13951.1, ups=1.17, wpb=11884.1, bsz=413.4, num_updates=13800, lr=0.000120386, gnorm=0.422, clip=0, loss_scale=16, train_wall=84, gb_free=15.6, wall=11830
2023-09-01 14:58:54 | INFO | train_inner | epoch 008:   1126 / 1826 loss=1.916, trans_loss=3.265, nll_loss=1.448, w2v_ctc_loss=1.092, task_loss=3.304, contrastive_loss=0, total=3967.4, n_correct=2547.33, ppl=2.73, accuracy=64.207, wps=13998.9, ups=1.17, wpb=11931.2, bsz=436.8, num_updates=13900, lr=0.000119952, gnorm=0.399, clip=0, loss_scale=16, train_wall=85, gb_free=15.1, wall=11915
2023-09-01 15:00:22 | INFO | train_inner | epoch 008:   1226 / 1826 loss=1.914, trans_loss=3.265, nll_loss=1.448, w2v_ctc_loss=1.089, task_loss=3.26, contrastive_loss=0, total=4000.75, n_correct=2568.6, ppl=2.73, accuracy=64.203, wps=13797, ups=1.15, wpb=12030.1, bsz=442.5, num_updates=14000, lr=0.000119523, gnorm=0.41, clip=0, loss_scale=16, train_wall=86, gb_free=16.5, wall=12002
2023-09-01 15:00:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 15:01:01 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 3.888 | trans_loss 5.15 | nll_loss 2.439 | w2v_ctc_loss 1.339 | task_loss 19.071 | contrastive_loss 0 | total 3505.91 | n_correct 2345 | ppl 5.42 | accuracy 66.887 | uer 18.721 | wer 20.633 | raw_wer 20.633 | bleu 28.7 | wps 1168.5 | wpb 3505.9 | bsz 119.3 | num_updates 14000 | best_bleu 28.7
2023-09-01 15:01:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14000 updates
2023-09-01 15:01:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_8_14000.pt
2023-09-01 15:01:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_8_14000.pt
2023-09-01 15:01:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_8_14000.pt (epoch 8 @ 14000 updates, score 28.7) (writing took 12.213503078994108 seconds)
2023-09-01 15:01:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-01 15:02:40 | INFO | train_inner | epoch 008:   1327 / 1826 loss=1.923, trans_loss=3.271, nll_loss=1.456, w2v_ctc_loss=1.098, task_loss=3.598, contrastive_loss=0, total=3927.43, n_correct=2513.43, ppl=2.74, accuracy=63.997, wps=8540.1, ups=0.72, wpb=11813.8, bsz=412, num_updates=14100, lr=0.000119098, gnorm=0.409, clip=0, loss_scale=8, train_wall=85, gb_free=15.6, wall=12140
2023-09-01 15:04:04 | INFO | train_inner | epoch 008:   1427 / 1826 loss=1.901, trans_loss=3.264, nll_loss=1.447, w2v_ctc_loss=1.069, task_loss=3.255, contrastive_loss=0, total=3986.48, n_correct=2556.72, ppl=2.73, accuracy=64.135, wps=14169.6, ups=1.18, wpb=11987, bsz=440.1, num_updates=14200, lr=0.000118678, gnorm=0.403, clip=0, loss_scale=8, train_wall=84, gb_free=16.8, wall=12225
2023-09-01 15:05:29 | INFO | train_inner | epoch 008:   1527 / 1826 loss=1.921, trans_loss=3.264, nll_loss=1.448, w2v_ctc_loss=1.101, task_loss=3.169, contrastive_loss=0, total=3998.9, n_correct=2565.08, ppl=2.73, accuracy=64.145, wps=14190.6, ups=1.18, wpb=12030.1, bsz=445.1, num_updates=14300, lr=0.000118262, gnorm=0.408, clip=0, loss_scale=8, train_wall=84, gb_free=11.1, wall=12310
2023-09-01 15:06:54 | INFO | train_inner | epoch 008:   1627 / 1826 loss=1.932, trans_loss=3.268, nll_loss=1.453, w2v_ctc_loss=1.115, task_loss=3.505, contrastive_loss=0, total=3954.43, n_correct=2533.26, ppl=2.74, accuracy=64.061, wps=14037.4, ups=1.18, wpb=11900, bsz=417.9, num_updates=14400, lr=0.000117851, gnorm=0.422, clip=0, loss_scale=8, train_wall=84, gb_free=16.1, wall=12395
2023-09-01 15:08:20 | INFO | train_inner | epoch 008:   1727 / 1826 loss=1.931, trans_loss=3.274, nll_loss=1.46, w2v_ctc_loss=1.109, task_loss=3.4, contrastive_loss=0, total=3950.04, n_correct=2521.23, ppl=2.75, accuracy=63.828, wps=13841.8, ups=1.17, wpb=11880.5, bsz=431.2, num_updates=14500, lr=0.000117444, gnorm=0.412, clip=0, loss_scale=8, train_wall=85, gb_free=12.3, wall=12480
2023-09-01 15:09:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 15:10:22 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 3.89 | trans_loss 5.138 | nll_loss 2.42 | w2v_ctc_loss 1.373 | task_loss 19.01 | contrastive_loss 0 | total 3505.91 | n_correct 2348.82 | ppl 5.35 | accuracy 66.996 | uer 18.605 | wer 20.461 | raw_wer 20.461 | bleu 28.72 | wps 1196.3 | wpb 3505.9 | bsz 119.3 | num_updates 14599 | best_bleu 28.72
2023-09-01 15:10:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14599 updates
2023-09-01 15:10:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 15:10:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 15:10:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 8 @ 14599 updates, score 28.72) (writing took 12.9055551510246 seconds)
2023-09-01 15:10:36 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-09-01 15:10:36 | INFO | train | epoch 008 | loss 1.93 | trans_loss 3.271 | nll_loss 1.456 | w2v_ctc_loss 1.106 | task_loss 3.425 | contrastive_loss 0 | total 3956.49 | n_correct 2527.52 | ppl 2.74 | accuracy 63.883 | wps 12977 | ups 1.09 | wpb 11900.5 | bsz 427.3 | num_updates 14599 | lr 0.000117045 | gnorm 0.412 | clip 0 | loss_scale 8 | train_wall 1549 | gb_free 16.6 | wall 12617
2023-09-01 15:10:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 15:10:36 | INFO | fairseq.trainer | begin training epoch 9
2023-09-01 15:10:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 15:10:45 | INFO | train_inner | epoch 009:      1 / 1826 loss=1.926, trans_loss=3.271, nll_loss=1.455, w2v_ctc_loss=1.106, task_loss=3.323, contrastive_loss=0, total=3954.05, n_correct=2530.82, ppl=2.74, accuracy=64.006, wps=8217.2, ups=0.69, wpb=11888.5, bsz=433.6, num_updates=14600, lr=0.000117041, gnorm=0.414, clip=0, loss_scale=8, train_wall=84, gb_free=14.6, wall=12625
2023-09-01 15:12:09 | INFO | train_inner | epoch 009:    101 / 1826 loss=1.883, trans_loss=3.243, nll_loss=1.418, w2v_ctc_loss=1.057, task_loss=3.438, contrastive_loss=0, total=3926.33, n_correct=2551, ppl=2.67, accuracy=64.972, wps=13940.6, ups=1.18, wpb=11803.6, bsz=420.9, num_updates=14700, lr=0.000116642, gnorm=0.414, clip=0, loss_scale=8, train_wall=84, gb_free=15.1, wall=12710
2023-09-01 15:13:34 | INFO | train_inner | epoch 009:    201 / 1826 loss=1.887, trans_loss=3.246, nll_loss=1.42, w2v_ctc_loss=1.064, task_loss=3.436, contrastive_loss=0, total=3979.12, n_correct=2586.68, ppl=2.68, accuracy=65.006, wps=14039.6, ups=1.17, wpb=11951.4, bsz=424.4, num_updates=14800, lr=0.000116248, gnorm=0.404, clip=0, loss_scale=8, train_wall=85, gb_free=15.5, wall=12795
2023-09-01 15:15:00 | INFO | train_inner | epoch 009:    301 / 1826 loss=1.887, trans_loss=3.243, nll_loss=1.42, w2v_ctc_loss=1.064, task_loss=3.301, contrastive_loss=0, total=4021.68, n_correct=2606.49, ppl=2.68, accuracy=64.811, wps=14086, ups=1.16, wpb=12092.8, bsz=445.4, num_updates=14900, lr=0.000115857, gnorm=0.406, clip=0, loss_scale=8, train_wall=85, gb_free=15.4, wall=12881
2023-09-01 15:16:26 | INFO | train_inner | epoch 009:    401 / 1826 loss=1.885, trans_loss=3.244, nll_loss=1.422, w2v_ctc_loss=1.063, task_loss=3.334, contrastive_loss=0, total=3977.48, n_correct=2584.08, ppl=2.68, accuracy=64.968, wps=13936.7, ups=1.16, wpb=11963.7, bsz=437.7, num_updates=15000, lr=0.00011547, gnorm=0.4, clip=0, loss_scale=8, train_wall=85, gb_free=10.2, wall=12967
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:0')
2023-09-01 15:17:29 | INFO | train_inner | epoch 009:    501 / 1826 loss=2.028, trans_loss=4.867, nll_loss=2.144, w2v_ctc_loss=0.819, task_loss=5.282, contrastive_loss=0, total=3929.06, n_correct=2537.88, ppl=4.42, accuracy=64.593, wps=12637.5, ups=1.6, wpb=7898.8, bsz=274.6, num_updates=15100, lr=0.000115087, gnorm=0.565, clip=0, loss_scale=8, train_wall=62, gb_free=11.4, wall=13029
2023-09-01 15:18:31 | INFO | train_inner | epoch 009:    601 / 1826 loss=2.019, trans_loss=4.888, nll_loss=2.15, w2v_ctc_loss=0.795, task_loss=4.889, contrastive_loss=0, total=3990.97, n_correct=2583.16, ppl=4.44, accuracy=64.725, wps=12814.7, ups=1.61, wpb=7981.9, bsz=293.4, num_updates=15200, lr=0.000114708, gnorm=0.549, clip=0, loss_scale=8, train_wall=62, gb_free=17.3, wall=13091
2023-09-01 15:19:33 | INFO | train_inner | epoch 009:    701 / 1826 loss=2.019, trans_loss=4.881, nll_loss=2.142, w2v_ctc_loss=0.803, task_loss=5.22, contrastive_loss=0, total=3920.39, n_correct=2545.97, ppl=4.41, accuracy=64.942, wps=12625, ups=1.61, wpb=7840.8, bsz=279.5, num_updates=15300, lr=0.000114332, gnorm=0.535, clip=0, loss_scale=8, train_wall=61, gb_free=14.3, wall=13154
2023-09-01 15:20:35 | INFO | train_inner | epoch 009:    801 / 1826 loss=2.031, trans_loss=4.898, nll_loss=2.163, w2v_ctc_loss=0.816, task_loss=5.183, contrastive_loss=0, total=3935.56, n_correct=2535.15, ppl=4.48, accuracy=64.416, wps=12658.6, ups=1.61, wpb=7871.1, bsz=284.5, num_updates=15400, lr=0.000113961, gnorm=0.594, clip=0, loss_scale=8, train_wall=62, gb_free=16, wall=13216
2023-09-01 15:21:38 | INFO | train_inner | epoch 009:    901 / 1826 loss=2.027, trans_loss=4.888, nll_loss=2.151, w2v_ctc_loss=0.817, task_loss=5.186, contrastive_loss=0, total=3924.5, n_correct=2541.86, ppl=4.44, accuracy=64.769, wps=12550.7, ups=1.6, wpb=7849, bsz=283.7, num_updates=15500, lr=0.000113592, gnorm=0.556, clip=0, loss_scale=8, train_wall=62, gb_free=10.2, wall=13278
2023-09-01 15:22:41 | INFO | train_inner | epoch 009:   1001 / 1826 loss=2.026, trans_loss=4.884, nll_loss=2.146, w2v_ctc_loss=0.817, task_loss=5.07, contrastive_loss=0, total=3972.59, n_correct=2568.22, ppl=4.42, accuracy=64.649, wps=12630.1, ups=1.59, wpb=7945.2, bsz=292.2, num_updates=15600, lr=0.000113228, gnorm=0.538, clip=0, loss_scale=8, train_wall=62, gb_free=14.7, wall=13341
2023-09-01 15:23:43 | INFO | train_inner | epoch 009:   1101 / 1826 loss=2.026, trans_loss=4.888, nll_loss=2.151, w2v_ctc_loss=0.815, task_loss=5.244, contrastive_loss=0, total=3941, n_correct=2545.17, ppl=4.44, accuracy=64.582, wps=12735.5, ups=1.62, wpb=7882, bsz=283.6, num_updates=15700, lr=0.000112867, gnorm=0.556, clip=0, loss_scale=8, train_wall=61, gb_free=15, wall=13403
2023-09-01 15:24:45 | INFO | train_inner | epoch 009:   1201 / 1826 loss=2.019, trans_loss=4.877, nll_loss=2.137, w2v_ctc_loss=0.808, task_loss=5.123, contrastive_loss=0, total=4009.72, n_correct=2605.16, ppl=4.4, accuracy=64.971, wps=12869.3, ups=1.6, wpb=8019.4, bsz=287.6, num_updates=15800, lr=0.000112509, gnorm=0.544, clip=0, loss_scale=8, train_wall=62, gb_free=12.4, wall=13465
2023-09-01 15:25:47 | INFO | train_inner | epoch 009:   1301 / 1826 loss=2.027, trans_loss=4.886, nll_loss=2.149, w2v_ctc_loss=0.817, task_loss=5.417, contrastive_loss=0, total=3908.1, n_correct=2533.29, ppl=4.44, accuracy=64.822, wps=12542.1, ups=1.6, wpb=7816.2, bsz=272, num_updates=15900, lr=0.000112154, gnorm=0.543, clip=0, loss_scale=8, train_wall=62, gb_free=15.4, wall=13528
2023-09-01 15:26:50 | INFO | train_inner | epoch 009:   1401 / 1826 loss=2.021, trans_loss=4.888, nll_loss=2.151, w2v_ctc_loss=0.808, task_loss=4.987, contrastive_loss=0, total=3974.66, n_correct=2572.04, ppl=4.44, accuracy=64.711, wps=12732.5, ups=1.6, wpb=7949.3, bsz=291.4, num_updates=16000, lr=0.000111803, gnorm=0.551, clip=0, loss_scale=8, train_wall=62, gb_free=17.4, wall=13590
2023-09-01 15:26:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:5')
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 15:27:29 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 3.877 | trans_loss 5.11 | nll_loss 2.389 | w2v_ctc_loss 1.392 | task_loss 19.098 | contrastive_loss 0 | total 3505.91 | n_correct 2360.91 | ppl 5.24 | accuracy 67.341 | uer 18.823 | wer 20.754 | raw_wer 20.754 | bleu 29.02 | wps 1179.9 | wpb 3505.9 | bsz 119.3 | num_updates 16000 | best_bleu 29.02
2023-09-01 15:27:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16000 updates
2023-09-01 15:27:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_9_16000.pt
2023-09-01 15:27:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_9_16000.pt
2023-09-01 15:27:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_9_16000.pt (epoch 9 @ 16000 updates, score 29.02) (writing took 10.911517119006021 seconds)
2023-09-01 15:28:42 | INFO | train_inner | epoch 009:   1501 / 1826 loss=2.028, trans_loss=4.883, nll_loss=2.145, w2v_ctc_loss=0.828, task_loss=5.279, contrastive_loss=0, total=3942.41, n_correct=2552.26, ppl=4.42, accuracy=64.739, wps=6992.2, ups=0.89, wpb=7884.8, bsz=282.3, num_updates=16100, lr=0.000111456, gnorm=0.579, clip=0, loss_scale=16, train_wall=61, gb_free=15.2, wall=13703
2023-09-01 15:29:44 | INFO | train_inner | epoch 009:   1601 / 1826 loss=2.013, trans_loss=4.872, nll_loss=2.131, w2v_ctc_loss=0.81, task_loss=4.705, contrastive_loss=0, total=4021.15, n_correct=2617.19, ppl=4.38, accuracy=65.086, wps=13118.1, ups=1.63, wpb=8042.3, bsz=306.6, num_updates=16200, lr=0.000111111, gnorm=0.545, clip=0, loss_scale=16, train_wall=61, gb_free=15.2, wall=13764
2023-09-01 15:30:45 | INFO | train_inner | epoch 009:   1701 / 1826 loss=2.025, trans_loss=4.879, nll_loss=2.14, w2v_ctc_loss=0.825, task_loss=5.424, contrastive_loss=0, total=3918.63, n_correct=2541.28, ppl=4.41, accuracy=64.851, wps=12718.3, ups=1.62, wpb=7837.3, bsz=272.4, num_updates=16300, lr=0.00011077, gnorm=0.55, clip=0, loss_scale=16, train_wall=61, gb_free=16.6, wall=13826
2023-09-01 15:31:47 | INFO | train_inner | epoch 009:   1801 / 1826 loss=2.027, trans_loss=4.885, nll_loss=2.148, w2v_ctc_loss=0.822, task_loss=5.494, contrastive_loss=0, total=3934.13, n_correct=2551.3, ppl=4.43, accuracy=64.85, wps=12693.5, ups=1.61, wpb=7868.3, bsz=270.5, num_updates=16400, lr=0.000110432, gnorm=0.555, clip=0, loss_scale=16, train_wall=61, gb_free=16.8, wall=13888
2023-09-01 15:32:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 15:32:42 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 3.891 | trans_loss 5.115 | nll_loss 2.399 | w2v_ctc_loss 1.428 | task_loss 19.133 | contrastive_loss 0 | total 3505.91 | n_correct 2356.45 | ppl 5.27 | accuracy 67.214 | uer 19.386 | wer 21.384 | raw_wer 21.384 | bleu 29.04 | wps 1197.7 | wpb 3505.9 | bsz 119.3 | num_updates 16425 | best_bleu 29.04
2023-09-01 15:32:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16425 updates
2023-09-01 15:32:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 15:32:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 15:32:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 9 @ 16425 updates, score 29.04) (writing took 11.076807390985778 seconds)
2023-09-01 15:32:53 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-09-01 15:32:53 | INFO | train | epoch 009 | loss 1.983 | trans_loss 4.394 | nll_loss 1.929 | w2v_ctc_loss 0.887 | task_loss 4.629 | contrastive_loss 0 | total 3956.37 | n_correct 2564.07 | ppl 3.81 | accuracy 64.809 | wps 12007.6 | ups 1.37 | wpb 8793.4 | bsz 316.5 | num_updates 16425 | lr 0.000110347 | gnorm 0.522 | clip 0 | loss_scale 16 | train_wall 1216 | gb_free 17.4 | wall 13954
2023-09-01 15:32:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 15:32:53 | INFO | fairseq.trainer | begin training epoch 10
2023-09-01 15:32:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 15:33:48 | INFO | train_inner | epoch 010:     75 / 1826 loss=2.01, trans_loss=4.863, nll_loss=2.119, w2v_ctc_loss=0.808, task_loss=5.071, contrastive_loss=0, total=3948.73, n_correct=2579.66, ppl=4.34, accuracy=65.329, wps=6538.3, ups=0.83, wpb=7897.5, bsz=287.7, num_updates=16500, lr=0.000110096, gnorm=0.545, clip=0, loss_scale=16, train_wall=62, gb_free=16.1, wall=14009
2023-09-01 15:34:50 | INFO | train_inner | epoch 010:    175 / 1826 loss=2.009, trans_loss=4.858, nll_loss=2.112, w2v_ctc_loss=0.805, task_loss=5.402, contrastive_loss=0, total=3931.53, n_correct=2570.46, ppl=4.32, accuracy=65.381, wps=12750.2, ups=1.62, wpb=7863.1, bsz=274.9, num_updates=16600, lr=0.000109764, gnorm=0.547, clip=0, loss_scale=16, train_wall=61, gb_free=11.8, wall=14070
2023-09-01 15:35:51 | INFO | train_inner | epoch 010:    275 / 1826 loss=1.994, trans_loss=4.841, nll_loss=2.091, w2v_ctc_loss=0.788, task_loss=5.076, contrastive_loss=0, total=3945.04, n_correct=2596.4, ppl=4.26, accuracy=65.814, wps=12906.6, ups=1.64, wpb=7890.1, bsz=282.5, num_updates=16700, lr=0.000109435, gnorm=0.544, clip=0, loss_scale=16, train_wall=61, gb_free=15.8, wall=14131
2023-09-01 15:36:53 | INFO | train_inner | epoch 010:    375 / 1826 loss=2.004, trans_loss=4.863, nll_loss=2.119, w2v_ctc_loss=0.795, task_loss=4.78, contrastive_loss=0, total=3996.07, n_correct=2607.9, ppl=4.34, accuracy=65.262, wps=12866.8, ups=1.61, wpb=7992.1, bsz=304.2, num_updates=16800, lr=0.000109109, gnorm=0.538, clip=0, loss_scale=16, train_wall=62, gb_free=16.5, wall=14194
2023-09-01 15:37:56 | INFO | train_inner | epoch 010:    475 / 1826 loss=2.004, trans_loss=4.857, nll_loss=2.111, w2v_ctc_loss=0.803, task_loss=5.053, contrastive_loss=0, total=3978.12, n_correct=2601.78, ppl=4.32, accuracy=65.402, wps=12728.5, ups=1.6, wpb=7956.2, bsz=289.2, num_updates=16900, lr=0.000108786, gnorm=0.555, clip=0, loss_scale=16, train_wall=62, gb_free=17.3, wall=14256
2023-09-01 15:38:58 | INFO | train_inner | epoch 010:    575 / 1826 loss=1.999, trans_loss=4.851, nll_loss=2.104, w2v_ctc_loss=0.796, task_loss=4.963, contrastive_loss=0, total=3964.3, n_correct=2601.15, ppl=4.3, accuracy=65.614, wps=12780.5, ups=1.61, wpb=7928.6, bsz=291.7, num_updates=17000, lr=0.000108465, gnorm=0.533, clip=0, loss_scale=16, train_wall=61, gb_free=17.4, wall=14318
2023-09-01 15:39:59 | INFO | train_inner | epoch 010:    675 / 1826 loss=2.004, trans_loss=4.855, nll_loss=2.11, w2v_ctc_loss=0.798, task_loss=5.213, contrastive_loss=0, total=3916.9, n_correct=2562.06, ppl=4.32, accuracy=65.41, wps=12652.3, ups=1.62, wpb=7833.8, bsz=282.3, num_updates=17100, lr=0.000108148, gnorm=0.552, clip=0, loss_scale=16, train_wall=61, gb_free=15.3, wall=14380
2023-09-01 15:41:01 | INFO | train_inner | epoch 010:    775 / 1826 loss=1.999, trans_loss=4.849, nll_loss=2.102, w2v_ctc_loss=0.795, task_loss=5.089, contrastive_loss=0, total=3952.03, n_correct=2593.23, ppl=4.29, accuracy=65.618, wps=12782.3, ups=1.62, wpb=7904.1, bsz=283.6, num_updates=17200, lr=0.000107833, gnorm=0.534, clip=0, loss_scale=16, train_wall=61, gb_free=16.2, wall=14442
2023-09-01 15:42:04 | INFO | train_inner | epoch 010:    875 / 1826 loss=2.008, trans_loss=4.855, nll_loss=2.11, w2v_ctc_loss=0.807, task_loss=5.528, contrastive_loss=0, total=3950.06, n_correct=2588.07, ppl=4.32, accuracy=65.52, wps=12655.5, ups=1.6, wpb=7900.1, bsz=273.7, num_updates=17300, lr=0.000107521, gnorm=0.553, clip=0, loss_scale=16, train_wall=62, gb_free=15.5, wall=14504
2023-09-01 15:43:06 | INFO | train_inner | epoch 010:    975 / 1826 loss=2.006, trans_loss=4.86, nll_loss=2.116, w2v_ctc_loss=0.804, task_loss=4.828, contrastive_loss=0, total=4054.65, n_correct=2649.81, ppl=4.33, accuracy=65.352, wps=13006.9, ups=1.6, wpb=8109.3, bsz=300.1, num_updates=17400, lr=0.000107211, gnorm=0.545, clip=0, loss_scale=16, train_wall=62, gb_free=16.4, wall=14567
2023-09-01 15:44:08 | INFO | train_inner | epoch 010:   1075 / 1826 loss=2.013, trans_loss=4.869, nll_loss=2.127, w2v_ctc_loss=0.81, task_loss=5.287, contrastive_loss=0, total=3915.5, n_correct=2553.81, ppl=4.37, accuracy=65.223, wps=12565.7, ups=1.6, wpb=7831, bsz=277.5, num_updates=17500, lr=0.000106904, gnorm=0.555, clip=0, loss_scale=16, train_wall=62, gb_free=10, wall=14629
2023-09-01 15:45:10 | INFO | train_inner | epoch 010:   1175 / 1826 loss=1.996, trans_loss=4.841, nll_loss=2.092, w2v_ctc_loss=0.796, task_loss=5.161, contrastive_loss=0, total=3940.51, n_correct=2595.57, ppl=4.26, accuracy=65.869, wps=12742.1, ups=1.62, wpb=7881, bsz=282, num_updates=17600, lr=0.0001066, gnorm=0.542, clip=0, loss_scale=16, train_wall=61, gb_free=14.3, wall=14691
2023-09-01 15:46:12 | INFO | train_inner | epoch 010:   1275 / 1826 loss=2.003, trans_loss=4.854, nll_loss=2.11, w2v_ctc_loss=0.805, task_loss=4.883, contrastive_loss=0, total=4017.71, n_correct=2633.64, ppl=4.32, accuracy=65.551, wps=12970.8, ups=1.61, wpb=8035.4, bsz=299.2, num_updates=17700, lr=0.000106299, gnorm=0.537, clip=0, loss_scale=16, train_wall=61, gb_free=11.7, wall=14753
2023-09-01 15:47:14 | INFO | train_inner | epoch 010:   1375 / 1826 loss=2.006, trans_loss=4.851, nll_loss=2.103, w2v_ctc_loss=0.805, task_loss=5.573, contrastive_loss=0, total=3918.39, n_correct=2571.58, ppl=4.3, accuracy=65.628, wps=12597.7, ups=1.61, wpb=7836.8, bsz=266, num_updates=17800, lr=0.000106, gnorm=0.548, clip=0, loss_scale=16, train_wall=62, gb_free=14.1, wall=14815
2023-09-01 15:48:17 | INFO | train_inner | epoch 010:   1475 / 1826 loss=1.995, trans_loss=4.836, nll_loss=2.086, w2v_ctc_loss=0.793, task_loss=5.368, contrastive_loss=0, total=3908.82, n_correct=2575.75, ppl=4.24, accuracy=65.896, wps=12587.4, ups=1.61, wpb=7817.6, bsz=271.1, num_updates=17900, lr=0.000105703, gnorm=0.541, clip=0, loss_scale=16, train_wall=62, gb_free=15.6, wall=14877
2023-09-01 15:49:19 | INFO | train_inner | epoch 010:   1575 / 1826 loss=1.991, trans_loss=4.833, nll_loss=2.082, w2v_ctc_loss=0.792, task_loss=5.031, contrastive_loss=0, total=3958.9, n_correct=2610.59, ppl=4.23, accuracy=65.942, wps=12739.5, ups=1.61, wpb=7917.8, bsz=285.8, num_updates=18000, lr=0.000105409, gnorm=0.556, clip=0, loss_scale=16, train_wall=62, gb_free=14.9, wall=14939
2023-09-01 15:49:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 15:49:59 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 3.85 | trans_loss 5.082 | nll_loss 2.353 | w2v_ctc_loss 1.364 | task_loss 19.018 | contrastive_loss 0 | total 3505.91 | n_correct 2374.91 | ppl 5.11 | accuracy 67.74 | uer 18.458 | wer 20.153 | raw_wer 20.153 | bleu 29.47 | wps 1174.5 | wpb 3505.9 | bsz 119.3 | num_updates 18000 | best_bleu 29.47
2023-09-01 15:49:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18000 updates
2023-09-01 15:49:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_10_18000.pt
2023-09-01 15:50:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_10_18000.pt
2023-09-01 15:50:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_10_18000.pt (epoch 10 @ 18000 updates, score 29.47) (writing took 11.440312872000504 seconds)
2023-09-01 15:51:13 | INFO | train_inner | epoch 010:   1675 / 1826 loss=2.002, trans_loss=4.85, nll_loss=2.103, w2v_ctc_loss=0.802, task_loss=5.302, contrastive_loss=0, total=3939.68, n_correct=2589.73, ppl=4.3, accuracy=65.735, wps=6886, ups=0.87, wpb=7879.4, bsz=278.2, num_updates=18100, lr=0.000105118, gnorm=0.55, clip=0, loss_scale=16, train_wall=61, gb_free=14.4, wall=15054
2023-09-01 15:52:15 | INFO | train_inner | epoch 010:   1775 / 1826 loss=1.994, trans_loss=4.853, nll_loss=2.107, w2v_ctc_loss=0.783, task_loss=4.77, contrastive_loss=0, total=3997.55, n_correct=2623.18, ppl=4.31, accuracy=65.62, wps=12848.2, ups=1.61, wpb=7995.1, bsz=302.4, num_updates=18200, lr=0.000104828, gnorm=0.534, clip=0, loss_scale=32, train_wall=62, gb_free=17, wall=15116
2023-09-01 15:52:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 15:53:26 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 3.852 | trans_loss 5.084 | nll_loss 2.356 | w2v_ctc_loss 1.368 | task_loss 19.013 | contrastive_loss 0 | total 3505.91 | n_correct 2380.09 | ppl 5.12 | accuracy 67.888 | uer 18.952 | wer 20.611 | raw_wer 20.611 | bleu 29.72 | wps 1198 | wpb 3505.9 | bsz 119.3 | num_updates 18251 | best_bleu 29.72
2023-09-01 15:53:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18251 updates
2023-09-01 15:53:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 15:53:32 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 15:53:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 10 @ 18251 updates, score 29.72) (writing took 11.07556674501393 seconds)
2023-09-01 15:53:37 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-09-01 15:53:37 | INFO | train | epoch 010 | loss 2.002 | trans_loss 4.852 | nll_loss 2.105 | w2v_ctc_loss 0.799 | task_loss 5.132 | contrastive_loss 0 | total 3956.37 | n_correct 2594.11 | ppl 4.3 | accuracy 65.568 | wps 11616.6 | ups 1.47 | wpb 7912.7 | bsz 284.8 | num_updates 18251 | lr 0.000104682 | gnorm 0.545 | clip 0 | loss_scale 32 | train_wall 1122 | gb_free 17 | wall 15198
2023-09-01 15:53:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 15:53:37 | INFO | fairseq.trainer | begin training epoch 11
2023-09-01 15:53:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 15:54:15 | INFO | train_inner | epoch 011:     49 / 1826 loss=1.994, trans_loss=4.84, nll_loss=2.091, w2v_ctc_loss=0.793, task_loss=5.211, contrastive_loss=0, total=3902.57, n_correct=2571.44, ppl=4.26, accuracy=65.891, wps=6522.6, ups=0.84, wpb=7805.1, bsz=280.6, num_updates=18300, lr=0.000104542, gnorm=0.551, clip=0, loss_scale=32, train_wall=61, gb_free=14.7, wall=15236
2023-09-01 15:55:19 | INFO | train_inner | epoch 011:    149 / 1826 loss=1.983, trans_loss=4.826, nll_loss=2.072, w2v_ctc_loss=0.78, task_loss=5.176, contrastive_loss=0, total=3987.55, n_correct=2636.82, ppl=4.2, accuracy=66.126, wps=12539, ups=1.57, wpb=7975.1, bsz=288.5, num_updates=18400, lr=0.000104257, gnorm=0.541, clip=0, loss_scale=32, train_wall=63, gb_free=15.3, wall=15299
2023-09-01 15:56:20 | INFO | train_inner | epoch 011:    249 / 1826 loss=1.988, trans_loss=4.827, nll_loss=2.074, w2v_ctc_loss=0.788, task_loss=5.26, contrastive_loss=0, total=3914.16, n_correct=2587.61, ppl=4.21, accuracy=66.109, wps=12713.3, ups=1.62, wpb=7828.3, bsz=278.6, num_updates=18500, lr=0.000103975, gnorm=0.549, clip=0, loss_scale=32, train_wall=61, gb_free=17.3, wall=15361
2023-09-01 15:57:22 | INFO | train_inner | epoch 011:    349 / 1826 loss=1.981, trans_loss=4.82, nll_loss=2.065, w2v_ctc_loss=0.779, task_loss=5.246, contrastive_loss=0, total=3936.35, n_correct=2607.01, ppl=4.18, accuracy=66.229, wps=12816.1, ups=1.63, wpb=7872.7, bsz=276.7, num_updates=18600, lr=0.000103695, gnorm=0.549, clip=0, loss_scale=32, train_wall=61, gb_free=16.1, wall=15422
2023-09-01 15:58:24 | INFO | train_inner | epoch 011:    449 / 1826 loss=1.982, trans_loss=4.82, nll_loss=2.065, w2v_ctc_loss=0.783, task_loss=5.301, contrastive_loss=0, total=3953.38, n_correct=2619.49, ppl=4.18, accuracy=66.26, wps=12672.9, ups=1.6, wpb=7906.8, bsz=281, num_updates=18700, lr=0.000103418, gnorm=0.549, clip=0, loss_scale=32, train_wall=62, gb_free=16.1, wall=15485
2023-09-01 15:59:26 | INFO | train_inner | epoch 011:    549 / 1826 loss=1.982, trans_loss=4.821, nll_loss=2.066, w2v_ctc_loss=0.783, task_loss=5.378, contrastive_loss=0, total=3939.99, n_correct=2615.04, ppl=4.19, accuracy=66.372, wps=12675, ups=1.61, wpb=7880, bsz=276.4, num_updates=18800, lr=0.000103142, gnorm=0.551, clip=0, loss_scale=32, train_wall=62, gb_free=15.3, wall=15547
2023-09-01 16:00:29 | INFO | train_inner | epoch 011:    649 / 1826 loss=1.989, trans_loss=4.827, nll_loss=2.075, w2v_ctc_loss=0.796, task_loss=5.378, contrastive_loss=0, total=3914.2, n_correct=2587.96, ppl=4.21, accuracy=66.117, wps=12492, ups=1.6, wpb=7828.4, bsz=276.9, num_updates=18900, lr=0.000102869, gnorm=0.546, clip=0, loss_scale=32, train_wall=62, gb_free=15.2, wall=15609
2023-09-01 16:01:31 | INFO | train_inner | epoch 011:    749 / 1826 loss=1.982, trans_loss=4.828, nll_loss=2.075, w2v_ctc_loss=0.782, task_loss=5.123, contrastive_loss=0, total=3946.93, n_correct=2613.31, ppl=4.21, accuracy=66.211, wps=12753.9, ups=1.62, wpb=7893.9, bsz=283, num_updates=19000, lr=0.000102598, gnorm=0.556, clip=0, loss_scale=32, train_wall=61, gb_free=17.5, wall=15671
2023-09-01 16:02:32 | INFO | train_inner | epoch 011:    849 / 1826 loss=1.985, trans_loss=4.829, nll_loss=2.076, w2v_ctc_loss=0.782, task_loss=5.289, contrastive_loss=0, total=3921.76, n_correct=2590.3, ppl=4.22, accuracy=66.049, wps=12742.1, ups=1.62, wpb=7843.5, bsz=278.4, num_updates=19100, lr=0.000102329, gnorm=0.548, clip=0, loss_scale=32, train_wall=61, gb_free=17.2, wall=15733
2023-09-01 16:03:35 | INFO | train_inner | epoch 011:    949 / 1826 loss=1.989, trans_loss=4.832, nll_loss=2.081, w2v_ctc_loss=0.787, task_loss=5.166, contrastive_loss=0, total=4002.83, n_correct=2641.66, ppl=4.23, accuracy=65.995, wps=12881.3, ups=1.61, wpb=8005.7, bsz=290.2, num_updates=19200, lr=0.000102062, gnorm=0.539, clip=0, loss_scale=32, train_wall=61, gb_free=16.8, wall=15795
2023-09-01 16:04:37 | INFO | train_inner | epoch 011:   1049 / 1826 loss=1.989, trans_loss=4.824, nll_loss=2.07, w2v_ctc_loss=0.797, task_loss=5.349, contrastive_loss=0, total=3960.75, n_correct=2620.85, ppl=4.2, accuracy=66.171, wps=12724.6, ups=1.61, wpb=7921.5, bsz=277.2, num_updates=19300, lr=0.000101797, gnorm=0.55, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=15857
2023-09-01 16:04:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-01 16:05:40 | INFO | train_inner | epoch 011:   1150 / 1826 loss=1.984, trans_loss=4.827, nll_loss=2.075, w2v_ctc_loss=0.781, task_loss=5.272, contrastive_loss=0, total=3944.53, n_correct=2610.04, ppl=4.21, accuracy=66.169, wps=12505.6, ups=1.59, wpb=7889.1, bsz=276.4, num_updates=19400, lr=0.000101535, gnorm=0.546, clip=0, loss_scale=16, train_wall=62, gb_free=15.7, wall=15920
2023-09-01 16:06:42 | INFO | train_inner | epoch 011:   1250 / 1826 loss=1.991, trans_loss=4.826, nll_loss=2.073, w2v_ctc_loss=0.797, task_loss=5.656, contrastive_loss=0, total=3869.34, n_correct=2554.96, ppl=4.21, accuracy=66.031, wps=12535.4, ups=1.62, wpb=7738.7, bsz=265.7, num_updates=19500, lr=0.000101274, gnorm=0.566, clip=0, loss_scale=16, train_wall=61, gb_free=12.1, wall=15982
2023-09-01 16:07:43 | INFO | train_inner | epoch 011:   1350 / 1826 loss=1.982, trans_loss=4.828, nll_loss=2.076, w2v_ctc_loss=0.785, task_loss=4.806, contrastive_loss=0, total=3983.96, n_correct=2634.88, ppl=4.22, accuracy=66.137, wps=13054, ups=1.64, wpb=7967.9, bsz=297.1, num_updates=19600, lr=0.000101015, gnorm=0.547, clip=0, loss_scale=16, train_wall=60, gb_free=17.1, wall=16043
2023-09-01 16:08:45 | INFO | train_inner | epoch 011:   1450 / 1826 loss=1.981, trans_loss=4.826, nll_loss=2.073, w2v_ctc_loss=0.785, task_loss=4.989, contrastive_loss=0, total=4000.92, n_correct=2651.96, ppl=4.21, accuracy=66.284, wps=12803.2, ups=1.6, wpb=8001.8, bsz=293.5, num_updates=19700, lr=0.000100759, gnorm=0.537, clip=0, loss_scale=16, train_wall=62, gb_free=15.6, wall=16106
2023-09-01 16:09:47 | INFO | train_inner | epoch 011:   1550 / 1826 loss=1.981, trans_loss=4.828, nll_loss=2.077, w2v_ctc_loss=0.784, task_loss=4.752, contrastive_loss=0, total=4013.12, n_correct=2660.34, ppl=4.22, accuracy=66.291, wps=12962.9, ups=1.62, wpb=8026.2, bsz=304.4, num_updates=19800, lr=0.000100504, gnorm=0.529, clip=0, loss_scale=16, train_wall=61, gb_free=15.5, wall=16168
2023-09-01 16:10:50 | INFO | train_inner | epoch 011:   1650 / 1826 loss=1.984, trans_loss=4.827, nll_loss=2.075, w2v_ctc_loss=0.789, task_loss=4.863, contrastive_loss=0, total=3975.76, n_correct=2635.8, ppl=4.21, accuracy=66.297, wps=12627.1, ups=1.59, wpb=7951.5, bsz=290.5, num_updates=19900, lr=0.000100251, gnorm=0.533, clip=0, loss_scale=16, train_wall=62, gb_free=16, wall=16231
2023-09-01 16:11:53 | INFO | train_inner | epoch 011:   1750 / 1826 loss=1.978, trans_loss=4.828, nll_loss=2.077, w2v_ctc_loss=0.774, task_loss=4.802, contrastive_loss=0, total=4033.09, n_correct=2670.29, ppl=4.22, accuracy=66.21, wps=12840.8, ups=1.59, wpb=8066.2, bsz=306.4, num_updates=20000, lr=0.0001, gnorm=0.545, clip=0, loss_scale=16, train_wall=62, gb_free=15.5, wall=16293
2023-09-01 16:11:53 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 16:12:31 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 3.845 | trans_loss 5.07 | nll_loss 2.341 | w2v_ctc_loss 1.378 | task_loss 19.042 | contrastive_loss 0 | total 3505.91 | n_correct 2385 | ppl 5.07 | accuracy 68.028 | uer 18.372 | wer 20.029 | raw_wer 20.029 | bleu 29.86 | wps 1217.8 | wpb 3505.9 | bsz 119.3 | num_updates 20000 | best_bleu 29.86
2023-09-01 16:12:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 20000 updates
2023-09-01 16:12:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_11_20000.pt
2023-09-01 16:12:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_11_20000.pt
2023-09-01 16:12:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_11_20000.pt (epoch 11 @ 20000 updates, score 29.86) (writing took 11.998653895017924 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:0')
2023-09-01 16:13:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:1')
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 16:14:09 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 3.842 | trans_loss 5.063 | nll_loss 2.333 | w2v_ctc_loss 1.38 | task_loss 19.066 | contrastive_loss 0 | total 3505.91 | n_correct 2386.09 | ppl 5.04 | accuracy 68.059 | uer 18.297 | wer 20.048 | raw_wer 20.048 | bleu 29.79 | wps 1193.3 | wpb 3505.9 | bsz 119.3 | num_updates 20076 | best_bleu 29.86
2023-09-01 16:14:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 20076 updates
2023-09-01 16:14:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_29.7907.pt
2023-09-01 16:14:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_29.7907.pt
2023-09-01 16:14:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_29.7907.pt (epoch 11 @ 20076 updates, score 29.79) (writing took 7.138673376990482 seconds)
2023-09-01 16:14:17 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-09-01 16:14:17 | INFO | train | epoch 011 | loss 1.984 | trans_loss 4.826 | nll_loss 2.073 | w2v_ctc_loss 0.785 | task_loss 5.144 | contrastive_loss 0 | total 3956.57 | n_correct 2619.09 | ppl 4.21 | accuracy 66.196 | wps 11644.3 | ups 1.47 | wpb 7913.1 | bsz 284.9 | num_updates 20076 | lr 9.98105e-05 | gnorm 0.545 | clip 0 | loss_scale 16 | train_wall 1124 | gb_free 15.6 | wall 16438
2023-09-01 16:14:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 16:14:18 | INFO | fairseq.trainer | begin training epoch 12
2023-09-01 16:14:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 16:14:40 | INFO | train_inner | epoch 012:     24 / 1826 loss=1.969, trans_loss=4.81, nll_loss=2.053, w2v_ctc_loss=0.769, task_loss=4.883, contrastive_loss=0, total=3952.69, n_correct=2632.73, ppl=4.15, accuracy=66.606, wps=4720, ups=0.6, wpb=7905.4, bsz=289, num_updates=20100, lr=9.97509e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=61, gb_free=16.8, wall=16461
2023-09-01 16:15:42 | INFO | train_inner | epoch 012:    124 / 1826 loss=1.977, trans_loss=4.81, nll_loss=2.052, w2v_ctc_loss=0.782, task_loss=5.587, contrastive_loss=0, total=3862.92, n_correct=2570.11, ppl=4.15, accuracy=66.533, wps=12602.5, ups=1.63, wpb=7725.8, bsz=267, num_updates=20200, lr=9.95037e-05, gnorm=0.551, clip=0, loss_scale=16, train_wall=61, gb_free=15.1, wall=16522
2023-09-01 16:16:44 | INFO | train_inner | epoch 012:    224 / 1826 loss=1.964, trans_loss=4.803, nll_loss=2.043, w2v_ctc_loss=0.762, task_loss=4.885, contrastive_loss=0, total=3993.93, n_correct=2666.86, ppl=4.12, accuracy=66.773, wps=12908.9, ups=1.62, wpb=7987.9, bsz=292.7, num_updates=20300, lr=9.92583e-05, gnorm=0.561, clip=0, loss_scale=16, train_wall=61, gb_free=11.7, wall=16584
2023-09-01 16:17:46 | INFO | train_inner | epoch 012:    324 / 1826 loss=1.962, trans_loss=4.801, nll_loss=2.041, w2v_ctc_loss=0.76, task_loss=4.974, contrastive_loss=0, total=3964.38, n_correct=2649.89, ppl=4.11, accuracy=66.842, wps=12748.6, ups=1.61, wpb=7928.8, bsz=290.2, num_updates=20400, lr=9.90148e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=62, gb_free=16.1, wall=16646
2023-09-01 16:18:48 | INFO | train_inner | epoch 012:    424 / 1826 loss=1.968, trans_loss=4.805, nll_loss=2.046, w2v_ctc_loss=0.773, task_loss=5.02, contrastive_loss=0, total=3984.39, n_correct=2660.14, ppl=4.13, accuracy=66.764, wps=12899.4, ups=1.62, wpb=7968.8, bsz=293.8, num_updates=20500, lr=9.8773e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=61, gb_free=16.3, wall=16708
2023-09-01 16:19:49 | INFO | train_inner | epoch 012:    524 / 1826 loss=1.964, trans_loss=4.797, nll_loss=2.035, w2v_ctc_loss=0.769, task_loss=5.131, contrastive_loss=0, total=3956.92, n_correct=2643.89, ppl=4.1, accuracy=66.817, wps=12858.1, ups=1.62, wpb=7913.8, bsz=283.7, num_updates=20600, lr=9.85329e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=61, gb_free=15.9, wall=16770
2023-09-01 16:20:51 | INFO | train_inner | epoch 012:    624 / 1826 loss=1.964, trans_loss=4.798, nll_loss=2.037, w2v_ctc_loss=0.767, task_loss=5.195, contrastive_loss=0, total=3953.03, n_correct=2645.17, ppl=4.11, accuracy=66.915, wps=12836.6, ups=1.62, wpb=7906.1, bsz=280.1, num_updates=20700, lr=9.82946e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=61, gb_free=14.4, wall=16831
2023-09-01 16:21:53 | INFO | train_inner | epoch 012:    724 / 1826 loss=1.972, trans_loss=4.806, nll_loss=2.048, w2v_ctc_loss=0.776, task_loss=5.21, contrastive_loss=0, total=3952.49, n_correct=2634.67, ppl=4.14, accuracy=66.658, wps=12699.4, ups=1.61, wpb=7905, bsz=282.3, num_updates=20800, lr=9.80581e-05, gnorm=0.552, clip=0, loss_scale=16, train_wall=62, gb_free=17, wall=16893
2023-09-01 16:22:55 | INFO | train_inner | epoch 012:    824 / 1826 loss=1.977, trans_loss=4.808, nll_loss=2.05, w2v_ctc_loss=0.78, task_loss=5.617, contrastive_loss=0, total=3920.99, n_correct=2609.15, ppl=4.14, accuracy=66.543, wps=12583.3, ups=1.6, wpb=7842, bsz=265.2, num_updates=20900, lr=9.78232e-05, gnorm=0.543, clip=0, loss_scale=16, train_wall=62, gb_free=15.9, wall=16956
2023-09-01 16:23:57 | INFO | train_inner | epoch 012:    924 / 1826 loss=1.955, trans_loss=4.793, nll_loss=2.032, w2v_ctc_loss=0.757, task_loss=4.427, contrastive_loss=0, total=4064.59, n_correct=2720.76, ppl=4.09, accuracy=66.938, wps=13102.8, ups=1.61, wpb=8129.2, bsz=315.7, num_updates=21000, lr=9.759e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=61, gb_free=17.1, wall=17018
2023-09-01 16:24:59 | INFO | train_inner | epoch 012:   1024 / 1826 loss=1.949, trans_loss=4.785, nll_loss=2.021, w2v_ctc_loss=0.749, task_loss=4.632, contrastive_loss=0, total=4027.9, n_correct=2704.82, ppl=4.06, accuracy=67.152, wps=13130.7, ups=1.63, wpb=8055.8, bsz=307.1, num_updates=21100, lr=9.73585e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=61, gb_free=16, wall=17079
2023-09-01 16:26:00 | INFO | train_inner | epoch 012:   1124 / 1826 loss=1.964, trans_loss=4.795, nll_loss=2.033, w2v_ctc_loss=0.771, task_loss=5.062, contrastive_loss=0, total=3982.26, n_correct=2663.29, ppl=4.09, accuracy=66.879, wps=12890.4, ups=1.62, wpb=7964.5, bsz=286.6, num_updates=21200, lr=9.71286e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=61, gb_free=15.7, wall=17141
2023-09-01 16:27:03 | INFO | train_inner | epoch 012:   1224 / 1826 loss=1.971, trans_loss=4.811, nll_loss=2.055, w2v_ctc_loss=0.775, task_loss=5.188, contrastive_loss=0, total=3949.09, n_correct=2634.44, ppl=4.16, accuracy=66.71, wps=12674.1, ups=1.6, wpb=7898.2, bsz=285.4, num_updates=21300, lr=9.69003e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=62, gb_free=12.3, wall=17203
2023-09-01 16:28:05 | INFO | train_inner | epoch 012:   1324 / 1826 loss=1.962, trans_loss=4.804, nll_loss=2.045, w2v_ctc_loss=0.758, task_loss=4.958, contrastive_loss=0, total=3963.34, n_correct=2644.25, ppl=4.13, accuracy=66.718, wps=12809.1, ups=1.62, wpb=7926.7, bsz=290.9, num_updates=21400, lr=9.66736e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=61, gb_free=12.8, wall=17265
2023-09-01 16:29:07 | INFO | train_inner | epoch 012:   1424 / 1826 loss=1.974, trans_loss=4.803, nll_loss=2.044, w2v_ctc_loss=0.781, task_loss=5.668, contrastive_loss=0, total=3926.15, n_correct=2617.07, ppl=4.12, accuracy=66.657, wps=12675.6, ups=1.61, wpb=7852.3, bsz=267.9, num_updates=21500, lr=9.64486e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=61, gb_free=15.7, wall=17327
2023-09-01 16:30:09 | INFO | train_inner | epoch 012:   1524 / 1826 loss=1.976, trans_loss=4.828, nll_loss=2.077, w2v_ctc_loss=0.775, task_loss=4.884, contrastive_loss=0, total=4009.37, n_correct=2661.2, ppl=4.22, accuracy=66.375, wps=12920.8, ups=1.61, wpb=8018.7, bsz=300.2, num_updates=21600, lr=9.6225e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=62, gb_free=15.4, wall=17389
2023-09-01 16:31:11 | INFO | train_inner | epoch 012:   1624 / 1826 loss=1.981, trans_loss=4.816, nll_loss=2.061, w2v_ctc_loss=0.784, task_loss=5.853, contrastive_loss=0, total=3883.94, n_correct=2579.19, ppl=4.17, accuracy=66.407, wps=12480.6, ups=1.61, wpb=7767.9, bsz=262.9, num_updates=21700, lr=9.60031e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=17451
2023-09-01 16:32:13 | INFO | train_inner | epoch 012:   1724 / 1826 loss=1.975, trans_loss=4.813, nll_loss=2.057, w2v_ctc_loss=0.788, task_loss=5.17, contrastive_loss=0, total=3919.88, n_correct=2611.03, ppl=4.16, accuracy=66.61, wps=12647.7, ups=1.61, wpb=7839.8, bsz=283, num_updates=21800, lr=9.57826e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=61, gb_free=14.9, wall=17513
2023-09-01 16:33:15 | INFO | train_inner | epoch 012:   1824 / 1826 loss=1.961, trans_loss=4.792, nll_loss=2.03, w2v_ctc_loss=0.76, task_loss=5.523, contrastive_loss=0, total=3908.91, n_correct=2618.48, ppl=4.09, accuracy=66.987, wps=12558.8, ups=1.61, wpb=7817.8, bsz=269, num_updates=21900, lr=9.55637e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=62, gb_free=16.1, wall=17576
2023-09-01 16:33:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 16:33:55 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 3.831 | trans_loss 5.045 | nll_loss 2.308 | w2v_ctc_loss 1.384 | task_loss 19.095 | contrastive_loss 0 | total 3505.91 | n_correct 2398.64 | ppl 4.95 | accuracy 68.417 | uer 18.305 | wer 19.977 | raw_wer 19.977 | bleu 30.24 | wps 1206 | wpb 3505.9 | bsz 119.3 | num_updates 21902 | best_bleu 30.24
2023-09-01 16:33:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 21902 updates
2023-09-01 16:33:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 16:34:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt
2023-09-01 16:34:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_best.pt (epoch 12 @ 21902 updates, score 30.24) (writing took 11.502800380025292 seconds)
2023-09-01 16:34:07 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-09-01 16:34:07 | INFO | train | epoch 012 | loss 1.967 | trans_loss 4.804 | nll_loss 2.045 | w2v_ctc_loss 0.77 | task_loss 5.141 | contrastive_loss 0 | total 3956.37 | n_correct 2640.61 | ppl 4.13 | accuracy 66.743 | wps 12143.4 | ups 1.53 | wpb 7912.7 | bsz 284.8 | num_updates 21902 | lr 9.55593e-05 | gnorm 0.537 | clip 0 | loss_scale 32 | train_wall 1120 | gb_free 10 | wall 17628
2023-09-01 16:34:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 16:34:07 | INFO | fairseq.trainer | begin training epoch 13
2023-09-01 16:34:07 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 16:35:16 | INFO | train_inner | epoch 013:     98 / 1826 loss=1.946, trans_loss=4.779, nll_loss=2.013, w2v_ctc_loss=0.747, task_loss=4.801, contrastive_loss=0, total=3964.32, n_correct=2668.74, ppl=4.04, accuracy=67.319, wps=6588.5, ups=0.83, wpb=7928.6, bsz=298.2, num_updates=22000, lr=9.53463e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=61, gb_free=17.3, wall=17696
2023-09-01 16:35:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 16:35:54 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.826 | trans_loss 5.047 | nll_loss 2.311 | w2v_ctc_loss 1.363 | task_loss 19.101 | contrastive_loss 0 | total 3505.91 | n_correct 2397.73 | ppl 4.96 | accuracy 68.391 | uer 17.798 | wer 19.59 | raw_wer 19.59 | bleu 29.91 | wps 1203.2 | wpb 3505.9 | bsz 119.3 | num_updates 22000 | best_bleu 30.24
2023-09-01 16:35:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 22000 updates
2023-09-01 16:35:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_13_22000.pt
2023-09-01 16:35:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_13_22000.pt
2023-09-01 16:36:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_13_22000.pt (epoch 13 @ 22000 updates, score 29.91) (writing took 9.82861379199312 seconds)
2023-09-01 16:37:10 | INFO | train_inner | epoch 013:    198 / 1826 loss=1.953, trans_loss=4.783, nll_loss=2.019, w2v_ctc_loss=0.759, task_loss=5.19, contrastive_loss=0, total=3968.9, n_correct=2669.95, ppl=4.05, accuracy=67.272, wps=6962.6, ups=0.88, wpb=7937.8, bsz=284.2, num_updates=22100, lr=9.51303e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=61, gb_free=15.5, wall=17810
2023-09-01 16:38:11 | INFO | train_inner | epoch 013:    298 / 1826 loss=1.949, trans_loss=4.783, nll_loss=2.018, w2v_ctc_loss=0.748, task_loss=5.136, contrastive_loss=0, total=3986, n_correct=2683.48, ppl=4.05, accuracy=67.323, wps=13009, ups=1.63, wpb=7972, bsz=284.1, num_updates=22200, lr=9.49158e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=61, gb_free=10.4, wall=17871
2023-09-01 16:39:14 | INFO | train_inner | epoch 013:    398 / 1826 loss=1.945, trans_loss=4.783, nll_loss=2.019, w2v_ctc_loss=0.739, task_loss=4.75, contrastive_loss=0, total=3998.65, n_correct=2684.64, ppl=4.05, accuracy=67.139, wps=12690.6, ups=1.59, wpb=7997.3, bsz=304.3, num_updates=22300, lr=9.47027e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=62, gb_free=16.3, wall=17934
2023-09-01 16:40:16 | INFO | train_inner | epoch 013:    498 / 1826 loss=1.956, trans_loss=4.782, nll_loss=2.017, w2v_ctc_loss=0.761, task_loss=5.207, contrastive_loss=0, total=3899.81, n_correct=2620.99, ppl=4.05, accuracy=67.208, wps=12602.8, ups=1.62, wpb=7799.6, bsz=284.4, num_updates=22400, lr=9.44911e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=61, gb_free=17.3, wall=17996
2023-09-01 16:41:17 | INFO | train_inner | epoch 013:    598 / 1826 loss=1.959, trans_loss=4.789, nll_loss=2.026, w2v_ctc_loss=0.765, task_loss=5.318, contrastive_loss=0, total=3901.11, n_correct=2615.88, ppl=4.07, accuracy=67.055, wps=12672, ups=1.62, wpb=7802.2, bsz=275.9, num_updates=22500, lr=9.42809e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=61, gb_free=15.7, wall=18058
2023-09-01 16:42:19 | INFO | train_inner | epoch 013:    698 / 1826 loss=1.964, trans_loss=4.789, nll_loss=2.026, w2v_ctc_loss=0.777, task_loss=5.461, contrastive_loss=0, total=3939.37, n_correct=2641.58, ppl=4.07, accuracy=67.056, wps=12725.4, ups=1.62, wpb=7878.7, bsz=273.3, num_updates=22600, lr=9.40721e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=61, gb_free=15.5, wall=18120
2023-09-01 16:43:22 | INFO | train_inner | epoch 013:    798 / 1826 loss=1.959, trans_loss=4.791, nll_loss=2.029, w2v_ctc_loss=0.762, task_loss=5.29, contrastive_loss=0, total=3978.46, n_correct=2664.93, ppl=4.08, accuracy=66.984, wps=12764.7, ups=1.6, wpb=7956.9, bsz=284.7, num_updates=22700, lr=9.38647e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=62, gb_free=15.6, wall=18182
2023-09-01 16:44:24 | INFO | train_inner | epoch 013:    898 / 1826 loss=1.953, trans_loss=4.781, nll_loss=2.015, w2v_ctc_loss=0.76, task_loss=5.103, contrastive_loss=0, total=3963.66, n_correct=2667.85, ppl=4.04, accuracy=67.308, wps=12687.9, ups=1.6, wpb=7927.3, bsz=283.7, num_updates=22800, lr=9.36586e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=62, gb_free=11.3, wall=18245
2023-09-01 16:45:25 | INFO | train_inner | epoch 013:    998 / 1826 loss=1.947, trans_loss=4.774, nll_loss=2.007, w2v_ctc_loss=0.749, task_loss=5.252, contrastive_loss=0, total=3903.96, n_correct=2630.08, ppl=4.02, accuracy=67.37, wps=12713.8, ups=1.63, wpb=7807.9, bsz=277.4, num_updates=22900, lr=9.34539e-05, gnorm=0.556, clip=0, loss_scale=32, train_wall=61, gb_free=15.2, wall=18306
2023-09-01 16:46:28 | INFO | train_inner | epoch 013:   1098 / 1826 loss=1.959, trans_loss=4.787, nll_loss=2.024, w2v_ctc_loss=0.764, task_loss=5.194, contrastive_loss=0, total=3984.36, n_correct=2670.88, ppl=4.07, accuracy=67.034, wps=12818.1, ups=1.61, wpb=7968.7, bsz=283.6, num_updates=23000, lr=9.32505e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=62, gb_free=16.6, wall=18368
2023-09-01 16:47:30 | INFO | train_inner | epoch 013:   1198 / 1826 loss=1.956, trans_loss=4.789, nll_loss=2.027, w2v_ctc_loss=0.761, task_loss=4.994, contrastive_loss=0, total=4004.3, n_correct=2684.14, ppl=4.08, accuracy=67.031, wps=12921, ups=1.61, wpb=8008.6, bsz=292, num_updates=23100, lr=9.30484e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=61, gb_free=16.1, wall=18430
2023-09-01 16:48:32 | INFO | train_inner | epoch 013:   1298 / 1826 loss=1.956, trans_loss=4.79, nll_loss=2.027, w2v_ctc_loss=0.757, task_loss=5.177, contrastive_loss=0, total=3983.14, n_correct=2676.04, ppl=4.08, accuracy=67.184, wps=12774.8, ups=1.6, wpb=7966.3, bsz=285, num_updates=23200, lr=9.28477e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=62, gb_free=16.7, wall=18492
2023-09-01 16:49:34 | INFO | train_inner | epoch 013:   1398 / 1826 loss=1.949, trans_loss=4.78, nll_loss=2.015, w2v_ctc_loss=0.753, task_loss=5.015, contrastive_loss=0, total=3950.68, n_correct=2662.16, ppl=4.04, accuracy=67.385, wps=12723.9, ups=1.61, wpb=7901.4, bsz=286.7, num_updates=23300, lr=9.26482e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=62, gb_free=14.5, wall=18555
2023-09-01 16:50:36 | INFO | train_inner | epoch 013:   1498 / 1826 loss=1.946, trans_loss=4.78, nll_loss=2.015, w2v_ctc_loss=0.744, task_loss=5.307, contrastive_loss=0, total=3943.21, n_correct=2655.75, ppl=4.04, accuracy=67.35, wps=12731.2, ups=1.61, wpb=7886.4, bsz=280.1, num_updates=23400, lr=9.245e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=18617
2023-09-01 16:50:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-01 16:51:38 | INFO | train_inner | epoch 013:   1599 / 1826 loss=1.954, trans_loss=4.779, nll_loss=2.014, w2v_ctc_loss=0.759, task_loss=5.379, contrastive_loss=0, total=3884, n_correct=2610.67, ppl=4.04, accuracy=67.216, wps=12460, ups=1.6, wpb=7768, bsz=272.8, num_updates=23500, lr=9.22531e-05, gnorm=0.548, clip=0, loss_scale=16, train_wall=62, gb_free=14.1, wall=18679
2023-09-01 16:52:41 | INFO | train_inner | epoch 013:   1699 / 1826 loss=1.956, trans_loss=4.785, nll_loss=2.021, w2v_ctc_loss=0.765, task_loss=5.163, contrastive_loss=0, total=3971.14, n_correct=2666.14, ppl=4.06, accuracy=67.138, wps=12750.3, ups=1.61, wpb=7942.3, bsz=286.1, num_updates=23600, lr=9.20575e-05, gnorm=0.546, clip=0, loss_scale=16, train_wall=62, gb_free=16.7, wall=18741
2023-09-01 16:53:43 | INFO | train_inner | epoch 013:   1799 / 1826 loss=1.949, trans_loss=4.794, nll_loss=2.034, w2v_ctc_loss=0.743, task_loss=4.753, contrastive_loss=0, total=4018.64, n_correct=2698.98, ppl=4.09, accuracy=67.162, wps=12988.9, ups=1.62, wpb=8037.3, bsz=301.2, num_updates=23700, lr=9.1863e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=61, gb_free=17.3, wall=18803
2023-09-01 16:53:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 16:54:38 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.831 | trans_loss 5.037 | nll_loss 2.301 | w2v_ctc_loss 1.405 | task_loss 19.053 | contrastive_loss 0 | total 3505.91 | n_correct 2403.91 | ppl 4.93 | accuracy 68.567 | uer 18.646 | wer 20.679 | raw_wer 20.679 | bleu 30.23 | wps 1196.4 | wpb 3505.9 | bsz 119.3 | num_updates 23727 | best_bleu 30.24
2023-09-01 16:54:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 23727 updates
2023-09-01 16:54:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.2305.pt
2023-09-01 16:54:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.2305.pt
2023-09-01 16:54:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.2305.pt (epoch 13 @ 23727 updates, score 30.23) (writing took 7.1858097259828355 seconds)
2023-09-01 16:54:46 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-09-01 16:54:46 | INFO | train | epoch 013 | loss 1.953 | trans_loss 4.784 | nll_loss 2.02 | w2v_ctc_loss 0.756 | task_loss 5.144 | contrastive_loss 0 | total 3956.67 | n_correct 2658.61 | ppl 4.06 | accuracy 67.193 | wps 11660.6 | ups 1.47 | wpb 7913.3 | bsz 284.9 | num_updates 23727 | lr 9.18108e-05 | gnorm 0.54 | clip 0 | loss_scale 16 | train_wall 1121 | gb_free 11.5 | wall 18866
2023-09-01 16:54:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 16:54:46 | INFO | fairseq.trainer | begin training epoch 14
2023-09-01 16:54:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 16:55:38 | INFO | train_inner | epoch 014:     73 / 1826 loss=1.945, trans_loss=4.767, nll_loss=1.998, w2v_ctc_loss=0.75, task_loss=5.384, contrastive_loss=0, total=3899.23, n_correct=2634.73, ppl=4, accuracy=67.571, wps=6727, ups=0.86, wpb=7798.5, bsz=272.6, num_updates=23800, lr=9.16698e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=61, gb_free=16.3, wall=18919
2023-09-01 16:56:40 | INFO | train_inner | epoch 014:    173 / 1826 loss=1.936, trans_loss=4.76, nll_loss=1.988, w2v_ctc_loss=0.741, task_loss=5.098, contrastive_loss=0, total=3935.95, n_correct=2665.84, ppl=3.97, accuracy=67.731, wps=12733.9, ups=1.62, wpb=7871.9, bsz=287.7, num_updates=23900, lr=9.14779e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=61, gb_free=16, wall=18981
2023-09-01 16:57:42 | INFO | train_inner | epoch 014:    273 / 1826 loss=1.936, trans_loss=4.764, nll_loss=1.995, w2v_ctc_loss=0.74, task_loss=4.904, contrastive_loss=0, total=3979.38, n_correct=2691.15, ppl=3.99, accuracy=67.627, wps=12872.7, ups=1.62, wpb=7958.8, bsz=296.7, num_updates=24000, lr=9.12871e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=61, gb_free=15.7, wall=19043
2023-09-01 16:57:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 16:58:21 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.817 | trans_loss 5.035 | nll_loss 2.297 | w2v_ctc_loss 1.36 | task_loss 19.015 | contrastive_loss 0 | total 3505.91 | n_correct 2406.18 | ppl 4.91 | accuracy 68.632 | uer 18.49 | wer 20.352 | raw_wer 20.352 | bleu 30.63 | wps 1198.6 | wpb 3505.9 | bsz 119.3 | num_updates 24000 | best_bleu 30.63
2023-09-01 16:58:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 24000 updates
2023-09-01 16:58:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_14_24000.pt
2023-09-01 16:58:23 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_14_24000.pt
2023-09-01 16:58:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_14_24000.pt (epoch 14 @ 24000 updates, score 30.63) (writing took 11.76305792297353 seconds)
2023-09-01 16:59:35 | INFO | train_inner | epoch 014:    373 / 1826 loss=1.939, trans_loss=4.768, nll_loss=2, w2v_ctc_loss=0.738, task_loss=5.24, contrastive_loss=0, total=3917.42, n_correct=2646.83, ppl=4, accuracy=67.566, wps=6946.6, ups=0.89, wpb=7834.8, bsz=280.6, num_updates=24100, lr=9.10975e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=61, gb_free=16.1, wall=19155
2023-09-01 17:00:37 | INFO | train_inner | epoch 014:    473 / 1826 loss=1.946, trans_loss=4.768, nll_loss=1.999, w2v_ctc_loss=0.753, task_loss=5.295, contrastive_loss=0, total=3905.2, n_correct=2635.66, ppl=4, accuracy=67.491, wps=12594, ups=1.61, wpb=7810.4, bsz=273.4, num_updates=24200, lr=9.09091e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=61, gb_free=16.5, wall=19217
2023-09-01 17:01:39 | INFO | train_inner | epoch 014:    573 / 1826 loss=1.939, trans_loss=4.766, nll_loss=1.997, w2v_ctc_loss=0.74, task_loss=5.242, contrastive_loss=0, total=3930.18, n_correct=2659.46, ppl=3.99, accuracy=67.668, wps=12601.4, ups=1.6, wpb=7860.4, bsz=278.7, num_updates=24300, lr=9.07218e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=62, gb_free=12.1, wall=19280
2023-09-01 17:02:41 | INFO | train_inner | epoch 014:    673 / 1826 loss=1.944, trans_loss=4.771, nll_loss=2.003, w2v_ctc_loss=0.75, task_loss=4.95, contrastive_loss=0, total=3952.35, n_correct=2665.7, ppl=4.01, accuracy=67.446, wps=12827.7, ups=1.62, wpb=7904.7, bsz=293.4, num_updates=24400, lr=9.05357e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=61, gb_free=17.4, wall=19341
2023-09-01 17:03:43 | INFO | train_inner | epoch 014:    773 / 1826 loss=1.929, trans_loss=4.758, nll_loss=1.987, w2v_ctc_loss=0.728, task_loss=4.93, contrastive_loss=0, total=4024.53, n_correct=2730.96, ppl=3.96, accuracy=67.858, wps=12988.9, ups=1.61, wpb=8049.1, bsz=293.9, num_updates=24500, lr=9.03508e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=61, gb_free=12.6, wall=19403
2023-09-01 17:04:45 | INFO | train_inner | epoch 014:    873 / 1826 loss=1.949, trans_loss=4.769, nll_loss=2, w2v_ctc_loss=0.755, task_loss=5.589, contrastive_loss=0, total=3990.77, n_correct=2693.11, ppl=4, accuracy=67.483, wps=12843.3, ups=1.61, wpb=7981.5, bsz=270.1, num_updates=24600, lr=9.0167e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=62, gb_free=16.8, wall=19466
2023-09-01 17:05:47 | INFO | train_inner | epoch 014:    973 / 1826 loss=1.935, trans_loss=4.766, nll_loss=1.997, w2v_ctc_loss=0.735, task_loss=4.801, contrastive_loss=0, total=4049.46, n_correct=2741.95, ppl=3.99, accuracy=67.711, wps=13018.1, ups=1.61, wpb=8098.9, bsz=303.6, num_updates=24700, lr=8.99843e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=62, gb_free=15.5, wall=19528
2023-09-01 17:06:50 | INFO | train_inner | epoch 014:   1073 / 1826 loss=1.937, trans_loss=4.759, nll_loss=1.988, w2v_ctc_loss=0.744, task_loss=5.162, contrastive_loss=0, total=3948.34, n_correct=2674.17, ppl=3.97, accuracy=67.729, wps=12631.1, ups=1.6, wpb=7896.7, bsz=283.4, num_updates=24800, lr=8.98027e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=62, gb_free=16, wall=19590
2023-09-01 17:07:52 | INFO | train_inner | epoch 014:   1173 / 1826 loss=1.944, trans_loss=4.771, nll_loss=2.004, w2v_ctc_loss=0.742, task_loss=5.239, contrastive_loss=0, total=3961.44, n_correct=2672.13, ppl=4.01, accuracy=67.454, wps=12793.1, ups=1.61, wpb=7922.9, bsz=282.8, num_updates=24900, lr=8.96221e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=61, gb_free=14.3, wall=19652
2023-09-01 17:08:53 | INFO | train_inner | epoch 014:   1273 / 1826 loss=1.935, trans_loss=4.764, nll_loss=1.994, w2v_ctc_loss=0.737, task_loss=5.012, contrastive_loss=0, total=3955.89, n_correct=2677.95, ppl=3.98, accuracy=67.695, wps=12933.1, ups=1.63, wpb=7911.8, bsz=289.3, num_updates=25000, lr=8.94427e-05, gnorm=0.529, clip=0, loss_scale=16, train_wall=61, gb_free=17, wall=19713
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:0')
2023-09-01 17:09:55 | INFO | train_inner | epoch 014:   1373 / 1826 loss=1.947, trans_loss=4.777, nll_loss=2.012, w2v_ctc_loss=0.749, task_loss=5.123, contrastive_loss=0, total=3930.24, n_correct=2643.41, ppl=4.03, accuracy=67.258, wps=12587, ups=1.6, wpb=7860.5, bsz=288.4, num_updates=25100, lr=8.92644e-05, gnorm=0.578, clip=0, loss_scale=16, train_wall=62, gb_free=16.3, wall=19776
2023-09-01 17:10:57 | INFO | train_inner | epoch 014:   1473 / 1826 loss=1.945, trans_loss=4.774, nll_loss=2.008, w2v_ctc_loss=0.748, task_loss=5.28, contrastive_loss=0, total=3957.41, n_correct=2671.43, ppl=4.02, accuracy=67.505, wps=12756.4, ups=1.61, wpb=7914.8, bsz=281, num_updates=25200, lr=8.90871e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=61, gb_free=15.5, wall=19838
2023-09-01 17:12:00 | INFO | train_inner | epoch 014:   1573 / 1826 loss=1.948, trans_loss=4.768, nll_loss=1.999, w2v_ctc_loss=0.754, task_loss=5.789, contrastive_loss=0, total=3882.9, n_correct=2625.52, ppl=4, accuracy=67.618, wps=12412, ups=1.6, wpb=7765.8, bsz=261.1, num_updates=25300, lr=8.89108e-05, gnorm=0.536, clip=0, loss_scale=16, train_wall=62, gb_free=16.3, wall=19901
2023-09-01 17:13:02 | INFO | train_inner | epoch 014:   1673 / 1826 loss=1.941, trans_loss=4.775, nll_loss=2.009, w2v_ctc_loss=0.747, task_loss=4.772, contrastive_loss=0, total=4028.71, n_correct=2718.61, ppl=4.02, accuracy=67.481, wps=13059.9, ups=1.62, wpb=8057.4, bsz=301.2, num_updates=25400, lr=8.87357e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=61, gb_free=15.3, wall=19962
2023-09-01 17:14:03 | INFO | train_inner | epoch 014:   1773 / 1826 loss=1.936, trans_loss=4.76, nll_loss=1.99, w2v_ctc_loss=0.74, task_loss=5.054, contrastive_loss=0, total=3970.96, n_correct=2693.53, ppl=3.97, accuracy=67.831, wps=12877, ups=1.62, wpb=7941.9, bsz=286.9, num_updates=25500, lr=8.85615e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=61, gb_free=17, wall=20024
2023-09-01 17:14:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:3')
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 17:15:15 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.816 | trans_loss 5.035 | nll_loss 2.3 | w2v_ctc_loss 1.359 | task_loss 19.122 | contrastive_loss 0 | total 3505.91 | n_correct 2398.09 | ppl 4.92 | accuracy 68.401 | uer 18.168 | wer 20.183 | raw_wer 20.183 | bleu 30.39 | wps 1186.4 | wpb 3505.9 | bsz 119.3 | num_updates 25553 | best_bleu 30.63
2023-09-01 17:15:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 25553 updates
2023-09-01 17:15:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.3900.pt
2023-09-01 17:15:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.3900.pt
2023-09-01 17:15:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.3900.pt (epoch 14 @ 25553 updates, score 30.39) (writing took 7.542774150002515 seconds)
2023-09-01 17:15:23 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-09-01 17:15:23 | INFO | train | epoch 014 | loss 1.94 | trans_loss 4.767 | nll_loss 1.998 | w2v_ctc_loss 0.744 | task_loss 5.144 | contrastive_loss 0 | total 3956.37 | n_correct 2674.55 | ppl 4 | accuracy 67.601 | wps 11676.7 | ups 1.48 | wpb 7912.7 | bsz 284.8 | num_updates 25553 | lr 8.84696e-05 | gnorm 0.531 | clip 0 | loss_scale 32 | train_wall 1122 | gb_free 16.9 | wall 20104
2023-09-01 17:15:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 17:15:23 | INFO | fairseq.trainer | begin training epoch 15
2023-09-01 17:15:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 17:16:00 | INFO | train_inner | epoch 015:     47 / 1826 loss=1.934, trans_loss=4.762, nll_loss=1.992, w2v_ctc_loss=0.741, task_loss=5.128, contrastive_loss=0, total=3892.38, n_correct=2641.31, ppl=3.98, accuracy=67.858, wps=6663.5, ups=0.86, wpb=7784.8, bsz=281.3, num_updates=25600, lr=8.83883e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=61, gb_free=14.6, wall=20141
2023-09-01 17:17:03 | INFO | train_inner | epoch 015:    147 / 1826 loss=1.93, trans_loss=4.75, nll_loss=1.976, w2v_ctc_loss=0.733, task_loss=5.186, contrastive_loss=0, total=3933.74, n_correct=2674.42, ppl=3.93, accuracy=67.987, wps=12590.9, ups=1.6, wpb=7867.5, bsz=280.2, num_updates=25700, lr=8.82162e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=62, gb_free=16.9, wall=20203
2023-09-01 17:18:05 | INFO | train_inner | epoch 015:    247 / 1826 loss=1.931, trans_loss=4.752, nll_loss=1.979, w2v_ctc_loss=0.734, task_loss=5.209, contrastive_loss=0, total=3951.59, n_correct=2683.56, ppl=3.94, accuracy=67.911, wps=12708.3, ups=1.61, wpb=7903.2, bsz=281.2, num_updates=25800, lr=8.80451e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=62, gb_free=15.3, wall=20265
2023-09-01 17:19:07 | INFO | train_inner | epoch 015:    347 / 1826 loss=1.924, trans_loss=4.739, nll_loss=1.962, w2v_ctc_loss=0.728, task_loss=5.116, contrastive_loss=0, total=3976.75, n_correct=2712.35, ppl=3.9, accuracy=68.205, wps=12882.6, ups=1.62, wpb=7953.5, bsz=284.7, num_updates=25900, lr=8.7875e-05, gnorm=0.522, clip=0, loss_scale=32, train_wall=61, gb_free=14.8, wall=20327
2023-09-01 17:20:08 | INFO | train_inner | epoch 015:    447 / 1826 loss=1.934, trans_loss=4.753, nll_loss=1.981, w2v_ctc_loss=0.737, task_loss=5.643, contrastive_loss=0, total=3878.95, n_correct=2635.04, ppl=3.95, accuracy=67.932, wps=12594.4, ups=1.62, wpb=7757.9, bsz=263.5, num_updates=26000, lr=8.77058e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=20389
2023-09-01 17:20:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 17:20:47 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.823 | trans_loss 5.033 | nll_loss 2.292 | w2v_ctc_loss 1.384 | task_loss 19.199 | contrastive_loss 0 | total 3505.91 | n_correct 2406.09 | ppl 4.9 | accuracy 68.63 | uer 17.98 | wer 19.965 | raw_wer 19.965 | bleu 30.33 | wps 1188.7 | wpb 3505.9 | bsz 119.3 | num_updates 26000 | best_bleu 30.63
2023-09-01 17:20:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 26000 updates
2023-09-01 17:20:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_15_26000.pt
2023-09-01 17:20:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_15_26000.pt
2023-09-01 17:20:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_15_26000.pt (epoch 15 @ 26000 updates, score 30.33) (writing took 6.612317057995824 seconds)
2023-09-01 17:21:59 | INFO | train_inner | epoch 015:    547 / 1826 loss=1.921, trans_loss=4.745, nll_loss=1.97, w2v_ctc_loss=0.72, task_loss=4.812, contrastive_loss=0, total=3934.71, n_correct=2680.41, ppl=3.92, accuracy=68.122, wps=7135.4, ups=0.91, wpb=7869.4, bsz=295.5, num_updates=26100, lr=8.75376e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=61, gb_free=13.5, wall=20499
2023-09-01 17:23:01 | INFO | train_inner | epoch 015:    647 / 1826 loss=1.927, trans_loss=4.746, nll_loss=1.972, w2v_ctc_loss=0.736, task_loss=4.908, contrastive_loss=0, total=3965.91, n_correct=2699.35, ppl=3.92, accuracy=68.064, wps=12659.8, ups=1.6, wpb=7931.8, bsz=293.5, num_updates=26200, lr=8.73704e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=62, gb_free=17.1, wall=20562
2023-09-01 17:24:02 | INFO | train_inner | epoch 015:    747 / 1826 loss=1.933, trans_loss=4.753, nll_loss=1.98, w2v_ctc_loss=0.736, task_loss=5.357, contrastive_loss=0, total=3895.08, n_correct=2648.99, ppl=3.95, accuracy=68.009, wps=12778.5, ups=1.64, wpb=7790.2, bsz=272, num_updates=26300, lr=8.72041e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=60, gb_free=11.4, wall=20623
2023-09-01 17:25:04 | INFO | train_inner | epoch 015:    847 / 1826 loss=1.933, trans_loss=4.748, nll_loss=1.974, w2v_ctc_loss=0.74, task_loss=5.526, contrastive_loss=0, total=3908.15, n_correct=2657.44, ppl=3.93, accuracy=67.997, wps=12704.8, ups=1.63, wpb=7816.3, bsz=269.5, num_updates=26400, lr=8.70388e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=61, gb_free=14.8, wall=20684
2023-09-01 17:26:06 | INFO | train_inner | epoch 015:    947 / 1826 loss=1.934, trans_loss=4.765, nll_loss=1.996, w2v_ctc_loss=0.733, task_loss=5.142, contrastive_loss=0, total=3982.31, n_correct=2693.55, ppl=3.99, accuracy=67.638, wps=12849.3, ups=1.61, wpb=7964.6, bsz=287.8, num_updates=26500, lr=8.68744e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=61, gb_free=16.1, wall=20746
2023-09-01 17:27:08 | INFO | train_inner | epoch 015:   1047 / 1826 loss=1.929, trans_loss=4.755, nll_loss=1.984, w2v_ctc_loss=0.73, task_loss=5.03, contrastive_loss=0, total=3986.82, n_correct=2709.14, ppl=3.96, accuracy=67.952, wps=12720.8, ups=1.6, wpb=7973.6, bsz=293.2, num_updates=26600, lr=8.6711e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=20809
2023-09-01 17:28:11 | INFO | train_inner | epoch 015:   1147 / 1826 loss=1.933, trans_loss=4.759, nll_loss=1.987, w2v_ctc_loss=0.736, task_loss=5.334, contrastive_loss=0, total=3982.39, n_correct=2705.14, ppl=3.97, accuracy=67.928, wps=12649.7, ups=1.59, wpb=7964.8, bsz=280.9, num_updates=26700, lr=8.65485e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=62, gb_free=16.9, wall=20872
2023-09-01 17:29:13 | INFO | train_inner | epoch 015:   1247 / 1826 loss=1.93, trans_loss=4.75, nll_loss=1.977, w2v_ctc_loss=0.737, task_loss=5.164, contrastive_loss=0, total=3965.88, n_correct=2699.62, ppl=3.94, accuracy=68.071, wps=12865.5, ups=1.62, wpb=7931.8, bsz=284, num_updates=26800, lr=8.63868e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=20933
2023-09-01 17:30:15 | INFO | train_inner | epoch 015:   1347 / 1826 loss=1.925, trans_loss=4.751, nll_loss=1.978, w2v_ctc_loss=0.723, task_loss=4.92, contrastive_loss=0, total=3978.53, n_correct=2700.37, ppl=3.94, accuracy=67.874, wps=12851.4, ups=1.62, wpb=7957.1, bsz=299.3, num_updates=26900, lr=8.62261e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=61, gb_free=17.3, wall=20995
2023-09-01 17:31:16 | INFO | train_inner | epoch 015:   1447 / 1826 loss=1.923, trans_loss=4.75, nll_loss=1.978, w2v_ctc_loss=0.729, task_loss=4.644, contrastive_loss=0, total=4031.16, n_correct=2746.53, ppl=3.94, accuracy=68.132, wps=13110.3, ups=1.63, wpb=8062.3, bsz=308.3, num_updates=27000, lr=8.60663e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=61, gb_free=15.3, wall=21057
2023-09-01 17:32:19 | INFO | train_inner | epoch 015:   1547 / 1826 loss=1.94, trans_loss=4.758, nll_loss=1.987, w2v_ctc_loss=0.752, task_loss=5.326, contrastive_loss=0, total=3916.89, n_correct=2657.59, ppl=3.96, accuracy=67.849, wps=12585.2, ups=1.61, wpb=7833.8, bsz=276.5, num_updates=27100, lr=8.59074e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=62, gb_free=16.6, wall=21119
2023-09-01 17:33:21 | INFO | train_inner | epoch 015:   1647 / 1826 loss=1.93, trans_loss=4.751, nll_loss=1.978, w2v_ctc_loss=0.736, task_loss=5.296, contrastive_loss=0, total=3982.71, n_correct=2709.16, ppl=3.94, accuracy=68.023, wps=12694.2, ups=1.59, wpb=7965.4, bsz=279.6, num_updates=27200, lr=8.57493e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=62, gb_free=15.8, wall=21182
2023-09-01 17:33:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-01 17:34:25 | INFO | train_inner | epoch 015:   1748 / 1826 loss=1.938, trans_loss=4.758, nll_loss=1.987, w2v_ctc_loss=0.745, task_loss=5.304, contrastive_loss=0, total=3981.99, n_correct=2704.82, ppl=3.96, accuracy=67.926, wps=12521.2, ups=1.57, wpb=7964, bsz=281.3, num_updates=27300, lr=8.55921e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=63, gb_free=15.1, wall=21246
2023-09-01 17:35:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 17:35:52 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.803 | trans_loss 5.019 | nll_loss 2.278 | w2v_ctc_loss 1.351 | task_loss 19.159 | contrastive_loss 0 | total 3505.91 | n_correct 2408.09 | ppl 4.85 | accuracy 68.687 | uer 17.739 | wer 19.571 | raw_wer 19.571 | bleu 30.37 | wps 1198.6 | wpb 3505.9 | bsz 119.3 | num_updates 27378 | best_bleu 30.63
2023-09-01 17:35:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 27378 updates
2023-09-01 17:35:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.3701.pt
2023-09-01 17:35:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.3701.pt
2023-09-01 17:35:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.3701.pt (epoch 15 @ 27378 updates, score 30.37) (writing took 6.563844372984022 seconds)
2023-09-01 17:35:59 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-09-01 17:35:59 | INFO | train | epoch 015 | loss 1.93 | trans_loss 4.752 | nll_loss 1.979 | w2v_ctc_loss 0.734 | task_loss 5.147 | contrastive_loss 0 | total 3956.55 | n_correct 2689.47 | ppl 3.94 | accuracy 67.975 | wps 11683.8 | ups 1.48 | wpb 7913.1 | bsz 284.8 | num_updates 27378 | lr 8.54701e-05 | gnorm 0.529 | clip 0 | loss_scale 16 | train_wall 1123 | gb_free 15.5 | wall 21340
2023-09-01 17:35:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 17:35:59 | INFO | fairseq.trainer | begin training epoch 16
2023-09-01 17:35:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 17:36:21 | INFO | train_inner | epoch 016:     22 / 1826 loss=1.928, trans_loss=4.754, nll_loss=1.982, w2v_ctc_loss=0.729, task_loss=4.976, contrastive_loss=0, total=3968.39, n_correct=2695.7, ppl=3.95, accuracy=67.929, wps=6863.7, ups=0.86, wpb=7936.8, bsz=290.5, num_updates=27400, lr=8.54358e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=61, gb_free=12.9, wall=21361
2023-09-01 17:37:22 | INFO | train_inner | epoch 016:    122 / 1826 loss=1.909, trans_loss=4.733, nll_loss=1.955, w2v_ctc_loss=0.706, task_loss=4.832, contrastive_loss=0, total=4003.43, n_correct=2739.67, ppl=3.88, accuracy=68.433, wps=12949.7, ups=1.62, wpb=8006.9, bsz=297.6, num_updates=27500, lr=8.52803e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=61, gb_free=16.6, wall=21423
2023-09-01 17:38:25 | INFO | train_inner | epoch 016:    222 / 1826 loss=1.921, trans_loss=4.741, nll_loss=1.965, w2v_ctc_loss=0.728, task_loss=4.944, contrastive_loss=0, total=3973, n_correct=2710.4, ppl=3.9, accuracy=68.22, wps=12734.1, ups=1.6, wpb=7946, bsz=293.2, num_updates=27600, lr=8.51257e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=62, gb_free=15.2, wall=21485
2023-09-01 17:39:27 | INFO | train_inner | epoch 016:    322 / 1826 loss=1.917, trans_loss=4.737, nll_loss=1.959, w2v_ctc_loss=0.715, task_loss=5.27, contrastive_loss=0, total=3907.57, n_correct=2665.72, ppl=3.89, accuracy=68.219, wps=12530.9, ups=1.6, wpb=7815.1, bsz=277.3, num_updates=27700, lr=8.49719e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=62, gb_free=15.9, wall=21548
2023-09-01 17:40:29 | INFO | train_inner | epoch 016:    422 / 1826 loss=1.915, trans_loss=4.739, nll_loss=1.963, w2v_ctc_loss=0.72, task_loss=4.859, contrastive_loss=0, total=3983.88, n_correct=2723.24, ppl=3.9, accuracy=68.356, wps=12982.2, ups=1.63, wpb=7967.8, bsz=298.2, num_updates=27800, lr=8.48189e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=61, gb_free=16.1, wall=21609
2023-09-01 17:41:30 | INFO | train_inner | epoch 016:    522 / 1826 loss=1.921, trans_loss=4.74, nll_loss=1.963, w2v_ctc_loss=0.719, task_loss=5.371, contrastive_loss=0, total=3942.7, n_correct=2690.72, ppl=3.9, accuracy=68.246, wps=12750.8, ups=1.62, wpb=7885.4, bsz=276.6, num_updates=27900, lr=8.46668e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=61, gb_free=17, wall=21671
2023-09-01 17:42:32 | INFO | train_inner | epoch 016:    622 / 1826 loss=1.922, trans_loss=4.737, nll_loss=1.959, w2v_ctc_loss=0.724, task_loss=5.227, contrastive_loss=0, total=3940.29, n_correct=2688.31, ppl=3.89, accuracy=68.226, wps=12761.8, ups=1.62, wpb=7880.6, bsz=283.8, num_updates=28000, lr=8.45154e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=61, gb_free=14.7, wall=21733
2023-09-01 17:42:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 17:43:12 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 3.834 | trans_loss 5.018 | nll_loss 2.275 | w2v_ctc_loss 1.457 | task_loss 19.207 | contrastive_loss 0 | total 3505.91 | n_correct 2409.82 | ppl 4.84 | accuracy 68.736 | uer 17.838 | wer 19.676 | raw_wer 19.676 | bleu 30.52 | wps 1174.4 | wpb 3505.9 | bsz 119.3 | num_updates 28000 | best_bleu 30.63
2023-09-01 17:43:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 28000 updates
2023-09-01 17:43:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_16_28000.pt
2023-09-01 17:43:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_16_28000.pt
2023-09-01 17:43:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_16_28000.pt (epoch 16 @ 28000 updates, score 30.52) (writing took 7.6415335920173675 seconds)
2023-09-01 17:44:22 | INFO | train_inner | epoch 016:    722 / 1826 loss=1.917, trans_loss=4.736, nll_loss=1.96, w2v_ctc_loss=0.722, task_loss=5.057, contrastive_loss=0, total=3964.31, n_correct=2711.2, ppl=3.89, accuracy=68.39, wps=7200.5, ups=0.91, wpb=7928.6, bsz=288.5, num_updates=28100, lr=8.43649e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=62, gb_free=16.2, wall=21843
2023-09-01 17:45:25 | INFO | train_inner | epoch 016:    822 / 1826 loss=1.921, trans_loss=4.734, nll_loss=1.956, w2v_ctc_loss=0.726, task_loss=5.425, contrastive_loss=0, total=3928.49, n_correct=2684.5, ppl=3.88, accuracy=68.334, wps=12619.8, ups=1.61, wpb=7857, bsz=274.1, num_updates=28200, lr=8.42152e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=62, gb_free=16.7, wall=21905
2023-09-01 17:46:27 | INFO | train_inner | epoch 016:    922 / 1826 loss=1.924, trans_loss=4.741, nll_loss=1.965, w2v_ctc_loss=0.729, task_loss=5.402, contrastive_loss=0, total=3936.55, n_correct=2685.15, ppl=3.9, accuracy=68.211, wps=12710.9, ups=1.61, wpb=7873.1, bsz=272.9, num_updates=28300, lr=8.40663e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=61, gb_free=16.5, wall=21967
2023-09-01 17:47:28 | INFO | train_inner | epoch 016:   1022 / 1826 loss=1.919, trans_loss=4.736, nll_loss=1.959, w2v_ctc_loss=0.723, task_loss=5.226, contrastive_loss=0, total=3931.11, n_correct=2685.56, ppl=3.89, accuracy=68.316, wps=12836.4, ups=1.63, wpb=7862.2, bsz=281.6, num_updates=28400, lr=8.39181e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=61, gb_free=17, wall=22028
2023-09-01 17:48:29 | INFO | train_inner | epoch 016:   1122 / 1826 loss=1.923, trans_loss=4.739, nll_loss=1.963, w2v_ctc_loss=0.727, task_loss=5.097, contrastive_loss=0, total=3983.75, n_correct=2716.26, ppl=3.9, accuracy=68.183, wps=13002.6, ups=1.63, wpb=7967.5, bsz=280.7, num_updates=28500, lr=8.37708e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=61, gb_free=16.7, wall=22090
2023-09-01 17:49:31 | INFO | train_inner | epoch 016:   1222 / 1826 loss=1.907, trans_loss=4.725, nll_loss=1.946, w2v_ctc_loss=0.705, task_loss=5.02, contrastive_loss=0, total=3959.67, n_correct=2716.28, ppl=3.85, accuracy=68.599, wps=12730.9, ups=1.61, wpb=7919.3, bsz=289.5, num_updates=28600, lr=8.36242e-05, gnorm=0.517, clip=0, loss_scale=16, train_wall=62, gb_free=16.7, wall=22152
2023-09-01 17:50:34 | INFO | train_inner | epoch 016:   1322 / 1826 loss=1.927, trans_loss=4.737, nll_loss=1.96, w2v_ctc_loss=0.734, task_loss=5.672, contrastive_loss=0, total=3904.89, n_correct=2663.37, ppl=3.89, accuracy=68.206, wps=12423.8, ups=1.59, wpb=7809.8, bsz=267.1, num_updates=28700, lr=8.34784e-05, gnorm=0.533, clip=0, loss_scale=16, train_wall=62, gb_free=15.9, wall=22215
2023-09-01 17:51:36 | INFO | train_inner | epoch 016:   1422 / 1826 loss=1.916, trans_loss=4.735, nll_loss=1.959, w2v_ctc_loss=0.716, task_loss=5.168, contrastive_loss=0, total=3969.02, n_correct=2709.8, ppl=3.89, accuracy=68.274, wps=12805.3, ups=1.61, wpb=7938, bsz=288.3, num_updates=28800, lr=8.33333e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=61, gb_free=16.9, wall=22277
2023-09-01 17:52:38 | INFO | train_inner | epoch 016:   1522 / 1826 loss=1.925, trans_loss=4.742, nll_loss=1.967, w2v_ctc_loss=0.736, task_loss=5.224, contrastive_loss=0, total=3950.42, n_correct=2691.68, ppl=3.91, accuracy=68.137, wps=12845.9, ups=1.63, wpb=7900.8, bsz=283.2, num_updates=28900, lr=8.3189e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=61, gb_free=16.3, wall=22338
2023-09-01 17:53:40 | INFO | train_inner | epoch 016:   1622 / 1826 loss=1.917, trans_loss=4.74, nll_loss=1.964, w2v_ctc_loss=0.719, task_loss=5.037, contrastive_loss=0, total=3987.85, n_correct=2722.11, ppl=3.9, accuracy=68.26, wps=12847.1, ups=1.61, wpb=7975.7, bsz=292.6, num_updates=29000, lr=8.30455e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=62, gb_free=16.3, wall=22400
2023-09-01 17:54:43 | INFO | train_inner | epoch 016:   1722 / 1826 loss=1.916, trans_loss=4.739, nll_loss=1.964, w2v_ctc_loss=0.719, task_loss=4.964, contrastive_loss=0, total=4019.17, n_correct=2740.79, ppl=3.9, accuracy=68.193, wps=12781.3, ups=1.59, wpb=8038.3, bsz=297.7, num_updates=29100, lr=8.29027e-05, gnorm=0.51, clip=0, loss_scale=16, train_wall=62, gb_free=17.5, wall=22463
2023-09-01 17:55:45 | INFO | train_inner | epoch 016:   1822 / 1826 loss=1.921, trans_loss=4.74, nll_loss=1.965, w2v_ctc_loss=0.728, task_loss=4.873, contrastive_loss=0, total=3975.07, n_correct=2716.48, ppl=3.9, accuracy=68.338, wps=12816.6, ups=1.61, wpb=7950.1, bsz=290.9, num_updates=29200, lr=8.27606e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=62, gb_free=16.2, wall=22525
2023-09-01 17:55:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 17:56:26 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 3.806 | trans_loss 5.018 | nll_loss 2.275 | w2v_ctc_loss 1.365 | task_loss 19.087 | contrastive_loss 0 | total 3505.91 | n_correct 2411.09 | ppl 4.84 | accuracy 68.772 | uer 17.728 | wer 19.605 | raw_wer 19.605 | bleu 30.56 | wps 1190 | wpb 3505.9 | bsz 119.3 | num_updates 29204 | best_bleu 30.63
2023-09-01 17:56:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 29204 updates
2023-09-01 17:56:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5604.pt
2023-09-01 17:56:28 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5604.pt
2023-09-01 17:56:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5604.pt (epoch 16 @ 29204 updates, score 30.56) (writing took 7.503338662994793 seconds)
2023-09-01 17:56:34 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-09-01 17:56:34 | INFO | train | epoch 016 | loss 1.919 | trans_loss 4.737 | nll_loss 1.961 | w2v_ctc_loss 0.722 | task_loss 5.144 | contrastive_loss 0 | total 3956.37 | n_correct 2701.81 | ppl 3.89 | accuracy 68.29 | wps 11700.9 | ups 1.48 | wpb 7912.7 | bsz 284.8 | num_updates 29204 | lr 8.27549e-05 | gnorm 0.525 | clip 0 | loss_scale 16 | train_wall 1122 | gb_free 15.6 | wall 22574
2023-09-01 17:56:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 17:56:34 | INFO | fairseq.trainer | begin training epoch 17
2023-09-01 17:56:34 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 17:57:40 | INFO | train_inner | epoch 017:     96 / 1826 loss=1.906, trans_loss=4.715, nll_loss=1.932, w2v_ctc_loss=0.71, task_loss=5.014, contrastive_loss=0, total=3925.48, n_correct=2697.93, ppl=3.82, accuracy=68.729, wps=6782.7, ups=0.86, wpb=7851, bsz=283.3, num_updates=29300, lr=8.26192e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=61, gb_free=16.7, wall=22641
2023-09-01 17:58:42 | INFO | train_inner | epoch 017:    196 / 1826 loss=1.893, trans_loss=4.702, nll_loss=1.915, w2v_ctc_loss=0.701, task_loss=4.743, contrastive_loss=0, total=3981.77, n_correct=2748.88, ppl=3.77, accuracy=69.037, wps=12999.9, ups=1.63, wpb=7963.5, bsz=300.5, num_updates=29400, lr=8.24786e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=61, gb_free=15.8, wall=22702
2023-09-01 17:59:43 | INFO | train_inner | epoch 017:    296 / 1826 loss=1.913, trans_loss=4.722, nll_loss=1.941, w2v_ctc_loss=0.718, task_loss=5.703, contrastive_loss=0, total=3872.12, n_correct=2658.96, ppl=3.84, accuracy=68.669, wps=12555.8, ups=1.62, wpb=7744.2, bsz=262.8, num_updates=29500, lr=8.23387e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=61, gb_free=16.7, wall=22764
2023-09-01 18:00:45 | INFO | train_inner | epoch 017:    396 / 1826 loss=1.91, trans_loss=4.718, nll_loss=1.936, w2v_ctc_loss=0.717, task_loss=5.249, contrastive_loss=0, total=3939.85, n_correct=2706.41, ppl=3.83, accuracy=68.693, wps=12683.2, ups=1.61, wpb=7879.7, bsz=283.5, num_updates=29600, lr=8.21995e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=62, gb_free=14.5, wall=22826
2023-09-01 18:01:48 | INFO | train_inner | epoch 017:    496 / 1826 loss=1.908, trans_loss=4.73, nll_loss=1.952, w2v_ctc_loss=0.71, task_loss=4.713, contrastive_loss=0, total=4011.56, n_correct=2748.43, ppl=3.87, accuracy=68.513, wps=12889.2, ups=1.61, wpb=8023.1, bsz=304.5, num_updates=29700, lr=8.2061e-05, gnorm=0.516, clip=0, loss_scale=32, train_wall=62, gb_free=16.6, wall=22888
2023-09-01 18:02:50 | INFO | train_inner | epoch 017:    596 / 1826 loss=1.908, trans_loss=4.724, nll_loss=1.944, w2v_ctc_loss=0.707, task_loss=5.052, contrastive_loss=0, total=3978.63, n_correct=2729.97, ppl=3.85, accuracy=68.616, wps=12750.9, ups=1.6, wpb=7957.3, bsz=287.7, num_updates=29800, lr=8.19232e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=62, gb_free=17.4, wall=22951
2023-09-01 18:03:52 | INFO | train_inner | epoch 017:    696 / 1826 loss=1.918, trans_loss=4.735, nll_loss=1.957, w2v_ctc_loss=0.721, task_loss=5.382, contrastive_loss=0, total=3916.31, n_correct=2676.44, ppl=3.88, accuracy=68.341, wps=12638.6, ups=1.61, wpb=7832.6, bsz=276.9, num_updates=29900, lr=8.17861e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=61, gb_free=16.9, wall=23013
2023-09-01 18:04:54 | INFO | train_inner | epoch 017:    796 / 1826 loss=1.915, trans_loss=4.733, nll_loss=1.955, w2v_ctc_loss=0.72, task_loss=5.399, contrastive_loss=0, total=3903, n_correct=2676.27, ppl=3.88, accuracy=68.57, wps=12630.9, ups=1.62, wpb=7806, bsz=276.5, num_updates=30000, lr=8.16497e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=23074
2023-09-01 18:04:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 18:05:33 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.813 | trans_loss 5.02 | nll_loss 2.279 | w2v_ctc_loss 1.38 | task_loss 19.018 | contrastive_loss 0 | total 3505.91 | n_correct 2409.91 | ppl 4.85 | accuracy 68.738 | uer 17.862 | wer 19.744 | raw_wer 19.744 | bleu 30.91 | wps 1190.5 | wpb 3505.9 | bsz 119.3 | num_updates 30000 | best_bleu 30.91
2023-09-01 18:05:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 30000 updates
2023-09-01 18:05:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_17_30000.pt
2023-09-01 18:05:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_17_30000.pt
2023-09-01 18:05:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_17_30000.pt (epoch 17 @ 30000 updates, score 30.91) (writing took 10.308675811014837 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:0')
2023-09-01 18:06:46 | INFO | train_inner | epoch 017:    896 / 1826 loss=1.914, trans_loss=4.727, nll_loss=1.947, w2v_ctc_loss=0.719, task_loss=5.285, contrastive_loss=0, total=3983.4, n_correct=2730.13, ppl=3.86, accuracy=68.538, wps=7115.6, ups=0.89, wpb=7966.8, bsz=278.3, num_updates=30100, lr=8.15139e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=62, gb_free=12.5, wall=23186
2023-09-01 18:07:48 | INFO | train_inner | epoch 017:    996 / 1826 loss=1.909, trans_loss=4.726, nll_loss=1.946, w2v_ctc_loss=0.707, task_loss=5.028, contrastive_loss=0, total=4001.1, n_correct=2742.71, ppl=3.85, accuracy=68.549, wps=12894.4, ups=1.61, wpb=8002.2, bsz=288.7, num_updates=30200, lr=8.13788e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=62, gb_free=14.1, wall=23248
2023-09-01 18:08:50 | INFO | train_inner | epoch 017:   1096 / 1826 loss=1.918, trans_loss=4.73, nll_loss=1.952, w2v_ctc_loss=0.72, task_loss=5.432, contrastive_loss=0, total=3928.39, n_correct=2689.38, ppl=3.87, accuracy=68.46, wps=12727.5, ups=1.62, wpb=7856.8, bsz=271.7, num_updates=30300, lr=8.12444e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=23310
2023-09-01 18:09:51 | INFO | train_inner | epoch 017:   1196 / 1826 loss=1.903, trans_loss=4.719, nll_loss=1.937, w2v_ctc_loss=0.702, task_loss=5.197, contrastive_loss=0, total=3937.57, n_correct=2709.88, ppl=3.83, accuracy=68.821, wps=12794.4, ups=1.62, wpb=7875.1, bsz=280.4, num_updates=30400, lr=8.11107e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=23372
2023-09-01 18:10:52 | INFO | train_inner | epoch 017:   1296 / 1826 loss=1.908, trans_loss=4.723, nll_loss=1.943, w2v_ctc_loss=0.715, task_loss=4.898, contrastive_loss=0, total=3991.29, n_correct=2739.18, ppl=3.85, accuracy=68.629, wps=13066.8, ups=1.64, wpb=7982.6, bsz=290.6, num_updates=30500, lr=8.09776e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=61, gb_free=15.2, wall=23433
2023-09-01 18:11:55 | INFO | train_inner | epoch 017:   1396 / 1826 loss=1.906, trans_loss=4.717, nll_loss=1.934, w2v_ctc_loss=0.71, task_loss=5.336, contrastive_loss=0, total=3921.13, n_correct=2694.78, ppl=3.82, accuracy=68.725, wps=12618.1, ups=1.61, wpb=7842.3, bsz=274.7, num_updates=30600, lr=8.08452e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=62, gb_free=17.4, wall=23495
2023-09-01 18:12:58 | INFO | train_inner | epoch 017:   1496 / 1826 loss=1.913, trans_loss=4.738, nll_loss=1.962, w2v_ctc_loss=0.713, task_loss=4.929, contrastive_loss=0, total=3959.99, n_correct=2704.66, ppl=3.9, accuracy=68.3, wps=12536.3, ups=1.58, wpb=7920, bsz=302.1, num_updates=30700, lr=8.07134e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=63, gb_free=14.2, wall=23558
2023-09-01 18:13:59 | INFO | train_inner | epoch 017:   1596 / 1826 loss=1.909, trans_loss=4.731, nll_loss=1.953, w2v_ctc_loss=0.707, task_loss=4.959, contrastive_loss=0, total=3988.94, n_correct=2732.69, ppl=3.87, accuracy=68.507, wps=12936.8, ups=1.62, wpb=7977.9, bsz=295.9, num_updates=30800, lr=8.05823e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=61, gb_free=16.8, wall=23620
2023-09-01 18:15:02 | INFO | train_inner | epoch 017:   1696 / 1826 loss=1.908, trans_loss=4.728, nll_loss=1.95, w2v_ctc_loss=0.706, task_loss=5.106, contrastive_loss=0, total=4009.04, n_correct=2748.48, ppl=3.86, accuracy=68.557, wps=12895.7, ups=1.61, wpb=8018.1, bsz=286.4, num_updates=30900, lr=8.04518e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=23682
2023-09-01 18:16:03 | INFO | train_inner | epoch 017:   1796 / 1826 loss=1.914, trans_loss=4.722, nll_loss=1.942, w2v_ctc_loss=0.724, task_loss=5.316, contrastive_loss=0, total=3940.88, n_correct=2704.93, ppl=3.84, accuracy=68.638, wps=12746.7, ups=1.62, wpb=7881.8, bsz=279.8, num_updates=31000, lr=8.03219e-05, gnorm=0.539, clip=0, loss_scale=32, train_wall=61, gb_free=16.5, wall=23744
2023-09-01 18:16:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:1')
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 18:17:01 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.804 | trans_loss 5.011 | nll_loss 2.266 | w2v_ctc_loss 1.373 | task_loss 18.983 | contrastive_loss 0 | total 3505.91 | n_correct 2420 | ppl 4.81 | accuracy 69.026 | uer 17.666 | wer 19.418 | raw_wer 19.418 | bleu 30.52 | wps 1196.8 | wpb 3505.9 | bsz 119.3 | num_updates 31030 | best_bleu 30.91
2023-09-01 18:17:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 31030 updates
2023-09-01 18:17:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5206.pt
2023-09-01 18:17:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5206.pt
2023-09-01 18:17:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5206.pt (epoch 17 @ 31030 updates, score 30.52) (writing took 7.938270806975197 seconds)
2023-09-01 18:17:10 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-09-01 18:17:10 | INFO | train | epoch 017 | loss 1.91 | trans_loss 4.724 | nll_loss 1.944 | w2v_ctc_loss 0.713 | task_loss 5.14 | contrastive_loss 0 | total 3956.37 | n_correct 2714.34 | ppl 3.85 | accuracy 68.607 | wps 11693.4 | ups 1.48 | wpb 7912.7 | bsz 284.8 | num_updates 31030 | lr 8.02831e-05 | gnorm 0.526 | clip 0 | loss_scale 32 | train_wall 1121 | gb_free 13.5 | wall 23810
2023-09-01 18:17:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 18:17:10 | INFO | fairseq.trainer | begin training epoch 18
2023-09-01 18:17:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 18:18:01 | INFO | train_inner | epoch 018:     70 / 1826 loss=1.895, trans_loss=4.708, nll_loss=1.923, w2v_ctc_loss=0.698, task_loss=4.792, contrastive_loss=0, total=4007.18, n_correct=2767.51, ppl=3.79, accuracy=69.064, wps=6813.6, ups=0.85, wpb=8014.4, bsz=299.7, num_updates=31100, lr=8.01927e-05, gnorm=0.526, clip=0, loss_scale=32, train_wall=62, gb_free=12.9, wall=23862
2023-09-01 18:19:03 | INFO | train_inner | epoch 018:    170 / 1826 loss=1.891, trans_loss=4.704, nll_loss=1.919, w2v_ctc_loss=0.69, task_loss=4.843, contrastive_loss=0, total=3997.3, n_correct=2762.76, ppl=3.78, accuracy=69.116, wps=12871.2, ups=1.61, wpb=7994.6, bsz=298.7, num_updates=31200, lr=8.00641e-05, gnorm=0.512, clip=0, loss_scale=32, train_wall=61, gb_free=15.1, wall=23924
2023-09-01 18:20:04 | INFO | train_inner | epoch 018:    270 / 1826 loss=1.893, trans_loss=4.706, nll_loss=1.921, w2v_ctc_loss=0.692, task_loss=4.892, contrastive_loss=0, total=4007.88, n_correct=2770.67, ppl=3.79, accuracy=69.131, wps=13095.3, ups=1.63, wpb=8015.8, bsz=292.2, num_updates=31300, lr=7.99361e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=61, gb_free=11.2, wall=23985
2023-09-01 18:21:07 | INFO | train_inner | epoch 018:    370 / 1826 loss=1.896, trans_loss=4.698, nll_loss=1.91, w2v_ctc_loss=0.702, task_loss=5.186, contrastive_loss=0, total=3957.19, n_correct=2733.07, ppl=3.76, accuracy=69.066, wps=12722.8, ups=1.61, wpb=7914.4, bsz=280.2, num_updates=31400, lr=7.98087e-05, gnorm=0.523, clip=0, loss_scale=64, train_wall=62, gb_free=15.1, wall=24047
2023-09-01 18:22:09 | INFO | train_inner | epoch 018:    470 / 1826 loss=1.904, trans_loss=4.711, nll_loss=1.927, w2v_ctc_loss=0.713, task_loss=5.145, contrastive_loss=0, total=3957.38, n_correct=2724.55, ppl=3.8, accuracy=68.847, wps=12743.8, ups=1.61, wpb=7914.8, bsz=285.8, num_updates=31500, lr=7.96819e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=61, gb_free=17.1, wall=24109
2023-09-01 18:23:11 | INFO | train_inner | epoch 018:    570 / 1826 loss=1.901, trans_loss=4.715, nll_loss=1.931, w2v_ctc_loss=0.706, task_loss=4.858, contrastive_loss=0, total=3997.5, n_correct=2749.05, ppl=3.81, accuracy=68.769, wps=12855.5, ups=1.61, wpb=7995, bsz=294.7, num_updates=31600, lr=7.95557e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=62, gb_free=16.3, wall=24171
2023-09-01 18:24:13 | INFO | train_inner | epoch 018:    670 / 1826 loss=1.906, trans_loss=4.715, nll_loss=1.932, w2v_ctc_loss=0.709, task_loss=5.492, contrastive_loss=0, total=3915.37, n_correct=2690.21, ppl=3.82, accuracy=68.709, wps=12524.7, ups=1.6, wpb=7830.7, bsz=271.9, num_updates=31700, lr=7.94301e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=62, gb_free=14.7, wall=24234
2023-09-01 18:25:15 | INFO | train_inner | epoch 018:    770 / 1826 loss=1.905, trans_loss=4.717, nll_loss=1.935, w2v_ctc_loss=0.704, task_loss=5.347, contrastive_loss=0, total=3952.92, n_correct=2715.89, ppl=3.82, accuracy=68.706, wps=12776.6, ups=1.62, wpb=7905.8, bsz=279.5, num_updates=31800, lr=7.93052e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=61, gb_free=12.5, wall=24296
2023-09-01 18:26:17 | INFO | train_inner | epoch 018:    870 / 1826 loss=1.895, trans_loss=4.706, nll_loss=1.92, w2v_ctc_loss=0.694, task_loss=5.19, contrastive_loss=0, total=3969.91, n_correct=2743.26, ppl=3.78, accuracy=69.101, wps=12869.4, ups=1.62, wpb=7939.8, bsz=281.6, num_updates=31900, lr=7.91808e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=61, gb_free=16.2, wall=24357
2023-09-01 18:27:20 | INFO | train_inner | epoch 018:    970 / 1826 loss=1.905, trans_loss=4.713, nll_loss=1.931, w2v_ctc_loss=0.717, task_loss=5.121, contrastive_loss=0, total=3934.97, n_correct=2709.86, ppl=3.81, accuracy=68.866, wps=12579.4, ups=1.6, wpb=7869.9, bsz=286.8, num_updates=32000, lr=7.90569e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=62, gb_free=15.8, wall=24420
2023-09-01 18:27:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 18:27:58 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.797 | trans_loss 5.004 | nll_loss 2.261 | w2v_ctc_loss 1.367 | task_loss 19.05 | contrastive_loss 0 | total 3505.91 | n_correct 2418.82 | ppl 4.79 | accuracy 68.993 | uer 17.527 | wer 19.35 | raw_wer 19.35 | bleu 30.61 | wps 1205.9 | wpb 3505.9 | bsz 119.3 | num_updates 32000 | best_bleu 30.91
2023-09-01 18:27:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 32000 updates
2023-09-01 18:27:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_18_32000.pt
2023-09-01 18:28:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_18_32000.pt
2023-09-01 18:28:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_18_32000.pt (epoch 18 @ 32000 updates, score 30.61) (writing took 7.193317555997055 seconds)
2023-09-01 18:29:08 | INFO | train_inner | epoch 018:   1070 / 1826 loss=1.894, trans_loss=4.712, nll_loss=1.929, w2v_ctc_loss=0.691, task_loss=4.811, contrastive_loss=0, total=4018.12, n_correct=2769.91, ppl=3.81, accuracy=68.935, wps=7405.7, ups=0.92, wpb=8036.2, bsz=303.8, num_updates=32100, lr=7.89337e-05, gnorm=0.518, clip=0, loss_scale=64, train_wall=62, gb_free=12.5, wall=24529
2023-09-01 18:30:11 | INFO | train_inner | epoch 018:   1170 / 1826 loss=1.897, trans_loss=4.706, nll_loss=1.921, w2v_ctc_loss=0.701, task_loss=5.158, contrastive_loss=0, total=3956.26, n_correct=2729.11, ppl=3.79, accuracy=68.982, wps=12649, ups=1.6, wpb=7912.5, bsz=283, num_updates=32200, lr=7.8811e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=62, gb_free=15.2, wall=24591
2023-09-01 18:31:12 | INFO | train_inner | epoch 018:   1270 / 1826 loss=1.902, trans_loss=4.711, nll_loss=1.927, w2v_ctc_loss=0.704, task_loss=5.252, contrastive_loss=0, total=3975.41, n_correct=2736.57, ppl=3.8, accuracy=68.837, wps=12855.2, ups=1.62, wpb=7950.8, bsz=278.7, num_updates=32300, lr=7.86889e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=61, gb_free=16.2, wall=24653
2023-09-01 18:32:14 | INFO | train_inner | epoch 018:   1370 / 1826 loss=1.897, trans_loss=4.718, nll_loss=1.936, w2v_ctc_loss=0.695, task_loss=5.012, contrastive_loss=0, total=3969.15, n_correct=2733.68, ppl=3.83, accuracy=68.873, wps=12814.9, ups=1.61, wpb=7938.3, bsz=291.1, num_updates=32400, lr=7.85674e-05, gnorm=0.519, clip=0, loss_scale=64, train_wall=61, gb_free=17.5, wall=24715
2023-09-01 18:33:16 | INFO | train_inner | epoch 018:   1470 / 1826 loss=1.9, trans_loss=4.709, nll_loss=1.925, w2v_ctc_loss=0.703, task_loss=5.102, contrastive_loss=0, total=3951.48, n_correct=2720.66, ppl=3.8, accuracy=68.852, wps=12777.2, ups=1.62, wpb=7903, bsz=284.5, num_updates=32500, lr=7.84465e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=15.3, wall=24777
2023-09-01 18:34:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-01 18:34:19 | INFO | train_inner | epoch 018:   1571 / 1826 loss=1.907, trans_loss=4.712, nll_loss=1.928, w2v_ctc_loss=0.712, task_loss=5.688, contrastive_loss=0, total=3821.28, n_correct=2632.51, ppl=3.81, accuracy=68.891, wps=12244.1, ups=1.6, wpb=7642.6, bsz=260.1, num_updates=32600, lr=7.8326e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=62, gb_free=14.5, wall=24839
2023-09-01 18:35:21 | INFO | train_inner | epoch 018:   1671 / 1826 loss=1.907, trans_loss=4.715, nll_loss=1.933, w2v_ctc_loss=0.716, task_loss=5.371, contrastive_loss=0, total=3922.44, n_correct=2696.95, ppl=3.82, accuracy=68.757, wps=12590.5, ups=1.6, wpb=7844.9, bsz=277.8, num_updates=32700, lr=7.82062e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=62, gb_free=17.3, wall=24902
2023-09-01 18:36:23 | INFO | train_inner | epoch 018:   1771 / 1826 loss=1.91, trans_loss=4.73, nll_loss=1.951, w2v_ctc_loss=0.709, task_loss=5.491, contrastive_loss=0, total=3899.6, n_correct=2673.53, ppl=3.87, accuracy=68.559, wps=12624.9, ups=1.62, wpb=7799.2, bsz=274.1, num_updates=32800, lr=7.80869e-05, gnorm=0.533, clip=0, loss_scale=32, train_wall=61, gb_free=13.9, wall=24963
2023-09-01 18:36:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 18:37:36 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.818 | trans_loss 5.011 | nll_loss 2.269 | w2v_ctc_loss 1.418 | task_loss 19.165 | contrastive_loss 0 | total 3505.91 | n_correct 2412.55 | ppl 4.82 | accuracy 68.814 | uer 17.854 | wer 19.785 | raw_wer 19.785 | bleu 30.54 | wps 1183.6 | wpb 3505.9 | bsz 119.3 | num_updates 32855 | best_bleu 30.91
2023-09-01 18:37:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 32855 updates
2023-09-01 18:37:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5407.pt
2023-09-01 18:37:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5407.pt
2023-09-01 18:37:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.5407.pt (epoch 18 @ 32855 updates, score 30.54) (writing took 6.288613721000729 seconds)
2023-09-01 18:37:42 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-09-01 18:37:42 | INFO | train | epoch 018 | loss 1.9 | trans_loss 4.712 | nll_loss 1.928 | w2v_ctc_loss 0.703 | task_loss 5.143 | contrastive_loss 0 | total 3956.04 | n_correct 2725.13 | ppl 3.81 | accuracy 68.885 | wps 11712.8 | ups 1.48 | wpb 7912.1 | bsz 284.7 | num_updates 32855 | lr 7.80215e-05 | gnorm 0.525 | clip 0 | loss_scale 32 | train_wall 1123 | gb_free 15.6 | wall 25043
2023-09-01 18:37:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 18:37:43 | INFO | fairseq.trainer | begin training epoch 19
2023-09-01 18:37:43 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 18:38:18 | INFO | train_inner | epoch 019:     45 / 1826 loss=1.899, trans_loss=4.715, nll_loss=1.932, w2v_ctc_loss=0.696, task_loss=5.109, contrastive_loss=0, total=3920.8, n_correct=2699.76, ppl=3.82, accuracy=68.857, wps=6799.4, ups=0.87, wpb=7841.6, bsz=281.7, num_updates=32900, lr=7.79681e-05, gnorm=0.527, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=25079
2023-09-01 18:39:20 | INFO | train_inner | epoch 019:    145 / 1826 loss=1.887, trans_loss=4.693, nll_loss=1.904, w2v_ctc_loss=0.689, task_loss=5.018, contrastive_loss=0, total=3999.19, n_correct=2768.72, ppl=3.74, accuracy=69.232, wps=12844.9, ups=1.61, wpb=7998.4, bsz=292.1, num_updates=33000, lr=7.78499e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=25141
2023-09-01 18:40:23 | INFO | train_inner | epoch 019:    245 / 1826 loss=1.887, trans_loss=4.691, nll_loss=1.902, w2v_ctc_loss=0.687, task_loss=5.233, contrastive_loss=0, total=3960.7, n_correct=2748.46, ppl=3.74, accuracy=69.393, wps=12710.6, ups=1.6, wpb=7921.4, bsz=279.6, num_updates=33100, lr=7.77322e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=62, gb_free=15.3, wall=25203
2023-09-01 18:41:25 | INFO | train_inner | epoch 019:    345 / 1826 loss=1.889, trans_loss=4.697, nll_loss=1.909, w2v_ctc_loss=0.689, task_loss=5.154, contrastive_loss=0, total=3949.38, n_correct=2737.76, ppl=3.76, accuracy=69.321, wps=12777.8, ups=1.62, wpb=7898.8, bsz=284.1, num_updates=33200, lr=7.76151e-05, gnorm=0.524, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=25265
2023-09-01 18:42:26 | INFO | train_inner | epoch 019:    445 / 1826 loss=1.888, trans_loss=4.693, nll_loss=1.904, w2v_ctc_loss=0.69, task_loss=5.041, contrastive_loss=0, total=3978.98, n_correct=2757.51, ppl=3.74, accuracy=69.302, wps=12896, ups=1.62, wpb=7958, bsz=287.6, num_updates=33300, lr=7.74984e-05, gnorm=0.531, clip=0, loss_scale=32, train_wall=61, gb_free=16.1, wall=25327
2023-09-01 18:43:28 | INFO | train_inner | epoch 019:    545 / 1826 loss=1.888, trans_loss=4.696, nll_loss=1.908, w2v_ctc_loss=0.687, task_loss=5.175, contrastive_loss=0, total=3975.57, n_correct=2750.31, ppl=3.75, accuracy=69.18, wps=12808.5, ups=1.61, wpb=7951.1, bsz=285.1, num_updates=33400, lr=7.73823e-05, gnorm=0.516, clip=0, loss_scale=32, train_wall=62, gb_free=13.3, wall=25389
2023-09-01 18:44:31 | INFO | train_inner | epoch 019:    645 / 1826 loss=1.892, trans_loss=4.705, nll_loss=1.92, w2v_ctc_loss=0.688, task_loss=5.148, contrastive_loss=0, total=3974.44, n_correct=2744.25, ppl=3.78, accuracy=69.047, wps=12729.6, ups=1.6, wpb=7948.9, bsz=287.9, num_updates=33500, lr=7.72667e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=62, gb_free=14.3, wall=25451
2023-09-01 18:45:33 | INFO | train_inner | epoch 019:    745 / 1826 loss=1.892, trans_loss=4.702, nll_loss=1.916, w2v_ctc_loss=0.692, task_loss=5.11, contrastive_loss=0, total=3997.11, n_correct=2765.27, ppl=3.77, accuracy=69.182, wps=12839.5, ups=1.61, wpb=7994.2, bsz=289.5, num_updates=33600, lr=7.71517e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=62, gb_free=16.5, wall=25514
2023-09-01 18:46:34 | INFO | train_inner | epoch 019:    845 / 1826 loss=1.903, trans_loss=4.71, nll_loss=1.926, w2v_ctc_loss=0.711, task_loss=5.248, contrastive_loss=0, total=3907.17, n_correct=2691.73, ppl=3.8, accuracy=68.892, wps=12727.4, ups=1.63, wpb=7814.3, bsz=279.6, num_updates=33700, lr=7.70371e-05, gnorm=0.549, clip=0, loss_scale=32, train_wall=61, gb_free=11.6, wall=25575
2023-09-01 18:47:36 | INFO | train_inner | epoch 019:    945 / 1826 loss=1.885, trans_loss=4.691, nll_loss=1.902, w2v_ctc_loss=0.685, task_loss=5.103, contrastive_loss=0, total=3939.76, n_correct=2731.07, ppl=3.74, accuracy=69.321, wps=12887.8, ups=1.64, wpb=7879.5, bsz=285.9, num_updates=33800, lr=7.69231e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=25636
2023-09-01 18:48:37 | INFO | train_inner | epoch 019:   1045 / 1826 loss=1.894, trans_loss=4.701, nll_loss=1.914, w2v_ctc_loss=0.7, task_loss=5.091, contrastive_loss=0, total=4013.68, n_correct=2773.29, ppl=3.77, accuracy=69.096, wps=12965.6, ups=1.62, wpb=8027.4, bsz=289.2, num_updates=33900, lr=7.68095e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=61, gb_free=11.7, wall=25698
2023-09-01 18:49:40 | INFO | train_inner | epoch 019:   1145 / 1826 loss=1.891, trans_loss=4.698, nll_loss=1.911, w2v_ctc_loss=0.694, task_loss=5.302, contrastive_loss=0, total=3884.31, n_correct=2688.2, ppl=3.76, accuracy=69.207, wps=12520.9, ups=1.61, wpb=7768.6, bsz=275.6, num_updates=34000, lr=7.66965e-05, gnorm=0.529, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=25760
2023-09-01 18:49:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 18:50:19 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.793 | trans_loss 5.007 | nll_loss 2.265 | w2v_ctc_loss 1.345 | task_loss 19.146 | contrastive_loss 0 | total 3505.91 | n_correct 2421.36 | ppl 4.81 | accuracy 69.065 | uer 17.43 | wer 19.29 | raw_wer 19.29 | bleu 31.1 | wps 1182.6 | wpb 3505.9 | bsz 119.3 | num_updates 34000 | best_bleu 31.1
2023-09-01 18:50:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 34000 updates
2023-09-01 18:50:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_19_34000.pt
2023-09-01 18:50:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_19_34000.pt
2023-09-01 18:50:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_19_34000.pt (epoch 19 @ 34000 updates, score 31.1) (writing took 11.644443264987785 seconds)
2023-09-01 18:51:33 | INFO | train_inner | epoch 019:   1245 / 1826 loss=1.891, trans_loss=4.701, nll_loss=1.915, w2v_ctc_loss=0.693, task_loss=4.937, contrastive_loss=0, total=3995.94, n_correct=2763.19, ppl=3.77, accuracy=69.15, wps=7063.2, ups=0.88, wpb=7991.9, bsz=298.1, num_updates=34100, lr=7.6584e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=61, gb_free=16.5, wall=25873
2023-09-01 18:52:34 | INFO | train_inner | epoch 019:   1345 / 1826 loss=1.898, trans_loss=4.7, nll_loss=1.913, w2v_ctc_loss=0.704, task_loss=5.383, contrastive_loss=0, total=3882.02, n_correct=2678.66, ppl=3.77, accuracy=69.002, wps=12606.8, ups=1.62, wpb=7764, bsz=276.1, num_updates=34200, lr=7.64719e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=61, gb_free=12.7, wall=25935
2023-09-01 18:53:36 | INFO | train_inner | epoch 019:   1445 / 1826 loss=1.892, trans_loss=4.704, nll_loss=1.919, w2v_ctc_loss=0.691, task_loss=5.107, contrastive_loss=0, total=3991.84, n_correct=2760.84, ppl=3.78, accuracy=69.162, wps=12874.4, ups=1.61, wpb=7983.7, bsz=289.4, num_updates=34300, lr=7.63604e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=61, gb_free=12.2, wall=25997
2023-09-01 18:54:38 | INFO | train_inner | epoch 019:   1545 / 1826 loss=1.897, trans_loss=4.705, nll_loss=1.92, w2v_ctc_loss=0.703, task_loss=5.341, contrastive_loss=0, total=3895.2, n_correct=2691.75, ppl=3.78, accuracy=69.104, wps=12715.5, ups=1.63, wpb=7790.4, bsz=272.3, num_updates=34400, lr=7.62493e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=26058
2023-09-01 18:55:39 | INFO | train_inner | epoch 019:   1645 / 1826 loss=1.894, trans_loss=4.705, nll_loss=1.921, w2v_ctc_loss=0.697, task_loss=4.977, contrastive_loss=0, total=3991.71, n_correct=2758.09, ppl=3.79, accuracy=69.095, wps=12900.3, ups=1.62, wpb=7983.4, bsz=290.2, num_updates=34500, lr=7.61387e-05, gnorm=0.522, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=26120
2023-09-01 18:56:42 | INFO | train_inner | epoch 019:   1745 / 1826 loss=1.897, trans_loss=4.713, nll_loss=1.931, w2v_ctc_loss=0.699, task_loss=5.076, contrastive_loss=0, total=3977.05, n_correct=2743.73, ppl=3.81, accuracy=68.989, wps=12705.1, ups=1.6, wpb=7954.1, bsz=289.7, num_updates=34600, lr=7.60286e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=62, gb_free=11.7, wall=26183
2023-09-01 18:57:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 18:58:11 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.804 | trans_loss 5.01 | nll_loss 2.267 | w2v_ctc_loss 1.376 | task_loss 19.202 | contrastive_loss 0 | total 3505.91 | n_correct 2415.73 | ppl 4.81 | accuracy 68.904 | uer 17.508 | wer 19.391 | raw_wer 19.391 | bleu 31.01 | wps 1174.4 | wpb 3505.9 | bsz 119.3 | num_updates 34681 | best_bleu 31.1
2023-09-01 18:58:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 34681 updates
2023-09-01 18:58:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_31.0104.pt
2023-09-01 18:58:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_31.0104.pt
2023-09-01 18:58:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_31.0104.pt (epoch 19 @ 34681 updates, score 31.01) (writing took 6.266948931006482 seconds)
2023-09-01 18:58:18 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-09-01 18:58:18 | INFO | train | epoch 019 | loss 1.892 | trans_loss 4.701 | nll_loss 1.914 | w2v_ctc_loss 0.694 | task_loss 5.146 | contrastive_loss 0 | total 3956.37 | n_correct 2735.84 | ppl 3.77 | accuracy 69.15 | wps 11695.6 | ups 1.48 | wpb 7912.7 | bsz 284.8 | num_updates 34681 | lr 7.59398e-05 | gnorm 0.526 | clip 0 | loss_scale 64 | train_wall 1120 | gb_free 16.7 | wall 26278
2023-09-01 18:58:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 18:58:18 | INFO | fairseq.trainer | begin training epoch 20
2023-09-01 18:58:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 18:58:37 | INFO | train_inner | epoch 020:     19 / 1826 loss=1.898, trans_loss=4.706, nll_loss=1.921, w2v_ctc_loss=0.702, task_loss=5.301, contrastive_loss=0, total=3914.44, n_correct=2700.87, ppl=3.79, accuracy=68.998, wps=6811.9, ups=0.87, wpb=7828.9, bsz=276.8, num_updates=34700, lr=7.5919e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=61, gb_free=15.2, wall=26298
2023-09-01 18:59:39 | INFO | train_inner | epoch 020:    119 / 1826 loss=1.871, trans_loss=4.68, nll_loss=1.887, w2v_ctc_loss=0.666, task_loss=4.988, contrastive_loss=0, total=3989.75, n_correct=2780.02, ppl=3.7, accuracy=69.679, wps=12850.5, ups=1.61, wpb=7979.5, bsz=291.1, num_updates=34800, lr=7.58098e-05, gnorm=0.516, clip=0, loss_scale=64, train_wall=62, gb_free=15.4, wall=26360
2023-09-01 19:00:42 | INFO | train_inner | epoch 020:    219 / 1826 loss=1.884, trans_loss=4.684, nll_loss=1.892, w2v_ctc_loss=0.685, task_loss=5.235, contrastive_loss=0, total=4021.64, n_correct=2793.8, ppl=3.71, accuracy=69.469, wps=12878, ups=1.6, wpb=8043.3, bsz=284.5, num_updates=34900, lr=7.57011e-05, gnorm=0.516, clip=0, loss_scale=64, train_wall=62, gb_free=16.6, wall=26422
2023-09-01 19:01:44 | INFO | train_inner | epoch 020:    319 / 1826 loss=1.877, trans_loss=4.687, nll_loss=1.897, w2v_ctc_loss=0.677, task_loss=4.73, contrastive_loss=0, total=4028.1, n_correct=2800.8, ppl=3.72, accuracy=69.532, wps=12970.4, ups=1.61, wpb=8056.2, bsz=300.1, num_updates=35000, lr=7.55929e-05, gnorm=0.511, clip=0, loss_scale=64, train_wall=62, gb_free=16.5, wall=26484
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:0')
2023-09-01 19:02:46 | INFO | train_inner | epoch 020:    419 / 1826 loss=1.881, trans_loss=4.682, nll_loss=1.89, w2v_ctc_loss=0.687, task_loss=5.027, contrastive_loss=0, total=3989.69, n_correct=2774.34, ppl=3.71, accuracy=69.538, wps=12805.9, ups=1.6, wpb=7979.4, bsz=288.8, num_updates=35100, lr=7.54851e-05, gnorm=0.521, clip=0, loss_scale=64, train_wall=62, gb_free=15, wall=26547
2023-09-01 19:03:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-01 19:03:48 | INFO | train_inner | epoch 020:    520 / 1826 loss=1.881, trans_loss=4.688, nll_loss=1.898, w2v_ctc_loss=0.677, task_loss=5.15, contrastive_loss=0, total=3935.37, n_correct=2732.19, ppl=3.73, accuracy=69.427, wps=12657.6, ups=1.61, wpb=7870.7, bsz=281.9, num_updates=35200, lr=7.53778e-05, gnorm=0.522, clip=0, loss_scale=32, train_wall=62, gb_free=15.2, wall=26609
2023-09-01 19:04:50 | INFO | train_inner | epoch 020:    620 / 1826 loss=1.887, trans_loss=4.689, nll_loss=1.899, w2v_ctc_loss=0.692, task_loss=5.236, contrastive_loss=0, total=3954.32, n_correct=2742.18, ppl=3.73, accuracy=69.346, wps=12759.5, ups=1.61, wpb=7908.6, bsz=281.4, num_updates=35300, lr=7.5271e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=61, gb_free=11.9, wall=26671
2023-09-01 19:05:53 | INFO | train_inner | epoch 020:    720 / 1826 loss=1.883, trans_loss=4.686, nll_loss=1.895, w2v_ctc_loss=0.682, task_loss=5.332, contrastive_loss=0, total=3927.01, n_correct=2725.47, ppl=3.72, accuracy=69.403, wps=12617.9, ups=1.61, wpb=7854, bsz=275, num_updates=35400, lr=7.51646e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=26733
2023-09-01 19:06:54 | INFO | train_inner | epoch 020:    820 / 1826 loss=1.884, trans_loss=4.693, nll_loss=1.905, w2v_ctc_loss=0.678, task_loss=5.332, contrastive_loss=0, total=3921.07, n_correct=2721.42, ppl=3.74, accuracy=69.405, wps=12688.4, ups=1.62, wpb=7842.1, bsz=276.8, num_updates=35500, lr=7.50587e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=26795
2023-09-01 19:07:56 | INFO | train_inner | epoch 020:    920 / 1826 loss=1.891, trans_loss=4.689, nll_loss=1.899, w2v_ctc_loss=0.698, task_loss=5.409, contrastive_loss=0, total=3850.69, n_correct=2669.68, ppl=3.73, accuracy=69.33, wps=12464, ups=1.62, wpb=7701.4, bsz=272.3, num_updates=35600, lr=7.49532e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=61, gb_free=16.6, wall=26857
2023-09-01 19:08:58 | INFO | train_inner | epoch 020:   1020 / 1826 loss=1.884, trans_loss=4.692, nll_loss=1.903, w2v_ctc_loss=0.682, task_loss=5.31, contrastive_loss=0, total=3948.3, n_correct=2740.79, ppl=3.74, accuracy=69.417, wps=12787.6, ups=1.62, wpb=7896.6, bsz=276.7, num_updates=35700, lr=7.48481e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=26918
2023-09-01 19:10:00 | INFO | train_inner | epoch 020:   1120 / 1826 loss=1.876, trans_loss=4.686, nll_loss=1.896, w2v_ctc_loss=0.673, task_loss=4.884, contrastive_loss=0, total=3985.06, n_correct=2774.02, ppl=3.72, accuracy=69.61, wps=12879.7, ups=1.62, wpb=7970.1, bsz=295.4, num_updates=35800, lr=7.47435e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=61, gb_free=15.7, wall=26980
2023-09-01 19:11:03 | INFO | train_inner | epoch 020:   1220 / 1826 loss=1.885, trans_loss=4.702, nll_loss=1.917, w2v_ctc_loss=0.683, task_loss=4.793, contrastive_loss=0, total=4009.25, n_correct=2773.67, ppl=3.78, accuracy=69.182, wps=12788.2, ups=1.59, wpb=8018.5, bsz=305.3, num_updates=35900, lr=7.46393e-05, gnorm=0.513, clip=0, loss_scale=32, train_wall=62, gb_free=15.5, wall=27043
2023-09-01 19:12:05 | INFO | train_inner | epoch 020:   1320 / 1826 loss=1.881, trans_loss=4.694, nll_loss=1.906, w2v_ctc_loss=0.676, task_loss=4.777, contrastive_loss=0, total=4041.07, n_correct=2800.04, ppl=3.75, accuracy=69.29, wps=12835, ups=1.59, wpb=8082.1, bsz=306.5, num_updates=36000, lr=7.45356e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=62, gb_free=11.3, wall=27106
2023-09-01 19:12:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2238, device='cuda:3')
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 19:12:45 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.791 | trans_loss 4.994 | nll_loss 2.248 | w2v_ctc_loss 1.367 | task_loss 19.109 | contrastive_loss 0 | total 3505.91 | n_correct 2427.36 | ppl 4.75 | accuracy 69.236 | uer 17.366 | wer 19.222 | raw_wer 19.222 | bleu 31.37 | wps 1151.2 | wpb 3505.9 | bsz 119.3 | num_updates 36000 | best_bleu 31.37
2023-09-01 19:12:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 36000 updates
2023-09-01 19:12:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_20_36000.pt
2023-09-01 19:12:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_20_36000.pt
2023-09-01 19:12:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint_20_36000.pt (epoch 20 @ 36000 updates, score 31.37) (writing took 11.549046048981836 seconds)
2023-09-01 19:13:59 | INFO | train_inner | epoch 020:   1420 / 1826 loss=1.885, trans_loss=4.682, nll_loss=1.89, w2v_ctc_loss=0.691, task_loss=5.354, contrastive_loss=0, total=3881.09, n_correct=2696.76, ppl=3.71, accuracy=69.485, wps=6812.2, ups=0.88, wpb=7762.2, bsz=274.3, num_updates=36100, lr=7.44323e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=27220
2023-09-01 19:15:02 | INFO | train_inner | epoch 020:   1520 / 1826 loss=1.898, trans_loss=4.694, nll_loss=1.905, w2v_ctc_loss=0.705, task_loss=5.842, contrastive_loss=0, total=3887.1, n_correct=2691.25, ppl=3.75, accuracy=69.235, wps=12413.6, ups=1.6, wpb=7774.2, bsz=260.1, num_updates=36200, lr=7.43294e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=62, gb_free=12.3, wall=27283
2023-09-01 19:15:55 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-01 19:16:05 | INFO | train_inner | epoch 020:   1621 / 1826 loss=1.887, trans_loss=4.691, nll_loss=1.902, w2v_ctc_loss=0.692, task_loss=5.252, contrastive_loss=0, total=3971.17, n_correct=2757.18, ppl=3.74, accuracy=69.43, wps=12626.4, ups=1.59, wpb=7942.3, bsz=282.2, num_updates=36300, lr=7.4227e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=62, gb_free=16.6, wall=27346
2023-09-01 19:17:07 | INFO | train_inner | epoch 020:   1721 / 1826 loss=1.894, trans_loss=4.694, nll_loss=1.906, w2v_ctc_loss=0.704, task_loss=5.358, contrastive_loss=0, total=3923.82, n_correct=2713.81, ppl=3.75, accuracy=69.162, wps=12606.7, ups=1.61, wpb=7847.6, bsz=277.2, num_updates=36400, lr=7.41249e-05, gnorm=0.528, clip=0, loss_scale=16, train_wall=62, gb_free=15.3, wall=27408
2023-09-01 19:18:10 | INFO | train_inner | epoch 020:   1821 / 1826 loss=1.89, trans_loss=4.707, nll_loss=1.923, w2v_ctc_loss=0.692, task_loss=4.788, contrastive_loss=0, total=3980.42, n_correct=2752.4, ppl=3.79, accuracy=69.148, wps=12771.2, ups=1.6, wpb=7960.8, bsz=299.6, num_updates=36500, lr=7.40233e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=62, gb_free=16.7, wall=27470
2023-09-01 19:18:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-01 19:18:52 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.811 | trans_loss 5 | nll_loss 2.255 | w2v_ctc_loss 1.422 | task_loss 19.202 | contrastive_loss 0 | total 3505.91 | n_correct 2422.64 | ppl 4.77 | accuracy 69.102 | uer 17.599 | wer 19.463 | raw_wer 19.463 | bleu 30.99 | wps 1186.7 | wpb 3505.9 | bsz 119.3 | num_updates 36505 | best_bleu 31.37
2023-09-01 19:18:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 36505 updates
2023-09-01 19:18:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.9908.pt
2023-09-01 19:18:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.9908.pt
2023-09-01 19:18:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_shrink_v2_merge_wmt_0901_soft_noCL_AT_sentence_mixup07_scale3.5_alpha0_mt0.5_lowgrad_highnograd/checkpoint.best_bleu_30.9908.pt (epoch 20 @ 36505 updates, score 30.99) (writing took 6.703548591991421 seconds)
2023-09-01 19:18:59 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-09-01 19:18:59 | INFO | train | epoch 020 | loss 1.884 | trans_loss 4.69 | nll_loss 1.901 | w2v_ctc_loss 0.686 | task_loss 5.148 | contrastive_loss 0 | total 3956.1 | n_correct 2745.29 | ppl 3.73 | accuracy 69.394 | wps 11630.6 | ups 1.47 | wpb 7912.2 | bsz 284.8 | num_updates 36505 | lr 7.40183e-05 | gnorm 0.523 | clip 0 | loss_scale 16 | train_wall 1124 | gb_free 15.4 | wall 27519
2023-09-01 19:18:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-01 19:18:59 | INFO | fairseq.trainer | begin training epoch 21
2023-09-01 19:18:59 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-01 19:20:06 | INFO | train_inner | epoch 021:     95 / 1826 loss=1.881, trans_loss=4.687, nll_loss=1.897, w2v_ctc_loss=0.684, task_loss=5.136, contrastive_loss=0, total=3944.95, n_correct=2745.08, ppl=3.72, accuracy=69.585, wps=6803.4, ups=0.86, wpb=7889.9, bsz=286.3, num_updates=36600, lr=7.39221e-05, gnorm=0.532, clip=0, loss_scale=16, train_wall=61, gb_free=13.3, wall=27586
2023-09-01 19:21:07 | INFO | train_inner | epoch 021:    195 / 1826 loss=1.869, trans_loss=4.673, nll_loss=1.879, w2v_ctc_loss=0.664, task_loss=4.929, contrastive_loss=0, total=3973.7, n_correct=2774.74, ppl=3.68, accuracy=69.828, wps=12926.1, ups=1.63, wpb=7947.4, bsz=292.4, num_updates=36700, lr=7.38213e-05, gnorm=0.516, clip=0, loss_scale=16, train_wall=61, gb_free=11.6, wall=27648
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 7 terminated with signal SIGKILL
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1065 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
