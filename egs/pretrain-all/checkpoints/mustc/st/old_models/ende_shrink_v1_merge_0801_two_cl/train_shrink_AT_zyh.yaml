train-subset: train_st,train_asr
valid-subset: dev_st

# ************/ Add AT per-task /************ #                               # //need_check//
#adversarial-training: True    # do AT task
#at-level: sentence            # the level of at task, support: sentence / token / window  
#at-scale: 1.0                 # set the weight of at loss
keep-mt-task: True            # keep update mt task and do not update weight
merge-mt-st: True             # merge (at,) mt, st task --> st
# mixup-rate: 0.5             # mixup rate, mixup-rate < 0 means random [0.2, 0.6]


max-epoch: 100
max-update: 50000

num-workers: 2
#patience: 12
no-progress-bar: True
log-interval: 100
seed: 1
report-accuracy: True

#load-pretrained-encoder-from:
#load-pretrained-decoder-from:

arch: s2t_joint
#arch: s2t_w2v2_ode_sead_s
#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec2.0/wav2vec_small.pt
#adapter-model-path: /workspace/MSP-ST/fairseq/egs/asr-pretrain/checkpoints/deep_transformer/avg_3_checkpoint.pt
#w2v-path: /workspace/pretrain/wav2vec_small.pt
w2v-path: /mnt/zhangyh/pretrain/hubert_base_ls960.pt
#mt-model-path: /workspace/MSP-ST/fairseq/egs/machine_translation/checkpoints/wmt-en2de/merge-lcrm/last5.ensemble.pt
#mt-model-path: /workspace/MSP-ST/fairseq/egs/machine_translation/checkpoints/wmt-en2fr/mustc-enfr-lc/last8.ensemble.pt

mt-model-path: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/mustc/ende-baseline/last8.ensemble.pt
decoder-embed-path: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_mustc_ende_baseline

#w2v2-model-path: /apdcephfs/share_1157259/users/adrienxu/st/wav2vec/wav2vec-tune/checkpoint_last.pt
share-decoder-input-output-embed: True
share-two-encoders: True
optimizer: adam
clip-norm: 10.0
lr-scheduler: inverse_sqrt
#lr-scheduler: polynomial_decay
#lr-scheduler: tri_stage
#phase-ratio: 0.1,0.4,0.5
#final-lr-scale: 0.05
warmup-init-lr: 1e-7
warmup-updates: 5000
lr: 2e-4
#weight-decay: 0.0001
adam-betas: (0.9,0.98)
adapter-dim: 4096
adapter-dropout: 0.0

ctc-weight: 0.3                                                                                   # //need_check//
criterion: label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge                              # //need_check//  
# criterion: label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT            
weight-steps: 5000
share-ctc-embed: true
use-ctc-shrink: true                                                                              # //need_check//
#train-st-without-ctc: true
#max_position_ctc: 100
label-smoothing: 0.2

use-two-contrastive: true   # double level contrastive learning                                 # //need_check//
contrastive-alpha: 1.0    # contrastive learning                                                  # //need_check//
contrastive-beta: 1.0
contrastive-temperature: 0.1
zero-infinity: true 
decrease-step: 0
post-process: sentencepiece
is-shrink: uniq


#wav2vec configuration
#macaron-style: False
use-cnn-module: False

cnn-module-kernel: 31
apply-mask: True
mask-prob: 0.5
mask-channel-prob: 0.25
mask-channel-length: 6
use-ctc-loss: True
add-position-embed: true
add-position-embed-after-ctc: true
adapter-layers: 0
sead-layers: 6
final-dropout: 0.1
freeze-finetune-updates: 3000

conv-kernel-sizes: 5,5
conv-channels: 512
dropout: 0.1
activation-fn: relu
encoder-embed-dim: 512
encoder-ffn-embed-dim: 2048
encoder-layers: 6
decoder-layers: 6
encoder-attention-heads: 8
#
decoder-embed-dim: 512
decoder-ffn-embed-dim: 2048
decoder-attention-heads: 8
attention-dropout: 0.1
activation-dropout: 0.1
