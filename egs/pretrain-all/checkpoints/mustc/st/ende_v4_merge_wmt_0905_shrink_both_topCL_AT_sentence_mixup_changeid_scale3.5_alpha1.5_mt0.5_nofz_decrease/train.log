2023-09-05 01:40:44 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:14604
2023-09-05 01:40:44 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:14604
2023-09-05 01:40:44 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:14604
2023-09-05 01:40:44 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:14604
2023-09-05 01:40:44 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:14604
2023-09-05 01:40:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-05 01:40:44 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:14604
2023-09-05 01:40:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-05 01:40:44 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:14604
2023-09-05 01:40:44 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:14604
2023-09-05 01:40:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-05 01:40:44 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 01:40:45 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 01:40:45 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-05 01:40:45 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 01:40:45 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 01:40:45 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-05 01:40:45 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 01:40:45 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-05 01:40:45 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 01:40:45 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-05 01:40:48 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:14604', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-05 01:40:48 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-05 01:40:48 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-05 01:40:48 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-05 01:40:48 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-05 01:40:48 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-05 01:40:52 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-05 01:40:52 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-05 01:40:52 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-05 01:40:54 | INFO | root | load pretrained hubert
2023-09-05 01:41:01 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-05 01:41:02 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-05 01:41:07 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-05 01:41:07 | INFO | root | share the sematic adapter and textual encoder
2023-09-05 01:41:07 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-05 01:41:07 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-05 01:41:07 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-05 01:41:07 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-05 01:41:07 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-05 01:41:07 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-05 01:41:07 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 01:41:07 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 01:41:07 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 01:41:07 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 01:41:23 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-05 01:41:28 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-05 01:41:28 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-05 01:41:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 01:41:28 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 01:41:28 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 01:41:28 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 01:41:28 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 01:41:28 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 01:41:28 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 01:41:28 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 01:41:28 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 01:41:28 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 01:41:28 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-05 01:41:28 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-05 01:41:28 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt
2023-09-05 01:41:28 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt
2023-09-05 01:41:28 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-05 01:41:28 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 01:41:28 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 01:41:28 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 01:41:30 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 01:41:32 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 01:42:12 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-05 01:42:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 01:42:12 | INFO | fairseq.trainer | begin training epoch 1
2023-09-05 01:42:12 | INFO | fairseq_cli.train | Start iterating over samples
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-05 01:42:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-05 01:42:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 01:42:33 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-05 01:42:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-05 01:43:29 | INFO | train_inner | epoch 001:    104 / 1474 loss=20.049, trans_loss=5.872, nll_loss=4.68, w2v_ctc_loss=22.335, task_loss=7.1, task_loss_gen=4.878, contrastive_loss=3.264, total=4219.72, n_correct=124.11, ppl=25.63, accuracy=2.941, wps=19395.9, ups=1.54, wpb=12589.4, bsz=473.3, num_updates=100, lr=4.098e-06, gnorm=2.825, clip=0, loss_scale=8, train_wall=69, gb_free=18.6, wall=121
2023-09-05 01:43:52 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-05 01:44:32 | INFO | train_inner | epoch 001:    205 / 1474 loss=16.572, trans_loss=5.854, nll_loss=4.684, w2v_ctc_loss=17.069, task_loss=5.577, task_loss_gen=5.21, contrastive_loss=3.225, total=4106.13, n_correct=115.77, ppl=25.7, accuracy=2.819, wps=19392.2, ups=1.58, wpb=12261, bsz=454.7, num_updates=200, lr=8.096e-06, gnorm=7.308, clip=17, loss_scale=4, train_wall=63, gb_free=19.3, wall=184
2023-09-05 01:45:35 | INFO | train_inner | epoch 001:    305 / 1474 loss=9.909, trans_loss=5.832, nll_loss=4.695, w2v_ctc_loss=6.887, task_loss=3.576, task_loss_gen=6.647, contrastive_loss=3.173, total=4080.91, n_correct=113.08, ppl=25.9, accuracy=2.771, wps=19427.6, ups=1.59, wpb=12190.4, bsz=439.4, num_updates=300, lr=1.2094e-05, gnorm=2.229, clip=0, loss_scale=4, train_wall=62, gb_free=18.6, wall=247
2023-09-05 01:46:37 | INFO | train_inner | epoch 001:    405 / 1474 loss=9.398, trans_loss=5.786, nll_loss=4.671, w2v_ctc_loss=6.108, task_loss=1.735, task_loss_gen=8.352, contrastive_loss=3.205, total=4176.41, n_correct=107.87, ppl=25.48, accuracy=2.583, wps=19952.7, ups=1.6, wpb=12470, bsz=461.3, num_updates=400, lr=1.6092e-05, gnorm=1.321, clip=0, loss_scale=4, train_wall=62, gb_free=19.4, wall=309
2023-09-05 01:47:40 | INFO | train_inner | epoch 001:    505 / 1474 loss=9.231, trans_loss=5.769, nll_loss=4.669, w2v_ctc_loss=5.811, task_loss=0.648, task_loss_gen=11.139, contrastive_loss=3.301, total=4192.13, n_correct=103.99, ppl=25.44, accuracy=2.481, wps=20054.9, ups=1.6, wpb=12526, bsz=489.2, num_updates=500, lr=2.009e-05, gnorm=1.15, clip=0, loss_scale=4, train_wall=62, gb_free=19.1, wall=372
2023-09-05 01:48:42 | INFO | train_inner | epoch 001:    605 / 1474 loss=9.042, trans_loss=5.687, nll_loss=4.553, w2v_ctc_loss=5.657, task_loss=0.266, task_loss_gen=14.308, contrastive_loss=3.255, total=4131.49, n_correct=109.83, ppl=23.47, accuracy=2.658, wps=19966.4, ups=1.62, wpb=12320.8, bsz=473.5, num_updates=600, lr=2.4088e-05, gnorm=1.216, clip=0, loss_scale=4, train_wall=61, gb_free=18.9, wall=433
2023-09-05 01:49:43 | INFO | train_inner | epoch 001:    705 / 1474 loss=9.006, trans_loss=5.76, nll_loss=4.658, w2v_ctc_loss=5.594, task_loss=0.065, task_loss_gen=20.581, contrastive_loss=3.133, total=4147.85, n_correct=94.95, ppl=25.25, accuracy=2.289, wps=20081.1, ups=1.62, wpb=12387.4, bsz=455.8, num_updates=700, lr=2.8086e-05, gnorm=1.047, clip=0, loss_scale=4, train_wall=61, gb_free=19.2, wall=495
2023-09-05 01:50:45 | INFO | train_inner | epoch 001:    805 / 1474 loss=8.893, trans_loss=5.86, nll_loss=4.779, w2v_ctc_loss=5.397, task_loss=0.023, task_loss_gen=24.14, contrastive_loss=3.131, total=4128.17, n_correct=78.89, ppl=27.45, accuracy=1.911, wps=19882.5, ups=1.61, wpb=12316.6, bsz=462.9, num_updates=800, lr=3.2084e-05, gnorm=1.287, clip=0, loss_scale=4, train_wall=61, gb_free=18.6, wall=557
2023-09-05 01:51:47 | INFO | train_inner | epoch 001:    905 / 1474 loss=8.714, trans_loss=5.866, nll_loss=4.784, w2v_ctc_loss=5.202, task_loss=0.008, task_loss_gen=28.222, contrastive_loss=3.026, total=4166.08, n_correct=77.64, ppl=27.56, accuracy=1.864, wps=20013.1, ups=1.61, wpb=12441.2, bsz=458.8, num_updates=900, lr=3.6082e-05, gnorm=1.737, clip=1, loss_scale=4, train_wall=62, gb_free=18.5, wall=619
2023-09-05 01:52:50 | INFO | train_inner | epoch 001:   1005 / 1474 loss=8.61, trans_loss=5.993, nll_loss=4.943, w2v_ctc_loss=4.975, task_loss=0.003, task_loss_gen=32.769, contrastive_loss=3.007, total=4131.01, n_correct=64.25, ppl=30.76, accuracy=1.555, wps=19779, ups=1.6, wpb=12339.5, bsz=456.7, num_updates=1000, lr=4.008e-05, gnorm=2.001, clip=0, loss_scale=4, train_wall=62, gb_free=19.1, wall=682
2023-09-05 01:53:52 | INFO | train_inner | epoch 001:   1105 / 1474 loss=8.551, trans_loss=6.176, nll_loss=5.165, w2v_ctc_loss=4.77, task_loss=3.794, task_loss_gen=13.149, contrastive_loss=2.943, total=4159.12, n_correct=31.62, ppl=35.87, accuracy=0.76, wps=20102.2, ups=1.62, wpb=12405, bsz=454.7, num_updates=1100, lr=4.4078e-05, gnorm=2.35, clip=0, loss_scale=4, train_wall=61, gb_free=18.8, wall=743
2023-09-05 01:54:54 | INFO | train_inner | epoch 001:   1205 / 1474 loss=8.425, trans_loss=6.296, nll_loss=5.318, w2v_ctc_loss=4.599, task_loss=6.117, task_loss_gen=6.193, contrastive_loss=2.813, total=4122.88, n_correct=16.72, ppl=39.9, accuracy=0.406, wps=19806.5, ups=1.61, wpb=12314.8, bsz=437.2, num_updates=1200, lr=4.8076e-05, gnorm=2.506, clip=0, loss_scale=4, train_wall=62, gb_free=19, wall=805
2023-09-05 01:55:55 | INFO | train_inner | epoch 001:   1305 / 1474 loss=8.16, trans_loss=6.177, nll_loss=5.167, w2v_ctc_loss=4.428, task_loss=3.686, task_loss_gen=6.565, contrastive_loss=2.713, total=4065.11, n_correct=22.78, ppl=35.92, accuracy=0.56, wps=19744.7, ups=1.63, wpb=12135.9, bsz=445.4, num_updates=1300, lr=5.2074e-05, gnorm=2.887, clip=0, loss_scale=4, train_wall=61, gb_free=19.2, wall=867
2023-09-05 01:56:58 | INFO | train_inner | epoch 001:   1405 / 1474 loss=7.937, trans_loss=6.152, nll_loss=5.137, w2v_ctc_loss=4.284, task_loss=1.867, task_loss_gen=8.596, contrastive_loss=2.723, total=4125.61, n_correct=25.14, ppl=35.2, accuracy=0.609, wps=19688.7, ups=1.6, wpb=12327.8, bsz=450.3, num_updates=1400, lr=5.6072e-05, gnorm=2.935, clip=0, loss_scale=4, train_wall=62, gb_free=18.6, wall=930
2023-09-05 01:57:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2023-09-05 01:58:27 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 11.94 | trans_loss 13.264 | nll_loss 12.995 | w2v_ctc_loss 5.599 | task_loss 9.327 | task_loss_gen 49.47 | contrastive_loss 3.709 | total 4003.4 | n_correct 20 | ppl 8162.6 | accuracy 0.5 | uer 70.687 | wer 68.901 | raw_wer 68.901 | bleu 0 | wps 1035.8 | wpb 4003.4 | bsz 141.8 | num_updates 1469
2023-09-05 01:58:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1469 updates
2023-09-05 01:58:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 01:58:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 01:58:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 1 @ 1469 updates, score 0.0) (writing took 5.030136525980197 seconds)
2023-09-05 01:58:32 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-09-05 01:58:32 | INFO | train | epoch 001 | loss 10.077 | trans_loss 5.942 | nll_loss 4.861 | w2v_ctc_loss 7.233 | task_loss 2.392 | task_loss_gen 13.436 | contrastive_loss 3.048 | total 4138.78 | n_correct 75.3091 | ppl 29.06 | accuracy 1.82 | wps 18744.3 | ups 1.52 | wpb 12356.2 | bsz 458.4 | num_updates 1469 | lr 5.88306e-05 | gnorm 2.377 | clip 1.2 | loss_scale 4 | train_wall 912 | gb_free 18.9 | wall 1024
2023-09-05 01:58:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 01:58:33 | INFO | fairseq.trainer | begin training epoch 2
2023-09-05 01:58:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 01:58:59 | INFO | train_inner | epoch 002:     31 / 1474 loss=7.737, trans_loss=6.111, nll_loss=5.09, w2v_ctc_loss=4.142, task_loss=1.399, task_loss_gen=9, contrastive_loss=2.632, total=4159.96, n_correct=27.4, ppl=34.06, accuracy=0.659, wps=10266.3, ups=0.83, wpb=12405.9, bsz=470.9, num_updates=1500, lr=6.007e-05, gnorm=2.91, clip=0, loss_scale=4, train_wall=61, gb_free=19.6, wall=1050
2023-09-05 02:00:00 | INFO | train_inner | epoch 002:    131 / 1474 loss=7.565, trans_loss=6.102, nll_loss=5.088, w2v_ctc_loss=4.062, task_loss=1.306, task_loss_gen=10.289, contrastive_loss=2.456, total=4158.42, n_correct=26.57, ppl=34.01, accuracy=0.639, wps=20163.1, ups=1.63, wpb=12405.1, bsz=453.8, num_updates=1600, lr=6.4068e-05, gnorm=2.589, clip=0, loss_scale=4, train_wall=61, gb_free=18.9, wall=1112
2023-09-05 02:01:02 | INFO | train_inner | epoch 002:    231 / 1474 loss=7.49, trans_loss=6.14, nll_loss=5.141, w2v_ctc_loss=3.916, task_loss=1.318, task_loss_gen=8.828, contrastive_loss=2.504, total=4199.99, n_correct=21.05, ppl=35.27, accuracy=0.501, wps=20400.5, ups=1.63, wpb=12542.2, bsz=493.1, num_updates=1700, lr=6.8066e-05, gnorm=2.806, clip=0, loss_scale=4, train_wall=61, gb_free=18.8, wall=1173
2023-09-05 02:02:04 | INFO | train_inner | epoch 002:    331 / 1474 loss=7.358, trans_loss=6.217, nll_loss=5.225, w2v_ctc_loss=3.869, task_loss=1.787, task_loss_gen=10.392, contrastive_loss=2.261, total=4133.34, n_correct=15.35, ppl=37.39, accuracy=0.371, wps=19936.4, ups=1.62, wpb=12340.6, bsz=446, num_updates=1800, lr=7.2064e-05, gnorm=2.738, clip=0, loss_scale=4, train_wall=61, gb_free=19.6, wall=1235
2023-09-05 02:03:05 | INFO | train_inner | epoch 002:    431 / 1474 loss=7.227, trans_loss=6.273, nll_loss=5.295, w2v_ctc_loss=3.809, task_loss=2.038, task_loss_gen=10.753, contrastive_loss=2.041, total=4031.76, n_correct=19.9, ppl=39.26, accuracy=0.494, wps=19484.1, ups=1.62, wpb=12051, bsz=413.3, num_updates=1900, lr=7.6062e-05, gnorm=2.381, clip=0, loss_scale=4, train_wall=61, gb_free=18.6, wall=1297
2023-09-05 02:04:08 | INFO | train_inner | epoch 002:    531 / 1474 loss=7.119, trans_loss=6.24, nll_loss=5.248, w2v_ctc_loss=3.669, task_loss=0.956, task_loss_gen=11.162, contrastive_loss=2.18, total=4176.84, n_correct=25.78, ppl=37.99, accuracy=0.617, wps=20029.8, ups=1.61, wpb=12461.7, bsz=466.9, num_updates=2000, lr=8.006e-05, gnorm=2.523, clip=0, loss_scale=4, train_wall=62, gb_free=18.5, wall=1359
2023-09-05 02:04:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 02:04:55 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 11.585 | trans_loss 13.546 | nll_loss 13.34 | w2v_ctc_loss 4.788 | task_loss 5.703 | task_loss_gen 57.843 | contrastive_loss 2.969 | total 4003.4 | n_correct 29 | ppl 10366.5 | accuracy 0.724 | uer 62.305 | wer 61.821 | raw_wer 61.821 | bleu 0 | wps 1038.6 | wpb 4003.4 | bsz 141.8 | num_updates 2000 | best_bleu 0
2023-09-05 02:04:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-09-05 02:04:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_2_2000.pt
2023-09-05 02:04:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_2_2000.pt
2023-09-05 02:05:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 14.068828241957817 seconds)
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.1755,  0.1312, -0.0106,  0.0329, -0.0560], device='cuda:0',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1930,  0.2588,  0.2544,  ..., -0.1055, -0.1771, -0.0543],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1194, -0.7734, -0.5698,  ...,  0.0511, -0.0533, -0.0638]],
       device='cuda:0', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.1755,  0.1312, -0.0106,  0.0329, -0.0560], device='cuda:0',
       dtype=torch.float16)
--------------------
2023-09-05 02:06:10 | INFO | train_inner | epoch 002:    631 / 1474 loss=6.981, trans_loss=6.258, nll_loss=5.275, w2v_ctc_loss=3.585, task_loss=0.475, task_loss_gen=13.391, contrastive_loss=1.967, total=4128.91, n_correct=13.5, ppl=38.73, accuracy=0.327, wps=10065.7, ups=0.82, wpb=12321.5, bsz=450, num_updates=2100, lr=8.4058e-05, gnorm=2.708, clip=0, loss_scale=4, train_wall=61, gb_free=19.4, wall=1482
2023-09-05 02:07:11 | INFO | train_inner | epoch 002:    731 / 1474 loss=6.831, trans_loss=6.154, nll_loss=5.15, w2v_ctc_loss=3.533, task_loss=0.223, task_loss_gen=16.045, contrastive_loss=2.03, total=4134.99, n_correct=21.14, ppl=35.5, accuracy=0.511, wps=20099.1, ups=1.63, wpb=12343.8, bsz=457.3, num_updates=2200, lr=8.8056e-05, gnorm=2.727, clip=0, loss_scale=8, train_wall=61, gb_free=18.9, wall=1543
2023-09-05 02:08:13 | INFO | train_inner | epoch 002:    831 / 1474 loss=6.686, trans_loss=6.124, nll_loss=5.112, w2v_ctc_loss=3.467, task_loss=0.118, task_loss_gen=18.461, contrastive_loss=1.967, total=4176.63, n_correct=30.14, ppl=34.58, accuracy=0.722, wps=20230.1, ups=1.62, wpb=12476, bsz=463.9, num_updates=2300, lr=9.2054e-05, gnorm=2.382, clip=0, loss_scale=8, train_wall=61, gb_free=18.6, wall=1605
2023-09-05 02:09:15 | INFO | train_inner | epoch 002:    931 / 1474 loss=6.549, trans_loss=6.154, nll_loss=5.159, w2v_ctc_loss=3.385, task_loss=0.053, task_loss_gen=22.102, contrastive_loss=1.843, total=4102.83, n_correct=26.18, ppl=35.72, accuracy=0.638, wps=19963.5, ups=1.63, wpb=12248.5, bsz=444.8, num_updates=2400, lr=9.6052e-05, gnorm=2.578, clip=0, loss_scale=8, train_wall=61, gb_free=18.5, wall=1666
2023-09-05 02:10:17 | INFO | train_inner | epoch 002:   1031 / 1474 loss=6.385, trans_loss=6.104, nll_loss=5.087, w2v_ctc_loss=3.326, task_loss=0.019, task_loss_gen=25.75, contrastive_loss=1.678, total=4094.52, n_correct=30.82, ppl=33.99, accuracy=0.753, wps=19473.4, ups=1.59, wpb=12223.5, bsz=452.4, num_updates=2500, lr=0.00010005, gnorm=2.229, clip=0, loss_scale=8, train_wall=62, gb_free=18.7, wall=1729
2023-09-05 02:11:19 | INFO | train_inner | epoch 002:   1131 / 1474 loss=6.392, trans_loss=6.189, nll_loss=5.195, w2v_ctc_loss=3.238, task_loss=0.008, task_loss_gen=24.836, contrastive_loss=1.831, total=4208.43, n_correct=26.86, ppl=36.63, accuracy=0.638, wps=20301, ups=1.62, wpb=12564.4, bsz=493.6, num_updates=2600, lr=0.000104048, gnorm=2.295, clip=0, loss_scale=8, train_wall=61, gb_free=19.6, wall=1791
2023-09-05 02:12:21 | INFO | train_inner | epoch 002:   1231 / 1474 loss=6.258, trans_loss=6.122, nll_loss=5.11, w2v_ctc_loss=3.191, task_loss=0.004, task_loss_gen=28.831, contrastive_loss=1.746, total=4225.14, n_correct=29.44, ppl=34.54, accuracy=0.697, wps=20330.2, ups=1.61, wpb=12605.4, bsz=493.8, num_updates=2700, lr=0.000108046, gnorm=2.245, clip=0, loss_scale=8, train_wall=61, gb_free=18.5, wall=1853
2023-09-05 02:13:23 | INFO | train_inner | epoch 002:   1331 / 1474 loss=6.108, trans_loss=6.135, nll_loss=5.128, w2v_ctc_loss=3.166, task_loss=0.002, task_loss_gen=33.73, contrastive_loss=1.409, total=4141.12, n_correct=26.01, ppl=34.98, accuracy=0.628, wps=20106.4, ups=1.62, wpb=12377.6, bsz=456.1, num_updates=2800, lr=0.000112044, gnorm=2.04, clip=0, loss_scale=8, train_wall=61, gb_free=19.1, wall=1914
2023-09-05 02:14:24 | INFO | train_inner | epoch 002:   1431 / 1474 loss=6.031, trans_loss=6.095, nll_loss=5.078, w2v_ctc_loss=3.12, task_loss=0.001, task_loss_gen=40.045, contrastive_loss=1.507, total=4066.11, n_correct=42.96, ppl=33.77, accuracy=1.057, wps=19741.1, ups=1.63, wpb=12140.4, bsz=444.1, num_updates=2900, lr=0.000116042, gnorm=2.136, clip=0, loss_scale=8, train_wall=61, gb_free=19, wall=1976
2023-09-05 02:14:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.7651,  1.1240, -0.1749,  0.4419, -0.0150], device='cuda:4',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [-5.7983e-02,  1.4355e-01, -2.1350e-01,  ..., -3.0045e-02,
         -1.2000e-01, -2.9831e-03],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 2.0203e-02,  5.3398e+00,  3.7773e+00,  ..., -7.7759e-02,
         -1.2207e-01, -1.5228e-02]], device='cuda:4', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.7651,  1.1240, -0.1749,  0.4419, -0.0150], device='cuda:4',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.0439,  0.0070,  0.0312, -0.0202, -0.0402], device='cuda:7',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0652,  0.2844, -0.2695,  ..., -0.0942, -0.0770,  0.0809],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0504,  0.3567,  0.1559,  ...,  0.0177, -0.0304,  0.0126]],
       device='cuda:7', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.0439,  0.0070,  0.0312, -0.0202, -0.0402], device='cuda:7',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.4512,  0.6426, -0.1616,  0.0486, -0.1772], device='cuda:6',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0767, -0.7007, -0.2124,  ..., -0.1765, -0.1261,  0.0249],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.3574,  1.1035, -0.6997,  ..., -0.1592, -0.1256, -0.1709]],
       device='cuda:6', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.4512,  0.6426, -0.1616,  0.0486, -0.1772], device='cuda:6',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.1329,  0.0538,  0.0455, -0.0379, -0.0587], device='cuda:5',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2179,  0.4810,  0.3142,  ..., -0.1221, -0.1119,  0.0014],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0150, -0.1040,  0.1118,  ...,  0.0475,  0.0495,  0.0038]],
       device='cuda:5', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.1329,  0.0538,  0.0455, -0.0379, -0.0587], device='cuda:5',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.0484,  0.0650,  0.0115,  0.0021, -0.0641], device='cuda:1',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1506,  0.1161,  0.2286,  ..., -0.1226, -0.1473,  0.0027],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0411, -0.0443,  0.1294,  ...,  0.0638,  0.0025,  0.0224]],
       device='cuda:1', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.0484,  0.0650,  0.0115,  0.0021, -0.0641], device='cuda:1',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-1.3164,  1.6904, -0.8228,  1.2959, -0.4692], device='cuda:3',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 1.3947e-02, -2.8271e-01,  6.3782e-03,  ..., -9.9609e-02,
         -2.5558e-04, -7.1960e-02],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 3.5913e-01,  2.4219e+00, -1.6309e+00,  ..., -1.4905e-01,
         -2.7026e-01, -2.6440e-01]], device='cuda:3', dtype=torch.float16)
task_net layer_norm.weight True tensor([-1.3164,  1.6904, -0.8228,  1.2959, -0.4692], device='cuda:3',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.2496,  0.3862, -0.0892,  0.0159, -0.0405], device='cuda:2',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0676,  0.2360,  0.2771,  ..., -0.1429, -0.1801,  0.0439],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.1556, -0.2773, -0.4543,  ...,  0.0097, -0.0670, -0.0750]],
       device='cuda:2', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.2496,  0.3862, -0.0892,  0.0159, -0.0405], device='cuda:2',
       dtype=torch.float16)
--------------------
2023-09-05 02:15:38 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 10.818 | trans_loss 13.4 | nll_loss 13.169 | w2v_ctc_loss 3.95 | task_loss 0.02 | task_loss_gen 192.844 | contrastive_loss 1.894 | total 4003.4 | n_correct 24.9 | ppl 9211.13 | accuracy 0.622 | uer 55.39 | wer 53.715 | raw_wer 53.715 | bleu 0 | wps 1037.7 | wpb 4003.4 | bsz 141.8 | num_updates 2943 | best_bleu 0
2023-09-05 02:15:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2943 updates
2023-09-05 02:15:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 02:15:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 02:15:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 2 @ 2943 updates, score 0.0) (writing took 14.841548838012386 seconds)
2023-09-05 02:15:53 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-09-05 02:15:53 | INFO | train | epoch 002 | loss 6.781 | trans_loss 6.163 | nll_loss 5.162 | w2v_ctc_loss 3.524 | task_loss 0.581 | task_loss_gen 19.972 | contrastive_loss 1.956 | total 4138.65 | n_correct 25.3073 | ppl 35.79 | accuracy 0.611 | wps 17500.2 | ups 1.42 | wpb 12355.8 | bsz 458.5 | num_updates 2943 | lr 0.000117761 | gnorm 2.436 | clip 0 | loss_scale 8 | train_wall 901 | gb_free 19 | wall 2065
2023-09-05 02:15:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 02:15:53 | INFO | fairseq.trainer | begin training epoch 3
2023-09-05 02:15:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 02:16:37 | INFO | train_inner | epoch 003:     57 / 1474 loss=5.951, trans_loss=6.144, nll_loss=5.138, w2v_ctc_loss=3.06, task_loss=0.001, task_loss_gen=40.291, contrastive_loss=1.338, total=4052.7, n_correct=31.01, ppl=35.21, accuracy=0.765, wps=9132.3, ups=0.75, wpb=12098.1, bsz=436.3, num_updates=3000, lr=0.00012004, gnorm=1.719, clip=0, loss_scale=8, train_wall=62, gb_free=19.4, wall=2108
2023-09-05 02:16:38 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-05 02:16:39 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-09-05 02:18:07 | INFO | train_inner | epoch 003:    159 / 1474 loss=5.242, trans_loss=5.427, nll_loss=4.221, w2v_ctc_loss=2.755, task_loss=4.268, task_loss_gen=13.252, contrastive_loss=1.313, total=4154.28, n_correct=307.75, ppl=18.64, accuracy=7.408, wps=13742.1, ups=1.11, wpb=12404.4, bsz=460.7, num_updates=3100, lr=0.000124038, gnorm=4.129, clip=3, loss_scale=2, train_wall=90, gb_free=16.1, wall=2199
2023-09-05 02:19:37 | INFO | train_inner | epoch 003:    259 / 1474 loss=4.679, trans_loss=5.162, nll_loss=3.87, w2v_ctc_loss=2.451, task_loss=3.198, task_loss_gen=9.353, contrastive_loss=1.154, total=4155.72, n_correct=507.28, ppl=14.63, accuracy=12.207, wps=13800.4, ups=1.11, wpb=12415.9, bsz=465.5, num_updates=3200, lr=0.000128036, gnorm=2.554, clip=0, loss_scale=2, train_wall=89, gb_free=17.3, wall=2289
2023-09-05 02:21:06 | INFO | train_inner | epoch 003:    359 / 1474 loss=4.389, trans_loss=4.967, nll_loss=3.6, w2v_ctc_loss=2.317, task_loss=2.56, task_loss_gen=9.268, contrastive_loss=1.145, total=4154.07, n_correct=650.61, ppl=12.13, accuracy=15.662, wps=13872.6, ups=1.12, wpb=12396.6, bsz=464.9, num_updates=3300, lr=0.000132034, gnorm=2.367, clip=0, loss_scale=2, train_wall=89, gb_free=15.2, wall=2378
2023-09-05 02:22:36 | INFO | train_inner | epoch 003:    459 / 1474 loss=4.139, trans_loss=4.808, nll_loss=3.384, w2v_ctc_loss=2.204, task_loss=1.988, task_loss_gen=10.083, contrastive_loss=1.012, total=4212.17, n_correct=815.03, ppl=10.44, accuracy=19.349, wps=14045.4, ups=1.12, wpb=12572.6, bsz=473.8, num_updates=3400, lr=0.000136032, gnorm=2.219, clip=0, loss_scale=2, train_wall=89, gb_free=15.3, wall=2468
2023-09-05 02:24:05 | INFO | train_inner | epoch 003:    559 / 1474 loss=3.937, trans_loss=4.699, nll_loss=3.241, w2v_ctc_loss=2.121, task_loss=2.057, task_loss_gen=11.374, contrastive_loss=0.914, total=4081.04, n_correct=888.02, ppl=9.45, accuracy=21.76, wps=13661.2, ups=1.12, wpb=12190.8, bsz=440.1, num_updates=3500, lr=0.00014003, gnorm=2.102, clip=0, loss_scale=2, train_wall=89, gb_free=16.1, wall=2557
2023-09-05 02:25:36 | INFO | train_inner | epoch 003:    659 / 1474 loss=3.816, trans_loss=4.606, nll_loss=3.116, w2v_ctc_loss=2.038, task_loss=1.958, task_loss_gen=8.554, contrastive_loss=0.982, total=4231.09, n_correct=1025.8, ppl=8.67, accuracy=24.244, wps=13810.6, ups=1.09, wpb=12615.6, bsz=484.4, num_updates=3600, lr=0.000144028, gnorm=2.068, clip=0, loss_scale=2, train_wall=91, gb_free=15.6, wall=2648
2023-09-05 02:27:05 | INFO | train_inner | epoch 003:    759 / 1474 loss=3.667, trans_loss=4.499, nll_loss=2.984, w2v_ctc_loss=2.001, task_loss=2.177, task_loss_gen=7.965, contrastive_loss=0.743, total=4160.74, n_correct=1110.04, ppl=7.91, accuracy=26.679, wps=14024.9, ups=1.13, wpb=12428.5, bsz=469.2, num_updates=3700, lr=0.000148026, gnorm=2.052, clip=0, loss_scale=2, train_wall=88, gb_free=16.5, wall=2737
2023-09-05 02:28:34 | INFO | train_inner | epoch 003:    859 / 1474 loss=3.528, trans_loss=4.404, nll_loss=2.863, w2v_ctc_loss=1.953, task_loss=2.484, task_loss_gen=8.691, contrastive_loss=0.669, total=4160.47, n_correct=1217.62, ppl=7.27, accuracy=29.266, wps=13952.9, ups=1.12, wpb=12423.6, bsz=455.4, num_updates=3800, lr=0.000152024, gnorm=1.978, clip=0, loss_scale=2, train_wall=88, gb_free=15.9, wall=2826
2023-09-05 02:30:03 | INFO | train_inner | epoch 003:    959 / 1474 loss=3.446, trans_loss=4.337, nll_loss=2.777, w2v_ctc_loss=1.914, task_loss=2.075, task_loss_gen=8.184, contrastive_loss=0.679, total=4162.26, n_correct=1306.29, ppl=6.85, accuracy=31.384, wps=13905.7, ups=1.12, wpb=12416.1, bsz=467.9, num_updates=3900, lr=0.000156022, gnorm=1.882, clip=0, loss_scale=2, train_wall=89, gb_free=17.5, wall=2915
2023-09-05 02:31:32 | INFO | train_inner | epoch 003:   1059 / 1474 loss=3.381, trans_loss=4.301, nll_loss=2.735, w2v_ctc_loss=1.899, task_loss=2.242, task_loss_gen=8.733, contrastive_loss=0.612, total=4062.67, n_correct=1300.07, ppl=6.66, accuracy=32, wps=13673.9, ups=1.13, wpb=12131.7, bsz=443.2, num_updates=4000, lr=0.00016002, gnorm=1.78, clip=0, loss_scale=2, train_wall=88, gb_free=15.3, wall=3004
2023-09-05 02:31:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 02:32:05 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.899 | trans_loss 7.434 | nll_loss 5.355 | w2v_ctc_loss 2.128 | task_loss 5.261 | task_loss_gen 38.78 | contrastive_loss 0.869 | total 4003.4 | n_correct 1361.3 | ppl 40.92 | accuracy 34.004 | uer 32.424 | wer 33.123 | raw_wer 33.123 | bleu 0.46 | wps 1592.9 | wpb 4003.4 | bsz 141.8 | num_updates 4000 | best_bleu 0.46
2023-09-05 02:32:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-09-05 02:32:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_3_4000.pt
2023-09-05 02:32:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_3_4000.pt
2023-09-05 02:32:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 0.46) (writing took 14.915068896021694 seconds)
2023-09-05 02:33:49 | INFO | train_inner | epoch 003:   1159 / 1474 loss=3.318, trans_loss=4.289, nll_loss=2.718, w2v_ctc_loss=1.854, task_loss=2.368, task_loss_gen=8.788, contrastive_loss=0.577, total=4046.76, n_correct=1320.8, ppl=6.58, accuracy=32.638, wps=8801.5, ups=0.73, wpb=12078.6, bsz=433.9, num_updates=4100, lr=0.000164018, gnorm=1.848, clip=0, loss_scale=2, train_wall=88, gb_free=15.7, wall=3141
2023-09-05 02:35:18 | INFO | train_inner | epoch 003:   1259 / 1474 loss=3.252, trans_loss=4.257, nll_loss=2.682, w2v_ctc_loss=1.82, task_loss=2.337, task_loss_gen=7.919, contrastive_loss=0.525, total=4064.26, n_correct=1349.53, ppl=6.42, accuracy=33.205, wps=13738.3, ups=1.13, wpb=12137.6, bsz=434, num_updates=4200, lr=0.000168016, gnorm=1.738, clip=0, loss_scale=2, train_wall=88, gb_free=16.3, wall=3229
2023-09-05 02:36:47 | INFO | train_inner | epoch 003:   1359 / 1474 loss=3.257, trans_loss=4.247, nll_loss=2.668, w2v_ctc_loss=1.781, task_loss=1.827, task_loss_gen=8.047, contrastive_loss=0.651, total=4137.36, n_correct=1390.06, ppl=6.35, accuracy=33.598, wps=13824.2, ups=1.12, wpb=12352.2, bsz=461.5, num_updates=4300, lr=0.000172014, gnorm=1.714, clip=0, loss_scale=2, train_wall=89, gb_free=15.7, wall=3319
2023-09-05 02:38:17 | INFO | train_inner | epoch 003:   1459 / 1474 loss=3.213, trans_loss=4.24, nll_loss=2.66, w2v_ctc_loss=1.755, task_loss=1.983, task_loss_gen=7.729, contrastive_loss=0.607, total=4207.75, n_correct=1433.84, ppl=6.32, accuracy=34.076, wps=13992.2, ups=1.11, wpb=12567.9, bsz=476.7, num_updates=4400, lr=0.000176012, gnorm=1.662, clip=0, loss_scale=2, train_wall=89, gb_free=17.1, wall=3409
2023-09-05 02:38:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 02:39:05 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.761 | trans_loss 7.334 | nll_loss 5.242 | w2v_ctc_loss 2.018 | task_loss 7.954 | task_loss_gen 32.516 | contrastive_loss 0.772 | total 4003.4 | n_correct 1412.8 | ppl 37.83 | accuracy 35.29 | uer 30.247 | wer 30.521 | raw_wer 30.521 | bleu 0.57 | wps 1461.7 | wpb 4003.4 | bsz 141.8 | num_updates 4415 | best_bleu 0.57
2023-09-05 02:39:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4415 updates
2023-09-05 02:39:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 02:39:11 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 02:39:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 3 @ 4415 updates, score 0.57) (writing took 11.876217279001139 seconds)
2023-09-05 02:39:18 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-09-05 02:39:18 | INFO | train | epoch 003 | loss 3.882 | trans_loss 4.646 | nll_loss 3.183 | w2v_ctc_loss 2.097 | task_loss 2.3 | task_loss_gen 10.312 | contrastive_loss 0.849 | total 4138.97 | n_correct 1008.65 | ppl 9.08 | accuracy 24.37 | wps 12951.6 | ups 1.05 | wpb 12356.7 | bsz 458.6 | num_updates 4415 | lr 0.000176612 | gnorm 2.13 | clip 0.2 | loss_scale 2 | train_wall 1293 | gb_free 15.9 | wall 3469
2023-09-05 02:39:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 02:39:18 | INFO | fairseq.trainer | begin training epoch 4
2023-09-05 02:39:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 02:40:40 | INFO | train_inner | epoch 004:     85 / 1474 loss=3.098, trans_loss=4.219, nll_loss=2.632, w2v_ctc_loss=1.713, task_loss=2.608, task_loss_gen=7.69, contrastive_loss=0.414, total=4095.18, n_correct=1409.18, ppl=6.2, accuracy=34.411, wps=8529.9, ups=0.7, wpb=12222.9, bsz=437.8, num_updates=4500, lr=0.00018001, gnorm=1.732, clip=0, loss_scale=2, train_wall=88, gb_free=12.1, wall=3552
2023-09-05 02:42:09 | INFO | train_inner | epoch 004:    185 / 1474 loss=3.086, trans_loss=4.203, nll_loss=2.612, w2v_ctc_loss=1.691, task_loss=1.984, task_loss_gen=6.75, contrastive_loss=0.45, total=4178.83, n_correct=1452.98, ppl=6.11, accuracy=34.77, wps=14050.3, ups=1.13, wpb=12476.1, bsz=469.5, num_updates=4600, lr=0.000184008, gnorm=1.519, clip=0, loss_scale=2, train_wall=88, gb_free=14.4, wall=3641
2023-09-05 02:43:39 | INFO | train_inner | epoch 004:    285 / 1474 loss=3.105, trans_loss=4.204, nll_loss=2.615, w2v_ctc_loss=1.689, task_loss=2.473, task_loss_gen=7.276, contrastive_loss=0.578, total=4142.3, n_correct=1443.53, ppl=6.13, accuracy=34.849, wps=13798.4, ups=1.12, wpb=12373.8, bsz=460.7, num_updates=4700, lr=0.000188006, gnorm=1.691, clip=0, loss_scale=2, train_wall=89, gb_free=12.8, wall=3730
2023-09-05 02:45:07 | INFO | train_inner | epoch 004:    385 / 1474 loss=3.039, trans_loss=4.205, nll_loss=2.613, w2v_ctc_loss=1.668, task_loss=2.357, task_loss_gen=6.939, contrastive_loss=0.387, total=4124.92, n_correct=1440.87, ppl=6.12, accuracy=34.931, wps=13922.2, ups=1.13, wpb=12307.7, bsz=444.2, num_updates=4800, lr=0.000192004, gnorm=1.52, clip=0, loss_scale=2, train_wall=88, gb_free=11.9, wall=3819
2023-09-05 02:46:37 | INFO | train_inner | epoch 004:    485 / 1474 loss=3.104, trans_loss=4.188, nll_loss=2.594, w2v_ctc_loss=1.627, task_loss=2.218, task_loss_gen=6.135, contrastive_loss=0.814, total=4216.09, n_correct=1492.1, ppl=6.04, accuracy=35.391, wps=14045, ups=1.12, wpb=12585.8, bsz=497.6, num_updates=4900, lr=0.000196002, gnorm=1.533, clip=0, loss_scale=2, train_wall=89, gb_free=16.4, wall=3908
2023-09-05 02:48:06 | INFO | train_inner | epoch 004:    585 / 1474 loss=3.04, trans_loss=4.179, nll_loss=2.581, w2v_ctc_loss=1.643, task_loss=2.632, task_loss_gen=6.105, contrastive_loss=0.491, total=4231.12, n_correct=1512.48, ppl=5.98, accuracy=35.747, wps=14158.4, ups=1.12, wpb=12629.4, bsz=490.1, num_updates=5000, lr=0.0002, gnorm=1.607, clip=0, loss_scale=2, train_wall=89, gb_free=15.7, wall=3998
mt_weight tensor(0.5000)
asr_weight tensor(0.2800, device='cuda:0')
2023-09-05 02:49:37 | INFO | train_inner | epoch 004:    685 / 1474 loss=2.997, trans_loss=4.175, nll_loss=2.573, w2v_ctc_loss=1.598, task_loss=3.603, task_loss_gen=6.103, contrastive_loss=0.508, total=4176.95, n_correct=1496.27, ppl=5.95, accuracy=35.822, wps=13724.9, ups=1.1, wpb=12451, bsz=455.2, num_updates=5100, lr=0.00019803, gnorm=1.002, clip=0, loss_scale=4, train_wall=90, gb_free=14.7, wall=4088
2023-09-05 02:51:06 | INFO | train_inner | epoch 004:    785 / 1474 loss=2.927, trans_loss=4.148, nll_loss=2.544, w2v_ctc_loss=1.592, task_loss=3.278, task_loss_gen=6.321, contrastive_loss=0.344, total=4016.91, n_correct=1451, ppl=5.83, accuracy=36.122, wps=13425.3, ups=1.12, wpb=11995.1, bsz=418.7, num_updates=5200, lr=0.000196116, gnorm=0.66, clip=0, loss_scale=4, train_wall=89, gb_free=15.7, wall=4178
2023-09-05 02:52:35 | INFO | train_inner | epoch 004:    885 / 1474 loss=2.96, trans_loss=4.125, nll_loss=2.515, w2v_ctc_loss=1.591, task_loss=2.356, task_loss_gen=6.334, contrastive_loss=0.528, total=4183.4, n_correct=1539.27, ppl=5.72, accuracy=36.795, wps=13980.1, ups=1.12, wpb=12493.2, bsz=465.4, num_updates=5300, lr=0.000194257, gnorm=0.638, clip=0, loss_scale=4, train_wall=89, gb_free=15.1, wall=4267
2023-09-05 02:54:05 | INFO | train_inner | epoch 004:    985 / 1474 loss=2.907, trans_loss=4.11, nll_loss=2.497, w2v_ctc_loss=1.578, task_loss=2.608, task_loss_gen=6.225, contrastive_loss=0.397, total=4128.78, n_correct=1541.85, ppl=5.64, accuracy=37.344, wps=13761.5, ups=1.12, wpb=12332, bsz=456.8, num_updates=5400, lr=0.00019245, gnorm=0.64, clip=0, loss_scale=4, train_wall=89, gb_free=15.6, wall=4357
2023-09-05 02:55:34 | INFO | train_inner | epoch 004:   1085 / 1474 loss=2.886, trans_loss=4.097, nll_loss=2.478, w2v_ctc_loss=1.579, task_loss=3.296, task_loss_gen=6.089, contrastive_loss=0.359, total=4080.2, n_correct=1547.55, ppl=5.57, accuracy=37.928, wps=13680.3, ups=1.12, wpb=12179.1, bsz=437.7, num_updates=5500, lr=0.000190693, gnorm=0.662, clip=0, loss_scale=4, train_wall=88, gb_free=15.7, wall=4446
2023-09-05 02:57:03 | INFO | train_inner | epoch 004:   1185 / 1474 loss=2.867, trans_loss=4.048, nll_loss=2.417, w2v_ctc_loss=1.564, task_loss=2.653, task_loss_gen=5.157, contrastive_loss=0.465, total=4163.45, n_correct=1655.32, ppl=5.34, accuracy=39.758, wps=14021.5, ups=1.13, wpb=12436, bsz=485.6, num_updates=5600, lr=0.000188982, gnorm=0.601, clip=0, loss_scale=4, train_wall=88, gb_free=14.8, wall=4534
2023-09-05 02:58:32 | INFO | train_inner | epoch 004:   1285 / 1474 loss=2.803, trans_loss=3.992, nll_loss=2.343, w2v_ctc_loss=1.561, task_loss=2.408, task_loss_gen=5.476, contrastive_loss=0.401, total=4152.41, n_correct=1728.64, ppl=5.07, accuracy=41.63, wps=13874, ups=1.12, wpb=12401.3, bsz=471.4, num_updates=5700, lr=0.000187317, gnorm=0.629, clip=0, loss_scale=4, train_wall=89, gb_free=12.4, wall=4624
2023-09-05 03:00:00 | INFO | train_inner | epoch 004:   1385 / 1474 loss=2.73, trans_loss=3.942, nll_loss=2.276, w2v_ctc_loss=1.559, task_loss=3.139, task_loss_gen=5.644, contrastive_loss=0.261, total=4103.57, n_correct=1793.97, ppl=4.84, accuracy=43.717, wps=13867.1, ups=1.13, wpb=12255.2, bsz=437.9, num_updates=5800, lr=0.000185695, gnorm=0.645, clip=0, loss_scale=4, train_wall=88, gb_free=16.4, wall=4712
2023-09-05 03:01:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2800, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2800, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2800, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2800, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2800, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2800, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2800, device='cuda:1')
2023-09-05 03:02:00 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 4.833 | trans_loss 6.214 | nll_loss 3.788 | w2v_ctc_loss 1.736 | task_loss 11.949 | task_loss_gen 21.757 | contrastive_loss 0.469 | total 4003.4 | n_correct 2042.1 | ppl 13.81 | accuracy 51.009 | uer 25.567 | wer 26.96 | raw_wer 26.96 | bleu 9.03 | wps 1181.3 | wpb 4003.4 | bsz 141.8 | num_updates 5889 | best_bleu 9.03
2023-09-05 03:02:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 5889 updates
2023-09-05 03:02:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 03:02:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 03:02:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 4 @ 5889 updates, score 9.03) (writing took 12.258150930982083 seconds)
2023-09-05 03:02:13 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-09-05 03:02:13 | INFO | train | epoch 004 | loss 2.95 | trans_loss 4.116 | nll_loss 2.501 | w2v_ctc_loss 1.613 | task_loss 2.705 | task_loss_gen 6.234 | contrastive_loss 0.451 | total 4138.65 | n_correct 1556.23 | ppl 5.66 | accuracy 37.602 | wps 13244.6 | ups 1.07 | wpb 12355.8 | bsz 458.5 | num_updates 5889 | lr 0.000184287 | gnorm 1.048 | clip 0 | loss_scale 4 | train_wall 1305 | gb_free 14.5 | wall 4844
2023-09-05 03:02:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 03:02:13 | INFO | fairseq.trainer | begin training epoch 5
2023-09-05 03:02:13 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 03:02:29 | INFO | train_inner | epoch 005:     11 / 1474 loss=2.675, trans_loss=3.887, nll_loss=2.205, w2v_ctc_loss=1.54, task_loss=3.116, task_loss_gen=5.552, contrastive_loss=0.267, total=4031.51, n_correct=1840.79, ppl=4.61, accuracy=45.66, wps=8094.3, ups=0.67, wpb=12037.5, bsz=436.7, num_updates=5900, lr=0.000184115, gnorm=0.704, clip=0, loss_scale=4, train_wall=87, gb_free=13.9, wall=4861
2023-09-05 03:03:59 | INFO | train_inner | epoch 005:    111 / 1474 loss=2.574, trans_loss=3.817, nll_loss=2.114, w2v_ctc_loss=1.461, task_loss=2.734, task_loss_gen=4.639, contrastive_loss=0.264, total=4256.63, n_correct=2051.47, ppl=4.33, accuracy=48.195, wps=14215.6, ups=1.12, wpb=12710, bsz=500.1, num_updates=6000, lr=0.000182574, gnorm=0.636, clip=0, loss_scale=4, train_wall=89, gb_free=15.8, wall=4950
2023-09-05 03:03:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 03:04:38 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.681 | trans_loss 6.043 | nll_loss 3.562 | w2v_ctc_loss 1.649 | task_loss 7.442 | task_loss_gen 25.376 | contrastive_loss 0.431 | total 4003.4 | n_correct 2121.9 | ppl 11.81 | accuracy 53.002 | uer 24.53 | wer 25.924 | raw_wer 25.924 | bleu 11.07 | wps 1243.3 | wpb 4003.4 | bsz 141.8 | num_updates 6000 | best_bleu 11.07
2023-09-05 03:04:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 6000 updates
2023-09-05 03:04:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_5_6000.pt
2023-09-05 03:04:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_5_6000.pt
2023-09-05 03:04:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_5_6000.pt (epoch 5 @ 6000 updates, score 11.07) (writing took 12.016433417971712 seconds)
2023-09-05 03:06:18 | INFO | train_inner | epoch 005:    211 / 1474 loss=2.604, trans_loss=3.798, nll_loss=2.088, w2v_ctc_loss=1.482, task_loss=3.486, task_loss_gen=4.752, contrastive_loss=0.466, total=4186.83, n_correct=2053.38, ppl=4.25, accuracy=49.044, wps=8978.6, ups=0.72, wpb=12492.2, bsz=485.3, num_updates=6100, lr=0.000181071, gnorm=0.736, clip=0, loss_scale=4, train_wall=87, gb_free=15.7, wall=5089
2023-09-05 03:07:46 | INFO | train_inner | epoch 005:    311 / 1474 loss=2.558, trans_loss=3.765, nll_loss=2.051, w2v_ctc_loss=1.495, task_loss=3.757, task_loss_gen=4.96, contrastive_loss=0.315, total=4094.07, n_correct=2039.08, ppl=4.14, accuracy=49.806, wps=13871.2, ups=1.13, wpb=12241.2, bsz=446, num_updates=6200, lr=0.000179605, gnorm=0.642, clip=0, loss_scale=4, train_wall=88, gb_free=15.8, wall=5178
2023-09-05 03:09:16 | INFO | train_inner | epoch 005:    411 / 1474 loss=2.52, trans_loss=3.732, nll_loss=2.007, w2v_ctc_loss=1.451, task_loss=3.821, task_loss_gen=4.61, contrastive_loss=0.375, total=4140.39, n_correct=2118.43, ppl=4.02, accuracy=51.165, wps=13748.9, ups=1.11, wpb=12372.8, bsz=467.9, num_updates=6300, lr=0.000178174, gnorm=0.691, clip=0, loss_scale=4, train_wall=89, gb_free=15.7, wall=5268
2023-09-05 03:10:44 | INFO | train_inner | epoch 005:    511 / 1474 loss=2.45, trans_loss=3.722, nll_loss=1.994, w2v_ctc_loss=1.458, task_loss=4.427, task_loss_gen=5.163, contrastive_loss=0.147, total=4026.21, n_correct=2074.99, ppl=3.98, accuracy=51.537, wps=13598.5, ups=1.13, wpb=12028.8, bsz=418.1, num_updates=6400, lr=0.000176777, gnorm=0.64, clip=0, loss_scale=4, train_wall=88, gb_free=16.7, wall=5356
2023-09-05 03:12:14 | INFO | train_inner | epoch 005:    611 / 1474 loss=2.49, trans_loss=3.717, nll_loss=1.984, w2v_ctc_loss=1.444, task_loss=5.069, task_loss_gen=4.778, contrastive_loss=0.34, total=4109.94, n_correct=2139.78, ppl=3.96, accuracy=52.064, wps=13753.2, ups=1.12, wpb=12260.2, bsz=449.8, num_updates=6500, lr=0.000175412, gnorm=0.715, clip=0, loss_scale=4, train_wall=89, gb_free=15, wall=5445
2023-09-05 03:13:42 | INFO | train_inner | epoch 005:    711 / 1474 loss=2.471, trans_loss=3.695, nll_loss=1.957, w2v_ctc_loss=1.434, task_loss=4.552, task_loss_gen=4.241, contrastive_loss=0.316, total=4176.83, n_correct=2210.6, ppl=3.88, accuracy=52.925, wps=14017.6, ups=1.12, wpb=12467.5, bsz=483.6, num_updates=6600, lr=0.000174078, gnorm=0.64, clip=0, loss_scale=4, train_wall=88, gb_free=16.7, wall=5534
2023-09-05 03:15:12 | INFO | train_inner | epoch 005:    811 / 1474 loss=2.433, trans_loss=3.68, nll_loss=1.938, w2v_ctc_loss=1.434, task_loss=4.739, task_loss_gen=4.75, contrastive_loss=0.228, total=4127.9, n_correct=2197.99, ppl=3.83, accuracy=53.247, wps=13823.7, ups=1.12, wpb=12321.2, bsz=447.5, num_updates=6700, lr=0.000172774, gnorm=0.679, clip=0, loss_scale=4, train_wall=89, gb_free=15.3, wall=5623
2023-09-05 03:16:41 | INFO | train_inner | epoch 005:    911 / 1474 loss=2.386, trans_loss=3.657, nll_loss=1.91, w2v_ctc_loss=1.415, task_loss=5.159, task_loss_gen=4.632, contrastive_loss=0.178, total=4101.19, n_correct=2212.58, ppl=3.76, accuracy=53.95, wps=13733.2, ups=1.12, wpb=12245.9, bsz=447.9, num_updates=6800, lr=0.000171499, gnorm=0.725, clip=0, loss_scale=4, train_wall=89, gb_free=16.8, wall=5713
2023-09-05 03:18:09 | INFO | train_inner | epoch 005:   1011 / 1474 loss=2.396, trans_loss=3.655, nll_loss=1.907, w2v_ctc_loss=1.415, task_loss=5.206, task_loss_gen=4.446, contrastive_loss=0.251, total=4164.27, n_correct=2256.73, ppl=3.75, accuracy=54.193, wps=14173.9, ups=1.14, wpb=12430.2, bsz=462, num_updates=6900, lr=0.000170251, gnorm=0.78, clip=0, loss_scale=4, train_wall=87, gb_free=14.6, wall=5800
2023-09-05 03:19:38 | INFO | train_inner | epoch 005:   1111 / 1474 loss=2.403, trans_loss=3.643, nll_loss=1.89, w2v_ctc_loss=1.422, task_loss=4.954, task_loss_gen=4.413, contrastive_loss=0.253, total=4168.94, n_correct=2279, ppl=3.71, accuracy=54.666, wps=13887.9, ups=1.12, wpb=12436.1, bsz=464.1, num_updates=7000, lr=0.000169031, gnorm=0.642, clip=0, loss_scale=4, train_wall=89, gb_free=16.2, wall=5890
2023-09-05 03:21:07 | INFO | train_inner | epoch 005:   1211 / 1474 loss=2.355, trans_loss=3.637, nll_loss=1.882, w2v_ctc_loss=1.394, task_loss=5.409, task_loss_gen=4.538, contrastive_loss=0.159, total=4171.16, n_correct=2286, ppl=3.69, accuracy=54.805, wps=13947, ups=1.12, wpb=12443.2, bsz=456.3, num_updates=7100, lr=0.000167836, gnorm=0.663, clip=0, loss_scale=8, train_wall=89, gb_free=15.5, wall=5979
2023-09-05 03:22:37 | INFO | train_inner | epoch 005:   1311 / 1474 loss=2.318, trans_loss=3.621, nll_loss=1.863, w2v_ctc_loss=1.38, task_loss=5.111, task_loss_gen=4.513, contrastive_loss=0.112, total=4126.97, n_correct=2284.67, ppl=3.64, accuracy=55.36, wps=13797.9, ups=1.12, wpb=12317.9, bsz=443.4, num_updates=7200, lr=0.000166667, gnorm=0.515, clip=0, loss_scale=8, train_wall=89, gb_free=15.1, wall=6068
2023-09-05 03:24:06 | INFO | train_inner | epoch 005:   1411 / 1474 loss=2.325, trans_loss=3.615, nll_loss=1.858, w2v_ctc_loss=1.374, task_loss=4.732, task_loss_gen=4.521, contrastive_loss=0.171, total=4138.54, n_correct=2301.4, ppl=3.63, accuracy=55.609, wps=13833.9, ups=1.12, wpb=12359.3, bsz=459, num_updates=7300, lr=0.000165521, gnorm=0.507, clip=0, loss_scale=8, train_wall=89, gb_free=16.3, wall=6158
2023-09-05 03:25:02 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 03:25:36 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 4.295 | trans_loss 5.558 | nll_loss 2.944 | w2v_ctc_loss 1.585 | task_loss 35.32 | task_loss_gen 19.725 | contrastive_loss 0.3 | total 4003.4 | n_correct 2418.3 | ppl 7.7 | accuracy 60.406 | uer 23.996 | wer 25.734 | raw_wer 25.734 | bleu 16.57 | wps 1569.9 | wpb 4003.4 | bsz 141.8 | num_updates 7363 | best_bleu 16.57
2023-09-05 03:25:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 7363 updates
2023-09-05 03:25:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 03:25:43 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 03:25:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 5 @ 7363 updates, score 16.57) (writing took 11.813909909978975 seconds)
2023-09-05 03:25:48 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-09-05 03:25:48 | INFO | train | epoch 005 | loss 2.446 | trans_loss 3.694 | nll_loss 1.957 | w2v_ctc_loss 1.431 | task_loss 4.506 | task_loss_gen 4.645 | contrastive_loss 0.255 | total 4138.65 | n_correct 2180.85 | ppl 3.88 | accuracy 52.695 | wps 12867 | ups 1.04 | wpb 12355.8 | bsz 458.5 | num_updates 7363 | lr 0.000164812 | gnorm 0.655 | clip 0 | loss_scale 8 | train_wall 1302 | gb_free 15.8 | wall 6260
2023-09-05 03:25:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 03:25:48 | INFO | fairseq.trainer | begin training epoch 6
2023-09-05 03:25:48 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 03:26:28 | INFO | train_inner | epoch 006:     37 / 1474 loss=2.316, trans_loss=3.597, nll_loss=1.833, w2v_ctc_loss=1.372, task_loss=5.107, task_loss_gen=4.729, contrastive_loss=0.172, total=4113.87, n_correct=2298.58, ppl=3.56, accuracy=55.874, wps=8620.4, ups=0.7, wpb=12276.7, bsz=447.4, num_updates=7400, lr=0.000164399, gnorm=0.556, clip=0, loss_scale=8, train_wall=89, gb_free=17.5, wall=6300
2023-09-05 03:27:57 | INFO | train_inner | epoch 006:    137 / 1474 loss=2.267, trans_loss=3.566, nll_loss=1.793, w2v_ctc_loss=1.318, task_loss=4.975, task_loss_gen=4.507, contrastive_loss=0.208, total=4161.2, n_correct=2366.34, ppl=3.47, accuracy=56.867, wps=13985.1, ups=1.13, wpb=12428.5, bsz=458.1, num_updates=7500, lr=0.000163299, gnorm=0.498, clip=0, loss_scale=8, train_wall=88, gb_free=16.5, wall=6389
2023-09-05 03:29:26 | INFO | train_inner | epoch 006:    237 / 1474 loss=2.266, trans_loss=3.572, nll_loss=1.803, w2v_ctc_loss=1.351, task_loss=5.379, task_loss_gen=4.893, contrastive_loss=0.112, total=4110.12, n_correct=2328.44, ppl=3.49, accuracy=56.651, wps=13877.6, ups=1.13, wpb=12279.2, bsz=437.3, num_updates=7600, lr=0.000162221, gnorm=0.541, clip=0, loss_scale=8, train_wall=88, gb_free=16.8, wall=6477
2023-09-05 03:30:56 | INFO | train_inner | epoch 006:    337 / 1474 loss=2.305, trans_loss=3.56, nll_loss=1.788, w2v_ctc_loss=1.299, task_loss=4.774, task_loss_gen=4.33, contrastive_loss=0.428, total=4170.52, n_correct=2381.92, ppl=3.45, accuracy=57.113, wps=13756.1, ups=1.1, wpb=12453.1, bsz=488.4, num_updates=7700, lr=0.000161165, gnorm=0.547, clip=0, loss_scale=8, train_wall=90, gb_free=15.3, wall=6568
2023-09-05 03:32:25 | INFO | train_inner | epoch 006:    437 / 1474 loss=2.234, trans_loss=3.553, nll_loss=1.778, w2v_ctc_loss=1.307, task_loss=4.964, task_loss_gen=4.364, contrastive_loss=0.128, total=4154.89, n_correct=2390.42, ppl=3.43, accuracy=57.533, wps=14035.9, ups=1.13, wpb=12405.6, bsz=470.4, num_updates=7800, lr=0.000160128, gnorm=0.489, clip=0, loss_scale=8, train_wall=88, gb_free=16, wall=6656
2023-09-05 03:33:55 | INFO | train_inner | epoch 006:    537 / 1474 loss=2.232, trans_loss=3.554, nll_loss=1.778, w2v_ctc_loss=1.316, task_loss=5.152, task_loss_gen=4.501, contrastive_loss=0.115, total=4174.46, n_correct=2405.99, ppl=3.43, accuracy=57.636, wps=13838.5, ups=1.11, wpb=12460.1, bsz=458.3, num_updates=7900, lr=0.000159111, gnorm=0.514, clip=0, loss_scale=8, train_wall=89, gb_free=16.9, wall=6746
2023-09-05 03:35:23 | INFO | train_inner | epoch 006:    637 / 1474 loss=2.235, trans_loss=3.553, nll_loss=1.777, w2v_ctc_loss=1.298, task_loss=5.029, task_loss_gen=4.407, contrastive_loss=0.171, total=4145.19, n_correct=2391.14, ppl=3.43, accuracy=57.685, wps=14020.8, ups=1.13, wpb=12372.7, bsz=470.9, num_updates=8000, lr=0.000158114, gnorm=0.528, clip=0, loss_scale=8, train_wall=88, gb_free=15.5, wall=6835
2023-09-05 03:35:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 03:35:57 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.223 | trans_loss 5.476 | nll_loss 2.833 | w2v_ctc_loss 1.557 | task_loss 19.687 | task_loss_gen 20.714 | contrastive_loss 0.267 | total 4003.4 | n_correct 2459.2 | ppl 7.13 | accuracy 61.428 | uer 22.849 | wer 24.581 | raw_wer 24.581 | bleu 17.49 | wps 1583 | wpb 4003.4 | bsz 141.8 | num_updates 8000 | best_bleu 17.49
2023-09-05 03:35:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8000 updates
2023-09-05 03:35:57 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_6_8000.pt
2023-09-05 03:35:59 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_6_8000.pt
2023-09-05 03:36:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_6_8000.pt (epoch 6 @ 8000 updates, score 17.49) (writing took 15.603109645016957 seconds)
2023-09-05 03:37:41 | INFO | train_inner | epoch 006:    737 / 1474 loss=2.229, trans_loss=3.552, nll_loss=1.777, w2v_ctc_loss=1.311, task_loss=5.448, task_loss_gen=4.675, contrastive_loss=0.121, total=4151.01, n_correct=2391.28, ppl=3.43, accuracy=57.607, wps=8969.3, ups=0.72, wpb=12393.7, bsz=454.2, num_updates=8100, lr=0.000157135, gnorm=0.525, clip=0, loss_scale=8, train_wall=88, gb_free=12.6, wall=6973
2023-09-05 03:39:10 | INFO | train_inner | epoch 006:    837 / 1474 loss=2.222, trans_loss=3.553, nll_loss=1.778, w2v_ctc_loss=1.305, task_loss=5.504, task_loss_gen=4.887, contrastive_loss=0.105, total=4108.83, n_correct=2363.99, ppl=3.43, accuracy=57.534, wps=13853.5, ups=1.13, wpb=12267.1, bsz=439.4, num_updates=8200, lr=0.000156174, gnorm=0.483, clip=0, loss_scale=8, train_wall=88, gb_free=16.8, wall=7061
2023-09-05 03:40:38 | INFO | train_inner | epoch 006:    937 / 1474 loss=2.245, trans_loss=3.552, nll_loss=1.776, w2v_ctc_loss=1.309, task_loss=5.649, task_loss_gen=4.771, contrastive_loss=0.203, total=4076.46, n_correct=2352.49, ppl=3.42, accuracy=57.709, wps=13729.4, ups=1.13, wpb=12166, bsz=443, num_updates=8300, lr=0.00015523, gnorm=0.53, clip=0, loss_scale=8, train_wall=88, gb_free=12.2, wall=7150
2023-09-05 03:42:08 | INFO | train_inner | epoch 006:   1037 / 1474 loss=2.237, trans_loss=3.537, nll_loss=1.757, w2v_ctc_loss=1.282, task_loss=5.012, task_loss_gen=4.227, contrastive_loss=0.279, total=4175.9, n_correct=2426.03, ppl=3.38, accuracy=58.096, wps=13926.6, ups=1.12, wpb=12465, bsz=480.4, num_updates=8400, lr=0.000154303, gnorm=0.491, clip=0, loss_scale=8, train_wall=89, gb_free=13.8, wall=7239
2023-09-05 03:43:36 | INFO | train_inner | epoch 006:   1137 / 1474 loss=2.21, trans_loss=3.538, nll_loss=1.758, w2v_ctc_loss=1.298, task_loss=5.752, task_loss_gen=4.969, contrastive_loss=0.108, total=4077.2, n_correct=2361.15, ppl=3.38, accuracy=57.911, wps=13719.5, ups=1.13, wpb=12172.7, bsz=430.6, num_updates=8500, lr=0.000153393, gnorm=0.539, clip=0, loss_scale=8, train_wall=88, gb_free=15.9, wall=7328
2023-09-05 03:45:06 | INFO | train_inner | epoch 006:   1237 / 1474 loss=2.259, trans_loss=3.528, nll_loss=1.748, w2v_ctc_loss=1.277, task_loss=5.172, task_loss_gen=4.407, contrastive_loss=0.426, total=4133.46, n_correct=2409.34, ppl=3.36, accuracy=58.289, wps=13827.7, ups=1.12, wpb=12346.4, bsz=470.3, num_updates=8600, lr=0.000152499, gnorm=0.492, clip=0, loss_scale=8, train_wall=89, gb_free=11.9, wall=7417
2023-09-05 03:46:34 | INFO | train_inner | epoch 006:   1337 / 1474 loss=2.183, trans_loss=3.531, nll_loss=1.747, w2v_ctc_loss=1.276, task_loss=5.17, task_loss_gen=4.449, contrastive_loss=0.089, total=4127.77, n_correct=2415.97, ppl=3.36, accuracy=58.53, wps=14009.4, ups=1.14, wpb=12312.7, bsz=454.1, num_updates=8700, lr=0.00015162, gnorm=0.491, clip=0, loss_scale=8, train_wall=87, gb_free=16.6, wall=7505
2023-09-05 03:48:03 | INFO | train_inner | epoch 006:   1437 / 1474 loss=2.177, trans_loss=3.523, nll_loss=1.739, w2v_ctc_loss=1.271, task_loss=5.27, task_loss_gen=4.543, contrastive_loss=0.093, total=4190.32, n_correct=2460.78, ppl=3.34, accuracy=58.725, wps=14067.8, ups=1.12, wpb=12507.6, bsz=460.5, num_updates=8800, lr=0.000150756, gnorm=0.483, clip=0, loss_scale=8, train_wall=88, gb_free=16.5, wall=7594
2023-09-05 03:48:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 03:49:08 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.151 | trans_loss 5.403 | nll_loss 2.737 | w2v_ctc_loss 1.489 | task_loss 48.145 | task_loss_gen 20.756 | contrastive_loss 0.258 | total 4003.4 | n_correct 2510.7 | ppl 6.67 | accuracy 62.714 | uer 22.029 | wer 23.638 | raw_wer 23.638 | bleu 18.89 | wps 1598.5 | wpb 4003.4 | bsz 141.8 | num_updates 8837 | best_bleu 18.89
2023-09-05 03:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8837 updates
2023-09-05 03:49:08 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 03:49:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 03:49:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 6 @ 8837 updates, score 18.89) (writing took 12.915614274970721 seconds)
2023-09-05 03:49:22 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-09-05 03:49:22 | INFO | train | epoch 006 | loss 2.235 | trans_loss 3.548 | nll_loss 1.771 | w2v_ctc_loss 1.301 | task_loss 5.209 | task_loss_gen 4.554 | contrastive_loss 0.184 | total 4138.65 | n_correct 2388.15 | ppl 3.41 | accuracy 57.704 | wps 12882.3 | ups 1.04 | wpb 12355.8 | bsz 458.5 | num_updates 8837 | lr 0.00015044 | gnorm 0.509 | clip 0 | loss_scale 8 | train_wall 1301 | gb_free 14.6 | wall 7674
2023-09-05 03:49:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 03:49:22 | INFO | fairseq.trainer | begin training epoch 7
2023-09-05 03:49:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 03:50:26 | INFO | train_inner | epoch 007:     63 / 1474 loss=2.152, trans_loss=3.508, nll_loss=1.721, w2v_ctc_loss=1.241, task_loss=4.998, task_loss_gen=4.406, contrastive_loss=0.11, total=4110.43, n_correct=2429.95, ppl=3.3, accuracy=59.117, wps=8571.4, ups=0.7, wpb=12272.9, bsz=462.8, num_updates=8900, lr=0.000149906, gnorm=0.494, clip=0, loss_scale=8, train_wall=88, gb_free=17.1, wall=7737
2023-09-05 03:51:55 | INFO | train_inner | epoch 007:    163 / 1474 loss=2.157, trans_loss=3.501, nll_loss=1.71, w2v_ctc_loss=1.232, task_loss=5.13, task_loss_gen=4.544, contrastive_loss=0.182, total=4109.53, n_correct=2434.79, ppl=3.27, accuracy=59.247, wps=13817.7, ups=1.13, wpb=12269.6, bsz=454.1, num_updates=9000, lr=0.000149071, gnorm=0.475, clip=0, loss_scale=8, train_wall=88, gb_free=13.2, wall=7826
2023-09-05 03:53:23 | INFO | train_inner | epoch 007:    263 / 1474 loss=2.131, trans_loss=3.492, nll_loss=1.698, w2v_ctc_loss=1.23, task_loss=4.903, task_loss_gen=4.384, contrastive_loss=0.089, total=4133.29, n_correct=2461.54, ppl=3.25, accuracy=59.554, wps=13989.6, ups=1.13, wpb=12335.8, bsz=455.5, num_updates=9100, lr=0.00014825, gnorm=0.465, clip=0, loss_scale=8, train_wall=88, gb_free=14.8, wall=7914
2023-09-05 03:54:52 | INFO | train_inner | epoch 007:    363 / 1474 loss=2.178, trans_loss=3.498, nll_loss=1.706, w2v_ctc_loss=1.218, task_loss=4.453, task_loss_gen=4.319, contrastive_loss=0.352, total=4194.76, n_correct=2492.43, ppl=3.26, accuracy=59.418, wps=14095.8, ups=1.13, wpb=12518.2, bsz=477.6, num_updates=9200, lr=0.000147442, gnorm=0.451, clip=0, loss_scale=16, train_wall=88, gb_free=12.6, wall=8003
2023-09-05 03:56:20 | INFO | train_inner | epoch 007:    463 / 1474 loss=2.162, trans_loss=3.493, nll_loss=1.702, w2v_ctc_loss=1.219, task_loss=4.218, task_loss_gen=4.497, contrastive_loss=0.273, total=4153.22, n_correct=2469.81, ppl=3.25, accuracy=59.467, wps=14067.2, ups=1.13, wpb=12403.3, bsz=463, num_updates=9300, lr=0.000146647, gnorm=0.435, clip=0, loss_scale=16, train_wall=87, gb_free=16.4, wall=8091
2023-09-05 03:57:48 | INFO | train_inner | epoch 007:    563 / 1474 loss=2.121, trans_loss=3.489, nll_loss=1.694, w2v_ctc_loss=1.218, task_loss=4.109, task_loss_gen=4.556, contrastive_loss=0.093, total=4168.14, n_correct=2495.16, ppl=3.23, accuracy=59.863, wps=14063.5, ups=1.13, wpb=12434.7, bsz=459.8, num_updates=9400, lr=0.000145865, gnorm=0.435, clip=0, loss_scale=16, train_wall=88, gb_free=16.5, wall=8180
2023-09-05 03:59:18 | INFO | train_inner | epoch 007:    663 / 1474 loss=2.111, trans_loss=3.487, nll_loss=1.692, w2v_ctc_loss=1.211, task_loss=3.882, task_loss_gen=4.692, contrastive_loss=0.083, total=4157.82, n_correct=2489.69, ppl=3.23, accuracy=59.88, wps=13850.7, ups=1.12, wpb=12406.4, bsz=455.4, num_updates=9500, lr=0.000145095, gnorm=0.424, clip=0, loss_scale=16, train_wall=89, gb_free=15.1, wall=8269
2023-09-05 04:00:47 | INFO | train_inner | epoch 007:    763 / 1474 loss=2.109, trans_loss=3.48, nll_loss=1.685, w2v_ctc_loss=1.212, task_loss=4.369, task_loss_gen=4.978, contrastive_loss=0.078, total=4122.1, n_correct=2468.01, ppl=3.22, accuracy=59.873, wps=13797.1, ups=1.12, wpb=12308.4, bsz=446.2, num_updates=9600, lr=0.000144338, gnorm=0.446, clip=0, loss_scale=16, train_wall=89, gb_free=15.3, wall=8359
2023-09-05 04:02:17 | INFO | train_inner | epoch 007:    863 / 1474 loss=2.111, trans_loss=3.488, nll_loss=1.694, w2v_ctc_loss=1.209, task_loss=3.892, task_loss_gen=4.774, contrastive_loss=0.095, total=4147.23, n_correct=2480.2, ppl=3.24, accuracy=59.804, wps=13799.5, ups=1.11, wpb=12377.6, bsz=460.9, num_updates=9700, lr=0.000143592, gnorm=0.434, clip=0, loss_scale=16, train_wall=89, gb_free=17.2, wall=8448
2023-09-05 04:03:46 | INFO | train_inner | epoch 007:    963 / 1474 loss=2.126, trans_loss=3.479, nll_loss=1.685, w2v_ctc_loss=1.2, task_loss=3.483, task_loss_gen=4.62, contrastive_loss=0.189, total=4140.14, n_correct=2486.95, ppl=3.21, accuracy=60.069, wps=13904.2, ups=1.13, wpb=12359.1, bsz=474.6, num_updates=9800, lr=0.000142857, gnorm=0.429, clip=0, loss_scale=16, train_wall=88, gb_free=15.5, wall=8537
2023-09-05 04:05:14 | INFO | train_inner | epoch 007:   1063 / 1474 loss=2.108, trans_loss=3.488, nll_loss=1.695, w2v_ctc_loss=1.213, task_loss=4.104, task_loss_gen=5.004, contrastive_loss=0.067, total=4103.51, n_correct=2457.57, ppl=3.24, accuracy=59.889, wps=13848.2, ups=1.13, wpb=12251.1, bsz=437.5, num_updates=9900, lr=0.000142134, gnorm=0.434, clip=0, loss_scale=16, train_wall=88, gb_free=16.4, wall=8626
2023-09-05 04:06:43 | INFO | train_inner | epoch 007:   1163 / 1474 loss=2.158, trans_loss=3.473, nll_loss=1.678, w2v_ctc_loss=1.198, task_loss=3.394, task_loss_gen=4.796, contrastive_loss=0.329, total=4137.04, n_correct=2489.96, ppl=3.2, accuracy=60.187, wps=13816.8, ups=1.12, wpb=12361.6, bsz=470.9, num_updates=10000, lr=0.000141421, gnorm=0.439, clip=0, loss_scale=16, train_wall=89, gb_free=15.6, wall=8715
2023-09-05 04:06:43 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 04:07:19 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.08 | trans_loss 5.342 | nll_loss 2.659 | w2v_ctc_loss 1.402 | task_loss 23.637 | task_loss_gen 19.508 | contrastive_loss 0.25 | total 4003.4 | n_correct 2547.4 | ppl 6.32 | accuracy 63.631 | uer 20.712 | wer 22.382 | raw_wer 22.382 | bleu 19.41 | wps 1479.5 | wpb 4003.4 | bsz 141.8 | num_updates 10000 | best_bleu 19.41
2023-09-05 04:07:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10000 updates
2023-09-05 04:07:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_7_10000.pt
2023-09-05 04:07:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_7_10000.pt
2023-09-05 04:07:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_7_10000.pt (epoch 7 @ 10000 updates, score 19.41) (writing took 13.184729649976362 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:0')
2023-09-05 04:09:01 | INFO | train_inner | epoch 007:   1263 / 1474 loss=2.1, trans_loss=3.477, nll_loss=1.683, w2v_ctc_loss=1.198, task_loss=3.808, task_loss_gen=5.046, contrastive_loss=0.093, total=4129.52, n_correct=2481, ppl=3.21, accuracy=60.08, wps=8991.3, ups=0.73, wpb=12331.4, bsz=450.2, num_updates=10100, lr=0.00014072, gnorm=0.428, clip=0, loss_scale=16, train_wall=87, gb_free=16.4, wall=8852
2023-09-05 04:10:29 | INFO | train_inner | epoch 007:   1363 / 1474 loss=2.112, trans_loss=3.471, nll_loss=1.674, w2v_ctc_loss=1.205, task_loss=3.412, task_loss_gen=4.591, contrastive_loss=0.124, total=4172.87, n_correct=2522.15, ppl=3.19, accuracy=60.442, wps=14142.1, ups=1.14, wpb=12458.1, bsz=476.2, num_updates=10200, lr=0.000140028, gnorm=0.406, clip=0, loss_scale=16, train_wall=88, gb_free=16.8, wall=8940
2023-09-05 04:11:59 | INFO | train_inner | epoch 007:   1463 / 1474 loss=2.121, trans_loss=3.474, nll_loss=1.679, w2v_ctc_loss=1.201, task_loss=3.889, task_loss_gen=5.275, contrastive_loss=0.191, total=4109.42, n_correct=2471.76, ppl=3.2, accuracy=60.149, wps=13559.6, ups=1.1, wpb=12278.1, bsz=443.8, num_updates=10300, lr=0.000139347, gnorm=0.427, clip=0, loss_scale=16, train_wall=90, gb_free=15.9, wall=9031
2023-09-05 04:12:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:1')
2023-09-05 04:12:42 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 4.087 | trans_loss 5.342 | nll_loss 2.663 | w2v_ctc_loss 1.417 | task_loss 19.56 | task_loss_gen 19.948 | contrastive_loss 0.256 | total 4003.4 | n_correct 2541.6 | ppl 6.34 | accuracy 63.486 | uer 21.514 | wer 23.344 | raw_wer 23.344 | bleu 19.36 | wps 1631.1 | wpb 4003.4 | bsz 141.8 | num_updates 10311 | best_bleu 19.41
2023-09-05 04:12:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 10311 updates
2023-09-05 04:12:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_19.3604.pt
2023-09-05 04:12:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_19.3604.pt
2023-09-05 04:12:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_19.3604.pt (epoch 7 @ 10311 updates, score 19.36) (writing took 7.694986767019145 seconds)
2023-09-05 04:12:50 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-09-05 04:12:50 | INFO | train | epoch 007 | loss 2.129 | trans_loss 3.485 | nll_loss 1.692 | w2v_ctc_loss 1.213 | task_loss 4.11 | task_loss_gen 4.712 | contrastive_loss 0.158 | total 4138.65 | n_correct 2476.14 | ppl 3.23 | accuracy 59.83 | wps 12935.7 | ups 1.05 | wpb 12355.8 | bsz 458.5 | num_updates 10311 | lr 0.000139272 | gnorm 0.441 | clip 0 | loss_scale 16 | train_wall 1301 | gb_free 12.7 | wall 9081
2023-09-05 04:12:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 04:12:50 | INFO | fairseq.trainer | begin training epoch 8
2023-09-05 04:12:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 04:14:16 | INFO | train_inner | epoch 008:     89 / 1474 loss=2.067, trans_loss=3.463, nll_loss=1.659, w2v_ctc_loss=1.163, task_loss=3.476, task_loss_gen=5.496, contrastive_loss=0.087, total=4116.25, n_correct=2499.83, ppl=3.16, accuracy=60.731, wps=8995.9, ups=0.73, wpb=12273, bsz=443.3, num_updates=10400, lr=0.000138675, gnorm=0.432, clip=0, loss_scale=16, train_wall=87, gb_free=16.6, wall=9167
2023-09-05 04:15:45 | INFO | train_inner | epoch 008:    189 / 1474 loss=2.074, trans_loss=3.453, nll_loss=1.647, w2v_ctc_loss=1.166, task_loss=3.928, task_loss_gen=5.356, contrastive_loss=0.112, total=4037.23, n_correct=2458.04, ppl=3.13, accuracy=60.884, wps=13539.2, ups=1.12, wpb=12041.5, bsz=428.6, num_updates=10500, lr=0.000138013, gnorm=0.471, clip=0, loss_scale=16, train_wall=88, gb_free=12.3, wall=9256
2023-09-05 04:17:14 | INFO | train_inner | epoch 008:    289 / 1474 loss=2.061, trans_loss=3.447, nll_loss=1.641, w2v_ctc_loss=1.159, task_loss=3.305, task_loss_gen=4.681, contrastive_loss=0.097, total=4207.78, n_correct=2572.8, ppl=3.12, accuracy=61.144, wps=14082.5, ups=1.12, wpb=12556.5, bsz=488.1, num_updates=10600, lr=0.000137361, gnorm=0.431, clip=0, loss_scale=16, train_wall=89, gb_free=12.4, wall=9345
2023-09-05 04:18:44 | INFO | train_inner | epoch 008:    389 / 1474 loss=2.082, trans_loss=3.452, nll_loss=1.648, w2v_ctc_loss=1.175, task_loss=3.397, task_loss_gen=5.487, contrastive_loss=0.131, total=4127.24, n_correct=2517.01, ppl=3.13, accuracy=60.985, wps=13700.5, ups=1.11, wpb=12316.2, bsz=441.4, num_updates=10700, lr=0.000136717, gnorm=0.419, clip=0, loss_scale=16, train_wall=89, gb_free=11.2, wall=9435
2023-09-05 04:20:14 | INFO | train_inner | epoch 008:    489 / 1474 loss=2.132, trans_loss=3.45, nll_loss=1.647, w2v_ctc_loss=1.151, task_loss=3.286, task_loss_gen=4.713, contrastive_loss=0.389, total=4203.76, n_correct=2563.66, ppl=3.13, accuracy=60.985, wps=13968.1, ups=1.11, wpb=12548.2, bsz=504.5, num_updates=10800, lr=0.000136083, gnorm=0.474, clip=0, loss_scale=16, train_wall=89, gb_free=14.1, wall=9525
2023-09-05 04:21:43 | INFO | train_inner | epoch 008:    589 / 1474 loss=2.07, trans_loss=3.449, nll_loss=1.648, w2v_ctc_loss=1.178, task_loss=3.754, task_loss_gen=5.567, contrastive_loss=0.067, total=4062.5, n_correct=2468.8, ppl=3.13, accuracy=60.77, wps=13634.1, ups=1.12, wpb=12145.4, bsz=427.9, num_updates=10900, lr=0.000135457, gnorm=0.434, clip=0, loss_scale=16, train_wall=89, gb_free=10.8, wall=9614
2023-09-05 04:23:11 | INFO | train_inner | epoch 008:    689 / 1474 loss=2.056, trans_loss=3.445, nll_loss=1.639, w2v_ctc_loss=1.164, task_loss=3.423, task_loss_gen=5.265, contrastive_loss=0.076, total=4142.78, n_correct=2536.17, ppl=3.11, accuracy=61.219, wps=13935.6, ups=1.13, wpb=12364.4, bsz=448.6, num_updates=11000, lr=0.00013484, gnorm=0.423, clip=0, loss_scale=16, train_wall=88, gb_free=15.4, wall=9703
2023-09-05 04:24:40 | INFO | train_inner | epoch 008:    789 / 1474 loss=2.07, trans_loss=3.441, nll_loss=1.638, w2v_ctc_loss=1.159, task_loss=3.196, task_loss_gen=5.405, contrastive_loss=0.162, total=4118.9, n_correct=2518.58, ppl=3.11, accuracy=61.147, wps=13846.8, ups=1.12, wpb=12310.9, bsz=447.8, num_updates=11100, lr=0.000134231, gnorm=0.417, clip=0, loss_scale=16, train_wall=88, gb_free=14.8, wall=9792
2023-09-05 04:26:09 | INFO | train_inner | epoch 008:    889 / 1474 loss=2.069, trans_loss=3.445, nll_loss=1.642, w2v_ctc_loss=1.145, task_loss=3.036, task_loss_gen=5.045, contrastive_loss=0.17, total=4169.01, n_correct=2552.62, ppl=3.12, accuracy=61.228, wps=14024.2, ups=1.13, wpb=12452.5, bsz=473.7, num_updates=11200, lr=0.000133631, gnorm=0.427, clip=0, loss_scale=32, train_wall=88, gb_free=15.6, wall=9881
2023-09-05 04:27:37 | INFO | train_inner | epoch 008:    989 / 1474 loss=2.04, trans_loss=3.443, nll_loss=1.638, w2v_ctc_loss=1.146, task_loss=2.8, task_loss_gen=5.253, contrastive_loss=0.071, total=4154.69, n_correct=2552.52, ppl=3.11, accuracy=61.437, wps=14060.6, ups=1.13, wpb=12403.4, bsz=464.9, num_updates=11300, lr=0.000133038, gnorm=0.407, clip=0, loss_scale=32, train_wall=88, gb_free=17.3, wall=9969
2023-09-05 04:28:34 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 04:29:08 | INFO | train_inner | epoch 008:   1090 / 1474 loss=2.083, trans_loss=3.444, nll_loss=1.639, w2v_ctc_loss=1.143, task_loss=2.77, task_loss_gen=5.756, contrastive_loss=0.3, total=4191.1, n_correct=2564.77, ppl=3.11, accuracy=61.196, wps=13813.9, ups=1.1, wpb=12510.3, bsz=464.8, num_updates=11400, lr=0.000132453, gnorm=0.429, clip=0, loss_scale=16, train_wall=90, gb_free=16.4, wall=10060
2023-09-05 04:30:37 | INFO | train_inner | epoch 008:   1190 / 1474 loss=2.045, trans_loss=3.437, nll_loss=1.632, w2v_ctc_loss=1.15, task_loss=3.08, task_loss_gen=5.171, contrastive_loss=0.079, total=4180.55, n_correct=2566.25, ppl=3.1, accuracy=61.385, wps=14080.1, ups=1.13, wpb=12487.2, bsz=472.7, num_updates=11500, lr=0.000131876, gnorm=0.473, clip=0, loss_scale=16, train_wall=88, gb_free=16.9, wall=10148
2023-09-05 04:32:05 | INFO | train_inner | epoch 008:   1290 / 1474 loss=2.06, trans_loss=3.443, nll_loss=1.639, w2v_ctc_loss=1.161, task_loss=3.366, task_loss_gen=5.559, contrastive_loss=0.103, total=4062.6, n_correct=2480.96, ppl=3.11, accuracy=61.068, wps=13709.7, ups=1.13, wpb=12135.3, bsz=437.8, num_updates=11600, lr=0.000131306, gnorm=0.466, clip=0, loss_scale=16, train_wall=88, gb_free=12.6, wall=10237
2023-09-05 04:33:33 | INFO | train_inner | epoch 008:   1390 / 1474 loss=2.067, trans_loss=3.444, nll_loss=1.64, w2v_ctc_loss=1.144, task_loss=3.014, task_loss_gen=5.242, contrastive_loss=0.169, total=4159.11, n_correct=2553.69, ppl=3.12, accuracy=61.4, wps=14073.3, ups=1.13, wpb=12419, bsz=468.7, num_updates=11700, lr=0.000130744, gnorm=0.436, clip=0, loss_scale=16, train_wall=88, gb_free=12.9, wall=10325
2023-09-05 04:34:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 04:35:24 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 4.031 | trans_loss 5.284 | nll_loss 2.584 | w2v_ctc_loss 1.374 | task_loss 24.86 | task_loss_gen 19.822 | contrastive_loss 0.237 | total 4003.4 | n_correct 2580.1 | ppl 6 | accuracy 64.448 | uer 20.216 | wer 21.878 | raw_wer 21.878 | bleu 20.01 | wps 1409.7 | wpb 4003.4 | bsz 141.8 | num_updates 11784 | best_bleu 20.01
2023-09-05 04:35:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 11784 updates
2023-09-05 04:35:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 04:35:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 04:35:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 8 @ 11784 updates, score 20.01) (writing took 12.492240978986956 seconds)
2023-09-05 04:35:37 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-09-05 04:35:37 | INFO | train | epoch 008 | loss 2.069 | trans_loss 3.447 | nll_loss 1.643 | w2v_ctc_loss 1.156 | task_loss 3.285 | task_loss_gen 5.286 | contrastive_loss 0.149 | total 4138.42 | n_correct 2530.01 | ppl 3.12 | accuracy 61.135 | wps 13313.6 | ups 1.08 | wpb 12355.1 | bsz 458.5 | num_updates 11784 | lr 0.000130277 | gnorm 0.443 | clip 0 | loss_scale 16 | train_wall 1301 | gb_free 16.5 | wall 10448
2023-09-05 04:35:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 04:35:37 | INFO | fairseq.trainer | begin training epoch 9
2023-09-05 04:35:37 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 04:35:59 | INFO | train_inner | epoch 009:     16 / 1474 loss=2.068, trans_loss=3.439, nll_loss=1.633, w2v_ctc_loss=1.129, task_loss=3.52, task_loss_gen=5.448, contrastive_loss=0.266, total=4121.25, n_correct=2535.13, ppl=3.1, accuracy=61.514, wps=8451.6, ups=0.69, wpb=12298.5, bsz=466, num_updates=11800, lr=0.000130189, gnorm=0.498, clip=0, loss_scale=16, train_wall=88, gb_free=17.4, wall=10471
2023-09-05 04:37:27 | INFO | train_inner | epoch 009:    116 / 1474 loss=2.01, trans_loss=3.413, nll_loss=1.6, w2v_ctc_loss=1.106, task_loss=3.082, task_loss_gen=5.029, contrastive_loss=0.101, total=4191.82, n_correct=2608.58, ppl=3.03, accuracy=62.23, wps=14188.1, ups=1.13, wpb=12518.2, bsz=479.9, num_updates=11900, lr=0.000129641, gnorm=0.433, clip=0, loss_scale=16, train_wall=88, gb_free=15.6, wall=10559
2023-09-05 04:38:56 | INFO | train_inner | epoch 009:    216 / 1474 loss=2.006, trans_loss=3.417, nll_loss=1.605, w2v_ctc_loss=1.109, task_loss=3.596, task_loss_gen=5.734, contrastive_loss=0.063, total=4061.27, n_correct=2518.91, ppl=3.04, accuracy=62.023, wps=13613.7, ups=1.12, wpb=12126.5, bsz=431.4, num_updates=12000, lr=0.000129099, gnorm=0.48, clip=0, loss_scale=16, train_wall=88, gb_free=17.2, wall=10648
2023-09-05 04:38:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 04:39:31 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.033 | trans_loss 5.296 | nll_loss 2.595 | w2v_ctc_loss 1.357 | task_loss 27.728 | task_loss_gen 19.421 | contrastive_loss 0.238 | total 4003.4 | n_correct 2569.9 | ppl 6.04 | accuracy 64.193 | uer 20.078 | wer 21.759 | raw_wer 21.759 | bleu 19.93 | wps 1534.9 | wpb 4003.4 | bsz 141.8 | num_updates 12000 | best_bleu 20.01
2023-09-05 04:39:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 12000 updates
2023-09-05 04:39:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_9_12000.pt
2023-09-05 04:39:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_9_12000.pt
2023-09-05 04:39:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_9_12000.pt (epoch 9 @ 12000 updates, score 19.93) (writing took 7.565855155989993 seconds)
2023-09-05 04:41:07 | INFO | train_inner | epoch 009:    316 / 1474 loss=2.002, trans_loss=3.407, nll_loss=1.594, w2v_ctc_loss=1.093, task_loss=3.446, task_loss_gen=5.235, contrastive_loss=0.108, total=4146.43, n_correct=2586.87, ppl=3.02, accuracy=62.388, wps=9479.9, ups=0.77, wpb=12389.9, bsz=474.6, num_updates=12100, lr=0.000128565, gnorm=0.514, clip=0, loss_scale=16, train_wall=88, gb_free=15.7, wall=10779
2023-09-05 04:42:37 | INFO | train_inner | epoch 009:    416 / 1474 loss=2.005, trans_loss=3.422, nll_loss=1.611, w2v_ctc_loss=1.105, task_loss=3.31, task_loss_gen=5.228, contrastive_loss=0.075, total=4194.84, n_correct=2597.22, ppl=3.05, accuracy=61.915, wps=13946.5, ups=1.11, wpb=12524.6, bsz=466.7, num_updates=12200, lr=0.000128037, gnorm=0.455, clip=0, loss_scale=16, train_wall=89, gb_free=15.8, wall=10868
2023-09-05 04:44:05 | INFO | train_inner | epoch 009:    516 / 1474 loss=2.037, trans_loss=3.423, nll_loss=1.611, w2v_ctc_loss=1.128, task_loss=3.58, task_loss_gen=5.636, contrastive_loss=0.129, total=4124.3, n_correct=2552.17, ppl=3.05, accuracy=61.881, wps=13943.5, ups=1.13, wpb=12310.3, bsz=439.5, num_updates=12300, lr=0.000127515, gnorm=0.461, clip=0, loss_scale=16, train_wall=88, gb_free=11, wall=10957
2023-09-05 04:45:34 | INFO | train_inner | epoch 009:    616 / 1474 loss=2.001, trans_loss=3.414, nll_loss=1.604, w2v_ctc_loss=1.099, task_loss=3.596, task_loss_gen=5.389, contrastive_loss=0.086, total=4120.96, n_correct=2559.3, ppl=3.04, accuracy=62.104, wps=13832.4, ups=1.12, wpb=12316.4, bsz=453.4, num_updates=12400, lr=0.000127, gnorm=0.466, clip=0, loss_scale=16, train_wall=88, gb_free=15.8, wall=11046
2023-09-05 04:47:02 | INFO | train_inner | epoch 009:    716 / 1474 loss=2.038, trans_loss=3.422, nll_loss=1.612, w2v_ctc_loss=1.123, task_loss=3.517, task_loss_gen=5.431, contrastive_loss=0.171, total=4088.53, n_correct=2529.46, ppl=3.06, accuracy=61.867, wps=13814.5, ups=1.13, wpb=12213.7, bsz=451.4, num_updates=12500, lr=0.000126491, gnorm=0.478, clip=0, loss_scale=16, train_wall=88, gb_free=16.6, wall=11134
2023-09-05 04:48:32 | INFO | train_inner | epoch 009:    816 / 1474 loss=2.066, trans_loss=3.414, nll_loss=1.604, w2v_ctc_loss=1.109, task_loss=3.037, task_loss_gen=4.762, contrastive_loss=0.311, total=4220.43, n_correct=2621.64, ppl=3.04, accuracy=62.118, wps=14111.4, ups=1.12, wpb=12611.1, bsz=501.1, num_updates=12600, lr=0.000125988, gnorm=0.46, clip=0, loss_scale=16, train_wall=89, gb_free=14, wall=11223
2023-09-05 04:50:02 | INFO | train_inner | epoch 009:    916 / 1474 loss=2.05, trans_loss=3.42, nll_loss=1.607, w2v_ctc_loss=1.11, task_loss=3.253, task_loss_gen=5.647, contrastive_loss=0.304, total=4146.05, n_correct=2570.78, ppl=3.05, accuracy=62.006, wps=13672, ups=1.11, wpb=12371.5, bsz=450.3, num_updates=12700, lr=0.000125491, gnorm=0.447, clip=0, loss_scale=16, train_wall=90, gb_free=17.4, wall=11314
2023-09-05 04:51:31 | INFO | train_inner | epoch 009:   1016 / 1474 loss=2.018, trans_loss=3.427, nll_loss=1.617, w2v_ctc_loss=1.117, task_loss=3.714, task_loss_gen=6.155, contrastive_loss=0.081, total=4101.48, n_correct=2538.19, ppl=3.07, accuracy=61.885, wps=13723.3, ups=1.12, wpb=12241.7, bsz=424.4, num_updates=12800, lr=0.000125, gnorm=0.483, clip=0, loss_scale=16, train_wall=89, gb_free=15.5, wall=11403
2023-09-05 04:53:00 | INFO | train_inner | epoch 009:   1116 / 1474 loss=2.013, trans_loss=3.425, nll_loss=1.611, w2v_ctc_loss=1.106, task_loss=3.381, task_loss_gen=4.968, contrastive_loss=0.097, total=4179.09, n_correct=2595.28, ppl=3.05, accuracy=62.102, wps=14019.2, ups=1.13, wpb=12457.7, bsz=474.7, num_updates=12900, lr=0.000124515, gnorm=0.452, clip=0, loss_scale=16, train_wall=88, gb_free=14.8, wall=11492
2023-09-05 04:54:30 | INFO | train_inner | epoch 009:   1216 / 1474 loss=2.015, trans_loss=3.421, nll_loss=1.611, w2v_ctc_loss=1.118, task_loss=3.632, task_loss_gen=5.958, contrastive_loss=0.08, total=4140.66, n_correct=2563.91, ppl=3.05, accuracy=61.92, wps=13828.9, ups=1.12, wpb=12363.4, bsz=448.1, num_updates=13000, lr=0.000124035, gnorm=0.511, clip=0, loss_scale=16, train_wall=89, gb_free=16.7, wall=11581
2023-09-05 04:55:58 | INFO | train_inner | epoch 009:   1316 / 1474 loss=2.041, trans_loss=3.414, nll_loss=1.6, w2v_ctc_loss=1.094, task_loss=3.333, task_loss_gen=4.92, contrastive_loss=0.273, total=4204.43, n_correct=2618.39, ppl=3.03, accuracy=62.277, wps=14161.9, ups=1.13, wpb=12544.9, bsz=492.5, num_updates=13100, lr=0.00012356, gnorm=0.492, clip=0, loss_scale=16, train_wall=88, gb_free=17.4, wall=11670
2023-09-05 04:57:27 | INFO | train_inner | epoch 009:   1416 / 1474 loss=2.011, trans_loss=3.427, nll_loss=1.617, w2v_ctc_loss=1.115, task_loss=3.407, task_loss_gen=5.875, contrastive_loss=0.063, total=4069.19, n_correct=2520.43, ppl=3.07, accuracy=61.939, wps=13694.2, ups=1.13, wpb=12143.2, bsz=427.7, num_updates=13200, lr=0.000123091, gnorm=0.465, clip=0, loss_scale=16, train_wall=88, gb_free=16.1, wall=11759
2023-09-05 04:58:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 04:58:51 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 4.003 | trans_loss 5.26 | nll_loss 2.555 | w2v_ctc_loss 1.342 | task_loss 83.128 | task_loss_gen 30.556 | contrastive_loss 0.234 | total 4003.4 | n_correct 2591.5 | ppl 5.88 | accuracy 64.732 | uer 19.399 | wer 21.14 | raw_wer 21.14 | bleu 20.31 | wps 1588.7 | wpb 4003.4 | bsz 141.8 | num_updates 13258 | best_bleu 20.31
2023-09-05 04:58:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 13258 updates
2023-09-05 04:58:51 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 04:58:58 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 04:59:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 9 @ 13258 updates, score 20.31) (writing took 12.545776618993841 seconds)
2023-09-05 04:59:04 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-09-05 04:59:04 | INFO | train | epoch 009 | loss 2.023 | trans_loss 3.419 | nll_loss 1.607 | w2v_ctc_loss 1.109 | task_loss 3.42 | task_loss_gen 5.397 | contrastive_loss 0.144 | total 4138.65 | n_correct 2568.36 | ppl 3.05 | accuracy 62.058 | wps 12938.4 | ups 1.05 | wpb 12355.8 | bsz 458.5 | num_updates 13258 | lr 0.000122822 | gnorm 0.473 | clip 0 | loss_scale 16 | train_wall 1302 | gb_free 11.1 | wall 11856
2023-09-05 04:59:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 04:59:05 | INFO | fairseq.trainer | begin training epoch 10
2023-09-05 04:59:05 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 04:59:49 | INFO | train_inner | epoch 010:     42 / 1474 loss=2.005, trans_loss=3.41, nll_loss=1.595, w2v_ctc_loss=1.087, task_loss=3.471, task_loss_gen=5.209, contrastive_loss=0.153, total=4100.8, n_correct=2565.56, ppl=3.02, accuracy=62.562, wps=8605.7, ups=0.7, wpb=12238.2, bsz=469.4, num_updates=13300, lr=0.000122628, gnorm=0.489, clip=0, loss_scale=16, train_wall=88, gb_free=15.9, wall=11901
2023-09-05 05:01:18 | INFO | train_inner | epoch 010:    142 / 1474 loss=1.956, trans_loss=3.39, nll_loss=1.571, w2v_ctc_loss=1.056, task_loss=3.053, task_loss_gen=5.158, contrastive_loss=0.075, total=4247.35, n_correct=2678.6, ppl=2.97, accuracy=63.065, wps=14314, ups=1.13, wpb=12684.5, bsz=479.6, num_updates=13400, lr=0.000122169, gnorm=0.446, clip=0, loss_scale=16, train_wall=88, gb_free=11.1, wall=11990
2023-09-05 05:02:46 | INFO | train_inner | epoch 010:    242 / 1474 loss=1.99, trans_loss=3.389, nll_loss=1.568, w2v_ctc_loss=1.07, task_loss=3.347, task_loss_gen=5.426, contrastive_loss=0.2, total=4122.82, n_correct=2597.13, ppl=2.96, accuracy=62.994, wps=13936.5, ups=1.13, wpb=12303.3, bsz=461.4, num_updates=13500, lr=0.000121716, gnorm=0.446, clip=0, loss_scale=32, train_wall=88, gb_free=15.9, wall=12078
2023-09-05 05:04:15 | INFO | train_inner | epoch 010:    342 / 1474 loss=1.968, trans_loss=3.387, nll_loss=1.571, w2v_ctc_loss=1.065, task_loss=2.717, task_loss_gen=5.809, contrastive_loss=0.115, total=4138.27, n_correct=2604.41, ppl=2.97, accuracy=62.935, wps=13935.3, ups=1.13, wpb=12371, bsz=453.8, num_updates=13600, lr=0.000121268, gnorm=0.417, clip=0, loss_scale=32, train_wall=88, gb_free=15.9, wall=12167
2023-09-05 05:05:45 | INFO | train_inner | epoch 010:    442 / 1474 loss=1.99, trans_loss=3.392, nll_loss=1.572, w2v_ctc_loss=1.049, task_loss=2.687, task_loss_gen=5.857, contrastive_loss=0.284, total=4196.37, n_correct=2641.19, ppl=2.97, accuracy=62.94, wps=13888.1, ups=1.11, wpb=12528, bsz=481.1, num_updates=13700, lr=0.000120824, gnorm=0.437, clip=0, loss_scale=32, train_wall=90, gb_free=15.6, wall=12257
2023-09-05 05:07:14 | INFO | train_inner | epoch 010:    542 / 1474 loss=1.979, trans_loss=3.403, nll_loss=1.583, w2v_ctc_loss=1.088, task_loss=2.974, task_loss_gen=6.514, contrastive_loss=0.064, total=4102.8, n_correct=2570.38, ppl=3, accuracy=62.649, wps=13689.8, ups=1.12, wpb=12234.1, bsz=437.8, num_updates=13800, lr=0.000120386, gnorm=0.429, clip=0, loss_scale=32, train_wall=89, gb_free=16.6, wall=12346
2023-09-05 05:08:44 | INFO | train_inner | epoch 010:    642 / 1474 loss=1.995, trans_loss=3.398, nll_loss=1.58, w2v_ctc_loss=1.077, task_loss=2.421, task_loss_gen=5.988, contrastive_loss=0.177, total=4176.56, n_correct=2623.85, ppl=2.99, accuracy=62.823, wps=13949.9, ups=1.12, wpb=12464, bsz=477.2, num_updates=13900, lr=0.000119952, gnorm=0.426, clip=0, loss_scale=32, train_wall=89, gb_free=15.7, wall=12436
2023-09-05 05:10:12 | INFO | train_inner | epoch 010:    742 / 1474 loss=1.98, trans_loss=3.395, nll_loss=1.577, w2v_ctc_loss=1.094, task_loss=2.455, task_loss_gen=6.249, contrastive_loss=0.063, total=4125.87, n_correct=2591.24, ppl=2.98, accuracy=62.805, wps=13976.1, ups=1.13, wpb=12315.3, bsz=454.4, num_updates=14000, lr=0.000119523, gnorm=0.423, clip=0, loss_scale=32, train_wall=88, gb_free=13.9, wall=12524
2023-09-05 05:10:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 05:10:45 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 3.998 | trans_loss 5.254 | nll_loss 2.545 | w2v_ctc_loss 1.339 | task_loss 30.974 | task_loss_gen 20.544 | contrastive_loss 0.236 | total 4003.4 | n_correct 2598.1 | ppl 5.84 | accuracy 64.897 | uer 19.728 | wer 21.543 | raw_wer 21.543 | bleu 20.31 | wps 1626.9 | wpb 4003.4 | bsz 141.8 | num_updates 14000 | best_bleu 20.31
2023-09-05 05:10:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14000 updates
2023-09-05 05:10:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_10_14000.pt
2023-09-05 05:10:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_10_14000.pt
2023-09-05 05:11:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_10_14000.pt (epoch 10 @ 14000 updates, score 20.31) (writing took 16.06563438603189 seconds)
2023-09-05 05:12:30 | INFO | train_inner | epoch 010:    842 / 1474 loss=1.956, trans_loss=3.392, nll_loss=1.574, w2v_ctc_loss=1.062, task_loss=2.396, task_loss_gen=6.382, contrastive_loss=0.065, total=4128.44, n_correct=2600.32, ppl=2.98, accuracy=62.986, wps=8955.7, ups=0.73, wpb=12327.8, bsz=456.3, num_updates=14100, lr=0.000119098, gnorm=0.409, clip=0, loss_scale=32, train_wall=87, gb_free=14.3, wall=12661
2023-09-05 05:13:58 | INFO | train_inner | epoch 010:    942 / 1474 loss=1.973, trans_loss=3.391, nll_loss=1.57, w2v_ctc_loss=1.072, task_loss=2.447, task_loss_gen=6.084, contrastive_loss=0.1, total=4160.94, n_correct=2618.98, ppl=2.97, accuracy=62.942, wps=14030.2, ups=1.13, wpb=12411.1, bsz=468.1, num_updates=14200, lr=0.000118678, gnorm=0.413, clip=0, loss_scale=32, train_wall=88, gb_free=15, wall=12750
2023-09-05 05:15:27 | INFO | train_inner | epoch 010:   1042 / 1474 loss=1.968, trans_loss=3.393, nll_loss=1.575, w2v_ctc_loss=1.074, task_loss=2.737, task_loss_gen=7.287, contrastive_loss=0.074, total=4067.53, n_correct=2556.33, ppl=2.98, accuracy=62.847, wps=13589.9, ups=1.12, wpb=12145, bsz=434.3, num_updates=14300, lr=0.000118262, gnorm=0.432, clip=0, loss_scale=32, train_wall=89, gb_free=16.5, wall=12839
2023-09-05 05:16:56 | INFO | train_inner | epoch 010:   1142 / 1474 loss=1.979, trans_loss=3.401, nll_loss=1.585, w2v_ctc_loss=1.089, task_loss=2.94, task_loss_gen=7.081, contrastive_loss=0.064, total=4044.03, n_correct=2528.08, ppl=3, accuracy=62.514, wps=13622.3, ups=1.13, wpb=12074.4, bsz=422.3, num_updates=14400, lr=0.000117851, gnorm=0.432, clip=0, loss_scale=32, train_wall=88, gb_free=17, wall=12928
2023-09-05 05:18:24 | INFO | train_inner | epoch 010:   1242 / 1474 loss=1.965, trans_loss=3.385, nll_loss=1.568, w2v_ctc_loss=1.081, task_loss=2.57, task_loss_gen=6.607, contrastive_loss=0.057, total=4110.41, n_correct=2585.06, ppl=2.97, accuracy=62.891, wps=13919.6, ups=1.13, wpb=12291.6, bsz=445.2, num_updates=14500, lr=0.000117444, gnorm=0.435, clip=0, loss_scale=32, train_wall=88, gb_free=16.1, wall=13016
2023-09-05 05:19:53 | INFO | train_inner | epoch 010:   1342 / 1474 loss=1.966, trans_loss=3.392, nll_loss=1.574, w2v_ctc_loss=1.075, task_loss=2.423, task_loss_gen=6.729, contrastive_loss=0.069, total=4121.38, n_correct=2596.1, ppl=2.98, accuracy=62.991, wps=13853.1, ups=1.13, wpb=12308.4, bsz=451, num_updates=14600, lr=0.000117041, gnorm=0.425, clip=0, loss_scale=32, train_wall=88, gb_free=13.5, wall=13105
2023-09-05 05:20:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 05:21:23 | INFO | train_inner | epoch 010:   1443 / 1474 loss=1.99, trans_loss=3.402, nll_loss=1.584, w2v_ctc_loss=1.063, task_loss=2.88, task_loss_gen=6.109, contrastive_loss=0.172, total=4177.73, n_correct=2624.4, ppl=3, accuracy=62.819, wps=13807.9, ups=1.11, wpb=12461.7, bsz=472.7, num_updates=14700, lr=0.000116642, gnorm=0.532, clip=0, loss_scale=16, train_wall=90, gb_free=15.9, wall=13195
2023-09-05 05:21:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 05:22:24 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 3.997 | trans_loss 5.243 | nll_loss 2.53 | w2v_ctc_loss 1.359 | task_loss 37.515 | task_loss_gen 21.608 | contrastive_loss 0.237 | total 4003.4 | n_correct 2605.4 | ppl 5.77 | accuracy 65.08 | uer 18.945 | wer 20.745 | raw_wer 20.745 | bleu 20.77 | wps 1598.6 | wpb 4003.4 | bsz 141.8 | num_updates 14731 | best_bleu 20.77
2023-09-05 05:22:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 14731 updates
2023-09-05 05:22:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 05:22:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 05:22:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 10 @ 14731 updates, score 20.77) (writing took 11.819849679013714 seconds)
2023-09-05 05:22:36 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-09-05 05:22:36 | INFO | train | epoch 010 | loss 1.976 | trans_loss 3.393 | nll_loss 1.575 | w2v_ctc_loss 1.071 | task_loss 2.742 | task_loss_gen 6.174 | contrastive_loss 0.124 | total 4137.35 | n_correct 2601.53 | ppl 2.98 | accuracy 62.879 | wps 12887.7 | ups 1.04 | wpb 12351.9 | bsz 457.7 | num_updates 14731 | lr 0.00011652 | gnorm 0.437 | clip 0 | loss_scale 16 | train_wall 1300 | gb_free 16.9 | wall 13268
2023-09-05 05:22:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 05:22:36 | INFO | fairseq.trainer | begin training epoch 11
2023-09-05 05:22:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 05:23:44 | INFO | train_inner | epoch 011:     69 / 1474 loss=1.955, trans_loss=3.374, nll_loss=1.55, w2v_ctc_loss=1.045, task_loss=3.181, task_loss_gen=5.535, contrastive_loss=0.147, total=4166, n_correct=2645.4, ppl=2.93, accuracy=63.5, wps=8836.9, ups=0.71, wpb=12436.1, bsz=475.7, num_updates=14800, lr=0.000116248, gnorm=0.521, clip=0, loss_scale=16, train_wall=87, gb_free=17.6, wall=13336
2023-09-05 05:25:13 | INFO | train_inner | epoch 011:    169 / 1474 loss=1.941, trans_loss=3.376, nll_loss=1.555, w2v_ctc_loss=1.047, task_loss=3.566, task_loss_gen=6.274, contrastive_loss=0.07, total=4100.74, n_correct=2601.93, ppl=2.94, accuracy=63.45, wps=13739, ups=1.12, wpb=12251.1, bsz=450.6, num_updates=14900, lr=0.000115857, gnorm=0.557, clip=0, loss_scale=16, train_wall=89, gb_free=14, wall=13425
2023-09-05 05:26:42 | INFO | train_inner | epoch 011:    269 / 1474 loss=1.93, trans_loss=3.373, nll_loss=1.549, w2v_ctc_loss=1.038, task_loss=3.451, task_loss_gen=5.79, contrastive_loss=0.058, total=4115.58, n_correct=2612.23, ppl=2.93, accuracy=63.472, wps=13920.8, ups=1.13, wpb=12290.6, bsz=444.2, num_updates=15000, lr=0.00011547, gnorm=0.476, clip=0, loss_scale=16, train_wall=88, gb_free=15.7, wall=13513
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:0')
2023-09-05 05:27:44 | INFO | train_inner | epoch 011:    369 / 1474 loss=2.051, trans_loss=5.015, nll_loss=2.306, w2v_ctc_loss=0.778, task_loss=5.408, task_loss_gen=8.804, contrastive_loss=0.05, total=4094.16, n_correct=2597.75, ppl=4.94, accuracy=63.45, wps=13160.3, ups=1.6, wpb=8227.1, bsz=296.5, num_updates=15100, lr=0.000115087, gnorm=0.696, clip=0, loss_scale=16, train_wall=62, gb_free=12.3, wall=13576
2023-09-05 05:28:48 | INFO | train_inner | epoch 011:    469 / 1474 loss=2.075, trans_loss=5.052, nll_loss=2.333, w2v_ctc_loss=0.781, task_loss=6.014, task_loss_gen=8.694, contrastive_loss=0.173, total=4112.8, n_correct=2594.3, ppl=5.04, accuracy=63.079, wps=12985.7, ups=1.58, wpb=8225.6, bsz=302.2, num_updates=15200, lr=0.000114708, gnorm=0.765, clip=0, loss_scale=16, train_wall=63, gb_free=16.7, wall=13639
2023-09-05 05:29:51 | INFO | train_inner | epoch 011:    569 / 1474 loss=2.074, trans_loss=5.047, nll_loss=2.328, w2v_ctc_loss=0.788, task_loss=5.662, task_loss_gen=8.816, contrastive_loss=0.17, total=4071.06, n_correct=2571.81, ppl=5.02, accuracy=63.173, wps=12793.1, ups=1.57, wpb=8142.1, bsz=292.6, num_updates=15300, lr=0.000114332, gnorm=0.695, clip=0, loss_scale=16, train_wall=63, gb_free=15.9, wall=13703
2023-09-05 05:30:54 | INFO | train_inner | epoch 011:    669 / 1474 loss=2.075, trans_loss=5.039, nll_loss=2.317, w2v_ctc_loss=0.783, task_loss=4.861, task_loss_gen=8.424, contrastive_loss=0.232, total=4156.4, n_correct=2632.26, ppl=4.98, accuracy=63.33, wps=13133.8, ups=1.58, wpb=8312.8, bsz=310.2, num_updates=15400, lr=0.000113961, gnorm=0.688, clip=0, loss_scale=16, train_wall=63, gb_free=15.7, wall=13766
2023-09-05 05:31:57 | INFO | train_inner | epoch 011:    769 / 1474 loss=2.063, trans_loss=5.05, nll_loss=2.331, w2v_ctc_loss=0.792, task_loss=5.81, task_loss_gen=8.471, contrastive_loss=0.052, total=4169.17, n_correct=2639.58, ppl=5.03, accuracy=63.312, wps=13247.5, ups=1.59, wpb=8338.3, bsz=304.8, num_updates=15500, lr=0.000113592, gnorm=0.655, clip=0, loss_scale=16, train_wall=62, gb_free=11.6, wall=13829
2023-09-05 05:33:00 | INFO | train_inner | epoch 011:    869 / 1474 loss=2.062, trans_loss=5.05, nll_loss=2.331, w2v_ctc_loss=0.785, task_loss=6.109, task_loss_gen=8.612, contrastive_loss=0.044, total=4120.01, n_correct=2600.98, ppl=5.03, accuracy=63.13, wps=13142.5, ups=1.59, wpb=8240, bsz=293.5, num_updates=15600, lr=0.000113228, gnorm=0.738, clip=0, loss_scale=16, train_wall=62, gb_free=13, wall=13892
2023-09-05 05:34:03 | INFO | train_inner | epoch 011:    969 / 1474 loss=2.059, trans_loss=5.048, nll_loss=2.328, w2v_ctc_loss=0.786, task_loss=5.134, task_loss_gen=8.258, contrastive_loss=0.053, total=4145.45, n_correct=2623.05, ppl=5.02, accuracy=63.275, wps=13249.4, ups=1.6, wpb=8290.9, bsz=303.7, num_updates=15700, lr=0.000112867, gnorm=0.605, clip=0, loss_scale=16, train_wall=62, gb_free=16.7, wall=13954
2023-09-05 05:35:05 | INFO | train_inner | epoch 011:   1069 / 1474 loss=2.061, trans_loss=5.044, nll_loss=2.323, w2v_ctc_loss=0.79, task_loss=5.512, task_loss_gen=7.891, contrastive_loss=0.069, total=4141.18, n_correct=2622.15, ppl=5, accuracy=63.319, wps=13271.6, ups=1.6, wpb=8282.4, bsz=309.4, num_updates=15800, lr=0.000112509, gnorm=0.686, clip=0, loss_scale=16, train_wall=62, gb_free=16.2, wall=14017
2023-09-05 05:36:09 | INFO | train_inner | epoch 011:   1169 / 1474 loss=2.063, trans_loss=5.051, nll_loss=2.333, w2v_ctc_loss=0.795, task_loss=5.853, task_loss_gen=8.361, contrastive_loss=0.056, total=4173.93, n_correct=2636.87, ppl=5.04, accuracy=63.175, wps=13113.4, ups=1.57, wpb=8347.9, bsz=307.2, num_updates=15900, lr=0.000112154, gnorm=0.86, clip=0, loss_scale=16, train_wall=63, gb_free=16.6, wall=14080
2023-09-05 05:37:12 | INFO | train_inner | epoch 011:   1269 / 1474 loss=2.067, trans_loss=5.042, nll_loss=2.322, w2v_ctc_loss=0.791, task_loss=5.5, task_loss_gen=7.661, contrastive_loss=0.123, total=4174.26, n_correct=2642.1, ppl=5, accuracy=63.295, wps=13121.7, ups=1.57, wpb=8348.5, bsz=314.4, num_updates=16000, lr=0.000111803, gnorm=0.698, clip=0, loss_scale=16, train_wall=63, gb_free=17.1, wall=14144
2023-09-05 05:37:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:1')
2023-09-05 05:37:46 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 3.991 | trans_loss 5.235 | nll_loss 2.52 | w2v_ctc_loss 1.357 | task_loss 74.231 | task_loss_gen 28.184 | contrastive_loss 0.237 | total 4003.4 | n_correct 2609.8 | ppl 5.74 | accuracy 65.19 | uer 18.807 | wer 20.588 | raw_wer 20.588 | bleu 20.82 | wps 1599.1 | wpb 4003.4 | bsz 141.8 | num_updates 16000 | best_bleu 20.82
2023-09-05 05:37:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16000 updates
2023-09-05 05:37:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_11_16000.pt
2023-09-05 05:37:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_11_16000.pt
2023-09-05 05:37:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_11_16000.pt (epoch 11 @ 16000 updates, score 20.82) (writing took 12.428518695000093 seconds)
2023-09-05 05:39:02 | INFO | train_inner | epoch 011:   1369 / 1474 loss=2.073, trans_loss=5.041, nll_loss=2.321, w2v_ctc_loss=0.773, task_loss=4.849, task_loss_gen=7.528, contrastive_loss=0.281, total=4191.56, n_correct=2653.64, ppl=5, accuracy=63.309, wps=7641.5, ups=0.91, wpb=8383.1, bsz=327.7, num_updates=16100, lr=0.000111456, gnorm=0.702, clip=0, loss_scale=16, train_wall=63, gb_free=17.2, wall=14254
2023-09-05 05:40:05 | INFO | train_inner | epoch 011:   1469 / 1474 loss=2.054, trans_loss=5.044, nll_loss=2.325, w2v_ctc_loss=0.78, task_loss=4.757, task_loss_gen=8.036, contrastive_loss=0.06, total=4161.81, n_correct=2638.7, ppl=5.01, accuracy=63.403, wps=13316.4, ups=1.6, wpb=8323.6, bsz=313.2, num_updates=16200, lr=0.000111111, gnorm=0.689, clip=0, loss_scale=16, train_wall=62, gb_free=16.4, wall=14316
2023-09-05 05:40:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 05:40:41 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 3.988 | trans_loss 5.235 | nll_loss 2.517 | w2v_ctc_loss 1.337 | task_loss 23.027 | task_loss_gen 21.426 | contrastive_loss 0.244 | total 4003.4 | n_correct 2616.9 | ppl 5.72 | accuracy 65.367 | uer 18.99 | wer 20.719 | raw_wer 20.719 | bleu 21.23 | wps 1580 | wpb 4003.4 | bsz 141.8 | num_updates 16205 | best_bleu 21.23
2023-09-05 05:40:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 16205 updates
2023-09-05 05:40:41 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 05:40:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 05:40:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 11 @ 16205 updates, score 21.23) (writing took 12.385108154034242 seconds)
2023-09-05 05:40:54 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-09-05 05:40:54 | INFO | train | epoch 011 | loss 2.033 | trans_loss 4.627 | nll_loss 2.131 | w2v_ctc_loss 0.849 | task_loss 4.941 | task_loss_gen 7.688 | contrastive_loss 0.102 | total 4138.65 | n_correct 2620.32 | ppl 4.38 | accuracy 63.313 | wps 12110.2 | ups 1.34 | wpb 9023.7 | bsz 333.5 | num_updates 16205 | lr 0.000111094 | gnorm 0.674 | clip 0 | loss_scale 16 | train_wall 988 | gb_free 16.8 | wall 14366
2023-09-05 05:40:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 05:40:55 | INFO | fairseq.trainer | begin training epoch 12
2023-09-05 05:40:55 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 05:42:01 | INFO | train_inner | epoch 012:     95 / 1474 loss=2.04, trans_loss=5.007, nll_loss=2.276, w2v_ctc_loss=0.768, task_loss=5.108, task_loss_gen=8.004, contrastive_loss=0.094, total=4139.2, n_correct=2649.91, ppl=4.84, accuracy=64.02, wps=7093.1, ups=0.86, wpb=8278.4, bsz=312.5, num_updates=16300, lr=0.00011077, gnorm=0.674, clip=0, loss_scale=16, train_wall=61, gb_free=15.6, wall=14433
2023-09-05 05:43:04 | INFO | train_inner | epoch 012:    195 / 1474 loss=2.042, trans_loss=5.013, nll_loss=2.284, w2v_ctc_loss=0.773, task_loss=5.258, task_loss_gen=8.62, contrastive_loss=0.046, total=4126.87, n_correct=2635.67, ppl=4.87, accuracy=63.866, wps=13157.2, ups=1.59, wpb=8253.7, bsz=295.9, num_updates=16400, lr=0.000110432, gnorm=0.637, clip=0, loss_scale=16, train_wall=62, gb_free=16.1, wall=14496
2023-09-05 05:44:08 | INFO | train_inner | epoch 012:    295 / 1474 loss=2.036, trans_loss=5.012, nll_loss=2.283, w2v_ctc_loss=0.757, task_loss=4.657, task_loss_gen=7.775, contrastive_loss=0.072, total=4203.54, n_correct=2688.18, ppl=4.87, accuracy=63.95, wps=13241.4, ups=1.58, wpb=8407.1, bsz=321.1, num_updates=16500, lr=0.000110096, gnorm=0.652, clip=0, loss_scale=16, train_wall=63, gb_free=14.2, wall=14559
2023-09-05 05:45:11 | INFO | train_inner | epoch 012:    395 / 1474 loss=2.038, trans_loss=5.014, nll_loss=2.284, w2v_ctc_loss=0.767, task_loss=5.46, task_loss_gen=8.129, contrastive_loss=0.058, total=4149.28, n_correct=2655.64, ppl=4.87, accuracy=64.002, wps=13145.3, ups=1.58, wpb=8298.6, bsz=307.1, num_updates=16600, lr=0.000109764, gnorm=0.684, clip=0, loss_scale=16, train_wall=63, gb_free=14.7, wall=14622
2023-09-05 05:46:13 | INFO | train_inner | epoch 012:    495 / 1474 loss=2.052, trans_loss=5.032, nll_loss=2.308, w2v_ctc_loss=0.78, task_loss=5.125, task_loss_gen=8.533, contrastive_loss=0.067, total=4106.46, n_correct=2614.13, ppl=4.95, accuracy=63.659, wps=13180.2, ups=1.6, wpb=8212.9, bsz=301.2, num_updates=16700, lr=0.000109435, gnorm=0.675, clip=0, loss_scale=32, train_wall=62, gb_free=17.4, wall=14685
2023-09-05 05:47:16 | INFO | train_inner | epoch 012:    595 / 1474 loss=2.046, trans_loss=5.014, nll_loss=2.285, w2v_ctc_loss=0.77, task_loss=4.673, task_loss_gen=8.095, contrastive_loss=0.129, total=4190.91, n_correct=2679.46, ppl=4.87, accuracy=63.935, wps=13225.5, ups=1.58, wpb=8381.8, bsz=316.5, num_updates=16800, lr=0.000109109, gnorm=0.587, clip=0, loss_scale=32, train_wall=63, gb_free=15.5, wall=14748
2023-09-05 05:48:20 | INFO | train_inner | epoch 012:    695 / 1474 loss=2.042, trans_loss=5.009, nll_loss=2.28, w2v_ctc_loss=0.754, task_loss=4.034, task_loss_gen=8.095, contrastive_loss=0.215, total=4203.66, n_correct=2695.16, ppl=4.86, accuracy=64.115, wps=13311.1, ups=1.58, wpb=8407.3, bsz=324.4, num_updates=16900, lr=0.000108786, gnorm=0.546, clip=0, loss_scale=32, train_wall=63, gb_free=17, wall=14811
2023-09-05 05:49:22 | INFO | train_inner | epoch 012:    795 / 1474 loss=2.037, trans_loss=5.009, nll_loss=2.279, w2v_ctc_loss=0.772, task_loss=4.223, task_loss_gen=9.1, contrastive_loss=0.051, total=4095.72, n_correct=2622.56, ppl=4.85, accuracy=64.032, wps=13070.2, ups=1.6, wpb=8191.4, bsz=298.9, num_updates=17000, lr=0.000108465, gnorm=0.569, clip=0, loss_scale=32, train_wall=62, gb_free=16.9, wall=14874
2023-09-05 05:50:26 | INFO | train_inner | epoch 012:    895 / 1474 loss=2.044, trans_loss=5.012, nll_loss=2.283, w2v_ctc_loss=0.771, task_loss=4.5, task_loss_gen=9.269, contrastive_loss=0.098, total=4162.82, n_correct=2660.79, ppl=4.87, accuracy=63.918, wps=13146.6, ups=1.58, wpb=8325.6, bsz=305.4, num_updates=17100, lr=0.000108148, gnorm=0.577, clip=0, loss_scale=32, train_wall=63, gb_free=15.7, wall=14937
2023-09-05 05:51:29 | INFO | train_inner | epoch 012:    995 / 1474 loss=2.046, trans_loss=5.016, nll_loss=2.288, w2v_ctc_loss=0.774, task_loss=4.367, task_loss_gen=9.606, contrastive_loss=0.112, total=4117.63, n_correct=2631.39, ppl=4.89, accuracy=63.905, wps=13085.5, ups=1.59, wpb=8235.3, bsz=301.6, num_updates=17200, lr=0.000107833, gnorm=0.597, clip=0, loss_scale=32, train_wall=62, gb_free=16.5, wall=15000
2023-09-05 05:52:31 | INFO | train_inner | epoch 012:   1095 / 1474 loss=2.058, trans_loss=5.02, nll_loss=2.294, w2v_ctc_loss=0.78, task_loss=4.569, task_loss_gen=9.705, contrastive_loss=0.161, total=4046.48, n_correct=2581.8, ppl=4.91, accuracy=63.804, wps=12874.2, ups=1.59, wpb=8093, bsz=289.6, num_updates=17300, lr=0.000107521, gnorm=0.609, clip=0, loss_scale=32, train_wall=62, gb_free=15.5, wall=15063
2023-09-05 05:53:34 | INFO | train_inner | epoch 012:   1195 / 1474 loss=2.06, trans_loss=5.037, nll_loss=2.316, w2v_ctc_loss=0.788, task_loss=3.595, task_loss_gen=9.138, contrastive_loss=0.123, total=4201.13, n_correct=2669.33, ppl=4.98, accuracy=63.538, wps=13348.3, ups=1.59, wpb=8402.3, bsz=319.2, num_updates=17400, lr=0.000107211, gnorm=0.564, clip=0, loss_scale=32, train_wall=62, gb_free=16.9, wall=15126
2023-09-05 05:54:38 | INFO | train_inner | epoch 012:   1295 / 1474 loss=2.046, trans_loss=5.017, nll_loss=2.29, w2v_ctc_loss=0.786, task_loss=4.617, task_loss_gen=10.149, contrastive_loss=0.04, total=4070.27, n_correct=2598.07, ppl=4.89, accuracy=63.83, wps=12829.3, ups=1.58, wpb=8140.5, bsz=286.1, num_updates=17500, lr=0.000106904, gnorm=0.575, clip=0, loss_scale=32, train_wall=63, gb_free=15.5, wall=15189
2023-09-05 05:55:41 | INFO | train_inner | epoch 012:   1395 / 1474 loss=2.051, trans_loss=5.025, nll_loss=2.301, w2v_ctc_loss=0.772, task_loss=4.161, task_loss_gen=10.09, contrastive_loss=0.143, total=4139.63, n_correct=2638.3, ppl=4.93, accuracy=63.733, wps=13077.9, ups=1.58, wpb=8279.3, bsz=305.8, num_updates=17600, lr=0.0001066, gnorm=0.62, clip=0, loss_scale=32, train_wall=63, gb_free=16.6, wall=15253
2023-09-05 05:56:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 05:57:04 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 3.976 | trans_loss 5.219 | nll_loss 2.5 | w2v_ctc_loss 1.337 | task_loss 25.852 | task_loss_gen 21.037 | contrastive_loss 0.246 | total 4003.4 | n_correct 2625.5 | ppl 5.66 | accuracy 65.582 | uer 18.687 | wer 20.421 | raw_wer 20.421 | bleu 21.45 | wps 1597.5 | wpb 4003.4 | bsz 141.8 | num_updates 17679 | best_bleu 21.45
2023-09-05 05:57:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 17679 updates
2023-09-05 05:57:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 05:57:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 05:57:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 12 @ 17679 updates, score 21.45) (writing took 12.34707294398686 seconds)
2023-09-05 05:57:17 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-09-05 05:57:17 | INFO | train | epoch 012 | loss 2.045 | trans_loss 5.017 | nll_loss 2.29 | w2v_ctc_loss 0.773 | task_loss 4.553 | task_loss_gen 8.918 | contrastive_loss 0.099 | total 4138.65 | n_correct 2643.59 | ppl 4.89 | accuracy 63.876 | wps 12419.2 | ups 1.5 | wpb 8277.3 | bsz 305.7 | num_updates 17679 | lr 0.000106362 | gnorm 0.61 | clip 0 | loss_scale 32 | train_wall 919 | gb_free 12.4 | wall 15349
2023-09-05 05:57:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 05:57:17 | INFO | fairseq.trainer | begin training epoch 13
2023-09-05 05:57:17 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 05:57:38 | INFO | train_inner | epoch 013:     21 / 1474 loss=2.047, trans_loss=5.025, nll_loss=2.299, w2v_ctc_loss=0.785, task_loss=3.844, task_loss_gen=10.005, contrastive_loss=0.048, total=4096.49, n_correct=2612.58, ppl=4.92, accuracy=63.776, wps=7034, ups=0.86, wpb=8193, bsz=295.4, num_updates=17700, lr=0.000106299, gnorm=0.583, clip=0, loss_scale=32, train_wall=62, gb_free=14.1, wall=15369
2023-09-05 05:58:41 | INFO | train_inner | epoch 013:    121 / 1474 loss=2.025, trans_loss=4.989, nll_loss=2.253, w2v_ctc_loss=0.757, task_loss=4.399, task_loss_gen=9.387, contrastive_loss=0.06, total=4160.97, n_correct=2680.25, ppl=4.77, accuracy=64.414, wps=13196.7, ups=1.59, wpb=8321.9, bsz=302.9, num_updates=17800, lr=0.000106, gnorm=0.587, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=15432
2023-09-05 05:59:44 | INFO | train_inner | epoch 013:    221 / 1474 loss=2.046, trans_loss=5, nll_loss=2.268, w2v_ctc_loss=0.757, task_loss=3.815, task_loss_gen=9.003, contrastive_loss=0.273, total=4212.08, n_correct=2707.58, ppl=4.82, accuracy=64.281, wps=13349.2, ups=1.58, wpb=8424.2, bsz=329.7, num_updates=17900, lr=0.000105703, gnorm=0.61, clip=0, loss_scale=32, train_wall=62, gb_free=14.2, wall=15495
2023-09-05 06:00:47 | INFO | train_inner | epoch 013:    321 / 1474 loss=2.02, trans_loss=4.983, nll_loss=2.244, w2v_ctc_loss=0.753, task_loss=4.598, task_loss_gen=10.202, contrastive_loss=0.045, total=4102.3, n_correct=2648.26, ppl=4.74, accuracy=64.555, wps=13000.2, ups=1.58, wpb=8204.6, bsz=294.1, num_updates=18000, lr=0.000105409, gnorm=0.621, clip=0, loss_scale=32, train_wall=62, gb_free=16.9, wall=15559
2023-09-05 06:00:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 06:01:21 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.987 | trans_loss 5.227 | nll_loss 2.51 | w2v_ctc_loss 1.359 | task_loss 71.792 | task_loss_gen 28.116 | contrastive_loss 0.242 | total 4003.4 | n_correct 2623.1 | ppl 5.7 | accuracy 65.522 | uer 19.282 | wer 21.245 | raw_wer 21.245 | bleu 21.24 | wps 1520.9 | wpb 4003.4 | bsz 141.8 | num_updates 18000 | best_bleu 21.45
2023-09-05 06:01:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 18000 updates
2023-09-05 06:01:21 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_13_18000.pt
2023-09-05 06:01:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_13_18000.pt
2023-09-05 06:01:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_13_18000.pt (epoch 13 @ 18000 updates, score 21.24) (writing took 8.464905117987655 seconds)
2023-09-05 06:02:33 | INFO | train_inner | epoch 013:    421 / 1474 loss=2.023, trans_loss=4.987, nll_loss=2.25, w2v_ctc_loss=0.754, task_loss=4.268, task_loss_gen=8.94, contrastive_loss=0.092, total=4177.29, n_correct=2697.39, ppl=4.76, accuracy=64.573, wps=7888.6, ups=0.94, wpb=8354.6, bsz=318.4, num_updates=18100, lr=0.000105118, gnorm=0.605, clip=0, loss_scale=32, train_wall=62, gb_free=17.1, wall=15664
2023-09-05 06:03:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 06:03:37 | INFO | train_inner | epoch 013:    522 / 1474 loss=2.034, trans_loss=4.997, nll_loss=2.263, w2v_ctc_loss=0.765, task_loss=3.713, task_loss_gen=9.218, contrastive_loss=0.124, total=4201.35, n_correct=2699.53, ppl=4.8, accuracy=64.254, wps=13123.9, ups=1.56, wpb=8402.7, bsz=320.5, num_updates=18200, lr=0.000104828, gnorm=0.661, clip=0, loss_scale=16, train_wall=63, gb_free=17, wall=15728
2023-09-05 06:04:40 | INFO | train_inner | epoch 013:    622 / 1474 loss=2.022, trans_loss=4.994, nll_loss=2.26, w2v_ctc_loss=0.754, task_loss=4.859, task_loss_gen=8.795, contrastive_loss=0.045, total=4158.04, n_correct=2683.07, ppl=4.79, accuracy=64.527, wps=13247.4, ups=1.59, wpb=8316.1, bsz=306.7, num_updates=18300, lr=0.000104542, gnorm=0.695, clip=0, loss_scale=16, train_wall=62, gb_free=12.9, wall=15791
2023-09-05 06:05:42 | INFO | train_inner | epoch 013:    722 / 1474 loss=2.039, trans_loss=5.001, nll_loss=2.268, w2v_ctc_loss=0.78, task_loss=5.548, task_loss_gen=9.51, contrastive_loss=0.045, total=4099.91, n_correct=2626.93, ppl=4.82, accuracy=64.073, wps=13069.8, ups=1.59, wpb=8199.8, bsz=285.5, num_updates=18400, lr=0.000104257, gnorm=0.736, clip=0, loss_scale=16, train_wall=62, gb_free=16.2, wall=15854
2023-09-05 06:06:46 | INFO | train_inner | epoch 013:    822 / 1474 loss=2.036, trans_loss=5.002, nll_loss=2.27, w2v_ctc_loss=0.763, task_loss=5.293, task_loss_gen=8.633, contrastive_loss=0.088, total=4122.78, n_correct=2643.55, ppl=4.82, accuracy=64.121, wps=12927.4, ups=1.57, wpb=8245.6, bsz=306, num_updates=18500, lr=0.000103975, gnorm=0.728, clip=0, loss_scale=16, train_wall=63, gb_free=14.5, wall=15918
2023-09-05 06:07:49 | INFO | train_inner | epoch 013:    922 / 1474 loss=2.027, trans_loss=4.997, nll_loss=2.263, w2v_ctc_loss=0.76, task_loss=5.322, task_loss_gen=8.709, contrastive_loss=0.052, total=4102.59, n_correct=2640.26, ppl=4.8, accuracy=64.356, wps=13089.7, ups=1.6, wpb=8205.2, bsz=296.6, num_updates=18600, lr=0.000103695, gnorm=0.676, clip=0, loss_scale=16, train_wall=62, gb_free=17, wall=15980
2023-09-05 06:08:52 | INFO | train_inner | epoch 013:   1022 / 1474 loss=2.043, trans_loss=5.004, nll_loss=2.273, w2v_ctc_loss=0.774, task_loss=5.664, task_loss_gen=8.944, contrastive_loss=0.105, total=4087.8, n_correct=2620.68, ppl=4.83, accuracy=64.11, wps=12849.2, ups=1.57, wpb=8175.6, bsz=293.7, num_updates=18700, lr=0.000103418, gnorm=0.739, clip=0, loss_scale=16, train_wall=63, gb_free=16.3, wall=16044
2023-09-05 06:09:55 | INFO | train_inner | epoch 013:   1122 / 1474 loss=2.023, trans_loss=4.989, nll_loss=2.253, w2v_ctc_loss=0.753, task_loss=5.235, task_loss_gen=8.369, contrastive_loss=0.085, total=4098.77, n_correct=2644.05, ppl=4.77, accuracy=64.508, wps=13138.1, ups=1.6, wpb=8197.5, bsz=304.8, num_updates=18800, lr=0.000103142, gnorm=0.67, clip=0, loss_scale=16, train_wall=62, gb_free=13.1, wall=16107
2023-09-05 06:10:58 | INFO | train_inner | epoch 013:   1222 / 1474 loss=2.034, trans_loss=5.004, nll_loss=2.274, w2v_ctc_loss=0.77, task_loss=5.385, task_loss_gen=8.832, contrastive_loss=0.044, total=4115.57, n_correct=2641.12, ppl=4.84, accuracy=64.174, wps=13123.3, ups=1.59, wpb=8231.1, bsz=295.9, num_updates=18900, lr=0.000102869, gnorm=0.66, clip=0, loss_scale=16, train_wall=62, gb_free=14.7, wall=16169
2023-09-05 06:12:00 | INFO | train_inner | epoch 013:   1322 / 1474 loss=2.037, trans_loss=4.994, nll_loss=2.261, w2v_ctc_loss=0.763, task_loss=5.384, task_loss_gen=8.313, contrastive_loss=0.146, total=4111.02, n_correct=2647.25, ppl=4.79, accuracy=64.394, wps=13107.2, ups=1.59, wpb=8222, bsz=307.8, num_updates=19000, lr=0.000102598, gnorm=0.768, clip=0, loss_scale=16, train_wall=62, gb_free=17.6, wall=16232
2023-09-05 06:13:04 | INFO | train_inner | epoch 013:   1422 / 1474 loss=2.041, trans_loss=5.001, nll_loss=2.27, w2v_ctc_loss=0.757, task_loss=5.299, task_loss_gen=8.318, contrastive_loss=0.162, total=4179.06, n_correct=2685.11, ppl=4.82, accuracy=64.252, wps=13208.2, ups=1.58, wpb=8358.1, bsz=311.9, num_updates=19100, lr=0.000102329, gnorm=0.813, clip=0, loss_scale=16, train_wall=63, gb_free=15.6, wall=16295
2023-09-05 06:13:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 06:14:10 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.971 | trans_loss 5.212 | nll_loss 2.484 | w2v_ctc_loss 1.34 | task_loss 57.89 | task_loss_gen 24.361 | contrastive_loss 0.238 | total 4003.4 | n_correct 2628.7 | ppl 5.59 | accuracy 65.662 | uer 18.533 | wer 20.201 | raw_wer 20.201 | bleu 21.2 | wps 1560.5 | wpb 4003.4 | bsz 141.8 | num_updates 19152 | best_bleu 21.45
2023-09-05 06:14:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 19152 updates
2023-09-05 06:14:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.2000.pt
2023-09-05 06:14:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.2000.pt
2023-09-05 06:14:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.2000.pt (epoch 13 @ 19152 updates, score 21.2) (writing took 7.673956367012579 seconds)
2023-09-05 06:14:18 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-09-05 06:14:18 | INFO | train | epoch 013 | loss 2.032 | trans_loss 4.995 | nll_loss 2.262 | w2v_ctc_loss 0.761 | task_loss 4.891 | task_loss_gen 8.919 | contrastive_loss 0.098 | total 4138.75 | n_correct 2662.76 | ppl 4.8 | accuracy 64.337 | wps 11936.8 | ups 1.44 | wpb 8277.5 | bsz 305.7 | num_updates 19152 | lr 0.00010219 | gnorm 0.687 | clip 0 | loss_scale 16 | train_wall 919 | gb_free 17.3 | wall 16370
2023-09-05 06:14:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 06:14:19 | INFO | fairseq.trainer | begin training epoch 14
2023-09-05 06:14:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 06:14:57 | INFO | train_inner | epoch 014:     48 / 1474 loss=2.013, trans_loss=4.971, nll_loss=2.231, w2v_ctc_loss=0.752, task_loss=5.508, task_loss_gen=8.037, contrastive_loss=0.057, total=4179.66, n_correct=2711.27, ppl=4.69, accuracy=64.868, wps=7375.9, ups=0.88, wpb=8359.3, bsz=322, num_updates=19200, lr=0.000102062, gnorm=0.888, clip=0, loss_scale=16, train_wall=63, gb_free=9.9, wall=16409
2023-09-05 06:15:59 | INFO | train_inner | epoch 014:    148 / 1474 loss=2.006, trans_loss=4.961, nll_loss=2.217, w2v_ctc_loss=0.747, task_loss=5.379, task_loss_gen=8.304, contrastive_loss=0.041, total=4081.01, n_correct=2654.63, ppl=4.65, accuracy=65.048, wps=13141.2, ups=1.61, wpb=8162, bsz=300.6, num_updates=19300, lr=0.000101797, gnorm=0.709, clip=0, loss_scale=16, train_wall=62, gb_free=15.6, wall=16471
2023-09-05 06:17:02 | INFO | train_inner | epoch 014:    248 / 1474 loss=2.026, trans_loss=4.979, nll_loss=2.24, w2v_ctc_loss=0.75, task_loss=5.993, task_loss_gen=9.073, contrastive_loss=0.146, total=4109.83, n_correct=2660.51, ppl=4.72, accuracy=64.735, wps=13144.1, ups=1.6, wpb=8219.7, bsz=295.2, num_updates=19400, lr=0.000101535, gnorm=0.858, clip=0, loss_scale=16, train_wall=62, gb_free=15.2, wall=16533
2023-09-05 06:18:04 | INFO | train_inner | epoch 014:    348 / 1474 loss=2.013, trans_loss=4.976, nll_loss=2.237, w2v_ctc_loss=0.742, task_loss=5.1, task_loss_gen=7.702, contrastive_loss=0.076, total=4171.83, n_correct=2703.38, ppl=4.71, accuracy=64.801, wps=13328.4, ups=1.6, wpb=8343.7, bsz=319.7, num_updates=19500, lr=0.000101274, gnorm=0.653, clip=0, loss_scale=16, train_wall=62, gb_free=16.4, wall=16596
2023-09-05 06:19:08 | INFO | train_inner | epoch 014:    448 / 1474 loss=2.014, trans_loss=4.98, nll_loss=2.242, w2v_ctc_loss=0.745, task_loss=6.261, task_loss_gen=8.273, contrastive_loss=0.052, total=4142.75, n_correct=2683.52, ppl=4.73, accuracy=64.776, wps=13070.9, ups=1.58, wpb=8285.5, bsz=302.2, num_updates=19600, lr=0.000101015, gnorm=0.833, clip=0, loss_scale=16, train_wall=63, gb_free=16.2, wall=16659
2023-09-05 06:20:11 | INFO | train_inner | epoch 014:    548 / 1474 loss=2.025, trans_loss=4.98, nll_loss=2.241, w2v_ctc_loss=0.766, task_loss=5.919, task_loss_gen=8.7, contrastive_loss=0.057, total=4073.76, n_correct=2629.3, ppl=4.73, accuracy=64.542, wps=12898.6, ups=1.58, wpb=8147.5, bsz=290.9, num_updates=19700, lr=0.000100759, gnorm=0.689, clip=0, loss_scale=16, train_wall=63, gb_free=15.1, wall=16722
2023-09-05 06:21:14 | INFO | train_inner | epoch 014:    648 / 1474 loss=2.025, trans_loss=4.982, nll_loss=2.244, w2v_ctc_loss=0.754, task_loss=5.587, task_loss_gen=8.244, contrastive_loss=0.116, total=4158.79, n_correct=2688.16, ppl=4.74, accuracy=64.638, wps=13048.1, ups=1.57, wpb=8317.6, bsz=306.8, num_updates=19800, lr=0.000100504, gnorm=0.747, clip=0, loss_scale=16, train_wall=63, gb_free=16.9, wall=16786
2023-09-05 06:22:17 | INFO | train_inner | epoch 014:    748 / 1474 loss=2.014, trans_loss=4.972, nll_loss=2.231, w2v_ctc_loss=0.753, task_loss=5.549, task_loss_gen=8.209, contrastive_loss=0.05, total=4145.47, n_correct=2689.19, ppl=4.69, accuracy=64.871, wps=13303.8, ups=1.6, wpb=8290.9, bsz=309.6, num_updates=19900, lr=0.000100251, gnorm=0.792, clip=0, loss_scale=16, train_wall=62, gb_free=15.8, wall=16848
2023-09-05 06:23:19 | INFO | train_inner | epoch 014:    848 / 1474 loss=2.023, trans_loss=4.971, nll_loss=2.23, w2v_ctc_loss=0.746, task_loss=5.307, task_loss_gen=7.712, contrastive_loss=0.16, total=4171.1, n_correct=2703.67, ppl=4.69, accuracy=64.819, wps=13310.1, ups=1.6, wpb=8342.2, bsz=319.7, num_updates=20000, lr=0.0001, gnorm=0.698, clip=0, loss_scale=16, train_wall=62, gb_free=16.3, wall=16911
2023-09-05 06:23:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 06:23:53 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.993 | trans_loss 5.209 | nll_loss 2.485 | w2v_ctc_loss 1.424 | task_loss 41.764 | task_loss_gen 22.043 | contrastive_loss 0.235 | total 4003.4 | n_correct 2636.1 | ppl 5.6 | accuracy 65.847 | uer 18.812 | wer 20.6 | raw_wer 20.6 | bleu 21.08 | wps 1638.9 | wpb 4003.4 | bsz 141.8 | num_updates 20000 | best_bleu 21.45
2023-09-05 06:23:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20000 updates
2023-09-05 06:23:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_14_20000.pt
2023-09-05 06:23:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_14_20000.pt
2023-09-05 06:24:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_14_20000.pt (epoch 14 @ 20000 updates, score 21.08) (writing took 10.246072976966389 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:0')
2023-09-05 06:25:06 | INFO | train_inner | epoch 014:    948 / 1474 loss=2.019, trans_loss=4.978, nll_loss=2.24, w2v_ctc_loss=0.748, task_loss=5.19, task_loss_gen=8.193, contrastive_loss=0.091, total=4167.75, n_correct=2691.45, ppl=4.72, accuracy=64.578, wps=7798.5, ups=0.94, wpb=8335.5, bsz=310.1, num_updates=20100, lr=9.97509e-05, gnorm=0.68, clip=0, loss_scale=16, train_wall=62, gb_free=15.9, wall=17018
2023-09-05 06:26:10 | INFO | train_inner | epoch 014:   1048 / 1474 loss=2.02, trans_loss=4.979, nll_loss=2.241, w2v_ctc_loss=0.75, task_loss=5.686, task_loss_gen=8.778, contrastive_loss=0.072, total=4143.92, n_correct=2682.09, ppl=4.73, accuracy=64.723, wps=13025.9, ups=1.57, wpb=8287.8, bsz=300.7, num_updates=20200, lr=9.95037e-05, gnorm=0.833, clip=0, loss_scale=16, train_wall=63, gb_free=15.5, wall=17082
2023-09-05 06:27:14 | INFO | train_inner | epoch 014:   1148 / 1474 loss=2.046, trans_loss=4.98, nll_loss=2.243, w2v_ctc_loss=0.758, task_loss=5.318, task_loss_gen=7.675, contrastive_loss=0.34, total=4228.69, n_correct=2731.16, ppl=4.73, accuracy=64.586, wps=13234, ups=1.56, wpb=8457.4, bsz=327.2, num_updates=20300, lr=9.92583e-05, gnorm=0.598, clip=0, loss_scale=32, train_wall=63, gb_free=15.3, wall=17146
2023-09-05 06:28:16 | INFO | train_inner | epoch 014:   1248 / 1474 loss=2.026, trans_loss=4.984, nll_loss=2.247, w2v_ctc_loss=0.768, task_loss=5.47, task_loss_gen=9.967, contrastive_loss=0.034, total=4021.19, n_correct=2597.54, ppl=4.75, accuracy=64.596, wps=12856.2, ups=1.6, wpb=8042.4, bsz=271.7, num_updates=20400, lr=9.90148e-05, gnorm=0.588, clip=0, loss_scale=32, train_wall=62, gb_free=16, wall=17208
2023-09-05 06:29:19 | INFO | train_inner | epoch 014:   1348 / 1474 loss=2.008, trans_loss=4.976, nll_loss=2.237, w2v_ctc_loss=0.743, task_loss=4.101, task_loss_gen=8.265, contrastive_loss=0.044, total=4213.9, n_correct=2731.64, ppl=4.71, accuracy=64.825, wps=13478.9, ups=1.6, wpb=8427.8, bsz=319.4, num_updates=20500, lr=9.8773e-05, gnorm=0.558, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=17271
2023-09-05 06:30:22 | INFO | train_inner | epoch 014:   1448 / 1474 loss=2.019, trans_loss=4.981, nll_loss=2.244, w2v_ctc_loss=0.749, task_loss=4.545, task_loss_gen=8.734, contrastive_loss=0.084, total=4130.28, n_correct=2674.4, ppl=4.74, accuracy=64.751, wps=13155.2, ups=1.59, wpb=8260.6, bsz=304.1, num_updates=20600, lr=9.85329e-05, gnorm=0.586, clip=0, loss_scale=32, train_wall=62, gb_free=14.9, wall=17333
2023-09-05 06:30:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:2')
2023-09-05 06:31:11 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.96 | trans_loss 5.205 | nll_loss 2.48 | w2v_ctc_loss 1.318 | task_loss 92.306 | task_loss_gen 33.229 | contrastive_loss 0.24 | total 4003.4 | n_correct 2638.1 | ppl 5.58 | accuracy 65.896 | uer 18.461 | wer 20.327 | raw_wer 20.327 | bleu 21.56 | wps 1608 | wpb 4003.4 | bsz 141.8 | num_updates 20626 | best_bleu 21.56
2023-09-05 06:31:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 20626 updates
2023-09-05 06:31:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 06:31:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 06:31:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 14 @ 20626 updates, score 21.56) (writing took 13.155470156983938 seconds)
2023-09-05 06:31:25 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-09-05 06:31:25 | INFO | train | epoch 014 | loss 2.02 | trans_loss 4.977 | nll_loss 2.238 | w2v_ctc_loss 0.751 | task_loss 5.38 | task_loss_gen 8.402 | contrastive_loss 0.097 | total 4138.65 | n_correct 2679.26 | ppl 4.72 | accuracy 64.738 | wps 11885.6 | ups 1.44 | wpb 8277.3 | bsz 305.7 | num_updates 20626 | lr 9.84708e-05 | gnorm 0.707 | clip 0 | loss_scale 32 | train_wall 918 | gb_free 15.9 | wall 17397
2023-09-05 06:31:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 06:31:25 | INFO | fairseq.trainer | begin training epoch 15
2023-09-05 06:31:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 06:32:19 | INFO | train_inner | epoch 015:     74 / 1474 loss=2.012, trans_loss=4.964, nll_loss=2.222, w2v_ctc_loss=0.738, task_loss=4.579, task_loss_gen=9.225, contrastive_loss=0.137, total=4083.88, n_correct=2655.91, ppl=4.67, accuracy=65.034, wps=6976.5, ups=0.85, wpb=8167.8, bsz=300.2, num_updates=20700, lr=9.82946e-05, gnorm=0.614, clip=0, loss_scale=32, train_wall=62, gb_free=15.6, wall=17451
2023-09-05 06:33:22 | INFO | train_inner | epoch 015:    174 / 1474 loss=2.003, trans_loss=4.955, nll_loss=2.208, w2v_ctc_loss=0.745, task_loss=4.1, task_loss_gen=9.686, contrastive_loss=0.044, total=4115.73, n_correct=2687.21, ppl=4.62, accuracy=65.291, wps=13044.9, ups=1.58, wpb=8231.5, bsz=297.8, num_updates=20800, lr=9.80581e-05, gnorm=0.588, clip=0, loss_scale=32, train_wall=62, gb_free=16.5, wall=17514
2023-09-05 06:34:25 | INFO | train_inner | epoch 015:    274 / 1474 loss=1.996, trans_loss=4.955, nll_loss=2.209, w2v_ctc_loss=0.735, task_loss=4.291, task_loss_gen=8.716, contrastive_loss=0.037, total=4193.15, n_correct=2739.55, ppl=4.62, accuracy=65.334, wps=13410.7, ups=1.6, wpb=8386.3, bsz=312.6, num_updates=20900, lr=9.78232e-05, gnorm=0.606, clip=0, loss_scale=32, train_wall=62, gb_free=12.3, wall=17576
2023-09-05 06:35:27 | INFO | train_inner | epoch 015:    374 / 1474 loss=2.001, trans_loss=4.951, nll_loss=2.204, w2v_ctc_loss=0.736, task_loss=4.232, task_loss_gen=9.097, contrastive_loss=0.056, total=4167.66, n_correct=2716.88, ppl=4.61, accuracy=65.19, wps=13276.4, ups=1.59, wpb=8335.3, bsz=306, num_updates=21000, lr=9.759e-05, gnorm=0.591, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=17639
2023-09-05 06:36:30 | INFO | train_inner | epoch 015:    474 / 1474 loss=2.008, trans_loss=4.954, nll_loss=2.208, w2v_ctc_loss=0.727, task_loss=4.011, task_loss_gen=9.779, contrastive_loss=0.151, total=4074.53, n_correct=2656.5, ppl=4.62, accuracy=65.198, wps=12938, ups=1.59, wpb=8149.1, bsz=294.2, num_updates=21100, lr=9.73585e-05, gnorm=0.595, clip=0, loss_scale=32, train_wall=62, gb_free=15.4, wall=17702
2023-09-05 06:37:34 | INFO | train_inner | epoch 015:    574 / 1474 loss=2.003, trans_loss=4.954, nll_loss=2.207, w2v_ctc_loss=0.747, task_loss=4.279, task_loss_gen=9.578, contrastive_loss=0.043, total=4140.59, n_correct=2696.8, ppl=4.62, accuracy=65.131, wps=13094.4, ups=1.58, wpb=8281.2, bsz=298.8, num_updates=21200, lr=9.71286e-05, gnorm=0.609, clip=0, loss_scale=32, train_wall=63, gb_free=11.6, wall=17765
2023-09-05 06:38:36 | INFO | train_inner | epoch 015:    674 / 1474 loss=2.012, trans_loss=4.953, nll_loss=2.207, w2v_ctc_loss=0.743, task_loss=4.155, task_loss_gen=9.357, contrastive_loss=0.121, total=4134.99, n_correct=2695.42, ppl=4.62, accuracy=65.186, wps=13150.9, ups=1.59, wpb=8270, bsz=307.1, num_updates=21300, lr=9.69003e-05, gnorm=0.6, clip=0, loss_scale=32, train_wall=62, gb_free=10.1, wall=17828
2023-09-05 06:39:40 | INFO | train_inner | epoch 015:    774 / 1474 loss=2.004, trans_loss=4.959, nll_loss=2.215, w2v_ctc_loss=0.745, task_loss=3.673, task_loss_gen=9.652, contrastive_loss=0.046, total=4173.66, n_correct=2716.77, ppl=4.64, accuracy=65.093, wps=13203.9, ups=1.58, wpb=8347.3, bsz=305, num_updates=21400, lr=9.66736e-05, gnorm=0.592, clip=0, loss_scale=32, train_wall=63, gb_free=16.4, wall=17891
2023-09-05 06:40:42 | INFO | train_inner | epoch 015:    874 / 1474 loss=2.007, trans_loss=4.962, nll_loss=2.22, w2v_ctc_loss=0.746, task_loss=4.719, task_loss_gen=9.69, contrastive_loss=0.042, total=4059.35, n_correct=2639.43, ppl=4.66, accuracy=65.021, wps=13013.8, ups=1.6, wpb=8118.7, bsz=288.3, num_updates=21500, lr=9.64486e-05, gnorm=0.604, clip=0, loss_scale=32, train_wall=62, gb_free=15.1, wall=17954
2023-09-05 06:41:45 | INFO | train_inner | epoch 015:    974 / 1474 loss=2.007, trans_loss=4.957, nll_loss=2.212, w2v_ctc_loss=0.737, task_loss=4.291, task_loss_gen=9.362, contrastive_loss=0.125, total=4122.87, n_correct=2690.75, ppl=4.63, accuracy=65.264, wps=13193.1, ups=1.6, wpb=8245.7, bsz=301.7, num_updates=21600, lr=9.6225e-05, gnorm=0.606, clip=0, loss_scale=32, train_wall=62, gb_free=17.2, wall=18016
2023-09-05 06:42:49 | INFO | train_inner | epoch 015:   1074 / 1474 loss=2.024, trans_loss=4.965, nll_loss=2.223, w2v_ctc_loss=0.737, task_loss=3.697, task_loss_gen=8.487, contrastive_loss=0.28, total=4192.24, n_correct=2725.52, ppl=4.67, accuracy=65.013, wps=13046.7, ups=1.56, wpb=8384.5, bsz=325.2, num_updates=21700, lr=9.60031e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=64, gb_free=16.8, wall=18080
2023-09-05 06:43:52 | INFO | train_inner | epoch 015:   1174 / 1474 loss=1.992, trans_loss=4.952, nll_loss=2.207, w2v_ctc_loss=0.723, task_loss=3.436, task_loss_gen=8.271, contrastive_loss=0.086, total=4185, n_correct=2738.59, ppl=4.62, accuracy=65.438, wps=13320.5, ups=1.59, wpb=8370, bsz=329.3, num_updates=21800, lr=9.57826e-05, gnorm=0.572, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=18143
2023-09-05 06:44:55 | INFO | train_inner | epoch 015:   1274 / 1474 loss=2.006, trans_loss=4.954, nll_loss=2.208, w2v_ctc_loss=0.753, task_loss=3.897, task_loss_gen=9.41, contrastive_loss=0.044, total=4152.04, n_correct=2702.16, ppl=4.62, accuracy=65.08, wps=13146.7, ups=1.58, wpb=8304.1, bsz=303.7, num_updates=21900, lr=9.55637e-05, gnorm=0.6, clip=0, loss_scale=32, train_wall=63, gb_free=16, wall=18207
2023-09-05 06:45:58 | INFO | train_inner | epoch 015:   1374 / 1474 loss=1.998, trans_loss=4.954, nll_loss=2.208, w2v_ctc_loss=0.737, task_loss=4.115, task_loss_gen=10.104, contrastive_loss=0.033, total=4100.21, n_correct=2674.24, ppl=4.62, accuracy=65.222, wps=13014, ups=1.59, wpb=8200.4, bsz=293.6, num_updates=22000, lr=9.53463e-05, gnorm=0.609, clip=0, loss_scale=32, train_wall=62, gb_free=17.1, wall=18270
2023-09-05 06:45:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 06:46:32 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.963 | trans_loss 5.206 | nll_loss 2.48 | w2v_ctc_loss 1.325 | task_loss 56.951 | task_loss_gen 24.363 | contrastive_loss 0.241 | total 4003.4 | n_correct 2636 | ppl 5.58 | accuracy 65.844 | uer 18.621 | wer 20.286 | raw_wer 20.286 | bleu 21.68 | wps 1535.4 | wpb 4003.4 | bsz 141.8 | num_updates 22000 | best_bleu 21.68
2023-09-05 06:46:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22000 updates
2023-09-05 06:46:32 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_15_22000.pt
2023-09-05 06:46:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_15_22000.pt
2023-09-05 06:46:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_15_22000.pt (epoch 15 @ 22000 updates, score 21.68) (writing took 13.277899067965336 seconds)
2023-09-05 06:47:50 | INFO | train_inner | epoch 015:   1474 / 1474 loss=2.01, trans_loss=4.964, nll_loss=2.223, w2v_ctc_loss=0.737, task_loss=4.386, task_loss_gen=8.719, contrastive_loss=0.117, total=4141.17, n_correct=2694.86, ppl=4.67, accuracy=65.075, wps=7394.6, ups=0.89, wpb=8282.3, bsz=314.3, num_updates=22100, lr=9.51303e-05, gnorm=0.652, clip=0, loss_scale=32, train_wall=63, gb_free=16.5, wall=18382
2023-09-05 06:47:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 06:48:23 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.961 | trans_loss 5.202 | nll_loss 2.477 | w2v_ctc_loss 1.328 | task_loss 27.945 | task_loss_gen 22.028 | contrastive_loss 0.248 | total 4003.4 | n_correct 2635.7 | ppl 5.57 | accuracy 65.837 | uer 18.318 | wer 20.1 | raw_wer 20.1 | bleu 21.33 | wps 1588.4 | wpb 4003.4 | bsz 141.8 | num_updates 22100 | best_bleu 21.68
2023-09-05 06:48:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 22100 updates
2023-09-05 06:48:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.3301.pt
2023-09-05 06:48:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.3301.pt
2023-09-05 06:48:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.3301.pt (epoch 15 @ 22100 updates, score 21.33) (writing took 8.399847951019183 seconds)
2023-09-05 06:48:32 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-09-05 06:48:32 | INFO | train | epoch 015 | loss 2.005 | trans_loss 4.956 | nll_loss 2.211 | w2v_ctc_loss 0.739 | task_loss 4.107 | task_loss_gen 9.249 | contrastive_loss 0.092 | total 4138.65 | n_correct 2697.82 | ppl 4.63 | accuracy 65.186 | wps 11872.6 | ups 1.43 | wpb 8277.3 | bsz 305.7 | num_updates 22100 | lr 9.51303e-05 | gnorm 0.599 | clip 0 | loss_scale 32 | train_wall 919 | gb_free 16.5 | wall 18424
2023-09-05 06:48:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 06:48:33 | INFO | fairseq.trainer | begin training epoch 16
2023-09-05 06:48:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 06:49:42 | INFO | train_inner | epoch 016:    100 / 1474 loss=1.986, trans_loss=4.937, nll_loss=2.187, w2v_ctc_loss=0.726, task_loss=3.558, task_loss_gen=8.845, contrastive_loss=0.057, total=4126.22, n_correct=2708.09, ppl=4.55, accuracy=65.631, wps=7337.2, ups=0.89, wpb=8252.4, bsz=315.6, num_updates=22200, lr=9.49158e-05, gnorm=0.601, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=18494
2023-09-05 06:50:46 | INFO | train_inner | epoch 016:    200 / 1474 loss=1.982, trans_loss=4.928, nll_loss=2.174, w2v_ctc_loss=0.72, task_loss=4.095, task_loss_gen=9.683, contrastive_loss=0.037, total=4100.6, n_correct=2696.74, ppl=4.51, accuracy=65.765, wps=12942.7, ups=1.58, wpb=8201.2, bsz=296.8, num_updates=22300, lr=9.47027e-05, gnorm=0.581, clip=0, loss_scale=64, train_wall=63, gb_free=12.2, wall=18557
2023-09-05 06:51:49 | INFO | train_inner | epoch 016:    300 / 1474 loss=1.995, trans_loss=4.937, nll_loss=2.186, w2v_ctc_loss=0.73, task_loss=3.422, task_loss_gen=9.573, contrastive_loss=0.108, total=4166.94, n_correct=2734.52, ppl=4.55, accuracy=65.624, wps=13168.9, ups=1.58, wpb=8333.9, bsz=308.9, num_updates=22400, lr=9.44911e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=63, gb_free=16.8, wall=18621
2023-09-05 06:52:51 | INFO | train_inner | epoch 016:    400 / 1474 loss=1.998, trans_loss=4.934, nll_loss=2.181, w2v_ctc_loss=0.733, task_loss=3.128, task_loss_gen=11.397, contrastive_loss=0.124, total=4073.3, n_correct=2672.41, ppl=4.54, accuracy=65.608, wps=13041.2, ups=1.6, wpb=8146.6, bsz=288.1, num_updates=22500, lr=9.42809e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=62, gb_free=16.6, wall=18683
2023-09-05 06:53:55 | INFO | train_inner | epoch 016:    500 / 1474 loss=1.988, trans_loss=4.938, nll_loss=2.188, w2v_ctc_loss=0.726, task_loss=3.593, task_loss_gen=9.541, contrastive_loss=0.065, total=4174.67, n_correct=2740.86, ppl=4.56, accuracy=65.655, wps=13227.8, ups=1.58, wpb=8349.3, bsz=319.1, num_updates=22600, lr=9.40721e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=62, gb_free=15.6, wall=18746
2023-09-05 06:54:57 | INFO | train_inner | epoch 016:    600 / 1474 loss=1.992, trans_loss=4.941, nll_loss=2.191, w2v_ctc_loss=0.736, task_loss=3.009, task_loss_gen=10.469, contrastive_loss=0.033, total=4124.65, n_correct=2701.8, ppl=4.57, accuracy=65.504, wps=13267.3, ups=1.61, wpb=8249.3, bsz=297.6, num_updates=22700, lr=9.38647e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=62, gb_free=15.9, wall=18808
2023-09-05 06:55:59 | INFO | train_inner | epoch 016:    700 / 1474 loss=1.989, trans_loss=4.936, nll_loss=2.185, w2v_ctc_loss=0.734, task_loss=2.851, task_loss_gen=11.29, contrastive_loss=0.035, total=4095.49, n_correct=2688.38, ppl=4.55, accuracy=65.642, wps=13068.7, ups=1.6, wpb=8191, bsz=296.3, num_updates=22800, lr=9.36586e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=62, gb_free=15.8, wall=18871
2023-09-05 06:57:02 | INFO | train_inner | epoch 016:    800 / 1474 loss=1.99, trans_loss=4.934, nll_loss=2.183, w2v_ctc_loss=0.721, task_loss=3.221, task_loss_gen=10.238, contrastive_loss=0.095, total=4174.94, n_correct=2741.25, ppl=4.54, accuracy=65.66, wps=13287.8, ups=1.59, wpb=8349.9, bsz=310.9, num_updates=22900, lr=9.34539e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=62, gb_free=16.1, wall=18934
2023-09-05 06:58:05 | INFO | train_inner | epoch 016:    900 / 1474 loss=1.99, trans_loss=4.935, nll_loss=2.185, w2v_ctc_loss=0.724, task_loss=3.3, task_loss_gen=10.318, contrastive_loss=0.086, total=4163.19, n_correct=2734.07, ppl=4.55, accuracy=65.672, wps=13237, ups=1.59, wpb=8326.4, bsz=310.6, num_updates=23000, lr=9.32505e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=62, gb_free=16.4, wall=18997
2023-09-05 06:58:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 06:59:09 | INFO | train_inner | epoch 016:   1001 / 1474 loss=2.001, trans_loss=4.944, nll_loss=2.196, w2v_ctc_loss=0.743, task_loss=2.884, task_loss_gen=11.102, contrastive_loss=0.083, total=4108.44, n_correct=2683.35, ppl=4.58, accuracy=65.313, wps=12888.3, ups=1.57, wpb=8216.9, bsz=298.9, num_updates=23100, lr=9.30484e-05, gnorm=0.576, clip=0, loss_scale=32, train_wall=63, gb_free=14.4, wall=19061
2023-09-05 07:00:12 | INFO | train_inner | epoch 016:   1101 / 1474 loss=2.001, trans_loss=4.949, nll_loss=2.202, w2v_ctc_loss=0.74, task_loss=3.542, task_loss_gen=10.492, contrastive_loss=0.06, total=4111.6, n_correct=2683.53, ppl=4.6, accuracy=65.267, wps=12991.1, ups=1.58, wpb=8223.2, bsz=295.6, num_updates=23200, lr=9.28477e-05, gnorm=0.605, clip=0, loss_scale=32, train_wall=63, gb_free=14.3, wall=19124
2023-09-05 07:00:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 07:01:16 | INFO | train_inner | epoch 016:   1202 / 1474 loss=2.001, trans_loss=4.946, nll_loss=2.199, w2v_ctc_loss=0.719, task_loss=3.759, task_loss_gen=9.92, contrastive_loss=0.156, total=4157.5, n_correct=2718.23, ppl=4.59, accuracy=65.381, wps=12967.4, ups=1.56, wpb=8315, bsz=308.5, num_updates=23300, lr=9.26482e-05, gnorm=0.769, clip=0, loss_scale=16, train_wall=64, gb_free=15.1, wall=19188
2023-09-05 07:02:20 | INFO | train_inner | epoch 016:   1302 / 1474 loss=2.003, trans_loss=4.944, nll_loss=2.195, w2v_ctc_loss=0.736, task_loss=4.248, task_loss_gen=8.535, contrastive_loss=0.139, total=4149.14, n_correct=2716.44, ppl=4.58, accuracy=65.47, wps=13051.6, ups=1.57, wpb=8298.3, bsz=311.9, num_updates=23400, lr=9.245e-05, gnorm=0.738, clip=0, loss_scale=16, train_wall=63, gb_free=10.8, wall=19252
2023-09-05 07:03:23 | INFO | train_inner | epoch 016:   1402 / 1474 loss=1.994, trans_loss=4.944, nll_loss=2.196, w2v_ctc_loss=0.734, task_loss=4.5, task_loss_gen=8.16, contrastive_loss=0.063, total=4200.01, n_correct=2750.34, ppl=4.58, accuracy=65.484, wps=13316.9, ups=1.59, wpb=8400, bsz=322.2, num_updates=23500, lr=9.22531e-05, gnorm=0.856, clip=0, loss_scale=16, train_wall=62, gb_free=16.5, wall=19315
2023-09-05 07:04:09 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 07:04:42 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 3.958 | trans_loss 5.193 | nll_loss 2.464 | w2v_ctc_loss 1.338 | task_loss 74.402 | task_loss_gen 29.127 | contrastive_loss 0.24 | total 4003.4 | n_correct 2645.4 | ppl 5.52 | accuracy 66.079 | uer 18.501 | wer 20.305 | raw_wer 20.305 | bleu 21.63 | wps 1587.6 | wpb 4003.4 | bsz 141.8 | num_updates 23572 | best_bleu 21.68
2023-09-05 07:04:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 23572 updates
2023-09-05 07:04:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.6307.pt
2023-09-05 07:04:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.6307.pt
2023-09-05 07:04:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.6307.pt (epoch 16 @ 23572 updates, score 21.63) (writing took 7.377363362000324 seconds)
2023-09-05 07:04:50 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-09-05 07:04:50 | INFO | train | epoch 016 | loss 1.994 | trans_loss 4.939 | nll_loss 2.189 | w2v_ctc_loss 0.73 | task_loss 3.583 | task_loss_gen 9.88 | contrastive_loss 0.091 | total 4138.25 | n_correct 2712.38 | ppl 4.56 | accuracy 65.544 | wps 12460.2 | ups 1.51 | wpb 8276.5 | bsz 305.7 | num_updates 23572 | lr 9.21121e-05 | gnorm 0.623 | clip 0 | loss_scale 16 | train_wall 919 | gb_free 15.1 | wall 19402
2023-09-05 07:04:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 07:04:50 | INFO | fairseq.trainer | begin training epoch 17
2023-09-05 07:04:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 07:05:16 | INFO | train_inner | epoch 017:     28 / 1474 loss=2.001, trans_loss=4.932, nll_loss=2.18, w2v_ctc_loss=0.718, task_loss=5.181, task_loss_gen=8.74, contrastive_loss=0.212, total=4141.79, n_correct=2716.6, ppl=4.53, accuracy=65.59, wps=7357, ups=0.89, wpb=8283.6, bsz=301.5, num_updates=23600, lr=9.20575e-05, gnorm=0.892, clip=0, loss_scale=16, train_wall=63, gb_free=15.7, wall=19427
2023-09-05 07:06:18 | INFO | train_inner | epoch 017:    128 / 1474 loss=1.981, trans_loss=4.92, nll_loss=2.163, w2v_ctc_loss=0.72, task_loss=4.567, task_loss_gen=8.775, contrastive_loss=0.043, total=4110.88, n_correct=2709.3, ppl=4.48, accuracy=65.906, wps=13122.4, ups=1.6, wpb=8221.8, bsz=295.4, num_updates=23700, lr=9.1863e-05, gnorm=0.704, clip=0, loss_scale=16, train_wall=62, gb_free=16, wall=19490
2023-09-05 07:07:21 | INFO | train_inner | epoch 017:    228 / 1474 loss=1.993, trans_loss=4.918, nll_loss=2.162, w2v_ctc_loss=0.712, task_loss=4.48, task_loss_gen=7.742, contrastive_loss=0.204, total=4171.95, n_correct=2749.41, ppl=4.48, accuracy=65.902, wps=13326.8, ups=1.6, wpb=8343.9, bsz=320.4, num_updates=23800, lr=9.16698e-05, gnorm=0.729, clip=0, loss_scale=16, train_wall=62, gb_free=15.5, wall=19553
2023-09-05 07:08:24 | INFO | train_inner | epoch 017:    328 / 1474 loss=1.994, trans_loss=4.925, nll_loss=2.17, w2v_ctc_loss=0.714, task_loss=5.211, task_loss_gen=8.068, contrastive_loss=0.211, total=4157.94, n_correct=2737.44, ppl=4.5, accuracy=65.836, wps=13207.5, ups=1.59, wpb=8315.9, bsz=305, num_updates=23900, lr=9.14779e-05, gnorm=0.824, clip=0, loss_scale=16, train_wall=62, gb_free=14, wall=19616
2023-09-05 07:09:27 | INFO | train_inner | epoch 017:    428 / 1474 loss=1.979, trans_loss=4.923, nll_loss=2.169, w2v_ctc_loss=0.721, task_loss=5.189, task_loss_gen=7.984, contrastive_loss=0.039, total=4141.8, n_correct=2732.2, ppl=4.5, accuracy=65.966, wps=13145.4, ups=1.59, wpb=8283.6, bsz=306.7, num_updates=24000, lr=9.12871e-05, gnorm=0.786, clip=0, loss_scale=16, train_wall=62, gb_free=16.9, wall=19679
2023-09-05 07:09:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 07:10:01 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.968 | trans_loss 5.192 | nll_loss 2.458 | w2v_ctc_loss 1.37 | task_loss 27.596 | task_loss_gen 21.629 | contrastive_loss 0.243 | total 4003.4 | n_correct 2646.2 | ppl 5.5 | accuracy 66.099 | uer 18.44 | wer 20.286 | raw_wer 20.286 | bleu 21.76 | wps 1555.9 | wpb 4003.4 | bsz 141.8 | num_updates 24000 | best_bleu 21.76
2023-09-05 07:10:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 24000 updates
2023-09-05 07:10:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_17_24000.pt
2023-09-05 07:10:03 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_17_24000.pt
2023-09-05 07:10:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_17_24000.pt (epoch 17 @ 24000 updates, score 21.76) (writing took 17.44332990096882 seconds)
2023-09-05 07:10:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-05 07:10:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-05 07:11:24 | INFO | train_inner | epoch 017:    530 / 1474 loss=1.998, trans_loss=4.941, nll_loss=2.191, w2v_ctc_loss=0.725, task_loss=6.7, task_loss_gen=8.016, contrastive_loss=0.092, total=4188.27, n_correct=2738.23, ppl=4.57, accuracy=65.379, wps=7170.4, ups=0.86, wpb=8376.5, bsz=309.3, num_updates=24100, lr=9.10975e-05, gnorm=3.345, clip=11, loss_scale=4, train_wall=64, gb_free=16.1, wall=19795
2023-09-05 07:12:26 | INFO | train_inner | epoch 017:    630 / 1474 loss=1.995, trans_loss=4.941, nll_loss=2.191, w2v_ctc_loss=0.723, task_loss=7.777, task_loss_gen=7.595, contrastive_loss=0.049, total=4158.81, n_correct=2723.99, ppl=4.57, accuracy=65.499, wps=13315.9, ups=1.6, wpb=8317.6, bsz=300.1, num_updates=24200, lr=9.09091e-05, gnorm=2.807, clip=4, loss_scale=4, train_wall=62, gb_free=15, wall=19858
2023-09-05 07:13:29 | INFO | train_inner | epoch 017:    730 / 1474 loss=2.006, trans_loss=4.945, nll_loss=2.195, w2v_ctc_loss=0.739, task_loss=7.146, task_loss_gen=7.127, contrastive_loss=0.088, total=4174.65, n_correct=2726.34, ppl=4.58, accuracy=65.307, wps=13385.5, ups=1.6, wpb=8349.3, bsz=310.5, num_updates=24300, lr=9.07218e-05, gnorm=2.708, clip=3, loss_scale=4, train_wall=62, gb_free=11.3, wall=19920
2023-09-05 07:14:31 | INFO | train_inner | epoch 017:    830 / 1474 loss=1.998, trans_loss=4.942, nll_loss=2.192, w2v_ctc_loss=0.728, task_loss=6.982, task_loss_gen=7.087, contrastive_loss=0.058, total=4083.91, n_correct=2673.5, ppl=4.57, accuracy=65.464, wps=13108.1, ups=1.6, wpb=8167.8, bsz=294.5, num_updates=24400, lr=9.05357e-05, gnorm=2.035, clip=2, loss_scale=4, train_wall=62, gb_free=16.7, wall=19983
2023-09-05 07:15:33 | INFO | train_inner | epoch 017:    930 / 1474 loss=1.997, trans_loss=4.944, nll_loss=2.194, w2v_ctc_loss=0.725, task_loss=6.586, task_loss_gen=6.692, contrastive_loss=0.061, total=4114.87, n_correct=2693.29, ppl=4.58, accuracy=65.453, wps=13230.2, ups=1.61, wpb=8229.7, bsz=307.5, num_updates=24500, lr=9.03508e-05, gnorm=2.416, clip=5, loss_scale=4, train_wall=62, gb_free=17.4, wall=20045
2023-09-05 07:16:35 | INFO | train_inner | epoch 017:   1030 / 1474 loss=2.001, trans_loss=4.942, nll_loss=2.193, w2v_ctc_loss=0.731, task_loss=7.41, task_loss_gen=7.181, contrastive_loss=0.063, total=4091.03, n_correct=2675.03, ppl=4.57, accuracy=65.388, wps=13120.9, ups=1.6, wpb=8182.1, bsz=298.8, num_updates=24600, lr=9.0167e-05, gnorm=2.39, clip=5, loss_scale=4, train_wall=62, gb_free=15.4, wall=20107
2023-09-05 07:17:38 | INFO | train_inner | epoch 017:   1130 / 1474 loss=1.992, trans_loss=4.939, nll_loss=2.189, w2v_ctc_loss=0.718, task_loss=6.952, task_loss_gen=6.749, contrastive_loss=0.052, total=4112.54, n_correct=2697.21, ppl=4.56, accuracy=65.585, wps=13209.3, ups=1.61, wpb=8225.1, bsz=301.7, num_updates=24700, lr=8.99843e-05, gnorm=2.109, clip=3, loss_scale=4, train_wall=61, gb_free=15.2, wall=20169
2023-09-05 07:18:41 | INFO | train_inner | epoch 017:   1230 / 1474 loss=2.032, trans_loss=4.957, nll_loss=2.212, w2v_ctc_loss=0.721, task_loss=6.744, task_loss_gen=6.708, contrastive_loss=0.352, total=4171.58, n_correct=2711.1, ppl=4.63, accuracy=64.99, wps=13183.7, ups=1.58, wpb=8343.2, bsz=325.2, num_updates=24800, lr=8.98027e-05, gnorm=2.435, clip=6, loss_scale=4, train_wall=63, gb_free=16.1, wall=20233
2023-09-05 07:19:44 | INFO | train_inner | epoch 017:   1330 / 1474 loss=1.996, trans_loss=4.944, nll_loss=2.194, w2v_ctc_loss=0.717, task_loss=6.514, task_loss_gen=6.724, contrastive_loss=0.062, total=4138.5, n_correct=2708.82, ppl=4.57, accuracy=65.454, wps=13211.3, ups=1.6, wpb=8277, bsz=301.4, num_updates=24900, lr=8.96221e-05, gnorm=1.861, clip=1, loss_scale=4, train_wall=62, gb_free=15.5, wall=20295
2023-09-05 07:20:47 | INFO | train_inner | epoch 017:   1430 / 1474 loss=1.995, trans_loss=4.948, nll_loss=2.2, w2v_ctc_loss=0.721, task_loss=6.451, task_loss_gen=6.598, contrastive_loss=0.053, total=4118.28, n_correct=2692.48, ppl=4.59, accuracy=65.379, wps=13100.8, ups=1.59, wpb=8236.6, bsz=303.9, num_updates=25000, lr=8.94427e-05, gnorm=1.88, clip=0, loss_scale=4, train_wall=62, gb_free=15.7, wall=20358
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:0')
2023-09-05 07:21:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:1')
2023-09-05 07:21:49 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.976 | trans_loss 5.205 | nll_loss 2.475 | w2v_ctc_loss 1.354 | task_loss 65.686 | task_loss_gen 25.411 | contrastive_loss 0.252 | total 4003.4 | n_correct 2636.6 | ppl 5.56 | accuracy 65.859 | uer 18.337 | wer 19.981 | raw_wer 19.981 | bleu 21.77 | wps 1552 | wpb 4003.4 | bsz 141.8 | num_updates 25044 | best_bleu 21.77
2023-09-05 07:21:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 25044 updates
2023-09-05 07:21:49 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 07:21:56 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 07:22:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 17 @ 25044 updates, score 21.77) (writing took 13.59661571396282 seconds)
2023-09-05 07:22:03 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-09-05 07:22:03 | INFO | train | epoch 017 | loss 1.997 | trans_loss 4.937 | nll_loss 2.186 | w2v_ctc_loss 0.723 | task_loss 6.329 | task_loss_gen 7.358 | contrastive_loss 0.101 | total 4138.4 | n_correct 2712.57 | ppl 4.55 | accuracy 65.546 | wps 11800.3 | ups 1.43 | wpb 8276.8 | bsz 305.6 | num_updates 25044 | lr 8.93641e-05 | gnorm 1.907 | clip 2.7 | loss_scale 4 | train_wall 914 | gb_free 15.9 | wall 20434
2023-09-05 07:22:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 07:22:03 | INFO | fairseq.trainer | begin training epoch 18
2023-09-05 07:22:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 07:22:46 | INFO | train_inner | epoch 018:     56 / 1474 loss=1.992, trans_loss=4.935, nll_loss=2.183, w2v_ctc_loss=0.727, task_loss=6.808, task_loss_gen=6.628, contrastive_loss=0.057, total=4131.1, n_correct=2712.22, ppl=4.54, accuracy=65.654, wps=6915.3, ups=0.84, wpb=8262.2, bsz=301.5, num_updates=25100, lr=8.92644e-05, gnorm=2.198, clip=1, loss_scale=4, train_wall=63, gb_free=16.5, wall=20478
2023-09-05 07:23:49 | INFO | train_inner | epoch 018:    156 / 1474 loss=1.984, trans_loss=4.911, nll_loss=2.153, w2v_ctc_loss=0.693, task_loss=5.89, task_loss_gen=6.143, contrastive_loss=0.179, total=4161.38, n_correct=2751.72, ppl=4.45, accuracy=66.125, wps=13302.6, ups=1.6, wpb=8322.8, bsz=315, num_updates=25200, lr=8.90871e-05, gnorm=1.555, clip=1, loss_scale=4, train_wall=62, gb_free=16.6, wall=20540
2023-09-05 07:24:51 | INFO | train_inner | epoch 018:    256 / 1474 loss=1.98, trans_loss=4.92, nll_loss=2.162, w2v_ctc_loss=0.711, task_loss=6.488, task_loss_gen=6.534, contrastive_loss=0.052, total=4153.17, n_correct=2740.16, ppl=4.48, accuracy=65.978, wps=13228.1, ups=1.59, wpb=8306.3, bsz=311.2, num_updates=25300, lr=8.89108e-05, gnorm=2.22, clip=2, loss_scale=4, train_wall=62, gb_free=16.5, wall=20603
2023-09-05 07:25:54 | INFO | train_inner | epoch 018:    356 / 1474 loss=1.988, trans_loss=4.929, nll_loss=2.175, w2v_ctc_loss=0.712, task_loss=6.847, task_loss_gen=6.668, contrastive_loss=0.067, total=4179.02, n_correct=2746.32, ppl=4.51, accuracy=65.717, wps=13281.6, ups=1.59, wpb=8358, bsz=302.1, num_updates=25400, lr=8.87357e-05, gnorm=1.765, clip=1, loss_scale=4, train_wall=62, gb_free=16.2, wall=20666
2023-09-05 07:26:58 | INFO | train_inner | epoch 018:    456 / 1474 loss=2.002, trans_loss=4.935, nll_loss=2.182, w2v_ctc_loss=0.715, task_loss=7.159, task_loss_gen=6.808, contrastive_loss=0.161, total=4069.37, n_correct=2668.68, ppl=4.54, accuracy=65.58, wps=12730.6, ups=1.56, wpb=8138.7, bsz=293.6, num_updates=25500, lr=8.85615e-05, gnorm=1.875, clip=1, loss_scale=4, train_wall=63, gb_free=15.2, wall=20730
2023-09-05 07:28:01 | INFO | train_inner | epoch 018:    556 / 1474 loss=1.973, trans_loss=4.915, nll_loss=2.158, w2v_ctc_loss=0.701, task_loss=5.527, task_loss_gen=5.74, contrastive_loss=0.063, total=4224.88, n_correct=2794.23, ppl=4.46, accuracy=66.137, wps=13464.3, ups=1.59, wpb=8449.8, bsz=330.4, num_updates=25600, lr=8.83883e-05, gnorm=1.522, clip=0, loss_scale=4, train_wall=62, gb_free=16.4, wall=20793
2023-09-05 07:29:03 | INFO | train_inner | epoch 018:    656 / 1474 loss=2.002, trans_loss=4.939, nll_loss=2.188, w2v_ctc_loss=0.73, task_loss=6.508, task_loss_gen=6.662, contrastive_loss=0.131, total=4087.72, n_correct=2681.44, ppl=4.56, accuracy=65.597, wps=13092.8, ups=1.6, wpb=8175.4, bsz=298.1, num_updates=25700, lr=8.82162e-05, gnorm=1.655, clip=0, loss_scale=4, train_wall=62, gb_free=16.2, wall=20855
2023-09-05 07:30:06 | INFO | train_inner | epoch 018:    756 / 1474 loss=1.999, trans_loss=4.93, nll_loss=2.177, w2v_ctc_loss=0.718, task_loss=6.05, task_loss_gen=5.877, contrastive_loss=0.219, total=4202.56, n_correct=2762.49, ppl=4.52, accuracy=65.734, wps=13353.3, ups=1.59, wpb=8405.1, bsz=322.9, num_updates=25800, lr=8.80451e-05, gnorm=1.824, clip=1, loss_scale=4, train_wall=62, gb_free=17.5, wall=20918
2023-09-05 07:31:09 | INFO | train_inner | epoch 018:    856 / 1474 loss=1.984, trans_loss=4.929, nll_loss=2.175, w2v_ctc_loss=0.715, task_loss=6.185, task_loss_gen=6.233, contrastive_loss=0.048, total=4181.64, n_correct=2751.5, ppl=4.51, accuracy=65.8, wps=13293.8, ups=1.59, wpb=8363.3, bsz=306.1, num_updates=25900, lr=8.7875e-05, gnorm=1.622, clip=1, loss_scale=4, train_wall=62, gb_free=15.2, wall=20981
2023-09-05 07:32:12 | INFO | train_inner | epoch 018:    956 / 1474 loss=1.973, trans_loss=4.918, nll_loss=2.161, w2v_ctc_loss=0.701, task_loss=5.874, task_loss_gen=5.762, contrastive_loss=0.052, total=4133.45, n_correct=2729.38, ppl=4.47, accuracy=66.032, wps=13285.1, ups=1.61, wpb=8266.9, bsz=312.7, num_updates=26000, lr=8.77058e-05, gnorm=1.773, clip=2, loss_scale=4, train_wall=62, gb_free=14.2, wall=21043
2023-09-05 07:32:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 07:32:45 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.97 | trans_loss 5.198 | nll_loss 2.466 | w2v_ctc_loss 1.353 | task_loss 48.988 | task_loss_gen 21.382 | contrastive_loss 0.252 | total 4003.4 | n_correct 2644.3 | ppl 5.53 | accuracy 66.051 | uer 18.308 | wer 20.126 | raw_wer 20.126 | bleu 21.86 | wps 1621.6 | wpb 4003.4 | bsz 141.8 | num_updates 26000 | best_bleu 21.86
2023-09-05 07:32:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26000 updates
2023-09-05 07:32:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_18_26000.pt
2023-09-05 07:32:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_18_26000.pt
2023-09-05 07:33:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_18_26000.pt (epoch 18 @ 26000 updates, score 21.86) (writing took 15.360402517020702 seconds)
2023-09-05 07:34:05 | INFO | train_inner | epoch 018:   1056 / 1474 loss=1.984, trans_loss=4.927, nll_loss=2.172, w2v_ctc_loss=0.711, task_loss=6.352, task_loss_gen=6.601, contrastive_loss=0.051, total=4133.66, n_correct=2722.67, ppl=4.51, accuracy=65.866, wps=7315.8, ups=0.88, wpb=8267.3, bsz=298.3, num_updates=26100, lr=8.75376e-05, gnorm=1.499, clip=0, loss_scale=8, train_wall=63, gb_free=16.5, wall=21156
2023-09-05 07:35:07 | INFO | train_inner | epoch 018:   1156 / 1474 loss=1.988, trans_loss=4.918, nll_loss=2.162, w2v_ctc_loss=0.711, task_loss=5.229, task_loss_gen=5.914, contrastive_loss=0.156, total=4156.35, n_correct=2743.63, ppl=4.48, accuracy=66.011, wps=13288.9, ups=1.6, wpb=8312.7, bsz=315.8, num_updates=26200, lr=8.73704e-05, gnorm=0.852, clip=0, loss_scale=8, train_wall=62, gb_free=13.3, wall=21219
2023-09-05 07:36:10 | INFO | train_inner | epoch 018:   1256 / 1474 loss=1.983, trans_loss=4.931, nll_loss=2.178, w2v_ctc_loss=0.713, task_loss=5.917, task_loss_gen=6.911, contrastive_loss=0.042, total=4093.35, n_correct=2694.37, ppl=4.53, accuracy=65.823, wps=13018.6, ups=1.59, wpb=8186.7, bsz=288, num_updates=26300, lr=8.72041e-05, gnorm=0.923, clip=0, loss_scale=8, train_wall=62, gb_free=15.8, wall=21282
2023-09-05 07:37:12 | INFO | train_inner | epoch 018:   1356 / 1474 loss=1.994, trans_loss=4.933, nll_loss=2.181, w2v_ctc_loss=0.733, task_loss=6.033, task_loss_gen=7.017, contrastive_loss=0.065, total=4056.71, n_correct=2664.31, ppl=4.53, accuracy=65.677, wps=13032.1, ups=1.61, wpb=8113.4, bsz=289.2, num_updates=26400, lr=8.70388e-05, gnorm=0.999, clip=0, loss_scale=8, train_wall=62, gb_free=16.9, wall=21344
2023-09-05 07:38:15 | INFO | train_inner | epoch 018:   1456 / 1474 loss=1.986, trans_loss=4.929, nll_loss=2.176, w2v_ctc_loss=0.725, task_loss=5.889, task_loss_gen=6.701, contrastive_loss=0.05, total=4125.39, n_correct=2716.37, ppl=4.52, accuracy=65.845, wps=13124.3, ups=1.59, wpb=8250.8, bsz=299, num_updates=26500, lr=8.68744e-05, gnorm=1.018, clip=0, loss_scale=8, train_wall=62, gb_free=15.7, wall=21407
2023-09-05 07:38:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 07:39:01 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.948 | trans_loss 5.189 | nll_loss 2.458 | w2v_ctc_loss 1.313 | task_loss 43.852 | task_loss_gen 20.94 | contrastive_loss 0.24 | total 4003.4 | n_correct 2642 | ppl 5.49 | accuracy 65.994 | uer 18.016 | wer 19.969 | raw_wer 19.969 | bleu 21.71 | wps 1525.3 | wpb 4003.4 | bsz 141.8 | num_updates 26518 | best_bleu 21.86
2023-09-05 07:39:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 26518 updates
2023-09-05 07:39:01 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.7102.pt
2023-09-05 07:39:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.7102.pt
2023-09-05 07:39:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.7102.pt (epoch 18 @ 26518 updates, score 21.71) (writing took 7.150595556013286 seconds)
2023-09-05 07:39:09 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-09-05 07:39:09 | INFO | train | epoch 018 | loss 1.987 | trans_loss 4.926 | nll_loss 2.172 | w2v_ctc_loss 0.713 | task_loss 6.148 | task_loss_gen 6.388 | contrastive_loss 0.099 | total 4138.65 | n_correct 2725.09 | ppl 4.51 | accuracy 65.845 | wps 11893.4 | ups 1.44 | wpb 8277.3 | bsz 305.7 | num_updates 26518 | lr 8.6845e-05 | gnorm 1.543 | clip 0.7 | loss_scale 8 | train_wall 918 | gb_free 15.6 | wall 21460
2023-09-05 07:39:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 07:39:09 | INFO | fairseq.trainer | begin training epoch 19
2023-09-05 07:39:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 07:40:08 | INFO | train_inner | epoch 019:     82 / 1474 loss=1.976, trans_loss=4.906, nll_loss=2.146, w2v_ctc_loss=0.707, task_loss=5.284, task_loss_gen=6.36, contrastive_loss=0.102, total=4098.8, n_correct=2712.96, ppl=4.43, accuracy=66.189, wps=7286.1, ups=0.89, wpb=8197.6, bsz=296.6, num_updates=26600, lr=8.6711e-05, gnorm=0.845, clip=0, loss_scale=8, train_wall=62, gb_free=16.5, wall=21519
2023-09-05 07:41:11 | INFO | train_inner | epoch 019:    182 / 1474 loss=1.973, trans_loss=4.903, nll_loss=2.143, w2v_ctc_loss=0.712, task_loss=4.734, task_loss_gen=6.039, contrastive_loss=0.086, total=4227.47, n_correct=2805.69, ppl=4.42, accuracy=66.368, wps=13310.5, ups=1.57, wpb=8454.9, bsz=325, num_updates=26700, lr=8.65485e-05, gnorm=0.886, clip=0, loss_scale=8, train_wall=63, gb_free=15.9, wall=21583
2023-09-05 07:42:14 | INFO | train_inner | epoch 019:    282 / 1474 loss=1.963, trans_loss=4.898, nll_loss=2.135, w2v_ctc_loss=0.7, task_loss=5.351, task_loss_gen=6.417, contrastive_loss=0.039, total=4188.96, n_correct=2782.97, ppl=4.39, accuracy=66.436, wps=13343.1, ups=1.59, wpb=8377.9, bsz=308.1, num_updates=26800, lr=8.63868e-05, gnorm=0.983, clip=0, loss_scale=8, train_wall=62, gb_free=16.5, wall=21646
2023-09-05 07:43:17 | INFO | train_inner | epoch 019:    382 / 1474 loss=1.977, trans_loss=4.9, nll_loss=2.138, w2v_ctc_loss=0.699, task_loss=5.431, task_loss_gen=6.219, contrastive_loss=0.148, total=4168.76, n_correct=2762.54, ppl=4.4, accuracy=66.268, wps=13322.6, ups=1.6, wpb=8337.5, bsz=310.6, num_updates=26900, lr=8.62261e-05, gnorm=0.945, clip=0, loss_scale=8, train_wall=62, gb_free=14.2, wall=21708
2023-09-05 07:44:19 | INFO | train_inner | epoch 019:    482 / 1474 loss=1.976, trans_loss=4.91, nll_loss=2.152, w2v_ctc_loss=0.718, task_loss=5.501, task_loss_gen=6.615, contrastive_loss=0.048, total=4107.91, n_correct=2715.28, ppl=4.44, accuracy=66.099, wps=13242.6, ups=1.61, wpb=8215.8, bsz=299.6, num_updates=27000, lr=8.60663e-05, gnorm=1.05, clip=0, loss_scale=8, train_wall=61, gb_free=11.4, wall=21770
2023-09-05 07:45:21 | INFO | train_inner | epoch 019:    582 / 1474 loss=1.97, trans_loss=4.904, nll_loss=2.143, w2v_ctc_loss=0.697, task_loss=5.337, task_loss_gen=6.189, contrastive_loss=0.115, total=4133.95, n_correct=2742.72, ppl=4.42, accuracy=66.346, wps=13216.7, ups=1.6, wpb=8267.9, bsz=306.3, num_updates=27100, lr=8.59074e-05, gnorm=1.043, clip=0, loss_scale=8, train_wall=62, gb_free=15.5, wall=21833
2023-09-05 07:46:24 | INFO | train_inner | epoch 019:    682 / 1474 loss=1.959, trans_loss=4.907, nll_loss=2.149, w2v_ctc_loss=0.689, task_loss=4.861, task_loss_gen=5.68, contrastive_loss=0.041, total=4199.37, n_correct=2789.65, ppl=4.43, accuracy=66.43, wps=13434.7, ups=1.6, wpb=8398.7, bsz=321.7, num_updates=27200, lr=8.57493e-05, gnorm=0.972, clip=0, loss_scale=8, train_wall=62, gb_free=16.7, wall=21895
2023-09-05 07:47:27 | INFO | train_inner | epoch 019:    782 / 1474 loss=1.97, trans_loss=4.905, nll_loss=2.145, w2v_ctc_loss=0.706, task_loss=5.266, task_loss_gen=6.548, contrastive_loss=0.054, total=4142.94, n_correct=2743.52, ppl=4.42, accuracy=66.222, wps=13143.5, ups=1.59, wpb=8285.9, bsz=305.4, num_updates=27300, lr=8.55921e-05, gnorm=0.977, clip=0, loss_scale=8, train_wall=62, gb_free=16.2, wall=21958
2023-09-05 07:48:30 | INFO | train_inner | epoch 019:    882 / 1474 loss=1.974, trans_loss=4.915, nll_loss=2.157, w2v_ctc_loss=0.711, task_loss=5.465, task_loss_gen=6.483, contrastive_loss=0.04, total=4153.23, n_correct=2742.72, ppl=4.46, accuracy=66.038, wps=13202.5, ups=1.59, wpb=8306.5, bsz=303.4, num_updates=27400, lr=8.54358e-05, gnorm=1.019, clip=0, loss_scale=8, train_wall=62, gb_free=16.5, wall=22021
2023-09-05 07:49:33 | INFO | train_inner | epoch 019:    982 / 1474 loss=1.993, trans_loss=4.92, nll_loss=2.164, w2v_ctc_loss=0.699, task_loss=5.314, task_loss_gen=6.309, contrastive_loss=0.275, total=4102.27, n_correct=2708.5, ppl=4.48, accuracy=66.024, wps=12912.9, ups=1.57, wpb=8204.5, bsz=308.7, num_updates=27500, lr=8.52803e-05, gnorm=1.058, clip=0, loss_scale=8, train_wall=63, gb_free=15.6, wall=22085
2023-09-05 07:50:36 | INFO | train_inner | epoch 019:   1082 / 1474 loss=1.978, trans_loss=4.919, nll_loss=2.163, w2v_ctc_loss=0.706, task_loss=5.659, task_loss_gen=6.588, contrastive_loss=0.082, total=4036.79, n_correct=2663.82, ppl=4.48, accuracy=65.989, wps=12948, ups=1.6, wpb=8073.6, bsz=291.6, num_updates=27600, lr=8.51257e-05, gnorm=1.138, clip=0, loss_scale=8, train_wall=62, gb_free=15.4, wall=22147
2023-09-05 07:51:39 | INFO | train_inner | epoch 019:   1182 / 1474 loss=1.994, trans_loss=4.922, nll_loss=2.167, w2v_ctc_loss=0.716, task_loss=5.101, task_loss_gen=6.639, contrastive_loss=0.165, total=4129.82, n_correct=2717.09, ppl=4.49, accuracy=65.792, wps=12993.6, ups=1.57, wpb=8259.6, bsz=305.9, num_updates=27700, lr=8.49719e-05, gnorm=0.984, clip=0, loss_scale=8, train_wall=63, gb_free=17.4, wall=22211
2023-09-05 07:52:42 | INFO | train_inner | epoch 019:   1282 / 1474 loss=1.974, trans_loss=4.914, nll_loss=2.157, w2v_ctc_loss=0.701, task_loss=5.276, task_loss_gen=6.335, contrastive_loss=0.064, total=4147.96, n_correct=2744.73, ppl=4.46, accuracy=66.171, wps=13254, ups=1.6, wpb=8295.9, bsz=301.4, num_updates=27800, lr=8.48189e-05, gnorm=0.839, clip=0, loss_scale=8, train_wall=62, gb_free=15.5, wall=22273
2023-09-05 07:53:44 | INFO | train_inner | epoch 019:   1382 / 1474 loss=1.972, trans_loss=4.911, nll_loss=2.153, w2v_ctc_loss=0.704, task_loss=5.208, task_loss_gen=6.346, contrastive_loss=0.05, total=4125.32, n_correct=2731.5, ppl=4.45, accuracy=66.213, wps=13140.6, ups=1.59, wpb=8250.6, bsz=300.4, num_updates=27900, lr=8.46668e-05, gnorm=0.874, clip=0, loss_scale=8, train_wall=62, gb_free=16.1, wall=22336
2023-09-05 07:54:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 07:55:16 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.956 | trans_loss 5.182 | nll_loss 2.444 | w2v_ctc_loss 1.35 | task_loss 69.978 | task_loss_gen 26.446 | contrastive_loss 0.25 | total 4003.4 | n_correct 2651.8 | ppl 5.44 | accuracy 66.239 | uer 18.201 | wer 19.962 | raw_wer 19.962 | bleu 21.76 | wps 1568.9 | wpb 4003.4 | bsz 141.8 | num_updates 27992 | best_bleu 21.86
2023-09-05 07:55:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 27992 updates
2023-09-05 07:55:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.7602.pt
2023-09-05 07:55:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.7602.pt
2023-09-05 07:55:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.7602.pt (epoch 19 @ 27992 updates, score 21.76) (writing took 7.745085411006585 seconds)
2023-09-05 07:55:24 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-09-05 07:55:24 | INFO | train | epoch 019 | loss 1.975 | trans_loss 4.909 | nll_loss 2.15 | w2v_ctc_loss 0.705 | task_loss 5.292 | task_loss_gen 6.337 | contrastive_loss 0.093 | total 4138.65 | n_correct 2739.87 | ppl 4.44 | accuracy 66.202 | wps 12501.4 | ups 1.51 | wpb 8277.3 | bsz 305.7 | num_updates 27992 | lr 8.45275e-05 | gnorm 0.988 | clip 0 | loss_scale 8 | train_wall 916 | gb_free 17 | wall 22436
2023-09-05 07:55:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 07:55:25 | INFO | fairseq.trainer | begin training epoch 20
2023-09-05 07:55:25 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 07:55:38 | INFO | train_inner | epoch 020:      8 / 1474 loss=1.971, trans_loss=4.9, nll_loss=2.138, w2v_ctc_loss=0.696, task_loss=5.519, task_loss_gen=6.152, contrastive_loss=0.133, total=4124.63, n_correct=2739.5, ppl=4.4, accuracy=66.418, wps=7296.5, ups=0.88, wpb=8249.3, bsz=304.8, num_updates=28000, lr=8.45154e-05, gnorm=1.178, clip=0, loss_scale=8, train_wall=62, gb_free=17, wall=22449
2023-09-05 07:55:38 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 07:56:11 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.951 | trans_loss 5.19 | nll_loss 2.454 | w2v_ctc_loss 1.316 | task_loss 32.316 | task_loss_gen 19.853 | contrastive_loss 0.249 | total 4003.4 | n_correct 2649 | ppl 5.48 | accuracy 66.169 | uer 18.236 | wer 20.089 | raw_wer 20.089 | bleu 21.65 | wps 1606 | wpb 4003.4 | bsz 141.8 | num_updates 28000 | best_bleu 21.86
2023-09-05 07:56:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 28000 updates
2023-09-05 07:56:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_20_28000.pt
2023-09-05 07:56:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_20_28000.pt
2023-09-05 07:56:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_20_28000.pt (epoch 20 @ 28000 updates, score 21.65) (writing took 8.587524094968103 seconds)
2023-09-05 07:57:22 | INFO | train_inner | epoch 020:    108 / 1474 loss=1.952, trans_loss=4.883, nll_loss=2.117, w2v_ctc_loss=0.686, task_loss=5.176, task_loss_gen=6.021, contrastive_loss=0.052, total=4199.19, n_correct=2805.23, ppl=4.34, accuracy=66.804, wps=8002.1, ups=0.95, wpb=8398.4, bsz=314, num_updates=28100, lr=8.43649e-05, gnorm=1.031, clip=0, loss_scale=16, train_wall=62, gb_free=14.4, wall=22554
2023-09-05 07:58:26 | INFO | train_inner | epoch 020:    208 / 1474 loss=1.964, trans_loss=4.891, nll_loss=2.126, w2v_ctc_loss=0.696, task_loss=5.109, task_loss_gen=6.459, contrastive_loss=0.105, total=4148.29, n_correct=2762.49, ppl=4.36, accuracy=66.593, wps=13096.4, ups=1.58, wpb=8296.6, bsz=300.5, num_updates=28200, lr=8.42152e-05, gnorm=0.704, clip=0, loss_scale=16, train_wall=63, gb_free=15.2, wall=22618
2023-09-05 07:59:29 | INFO | train_inner | epoch 020:    308 / 1474 loss=1.95, trans_loss=4.885, nll_loss=2.119, w2v_ctc_loss=0.691, task_loss=4.099, task_loss_gen=6.135, contrastive_loss=0.042, total=4191.34, n_correct=2796.17, ppl=4.34, accuracy=66.713, wps=13364.6, ups=1.59, wpb=8382.7, bsz=326.2, num_updates=28300, lr=8.40663e-05, gnorm=0.694, clip=0, loss_scale=16, train_wall=62, gb_free=15.3, wall=22680
2023-09-05 08:00:31 | INFO | train_inner | epoch 020:    408 / 1474 loss=1.952, trans_loss=4.882, nll_loss=2.115, w2v_ctc_loss=0.688, task_loss=4.738, task_loss_gen=6.462, contrastive_loss=0.043, total=4114.19, n_correct=2747.89, ppl=4.33, accuracy=66.791, wps=13092.9, ups=1.59, wpb=8228.4, bsz=297.5, num_updates=28400, lr=8.39181e-05, gnorm=0.71, clip=0, loss_scale=16, train_wall=62, gb_free=14.9, wall=22743
2023-09-05 08:01:34 | INFO | train_inner | epoch 020:    508 / 1474 loss=1.964, trans_loss=4.894, nll_loss=2.13, w2v_ctc_loss=0.687, task_loss=5.358, task_loss_gen=6.7, contrastive_loss=0.13, total=4108.2, n_correct=2737.43, ppl=4.38, accuracy=66.633, wps=13139.4, ups=1.6, wpb=8216.4, bsz=299.5, num_updates=28500, lr=8.37708e-05, gnorm=0.827, clip=0, loss_scale=16, train_wall=62, gb_free=15.3, wall=22806
2023-09-05 08:02:36 | INFO | train_inner | epoch 020:    608 / 1474 loss=1.971, trans_loss=4.893, nll_loss=2.129, w2v_ctc_loss=0.699, task_loss=4.453, task_loss_gen=6.995, contrastive_loss=0.132, total=4092.44, n_correct=2723.59, ppl=4.38, accuracy=66.552, wps=13145.3, ups=1.61, wpb=8184.9, bsz=295.9, num_updates=28600, lr=8.36242e-05, gnorm=0.663, clip=0, loss_scale=16, train_wall=62, gb_free=17.2, wall=22868
2023-09-05 08:03:39 | INFO | train_inner | epoch 020:    708 / 1474 loss=1.96, trans_loss=4.894, nll_loss=2.13, w2v_ctc_loss=0.7, task_loss=4.551, task_loss_gen=6.474, contrastive_loss=0.036, total=4137.06, n_correct=2749.5, ppl=4.38, accuracy=66.46, wps=13196.9, ups=1.59, wpb=8274.1, bsz=300.5, num_updates=28700, lr=8.34784e-05, gnorm=0.66, clip=0, loss_scale=16, train_wall=62, gb_free=16.2, wall=22931
2023-09-05 08:04:42 | INFO | train_inner | epoch 020:    808 / 1474 loss=1.958, trans_loss=4.895, nll_loss=2.132, w2v_ctc_loss=0.698, task_loss=4.767, task_loss_gen=6.648, contrastive_loss=0.038, total=4146.78, n_correct=2759.45, ppl=4.38, accuracy=66.544, wps=13241.5, ups=1.6, wpb=8293.6, bsz=307.3, num_updates=28800, lr=8.33333e-05, gnorm=0.831, clip=0, loss_scale=16, train_wall=62, gb_free=14.8, wall=22993
2023-09-05 08:05:45 | INFO | train_inner | epoch 020:    908 / 1474 loss=1.989, trans_loss=4.9, nll_loss=2.139, w2v_ctc_loss=0.695, task_loss=4.089, task_loss_gen=6.156, contrastive_loss=0.328, total=4161, n_correct=2760.21, ppl=4.4, accuracy=66.335, wps=13081.6, ups=1.57, wpb=8322, bsz=323.2, num_updates=28900, lr=8.3189e-05, gnorm=0.651, clip=0, loss_scale=16, train_wall=63, gb_free=17.1, wall=23057
2023-09-05 08:06:48 | INFO | train_inner | epoch 020:   1008 / 1474 loss=1.953, trans_loss=4.892, nll_loss=2.128, w2v_ctc_loss=0.686, task_loss=4.672, task_loss_gen=6.591, contrastive_loss=0.042, total=4168.14, n_correct=2778.68, ppl=4.37, accuracy=66.665, wps=13199, ups=1.58, wpb=8336.3, bsz=307.3, num_updates=29000, lr=8.30455e-05, gnorm=0.748, clip=0, loss_scale=16, train_wall=63, gb_free=16, wall=23120
2023-09-05 08:07:51 | INFO | train_inner | epoch 020:   1108 / 1474 loss=1.971, trans_loss=4.895, nll_loss=2.132, w2v_ctc_loss=0.691, task_loss=3.943, task_loss_gen=6.289, contrastive_loss=0.177, total=4166.49, n_correct=2770.52, ppl=4.38, accuracy=66.495, wps=13268.3, ups=1.59, wpb=8333, bsz=315.4, num_updates=29100, lr=8.29027e-05, gnorm=0.692, clip=0, loss_scale=16, train_wall=62, gb_free=14.4, wall=23183
2023-09-05 08:08:54 | INFO | train_inner | epoch 020:   1208 / 1474 loss=1.961, trans_loss=4.886, nll_loss=2.12, w2v_ctc_loss=0.705, task_loss=4.88, task_loss_gen=7.001, contrastive_loss=0.034, total=4029.18, n_correct=2681.3, ppl=4.35, accuracy=66.547, wps=12847.3, ups=1.59, wpb=8058.4, bsz=284, num_updates=29200, lr=8.27606e-05, gnorm=0.717, clip=0, loss_scale=16, train_wall=62, gb_free=11.3, wall=23246
2023-09-05 08:09:57 | INFO | train_inner | epoch 020:   1308 / 1474 loss=1.958, trans_loss=4.895, nll_loss=2.132, w2v_ctc_loss=0.695, task_loss=4.597, task_loss_gen=6.638, contrastive_loss=0.037, total=4123.21, n_correct=2741.6, ppl=4.38, accuracy=66.492, wps=13054.3, ups=1.58, wpb=8246.4, bsz=297.1, num_updates=29300, lr=8.26192e-05, gnorm=0.664, clip=0, loss_scale=16, train_wall=63, gb_free=17, wall=23309
2023-09-05 08:11:00 | INFO | train_inner | epoch 020:   1408 / 1474 loss=1.961, trans_loss=4.896, nll_loss=2.134, w2v_ctc_loss=0.7, task_loss=4.613, task_loss_gen=6.952, contrastive_loss=0.035, total=4116.28, n_correct=2733.59, ppl=4.39, accuracy=66.409, wps=13074.1, ups=1.59, wpb=8232.6, bsz=295, num_updates=29400, lr=8.24786e-05, gnorm=0.74, clip=0, loss_scale=16, train_wall=62, gb_free=17.1, wall=23372
2023-09-05 08:11:41 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 08:12:15 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.937 | trans_loss 5.183 | nll_loss 2.447 | w2v_ctc_loss 1.295 | task_loss 46.3 | task_loss_gen 21.253 | contrastive_loss 0.242 | total 4003.4 | n_correct 2655.5 | ppl 5.45 | accuracy 66.331 | uer 17.686 | wer 19.615 | raw_wer 19.615 | bleu 22.26 | wps 1601.9 | wpb 4003.4 | bsz 141.8 | num_updates 29466 | best_bleu 22.26
2023-09-05 08:12:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 29466 updates
2023-09-05 08:12:15 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 08:12:22 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 08:12:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 20 @ 29466 updates, score 22.26) (writing took 13.619859894970432 seconds)
2023-09-05 08:12:29 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-09-05 08:12:29 | INFO | train | epoch 020 | loss 1.961 | trans_loss 4.892 | nll_loss 2.128 | w2v_ctc_loss 0.694 | task_loss 4.593 | task_loss_gen 6.49 | contrastive_loss 0.088 | total 4138.65 | n_correct 2755.26 | ppl 4.37 | accuracy 66.574 | wps 11909.5 | ups 1.44 | wpb 8277.3 | bsz 305.7 | num_updates 29466 | lr 8.23862e-05 | gnorm 0.736 | clip 0 | loss_scale 16 | train_wall 917 | gb_free 15.7 | wall 23461
2023-09-05 08:12:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 08:12:29 | INFO | fairseq.trainer | begin training epoch 21
2023-09-05 08:12:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 08:12:59 | INFO | train_inner | epoch 021:     34 / 1474 loss=1.965, trans_loss=4.892, nll_loss=2.129, w2v_ctc_loss=0.69, task_loss=3.919, task_loss_gen=6.2, contrastive_loss=0.153, total=4152.26, n_correct=2764.49, ppl=4.37, accuracy=66.578, wps=7000.3, ups=0.84, wpb=8304.5, bsz=316.3, num_updates=29500, lr=8.23387e-05, gnorm=0.716, clip=0, loss_scale=16, train_wall=62, gb_free=16.1, wall=23490
2023-09-05 08:14:01 | INFO | train_inner | epoch 021:    134 / 1474 loss=1.954, trans_loss=4.871, nll_loss=2.101, w2v_ctc_loss=0.682, task_loss=3.773, task_loss_gen=6.034, contrastive_loss=0.147, total=4195.08, n_correct=2810.09, ppl=4.29, accuracy=66.985, wps=13392, ups=1.6, wpb=8390.2, bsz=319.6, num_updates=29600, lr=8.21995e-05, gnorm=0.649, clip=0, loss_scale=16, train_wall=62, gb_free=16.6, wall=23553
2023-09-05 08:15:04 | INFO | train_inner | epoch 021:    234 / 1474 loss=1.945, trans_loss=4.873, nll_loss=2.104, w2v_ctc_loss=0.674, task_loss=4.264, task_loss_gen=6.346, contrastive_loss=0.102, total=4155.31, n_correct=2784.34, ppl=4.3, accuracy=67.007, wps=13255.5, ups=1.6, wpb=8310.6, bsz=312.6, num_updates=29700, lr=8.2061e-05, gnorm=0.788, clip=0, loss_scale=16, train_wall=62, gb_free=16.4, wall=23616
2023-09-05 08:16:07 | INFO | train_inner | epoch 021:    334 / 1474 loss=1.955, trans_loss=4.877, nll_loss=2.109, w2v_ctc_loss=0.689, task_loss=3.982, task_loss_gen=6.456, contrastive_loss=0.105, total=4151.51, n_correct=2774.34, ppl=4.31, accuracy=66.827, wps=13178.9, ups=1.59, wpb=8303, bsz=310.8, num_updates=29800, lr=8.19232e-05, gnorm=0.643, clip=0, loss_scale=16, train_wall=62, gb_free=15.2, wall=23679
2023-09-05 08:17:09 | INFO | train_inner | epoch 021:    434 / 1474 loss=1.942, trans_loss=4.873, nll_loss=2.104, w2v_ctc_loss=0.678, task_loss=3.943, task_loss_gen=6.465, contrastive_loss=0.032, total=4180.85, n_correct=2805.07, ppl=4.3, accuracy=67.093, wps=13426.9, ups=1.61, wpb=8361.7, bsz=306.8, num_updates=29900, lr=8.17861e-05, gnorm=0.724, clip=0, loss_scale=16, train_wall=62, gb_free=15.3, wall=23741
2023-09-05 08:18:12 | INFO | train_inner | epoch 021:    534 / 1474 loss=1.944, trans_loss=4.868, nll_loss=2.096, w2v_ctc_loss=0.688, task_loss=4.218, task_loss_gen=6.821, contrastive_loss=0.03, total=4083.98, n_correct=2739.19, ppl=4.28, accuracy=67.072, wps=13047.8, ups=1.6, wpb=8168, bsz=295.1, num_updates=30000, lr=8.16497e-05, gnorm=0.705, clip=0, loss_scale=16, train_wall=62, gb_free=12.6, wall=23804
2023-09-05 08:18:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 08:18:46 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.947 | trans_loss 5.185 | nll_loss 2.451 | w2v_ctc_loss 1.326 | task_loss 40.387 | task_loss_gen 21.029 | contrastive_loss 0.231 | total 4003.4 | n_correct 2653.7 | ppl 5.47 | accuracy 66.286 | uer 17.782 | wer 19.716 | raw_wer 19.716 | bleu 21.84 | wps 1547.7 | wpb 4003.4 | bsz 141.8 | num_updates 30000 | best_bleu 22.26
2023-09-05 08:18:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30000 updates
2023-09-05 08:18:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_21_30000.pt
2023-09-05 08:18:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_21_30000.pt
2023-09-05 08:18:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_21_30000.pt (epoch 21 @ 30000 updates, score 21.84) (writing took 11.27359192701988 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:0')
2023-09-05 08:20:01 | INFO | train_inner | epoch 021:    634 / 1474 loss=1.957, trans_loss=4.872, nll_loss=2.103, w2v_ctc_loss=0.678, task_loss=4.262, task_loss_gen=6.468, contrastive_loss=0.202, total=4215.41, n_correct=2821.82, ppl=4.3, accuracy=66.941, wps=7734.2, ups=0.92, wpb=8430.8, bsz=315.4, num_updates=30100, lr=8.15139e-05, gnorm=0.756, clip=0, loss_scale=16, train_wall=62, gb_free=10.7, wall=23913
2023-09-05 08:21:04 | INFO | train_inner | epoch 021:    734 / 1474 loss=1.949, trans_loss=4.881, nll_loss=2.114, w2v_ctc_loss=0.68, task_loss=3.803, task_loss_gen=6.582, contrastive_loss=0.06, total=4152.97, n_correct=2776.74, ppl=4.33, accuracy=66.862, wps=13226.4, ups=1.59, wpb=8305.9, bsz=309.3, num_updates=30200, lr=8.13788e-05, gnorm=0.618, clip=0, loss_scale=32, train_wall=62, gb_free=16, wall=23975
2023-09-05 08:21:08 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 08:22:08 | INFO | train_inner | epoch 021:    835 / 1474 loss=1.955, trans_loss=4.888, nll_loss=2.123, w2v_ctc_loss=0.69, task_loss=4.493, task_loss_gen=7.431, contrastive_loss=0.034, total=4056.72, n_correct=2702.27, ppl=4.36, accuracy=66.612, wps=12701.9, ups=1.57, wpb=8113.4, bsz=290.1, num_updates=30300, lr=8.12444e-05, gnorm=0.796, clip=0, loss_scale=16, train_wall=63, gb_free=15.9, wall=24039
2023-09-05 08:23:10 | INFO | train_inner | epoch 021:    935 / 1474 loss=1.948, trans_loss=4.876, nll_loss=2.108, w2v_ctc_loss=0.686, task_loss=4.11, task_loss_gen=6.525, contrastive_loss=0.046, total=4091.88, n_correct=2739.33, ppl=4.31, accuracy=66.946, wps=13130.1, ups=1.6, wpb=8183.8, bsz=300, num_updates=30400, lr=8.11107e-05, gnorm=0.704, clip=0, loss_scale=16, train_wall=62, gb_free=16.7, wall=24102
2023-09-05 08:24:12 | INFO | train_inner | epoch 021:   1035 / 1474 loss=1.954, trans_loss=4.888, nll_loss=2.123, w2v_ctc_loss=0.69, task_loss=3.963, task_loss_gen=6.614, contrastive_loss=0.043, total=4107.66, n_correct=2737.79, ppl=4.36, accuracy=66.651, wps=13236.6, ups=1.61, wpb=8215.3, bsz=299.5, num_updates=30500, lr=8.09776e-05, gnorm=0.737, clip=0, loss_scale=16, train_wall=61, gb_free=14.2, wall=24164
2023-09-05 08:25:15 | INFO | train_inner | epoch 021:   1135 / 1474 loss=1.951, trans_loss=4.877, nll_loss=2.109, w2v_ctc_loss=0.689, task_loss=4.101, task_loss_gen=7.159, contrastive_loss=0.048, total=4118.94, n_correct=2755.4, ppl=4.31, accuracy=66.896, wps=13113.5, ups=1.59, wpb=8237.9, bsz=294.9, num_updates=30600, lr=8.08452e-05, gnorm=0.728, clip=0, loss_scale=16, train_wall=62, gb_free=15.5, wall=24227
2023-09-05 08:26:17 | INFO | train_inner | epoch 021:   1235 / 1474 loss=1.955, trans_loss=4.881, nll_loss=2.115, w2v_ctc_loss=0.686, task_loss=3.818, task_loss_gen=6.336, contrastive_loss=0.098, total=4151.84, n_correct=2774.6, ppl=4.33, accuracy=66.828, wps=13308.1, ups=1.6, wpb=8303.7, bsz=309.1, num_updates=30700, lr=8.07134e-05, gnorm=0.707, clip=0, loss_scale=16, train_wall=62, gb_free=14.7, wall=24289
2023-09-05 08:27:20 | INFO | train_inner | epoch 021:   1335 / 1474 loss=1.949, trans_loss=4.879, nll_loss=2.113, w2v_ctc_loss=0.684, task_loss=3.902, task_loss_gen=6.25, contrastive_loss=0.057, total=4145.91, n_correct=2773.29, ppl=4.33, accuracy=66.892, wps=13183.1, ups=1.59, wpb=8291.8, bsz=312.1, num_updates=30800, lr=8.05823e-05, gnorm=0.635, clip=0, loss_scale=16, train_wall=62, gb_free=15.9, wall=24352
2023-09-05 08:28:24 | INFO | train_inner | epoch 021:   1435 / 1474 loss=1.968, trans_loss=4.889, nll_loss=2.124, w2v_ctc_loss=0.704, task_loss=4.361, task_loss_gen=6.916, contrastive_loss=0.106, total=4136.27, n_correct=2753.99, ppl=4.36, accuracy=66.581, wps=13034.3, ups=1.58, wpb=8272.5, bsz=304.5, num_updates=30900, lr=8.04518e-05, gnorm=0.837, clip=0, loss_scale=16, train_wall=63, gb_free=16.2, wall=24415
2023-09-05 08:28:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:1')
2023-09-05 08:29:23 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.947 | trans_loss 5.182 | nll_loss 2.447 | w2v_ctc_loss 1.325 | task_loss 38.697 | task_loss_gen 20.75 | contrastive_loss 0.24 | total 4003.4 | n_correct 2650 | ppl 5.45 | accuracy 66.194 | uer 18.018 | wer 19.813 | raw_wer 19.813 | bleu 22.02 | wps 1510.8 | wpb 4003.4 | bsz 141.8 | num_updates 30939 | best_bleu 22.26
2023-09-05 08:29:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 30939 updates
2023-09-05 08:29:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.0202.pt
2023-09-05 08:29:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.0202.pt
2023-09-05 08:29:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.0202.pt (epoch 21 @ 30939 updates, score 22.02) (writing took 7.168018449970987 seconds)
2023-09-05 08:29:30 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-09-05 08:29:30 | INFO | train | epoch 021 | loss 1.952 | trans_loss 4.878 | nll_loss 2.11 | w2v_ctc_loss 0.685 | task_loss 4.07 | task_loss_gen 6.593 | contrastive_loss 0.084 | total 4137.49 | n_correct 2766.77 | ppl 4.32 | accuracy 66.871 | wps 11933.1 | ups 1.44 | wpb 8275 | bsz 305.3 | num_updates 30939 | lr 8.04011e-05 | gnorm 0.715 | clip 0 | loss_scale 16 | train_wall 915 | gb_free 15.1 | wall 24482
2023-09-05 08:29:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 08:29:31 | INFO | fairseq.trainer | begin training epoch 22
2023-09-05 08:29:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 08:30:17 | INFO | train_inner | epoch 022:     61 / 1474 loss=1.941, trans_loss=4.864, nll_loss=2.092, w2v_ctc_loss=0.682, task_loss=3.962, task_loss_gen=6.572, contrastive_loss=0.033, total=4133.81, n_correct=2779.41, ppl=4.26, accuracy=67.236, wps=7282.6, ups=0.88, wpb=8267.6, bsz=300.5, num_updates=31000, lr=8.03219e-05, gnorm=0.753, clip=0, loss_scale=16, train_wall=62, gb_free=16, wall=24529
2023-09-05 08:31:20 | INFO | train_inner | epoch 022:    161 / 1474 loss=1.951, trans_loss=4.866, nll_loss=2.095, w2v_ctc_loss=0.684, task_loss=3.88, task_loss_gen=6.627, contrastive_loss=0.108, total=4116.11, n_correct=2758.32, ppl=4.27, accuracy=67.013, wps=13066.8, ups=1.59, wpb=8232.2, bsz=306.9, num_updates=31100, lr=8.01927e-05, gnorm=0.734, clip=0, loss_scale=16, train_wall=62, gb_free=16.4, wall=24592
2023-09-05 08:32:23 | INFO | train_inner | epoch 022:    261 / 1474 loss=1.931, trans_loss=4.857, nll_loss=2.083, w2v_ctc_loss=0.668, task_loss=3.372, task_loss_gen=5.756, contrastive_loss=0.049, total=4272.11, n_correct=2881.13, ppl=4.24, accuracy=67.44, wps=13561.6, ups=1.59, wpb=8544.2, bsz=331.4, num_updates=31200, lr=8.00641e-05, gnorm=0.672, clip=0, loss_scale=16, train_wall=62, gb_free=17.6, wall=24655
2023-09-05 08:33:27 | INFO | train_inner | epoch 022:    361 / 1474 loss=1.965, trans_loss=4.871, nll_loss=2.101, w2v_ctc_loss=0.682, task_loss=4.22, task_loss_gen=6.476, contrastive_loss=0.218, total=4178.4, n_correct=2796.09, ppl=4.29, accuracy=66.918, wps=13094.8, ups=1.57, wpb=8356.8, bsz=310, num_updates=31300, lr=7.99361e-05, gnorm=0.792, clip=0, loss_scale=16, train_wall=63, gb_free=14.8, wall=24719
2023-09-05 08:34:30 | INFO | train_inner | epoch 022:    461 / 1474 loss=1.951, trans_loss=4.869, nll_loss=2.097, w2v_ctc_loss=0.682, task_loss=4.157, task_loss_gen=6.724, contrastive_loss=0.092, total=4132.96, n_correct=2773.71, ppl=4.28, accuracy=67.112, wps=13115.8, ups=1.59, wpb=8265.9, bsz=297.5, num_updates=31400, lr=7.98087e-05, gnorm=0.694, clip=0, loss_scale=16, train_wall=62, gb_free=16.6, wall=24782
2023-09-05 08:35:33 | INFO | train_inner | epoch 022:    561 / 1474 loss=1.939, trans_loss=4.863, nll_loss=2.091, w2v_ctc_loss=0.677, task_loss=4.119, task_loss_gen=6.496, contrastive_loss=0.04, total=4158.17, n_correct=2790.84, ppl=4.26, accuracy=67.117, wps=13148.8, ups=1.58, wpb=8316.3, bsz=307.8, num_updates=31500, lr=7.96819e-05, gnorm=0.807, clip=0, loss_scale=16, train_wall=63, gb_free=16.2, wall=24845
2023-09-05 08:36:35 | INFO | train_inner | epoch 022:    661 / 1474 loss=1.939, trans_loss=4.857, nll_loss=2.083, w2v_ctc_loss=0.665, task_loss=3.853, task_loss_gen=5.881, contrastive_loss=0.125, total=4139.66, n_correct=2784.83, ppl=4.24, accuracy=67.272, wps=13319.5, ups=1.61, wpb=8279.3, bsz=311.1, num_updates=31600, lr=7.95557e-05, gnorm=0.689, clip=0, loss_scale=16, train_wall=61, gb_free=15.8, wall=24907
2023-09-05 08:37:39 | INFO | train_inner | epoch 022:    761 / 1474 loss=1.941, trans_loss=4.862, nll_loss=2.089, w2v_ctc_loss=0.683, task_loss=4.042, task_loss_gen=6.482, contrastive_loss=0.041, total=4167.89, n_correct=2798.53, ppl=4.26, accuracy=67.145, wps=13185.5, ups=1.58, wpb=8335.8, bsz=303.9, num_updates=31700, lr=7.94301e-05, gnorm=0.773, clip=0, loss_scale=16, train_wall=63, gb_free=12.4, wall=24970
2023-09-05 08:38:41 | INFO | train_inner | epoch 022:    861 / 1474 loss=1.947, trans_loss=4.873, nll_loss=2.104, w2v_ctc_loss=0.684, task_loss=4.243, task_loss_gen=7.027, contrastive_loss=0.032, total=4075.79, n_correct=2723.94, ppl=4.3, accuracy=66.832, wps=12997, ups=1.59, wpb=8151.6, bsz=289, num_updates=31800, lr=7.93052e-05, gnorm=0.777, clip=0, loss_scale=16, train_wall=62, gb_free=15.9, wall=25033
2023-09-05 08:39:44 | INFO | train_inner | epoch 022:    961 / 1474 loss=1.937, trans_loss=4.863, nll_loss=2.091, w2v_ctc_loss=0.673, task_loss=4.229, task_loss_gen=6.353, contrastive_loss=0.031, total=4134.72, n_correct=2775.61, ppl=4.26, accuracy=67.129, wps=13135.4, ups=1.59, wpb=8269.4, bsz=303.2, num_updates=31900, lr=7.91808e-05, gnorm=0.763, clip=0, loss_scale=16, train_wall=62, gb_free=13.7, wall=25096
2023-09-05 08:40:47 | INFO | train_inner | epoch 022:   1061 / 1474 loss=1.947, trans_loss=4.859, nll_loss=2.086, w2v_ctc_loss=0.667, task_loss=3.946, task_loss_gen=5.916, contrastive_loss=0.2, total=4160.57, n_correct=2798.55, ppl=4.25, accuracy=67.264, wps=13207.6, ups=1.59, wpb=8321.1, bsz=315.5, num_updates=32000, lr=7.90569e-05, gnorm=0.722, clip=0, loss_scale=16, train_wall=62, gb_free=16.9, wall=25159
2023-09-05 08:40:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 08:41:22 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.945 | trans_loss 5.176 | nll_loss 2.442 | w2v_ctc_loss 1.336 | task_loss 56.272 | task_loss_gen 23.507 | contrastive_loss 0.24 | total 4003.4 | n_correct 2662.3 | ppl 5.43 | accuracy 66.501 | uer 17.816 | wer 19.667 | raw_wer 19.667 | bleu 22.3 | wps 1491.7 | wpb 4003.4 | bsz 141.8 | num_updates 32000 | best_bleu 22.3
2023-09-05 08:41:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32000 updates
2023-09-05 08:41:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_22_32000.pt
2023-09-05 08:41:25 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_22_32000.pt
2023-09-05 08:41:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_22_32000.pt (epoch 22 @ 32000 updates, score 22.3) (writing took 13.146232522965875 seconds)
2023-09-05 08:42:38 | INFO | train_inner | epoch 022:   1161 / 1474 loss=1.956, trans_loss=4.882, nll_loss=2.116, w2v_ctc_loss=0.689, task_loss=3.773, task_loss_gen=6.719, contrastive_loss=0.081, total=4099.59, n_correct=2740.51, ppl=4.34, accuracy=66.848, wps=7384, ups=0.9, wpb=8199.2, bsz=296.2, num_updates=32100, lr=7.89337e-05, gnorm=0.786, clip=0, loss_scale=16, train_wall=62, gb_free=14.5, wall=25270
2023-09-05 08:43:42 | INFO | train_inner | epoch 022:   1261 / 1474 loss=1.948, trans_loss=4.879, nll_loss=2.112, w2v_ctc_loss=0.682, task_loss=3.433, task_loss_gen=5.832, contrastive_loss=0.071, total=4182.05, n_correct=2798.2, ppl=4.32, accuracy=66.91, wps=13233, ups=1.58, wpb=8364.1, bsz=323, num_updates=32200, lr=7.8811e-05, gnorm=0.707, clip=0, loss_scale=16, train_wall=62, gb_free=15.1, wall=25333
2023-09-05 08:44:44 | INFO | train_inner | epoch 022:   1361 / 1474 loss=1.942, trans_loss=4.862, nll_loss=2.09, w2v_ctc_loss=0.672, task_loss=3.641, task_loss_gen=6.234, contrastive_loss=0.099, total=4062.31, n_correct=2729.03, ppl=4.26, accuracy=67.179, wps=13092.3, ups=1.61, wpb=8124.6, bsz=299, num_updates=32300, lr=7.86889e-05, gnorm=0.599, clip=0, loss_scale=32, train_wall=61, gb_free=14.6, wall=25395
2023-09-05 08:45:47 | INFO | train_inner | epoch 022:   1461 / 1474 loss=1.951, trans_loss=4.879, nll_loss=2.112, w2v_ctc_loss=0.69, task_loss=3.605, task_loss_gen=7.281, contrastive_loss=0.044, total=4081.88, n_correct=2726.65, ppl=4.32, accuracy=66.799, wps=12978.2, ups=1.59, wpb=8163.8, bsz=288.9, num_updates=32400, lr=7.85674e-05, gnorm=0.598, clip=0, loss_scale=32, train_wall=62, gb_free=16.9, wall=25458
2023-09-05 08:45:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 08:46:29 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.938 | trans_loss 5.169 | nll_loss 2.43 | w2v_ctc_loss 1.328 | task_loss 22.208 | task_loss_gen 21.049 | contrastive_loss 0.243 | total 4003.4 | n_correct 2661.7 | ppl 5.39 | accuracy 66.486 | uer 17.915 | wer 19.731 | raw_wer 19.731 | bleu 21.88 | wps 1563.4 | wpb 4003.4 | bsz 141.8 | num_updates 32413 | best_bleu 22.3
2023-09-05 08:46:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32413 updates
2023-09-05 08:46:29 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.8806.pt
2023-09-05 08:46:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.8806.pt
2023-09-05 08:46:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.8806.pt (epoch 22 @ 32413 updates, score 21.88) (writing took 7.232558878022246 seconds)
2023-09-05 08:46:36 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-09-05 08:46:36 | INFO | train | epoch 022 | loss 1.946 | trans_loss 4.867 | nll_loss 2.096 | w2v_ctc_loss 0.679 | task_loss 3.891 | task_loss_gen 6.422 | contrastive_loss 0.086 | total 4138.65 | n_correct 2776.4 | ppl 4.28 | accuracy 67.085 | wps 11894.1 | ups 1.44 | wpb 8277.3 | bsz 305.7 | num_updates 32413 | lr 7.85517e-05 | gnorm 0.726 | clip 0 | loss_scale 32 | train_wall 917 | gb_free 11.2 | wall 25508
2023-09-05 08:46:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 08:46:36 | INFO | fairseq.trainer | begin training epoch 23
2023-09-05 08:46:36 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 08:47:39 | INFO | train_inner | epoch 023:     87 / 1474 loss=1.931, trans_loss=4.848, nll_loss=2.071, w2v_ctc_loss=0.678, task_loss=3.491, task_loss_gen=6.902, contrastive_loss=0.034, total=4096.09, n_correct=2764.15, ppl=4.2, accuracy=67.483, wps=7296.9, ups=0.89, wpb=8192.2, bsz=301.2, num_updates=32500, lr=7.84465e-05, gnorm=0.613, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=25570
2023-09-05 08:48:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 08:48:20 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-05 08:48:43 | INFO | train_inner | epoch 023:    189 / 1474 loss=1.931, trans_loss=4.845, nll_loss=2.067, w2v_ctc_loss=0.67, task_loss=3.881, task_loss_gen=7.062, contrastive_loss=0.037, total=4114.8, n_correct=2776.69, ppl=4.19, accuracy=67.481, wps=12875.4, ups=1.56, wpb=8229.6, bsz=295.2, num_updates=32600, lr=7.8326e-05, gnorm=0.836, clip=0, loss_scale=8, train_wall=63, gb_free=15.4, wall=25634
2023-09-05 08:49:46 | INFO | train_inner | epoch 023:    289 / 1474 loss=1.939, trans_loss=4.857, nll_loss=2.082, w2v_ctc_loss=0.665, task_loss=4.968, task_loss_gen=6.395, contrastive_loss=0.109, total=4142.43, n_correct=2789.05, ppl=4.23, accuracy=67.329, wps=13031.4, ups=1.57, wpb=8284.9, bsz=303.4, num_updates=32700, lr=7.82062e-05, gnorm=1.54, clip=0, loss_scale=8, train_wall=63, gb_free=16.3, wall=25698
2023-09-05 08:50:49 | INFO | train_inner | epoch 023:    389 / 1474 loss=1.931, trans_loss=4.847, nll_loss=2.069, w2v_ctc_loss=0.669, task_loss=3.954, task_loss_gen=6.23, contrastive_loss=0.032, total=4115.12, n_correct=2777.49, ppl=4.2, accuracy=67.495, wps=13176.2, ups=1.6, wpb=8230.2, bsz=294.4, num_updates=32800, lr=7.80869e-05, gnorm=0.907, clip=0, loss_scale=8, train_wall=62, gb_free=12.8, wall=25760
2023-09-05 08:51:52 | INFO | train_inner | epoch 023:    489 / 1474 loss=1.941, trans_loss=4.861, nll_loss=2.087, w2v_ctc_loss=0.67, task_loss=4.132, task_loss_gen=5.869, contrastive_loss=0.086, total=4156.86, n_correct=2793.98, ppl=4.25, accuracy=67.214, wps=13245.4, ups=1.59, wpb=8313.7, bsz=312.1, num_updates=32900, lr=7.79681e-05, gnorm=0.986, clip=0, loss_scale=8, train_wall=62, gb_free=14.7, wall=25823
2023-09-05 08:52:54 | INFO | train_inner | epoch 023:    589 / 1474 loss=1.93, trans_loss=4.85, nll_loss=2.074, w2v_ctc_loss=0.668, task_loss=4.543, task_loss_gen=5.532, contrastive_loss=0.034, total=4172.99, n_correct=2815.41, ppl=4.21, accuracy=67.467, wps=13416.4, ups=1.61, wpb=8346, bsz=316.1, num_updates=33000, lr=7.78499e-05, gnorm=1.224, clip=0, loss_scale=8, train_wall=61, gb_free=15.5, wall=25885
2023-09-05 08:53:57 | INFO | train_inner | epoch 023:    689 / 1474 loss=1.939, trans_loss=4.859, nll_loss=2.086, w2v_ctc_loss=0.669, task_loss=4.253, task_loss_gen=5.901, contrastive_loss=0.073, total=4135.85, n_correct=2784.1, ppl=4.24, accuracy=67.316, wps=13159.1, ups=1.59, wpb=8271.7, bsz=301.9, num_updates=33100, lr=7.77322e-05, gnorm=1.094, clip=0, loss_scale=8, train_wall=62, gb_free=13.7, wall=25948
2023-09-05 08:54:59 | INFO | train_inner | epoch 023:    789 / 1474 loss=1.94, trans_loss=4.86, nll_loss=2.087, w2v_ctc_loss=0.678, task_loss=4.356, task_loss_gen=5.89, contrastive_loss=0.051, total=4155.54, n_correct=2793.16, ppl=4.25, accuracy=67.215, wps=13228.6, ups=1.59, wpb=8311.1, bsz=306.6, num_updates=33200, lr=7.76151e-05, gnorm=0.942, clip=0, loss_scale=8, train_wall=62, gb_free=16.5, wall=26011
2023-09-05 08:56:02 | INFO | train_inner | epoch 023:    889 / 1474 loss=1.94, trans_loss=4.853, nll_loss=2.079, w2v_ctc_loss=0.668, task_loss=3.936, task_loss_gen=5.261, contrastive_loss=0.13, total=4180.56, n_correct=2817.78, ppl=4.22, accuracy=67.402, wps=13330.6, ups=1.59, wpb=8361.1, bsz=324.5, num_updates=33300, lr=7.74984e-05, gnorm=0.993, clip=0, loss_scale=8, train_wall=62, gb_free=16, wall=26074
2023-09-05 08:57:06 | INFO | train_inner | epoch 023:    989 / 1474 loss=1.957, trans_loss=4.857, nll_loss=2.083, w2v_ctc_loss=0.668, task_loss=4.617, task_loss_gen=5.486, contrastive_loss=0.293, total=4163.63, n_correct=2800.1, ppl=4.24, accuracy=67.251, wps=13109.9, ups=1.57, wpb=8327.3, bsz=309.2, num_updates=33400, lr=7.73823e-05, gnorm=1.056, clip=0, loss_scale=8, train_wall=63, gb_free=11.2, wall=26137
2023-09-05 08:58:09 | INFO | train_inner | epoch 023:   1089 / 1474 loss=1.946, trans_loss=4.867, nll_loss=2.095, w2v_ctc_loss=0.685, task_loss=4.405, task_loss_gen=6.128, contrastive_loss=0.04, total=4094.62, n_correct=2747.26, ppl=4.27, accuracy=67.094, wps=12992.8, ups=1.59, wpb=8189.2, bsz=291.3, num_updates=33500, lr=7.72667e-05, gnorm=1.057, clip=0, loss_scale=8, train_wall=62, gb_free=17, wall=26200
2023-09-05 08:59:12 | INFO | train_inner | epoch 023:   1189 / 1474 loss=1.939, trans_loss=4.865, nll_loss=2.094, w2v_ctc_loss=0.681, task_loss=4.517, task_loss_gen=5.76, contrastive_loss=0.033, total=4161.7, n_correct=2793.62, ppl=4.27, accuracy=67.127, wps=13133.5, ups=1.58, wpb=8323.4, bsz=309.2, num_updates=33600, lr=7.71517e-05, gnorm=1.168, clip=0, loss_scale=8, train_wall=63, gb_free=16.6, wall=26264
2023-09-05 09:00:14 | INFO | train_inner | epoch 023:   1289 / 1474 loss=1.933, trans_loss=4.859, nll_loss=2.085, w2v_ctc_loss=0.667, task_loss=4.632, task_loss_gen=5.533, contrastive_loss=0.044, total=4133.96, n_correct=2783.89, ppl=4.24, accuracy=67.342, wps=13283.5, ups=1.61, wpb=8267.9, bsz=309.3, num_updates=33700, lr=7.70371e-05, gnorm=1.237, clip=0, loss_scale=8, train_wall=62, gb_free=16.4, wall=26326
2023-09-05 09:01:17 | INFO | train_inner | epoch 023:   1389 / 1474 loss=1.953, trans_loss=4.873, nll_loss=2.104, w2v_ctc_loss=0.675, task_loss=4.495, task_loss_gen=5.616, contrastive_loss=0.12, total=4159.95, n_correct=2787.29, ppl=4.3, accuracy=67.003, wps=13200.7, ups=1.59, wpb=8319.9, bsz=308.8, num_updates=33800, lr=7.69231e-05, gnorm=0.912, clip=0, loss_scale=8, train_wall=62, gb_free=10.2, wall=26389
2023-09-05 09:02:11 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 09:02:45 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 3.94 | trans_loss 5.175 | nll_loss 2.436 | w2v_ctc_loss 1.319 | task_loss 62.348 | task_loss_gen 25.388 | contrastive_loss 0.242 | total 4003.4 | n_correct 2657.5 | ppl 5.41 | accuracy 66.381 | uer 17.511 | wer 19.242 | raw_wer 19.242 | bleu 21.99 | wps 1571.9 | wpb 4003.4 | bsz 141.8 | num_updates 33885 | best_bleu 22.3
2023-09-05 09:02:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33885 updates
2023-09-05 09:02:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.9904.pt
2023-09-05 09:02:48 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.9904.pt
2023-09-05 09:02:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_21.9904.pt (epoch 23 @ 33885 updates, score 21.99) (writing took 7.6880038809613325 seconds)
2023-09-05 09:02:53 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-09-05 09:02:53 | INFO | train | epoch 023 | loss 1.94 | trans_loss 4.858 | nll_loss 2.084 | w2v_ctc_loss 0.672 | task_loss 4.311 | task_loss_gen 5.933 | contrastive_loss 0.087 | total 4138.46 | n_correct 2784.51 | ppl 4.24 | accuracy 67.284 | wps 12475.1 | ups 1.51 | wpb 8276.9 | bsz 305.6 | num_updates 33885 | lr 7.68265e-05 | gnorm 1.039 | clip 0 | loss_scale 8 | train_wall 917 | gb_free 13.1 | wall 26485
2023-09-05 09:02:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 09:02:53 | INFO | fairseq.trainer | begin training epoch 24
2023-09-05 09:02:53 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 09:03:10 | INFO | train_inner | epoch 024:     15 / 1474 loss=1.958, trans_loss=4.868, nll_loss=2.097, w2v_ctc_loss=0.669, task_loss=4.271, task_loss_gen=5.518, contrastive_loss=0.225, total=4099.91, n_correct=2746.84, ppl=4.28, accuracy=66.998, wps=7252.9, ups=0.88, wpb=8199.8, bsz=308.7, num_updates=33900, lr=7.68095e-05, gnorm=0.97, clip=0, loss_scale=8, train_wall=62, gb_free=16.6, wall=26502
2023-09-05 09:04:13 | INFO | train_inner | epoch 024:    115 / 1474 loss=1.929, trans_loss=4.837, nll_loss=2.056, w2v_ctc_loss=0.657, task_loss=4.199, task_loss_gen=5.298, contrastive_loss=0.135, total=4147.74, n_correct=2809.37, ppl=4.16, accuracy=67.733, wps=13273.5, ups=1.6, wpb=8295.5, bsz=317.7, num_updates=34000, lr=7.66965e-05, gnorm=1.135, clip=0, loss_scale=8, train_wall=62, gb_free=15.3, wall=26565
2023-09-05 09:04:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 09:04:47 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.937 | trans_loss 5.177 | nll_loss 2.437 | w2v_ctc_loss 1.31 | task_loss 55.586 | task_loss_gen 23.841 | contrastive_loss 0.232 | total 4003.4 | n_correct 2655.9 | ppl 5.41 | accuracy 66.341 | uer 17.493 | wer 19.399 | raw_wer 19.399 | bleu 21.95 | wps 1591.6 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 22.3
2023-09-05 09:04:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-09-05 09:04:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_24_34000.pt
2023-09-05 09:04:50 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_24_34000.pt
2023-09-05 09:04:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 21.95) (writing took 9.324588711955585 seconds)
2023-09-05 09:06:00 | INFO | train_inner | epoch 024:    215 / 1474 loss=1.94, trans_loss=4.843, nll_loss=2.066, w2v_ctc_loss=0.651, task_loss=3.666, task_loss_gen=4.72, contrastive_loss=0.252, total=4244.96, n_correct=2868.3, ppl=4.19, accuracy=67.57, wps=7958.2, ups=0.94, wpb=8489.9, bsz=340, num_updates=34100, lr=7.6584e-05, gnorm=0.935, clip=0, loss_scale=8, train_wall=62, gb_free=15.2, wall=26671
2023-09-05 09:07:02 | INFO | train_inner | epoch 024:    315 / 1474 loss=1.926, trans_loss=4.845, nll_loss=2.067, w2v_ctc_loss=0.666, task_loss=3.973, task_loss_gen=5.43, contrastive_loss=0.031, total=4144.64, n_correct=2799.54, ppl=4.19, accuracy=67.546, wps=13192.5, ups=1.59, wpb=8289.3, bsz=309.2, num_updates=34200, lr=7.64719e-05, gnorm=0.908, clip=0, loss_scale=8, train_wall=62, gb_free=16.3, wall=26734
2023-09-05 09:08:06 | INFO | train_inner | epoch 024:    415 / 1474 loss=1.95, trans_loss=4.845, nll_loss=2.067, w2v_ctc_loss=0.673, task_loss=4.726, task_loss_gen=6.069, contrastive_loss=0.174, total=4152.59, n_correct=2799.04, ppl=4.19, accuracy=67.405, wps=13079.7, ups=1.57, wpb=8305.2, bsz=297.1, num_updates=34300, lr=7.63604e-05, gnorm=1.18, clip=0, loss_scale=8, train_wall=63, gb_free=10.4, wall=26798
2023-09-05 09:09:09 | INFO | train_inner | epoch 024:    515 / 1474 loss=1.937, trans_loss=4.847, nll_loss=2.07, w2v_ctc_loss=0.666, task_loss=4.613, task_loss_gen=5.651, contrastive_loss=0.103, total=4135.54, n_correct=2790.13, ppl=4.2, accuracy=67.467, wps=13173.1, ups=1.59, wpb=8271.1, bsz=300.8, num_updates=34400, lr=7.62493e-05, gnorm=1.03, clip=0, loss_scale=8, train_wall=62, gb_free=16.1, wall=26860
2023-09-05 09:10:12 | INFO | train_inner | epoch 024:    615 / 1474 loss=1.929, trans_loss=4.846, nll_loss=2.069, w2v_ctc_loss=0.66, task_loss=4.454, task_loss_gen=5.535, contrastive_loss=0.063, total=4163.63, n_correct=2810.74, ppl=4.2, accuracy=67.507, wps=13205.5, ups=1.59, wpb=8327.3, bsz=309.2, num_updates=34500, lr=7.61387e-05, gnorm=1.024, clip=1, loss_scale=8, train_wall=62, gb_free=16.3, wall=26924
2023-09-05 09:11:15 | INFO | train_inner | epoch 024:    715 / 1474 loss=1.938, trans_loss=4.857, nll_loss=2.082, w2v_ctc_loss=0.666, task_loss=4.361, task_loss_gen=5.601, contrastive_loss=0.079, total=4100.27, n_correct=2759.63, ppl=4.23, accuracy=67.304, wps=13079.7, ups=1.59, wpb=8200.5, bsz=295.4, num_updates=34600, lr=7.60286e-05, gnorm=0.966, clip=0, loss_scale=8, train_wall=62, gb_free=15.3, wall=26986
2023-09-05 09:12:18 | INFO | train_inner | epoch 024:    815 / 1474 loss=1.934, trans_loss=4.857, nll_loss=2.084, w2v_ctc_loss=0.669, task_loss=3.933, task_loss_gen=5.687, contrastive_loss=0.049, total=4129.03, n_correct=2781.92, ppl=4.24, accuracy=67.375, wps=13108.9, ups=1.59, wpb=8258.1, bsz=307.8, num_updates=34700, lr=7.5919e-05, gnorm=0.749, clip=0, loss_scale=16, train_wall=62, gb_free=17.2, wall=27049
2023-09-05 09:13:20 | INFO | train_inner | epoch 024:    915 / 1474 loss=1.937, trans_loss=4.853, nll_loss=2.076, w2v_ctc_loss=0.677, task_loss=4.499, task_loss_gen=6.256, contrastive_loss=0.027, total=4035.8, n_correct=2715.77, ppl=4.22, accuracy=67.292, wps=12962.5, ups=1.61, wpb=8071.6, bsz=278.8, num_updates=34800, lr=7.58098e-05, gnorm=0.805, clip=0, loss_scale=16, train_wall=62, gb_free=11.8, wall=27112
2023-09-05 09:14:23 | INFO | train_inner | epoch 024:   1015 / 1474 loss=1.927, trans_loss=4.849, nll_loss=2.072, w2v_ctc_loss=0.661, task_loss=3.944, task_loss_gen=5.906, contrastive_loss=0.031, total=4124.2, n_correct=2784.32, ppl=4.2, accuracy=67.512, wps=13090.6, ups=1.59, wpb=8248.4, bsz=295.7, num_updates=34900, lr=7.57011e-05, gnorm=0.693, clip=0, loss_scale=16, train_wall=62, gb_free=12.9, wall=27175
2023-09-05 09:15:26 | INFO | train_inner | epoch 024:   1115 / 1474 loss=1.927, trans_loss=4.836, nll_loss=2.056, w2v_ctc_loss=0.668, task_loss=3.684, task_loss_gen=5.385, contrastive_loss=0.071, total=4133.96, n_correct=2800.39, ppl=4.16, accuracy=67.741, wps=13108.4, ups=1.59, wpb=8267.9, bsz=310.6, num_updates=35000, lr=7.55929e-05, gnorm=0.768, clip=0, loss_scale=16, train_wall=63, gb_free=16, wall=27238
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:0')
2023-09-05 09:16:29 | INFO | train_inner | epoch 024:   1215 / 1474 loss=1.928, trans_loss=4.848, nll_loss=2.071, w2v_ctc_loss=0.662, task_loss=3.946, task_loss_gen=5.559, contrastive_loss=0.056, total=4152.6, n_correct=2805.37, ppl=4.2, accuracy=67.557, wps=13235.9, ups=1.59, wpb=8305.2, bsz=310.7, num_updates=35100, lr=7.54851e-05, gnorm=0.765, clip=0, loss_scale=16, train_wall=62, gb_free=17, wall=27300
2023-09-05 09:17:32 | INFO | train_inner | epoch 024:   1315 / 1474 loss=1.936, trans_loss=4.854, nll_loss=2.079, w2v_ctc_loss=0.678, task_loss=4.04, task_loss_gen=5.946, contrastive_loss=0.033, total=4108.12, n_correct=2768.44, ppl=4.23, accuracy=67.389, wps=12949.5, ups=1.58, wpb=8216.2, bsz=293.3, num_updates=35200, lr=7.53778e-05, gnorm=0.768, clip=0, loss_scale=16, train_wall=63, gb_free=12.5, wall=27364
2023-09-05 09:18:34 | INFO | train_inner | epoch 024:   1415 / 1474 loss=1.933, trans_loss=4.852, nll_loss=2.077, w2v_ctc_loss=0.675, task_loss=3.917, task_loss_gen=5.839, contrastive_loss=0.033, total=4099.36, n_correct=2766.24, ppl=4.22, accuracy=67.48, wps=13143.1, ups=1.6, wpb=8198.7, bsz=294.7, num_updates=35300, lr=7.5271e-05, gnorm=0.716, clip=0, loss_scale=16, train_wall=62, gb_free=14.8, wall=27426
2023-09-05 09:19:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2109, device='cuda:2')
2023-09-05 09:19:45 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.928 | trans_loss 5.171 | nll_loss 2.432 | w2v_ctc_loss 1.296 | task_loss 27.48 | task_loss_gen 20.587 | contrastive_loss 0.229 | total 4003.4 | n_correct 2664 | ppl 5.4 | accuracy 66.543 | uer 17.514 | wer 19.436 | raw_wer 19.436 | bleu 22.54 | wps 1643.2 | wpb 4003.4 | bsz 141.8 | num_updates 35359 | best_bleu 22.54
2023-09-05 09:19:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35359 updates
2023-09-05 09:19:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 09:19:52 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 09:19:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 24 @ 35359 updates, score 22.54) (writing took 12.580412634008098 seconds)
2023-09-05 09:19:58 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-09-05 09:19:58 | INFO | train | epoch 024 | loss 1.933 | trans_loss 4.848 | nll_loss 2.071 | w2v_ctc_loss 0.666 | task_loss 4.101 | task_loss_gen 5.597 | contrastive_loss 0.086 | total 4138.65 | n_correct 2793.58 | ppl 4.2 | accuracy 67.5 | wps 11901.9 | ups 1.44 | wpb 8277.3 | bsz 305.7 | num_updates 35359 | lr 7.52082e-05 | gnorm 0.886 | clip 0.1 | loss_scale 16 | train_wall 918 | gb_free 15.8 | wall 27510
2023-09-05 09:19:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 09:19:58 | INFO | fairseq.trainer | begin training epoch 25
2023-09-05 09:19:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 09:20:31 | INFO | train_inner | epoch 025:     41 / 1474 loss=1.92, trans_loss=4.837, nll_loss=2.058, w2v_ctc_loss=0.659, task_loss=3.671, task_loss_gen=5.65, contrastive_loss=0.038, total=4161.08, n_correct=2820.67, ppl=4.16, accuracy=67.787, wps=7138.8, ups=0.86, wpb=8322.2, bsz=309.6, num_updates=35400, lr=7.51646e-05, gnorm=0.754, clip=0, loss_scale=16, train_wall=62, gb_free=16.3, wall=27543
2023-09-05 09:20:49 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-05 09:21:34 | INFO | train_inner | epoch 025:    142 / 1474 loss=1.911, trans_loss=4.826, nll_loss=2.042, w2v_ctc_loss=0.648, task_loss=3.714, task_loss_gen=5.414, contrastive_loss=0.038, total=4138.76, n_correct=2818.01, ppl=4.12, accuracy=68.088, wps=13106.1, ups=1.58, wpb=8277.5, bsz=310.5, num_updates=35500, lr=7.50587e-05, gnorm=0.933, clip=0, loss_scale=8, train_wall=62, gb_free=16.8, wall=27606
2023-09-05 09:22:37 | INFO | train_inner | epoch 025:    242 / 1474 loss=1.921, trans_loss=4.832, nll_loss=2.05, w2v_ctc_loss=0.66, task_loss=4.158, task_loss_gen=5.369, contrastive_loss=0.041, total=4122.7, n_correct=2795.82, ppl=4.14, accuracy=67.815, wps=13116, ups=1.59, wpb=8245.4, bsz=302.9, num_updates=35600, lr=7.49532e-05, gnorm=0.995, clip=0, loss_scale=8, train_wall=62, gb_free=16.4, wall=27669
2023-09-05 09:23:40 | INFO | train_inner | epoch 025:    342 / 1474 loss=1.925, trans_loss=4.834, nll_loss=2.051, w2v_ctc_loss=0.656, task_loss=4.33, task_loss_gen=5.866, contrastive_loss=0.075, total=4132.6, n_correct=2799.51, ppl=4.14, accuracy=67.742, wps=13125, ups=1.59, wpb=8265.2, bsz=294.3, num_updates=35700, lr=7.48481e-05, gnorm=1.067, clip=0, loss_scale=8, train_wall=62, gb_free=16.6, wall=27732
2023-09-05 09:24:44 | INFO | train_inner | epoch 025:    442 / 1474 loss=1.944, trans_loss=4.837, nll_loss=2.057, w2v_ctc_loss=0.675, task_loss=4.212, task_loss_gen=5.634, contrastive_loss=0.153, total=4169.31, n_correct=2820.57, ppl=4.16, accuracy=67.651, wps=13096.5, ups=1.57, wpb=8338.6, bsz=296.4, num_updates=35800, lr=7.47435e-05, gnorm=0.883, clip=0, loss_scale=8, train_wall=63, gb_free=14.9, wall=27795
2023-09-05 09:25:46 | INFO | train_inner | epoch 025:    542 / 1474 loss=1.924, trans_loss=4.844, nll_loss=2.065, w2v_ctc_loss=0.658, task_loss=4.206, task_loss_gen=5.094, contrastive_loss=0.043, total=4157.91, n_correct=2813.08, ppl=4.18, accuracy=67.656, wps=13311.2, ups=1.6, wpb=8315.8, bsz=314.5, num_updates=35900, lr=7.46393e-05, gnorm=1.1, clip=0, loss_scale=8, train_wall=62, gb_free=15.1, wall=27858
2023-09-05 09:26:49 | INFO | train_inner | epoch 025:    642 / 1474 loss=1.928, trans_loss=4.832, nll_loss=2.051, w2v_ctc_loss=0.663, task_loss=4.233, task_loss_gen=5.196, contrastive_loss=0.107, total=4158.52, n_correct=2816.59, ppl=4.14, accuracy=67.731, wps=13157.9, ups=1.58, wpb=8317, bsz=309.9, num_updates=36000, lr=7.45356e-05, gnorm=1.135, clip=0, loss_scale=8, train_wall=63, gb_free=16, wall=27921
2023-09-05 09:26:49 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 09:27:24 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.944 | trans_loss 5.178 | nll_loss 2.439 | w2v_ctc_loss 1.332 | task_loss 38.513 | task_loss_gen 20.422 | contrastive_loss 0.229 | total 4003.4 | n_correct 2665.5 | ppl 5.42 | accuracy 66.581 | uer 17.246 | wer 19.149 | raw_wer 19.149 | bleu 22.19 | wps 1548.7 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 22.54
2023-09-05 09:27:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-09-05 09:27:24 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_25_36000.pt
2023-09-05 09:27:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_25_36000.pt
2023-09-05 09:27:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 22.19) (writing took 8.65020337200258 seconds)
--Backword ST Loss tensor(1192.0677, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(678.0402, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 09:28:35 | INFO | train_inner | epoch 025:    742 / 1474 loss=1.929, trans_loss=4.833, nll_loss=2.053, w2v_ctc_loss=0.661, task_loss=4.455, task_loss_gen=5.516, contrastive_loss=0.101, total=4138.4, n_correct=2804.03, ppl=4.15, accuracy=67.756, wps=7807.3, ups=0.94, wpb=8276.8, bsz=304.5, num_updates=36100, lr=7.44323e-05, gnorm=1.089, clip=0, loss_scale=8, train_wall=62, gb_free=16, wall=28027
2023-09-05 09:29:38 | INFO | train_inner | epoch 025:    842 / 1474 loss=1.92, trans_loss=4.838, nll_loss=2.058, w2v_ctc_loss=0.658, task_loss=3.753, task_loss_gen=4.845, contrastive_loss=0.048, total=4171.94, n_correct=2827.48, ppl=4.16, accuracy=67.774, wps=13300.7, ups=1.59, wpb=8343.9, bsz=324, num_updates=36200, lr=7.43294e-05, gnorm=1.047, clip=0, loss_scale=8, train_wall=62, gb_free=15.2, wall=28090
2023-09-05 09:30:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-05 09:30:10 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-09-05 09:30:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-09-05 09:30:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-09-05 09:30:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-09-05 09:30:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-09-05 09:30:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2023-09-05 09:30:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2023-09-05 09:30:26 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2023-09-05 09:30:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125
2023-09-05 09:30:29 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00390625
2023-09-05 09:30:30 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.001953125
2023-09-05 09:30:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0009765625
2023-09-05 09:30:31 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00048828125
2023-09-05 09:30:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.000244140625
2023-09-05 09:30:32 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0001220703125
2023-09-05 09:30:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/crash.pt
--Backword ST Loss tensor(1402.9390, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(731.4300, device='cuda:5', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1280.9152, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(739.4979, device='cuda:3', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1006.6606, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(541.0554, device='cuda:7', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1542.0736, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(883.2940, device='cuda:4', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1924.3320, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(1129.6287, device='cuda:1', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1675.3921, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(978.6556, device='cuda:2', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1368.8972, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(777.6526, device='cuda:6', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
2023-09-05 09:30:36 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/crash.pt
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 5 terminated with the following error:
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 936, in train_step
    grad_norm = self.clip_grad_norm(self.cfg.optimization.clip_norm)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 1270, in clip_grad_norm
    return self.optimizer.clip_grad_norm(
  File "/mnt/zhangyh/fairseq-AT/fairseq/optim/fp16_optimizer.py", line 201, in clip_grad_norm
    self.scaler.check_overflow(grad_norm)
  File "/mnt/zhangyh/fairseq-AT/fairseq/optim/dynamic_loss_scaler.py", line 61, in check_overflow
    raise FloatingPointError(
FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 983, in train_step
    self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 680, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 508, in _per_task_train_loss
    loss_at, loss_at_gen, loss_st, loss_mt, sample_size, logging_output = criterion(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge.py", line 219, in forward
    encoder_out_st = model.acoustic_encoder(src_tokens_st, src_lengths_st, mixup_rate=self.mixup_rate if ((per_task == 'at' or multi_st) and st_update) else 0.0, mixup_for_whole_model=self.mixup_for_whole_model, textual_encoder=model.textual_encoder, update_num=update_num)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1564, in _call_impl
    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
StopIteration

/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1072 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-09-05 09:33:41 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:16380
2023-09-05 09:33:41 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:16380
2023-09-05 09:33:41 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:16380
2023-09-05 09:33:41 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:16380
2023-09-05 09:33:42 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:16380
2023-09-05 09:33:42 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:16380
2023-09-05 09:33:42 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:16380
2023-09-05 09:33:42 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:16380
2023-09-05 09:33:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-05 09:33:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-05 09:33:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-05 09:33:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 09:33:43 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 09:33:43 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-05 09:33:43 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 09:33:43 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 09:33:43 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 09:33:43 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 09:33:43 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-05 09:33:43 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 09:33:43 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-05 09:33:46 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:16380', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-05 09:33:46 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-05 09:33:46 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-05 09:33:46 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-05 09:33:46 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-05 09:33:46 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-05 09:33:50 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-05 09:33:50 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-05 09:33:50 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-05 09:33:52 | INFO | root | load pretrained hubert
2023-09-05 09:33:59 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-05 09:34:01 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-05 09:34:05 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-05 09:34:06 | INFO | root | share the sematic adapter and textual encoder
2023-09-05 09:34:06 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-05 09:34:06 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-05 09:34:06 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-05 09:34:06 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-05 09:34:06 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-05 09:34:06 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-05 09:34:06 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 09:34:06 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 09:34:06 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 09:34:06 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 09:34:22 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-05 09:34:26 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-05 09:34:26 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-05 09:34:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 09:34:26 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 09:34:26 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 09:34:26 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 09:34:26 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 09:34:26 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 09:34:26 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 09:34:26 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 09:34:26 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 09:34:26 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 09:34:26 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-05 09:34:26 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-05 09:34:26 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt
2023-09-05 09:34:28 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
2023-09-05 09:34:47 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-05 09:34:48 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt (epoch 25 @ 36000 updates)
2023-09-05 09:34:48 | INFO | fairseq.trainer | loading train data for epoch 25
2023-09-05 09:34:48 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 09:34:48 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 09:34:48 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 09:34:50 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 09:34:52 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 09:35:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 09:35:32 | INFO | fairseq.trainer | begin training epoch 25
2023-09-05 09:35:32 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
--Backword ST Loss tensor(1212.0249, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(678.0402, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 09:36:47 | INFO | train_inner | epoch 025:    742 / 1474 loss=1.93, trans_loss=4.834, nll_loss=2.054, w2v_ctc_loss=0.66, task_loss=4.492, task_loss_gen=5.547, contrastive_loss=0.102, total=4138.4, n_correct=2803.2, ppl=4.15, accuracy=67.736, wps=5378.7, ups=0.65, wpb=8276.8, bsz=304.5, num_updates=36100, lr=7.44323e-05, gnorm=1.119, clip=0, loss_scale=8, train_wall=68, gb_free=16, wall=0
2023-09-05 09:37:51 | INFO | train_inner | epoch 025:    842 / 1474 loss=1.92, trans_loss=4.837, nll_loss=2.057, w2v_ctc_loss=0.657, task_loss=3.677, task_loss_gen=4.875, contrastive_loss=0.049, total=4171.94, n_correct=2826.46, ppl=4.16, accuracy=67.749, wps=13093, ups=1.57, wpb=8343.9, bsz=324, num_updates=36200, lr=7.43294e-05, gnorm=1.009, clip=0, loss_scale=8, train_wall=63, gb_free=15.2, wall=0
2023-09-05 09:38:57 | INFO | train_inner | epoch 025:    942 / 1474 loss=1.931, trans_loss=4.842, nll_loss=2.064, w2v_ctc_loss=0.664, task_loss=4.271, task_loss_gen=5.478, contrastive_loss=0.108, total=4148.78, n_correct=2805.39, ppl=4.18, accuracy=67.62, wps=12636.3, ups=1.52, wpb=8297.6, bsz=314.3, num_updates=36300, lr=7.4227e-05, gnorm=1.268, clip=0, loss_scale=8, train_wall=65, gb_free=11.9, wall=0
2023-09-05 09:40:01 | INFO | train_inner | epoch 025:   1042 / 1474 loss=1.941, trans_loss=4.847, nll_loss=2.07, w2v_ctc_loss=0.655, task_loss=4.03, task_loss_gen=5.109, contrastive_loss=0.218, total=4176.78, n_correct=2819.13, ppl=4.2, accuracy=67.495, wps=13026.5, ups=1.56, wpb=8353.6, bsz=310.2, num_updates=36400, lr=7.41249e-05, gnorm=0.966, clip=1, loss_scale=8, train_wall=64, gb_free=16.3, wall=0
2023-09-05 09:40:51 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-05 09:41:05 | INFO | train_inner | epoch 025:   1143 / 1474 loss=1.923, trans_loss=4.841, nll_loss=2.062, w2v_ctc_loss=0.657, task_loss=4.474, task_loss_gen=5.78, contrastive_loss=0.029, total=4043.22, n_correct=2735.28, ppl=4.18, accuracy=67.651, wps=12588, ups=1.56, wpb=8086.4, bsz=286.5, num_updates=36500, lr=7.40233e-05, gnorm=1.363, clip=1, loss_scale=4, train_wall=64, gb_free=17.3, wall=0
2023-09-05 09:41:44 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-09-05 09:41:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-09-05 09:41:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.5
2023-09-05 09:42:09 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.25
2023-09-05 09:42:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.125
2023-09-05 09:42:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0625
2023-09-05 09:42:12 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.03125
2023-09-05 09:42:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.015625
2023-09-05 09:42:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0078125
2023-09-05 09:42:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00390625
2023-09-05 09:42:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.001953125
2023-09-05 09:42:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0009765625
2023-09-05 09:42:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.00048828125
2023-09-05 09:42:16 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.000244140625
2023-09-05 09:42:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 0.0001220703125
2023-09-05 09:42:17 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/crash.pt
--Backword ST Loss tensor(978.1849, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(541.0554, device='cuda:7', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1884.3074, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(1129.6287, device='cuda:1', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1647.1366, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(978.6556, device='cuda:2', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1332.5327, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(733.9202, device='cuda:3', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1325.2070, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(731.4300, device='cuda:5', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1380.2736, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(777.6526, device='cuda:6', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
--Backword ST Loss tensor(1668.0999, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(883.2940, device='cuda:4', grad_fn=<MulBackward0>)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1344: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when the forward contains multiple autograd Nodes "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1309: UserWarning: Using non-full backward hooks on a Module that does not return a single Tensor or a tuple of Tensors is deprecated and will be removed in future versions. This hook will be missing some of the grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using non-full backward hooks on a Module that does not return a "
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py:1334: UserWarning: Using a non-full backward hook when outputs are generated by different autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_output. Please use register_full_backward_hook to get the documented behavior.
  warnings.warn("Using a non-full backward hook when outputs are generated by different autograd Nodes "
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 160, in join
    raise ProcessRaisedException(msg, error_index, failed_process.pid)
torch.multiprocessing.spawn.ProcessRaisedException: 

-- Process 7 terminated with the following error:
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 936, in train_step
    grad_norm = self.clip_grad_norm(self.cfg.optimization.clip_norm)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 1270, in clip_grad_norm
    return self.optimizer.clip_grad_norm(
  File "/mnt/zhangyh/fairseq-AT/fairseq/optim/fp16_optimizer.py", line 201, in clip_grad_norm
    self.scaler.check_overflow(grad_norm)
  File "/mnt/zhangyh/fairseq-AT/fairseq/optim/dynamic_loss_scaler.py", line 61, in check_overflow
    raise FloatingPointError(
FloatingPointError: Minimum loss scale reached (0.0001). Your loss is probably exploding. Try lowering the learning rate, using gradient clipping or increasing the batch size.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 328, in distributed_main
    main(cfg, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 190, in main
    valid_losses, should_stop = train(cfg, trainer, task, epoch_itr)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 316, in train
    log_output = trainer.train_step(samples)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/mnt/zhangyh/fairseq-AT/fairseq/trainer.py", line 983, in train_step
    self.task.train_step(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 680, in train_step
    loss, sample_size, logging_output, norm_list = self._per_task_train_loss(
  File "/mnt/zhangyh/fairseq-AT/fairseq/tasks/joint_triple_pretraining_merge.py", line 508, in _per_task_train_loss
    loss_at, loss_at_gen, loss_st, loss_mt, sample_size, logging_output = criterion(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/zhangyh/fairseq-AT/fairseq/criterions/label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge.py", line 219, in forward
    encoder_out_st = model.acoustic_encoder(src_tokens_st, src_lengths_st, mixup_rate=self.mixup_rate if ((per_task == 'at' or multi_st) and st_update) else 0.0, mixup_for_whole_model=self.mixup_for_whole_model, textual_encoder=model.textual_encoder, update_num=update_num)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1564, in _call_impl
    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))
StopIteration

/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-09-05 21:31:24 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:17569
2023-09-05 21:31:24 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:17569
2023-09-05 21:31:24 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:17569
2023-09-05 21:31:24 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:17569
2023-09-05 21:31:24 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:17569
2023-09-05 21:31:24 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:17569
2023-09-05 21:31:24 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:17569
2023-09-05 21:31:24 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:17569
2023-09-05 21:31:24 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-05 21:31:24 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-05 21:31:24 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 21:31:25 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 21:31:25 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-05 21:31:25 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 21:31:25 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 21:31:25 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-05 21:31:25 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-05 21:31:25 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-05 21:31:25 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 21:31:25 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-05 21:31:28 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:17569', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=1.5, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_ende_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-05 21:31:29 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-05 21:31:29 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-05 21:31:29 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-05 21:31:29 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-05 21:31:29 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-05 21:31:33 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-05 21:31:33 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_ende_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-05 21:31:33 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-05 21:31:35 | INFO | root | load pretrained hubert
2023-09-05 21:31:42 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_ende_baseline
2023-09-05 21:31:46 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-05 21:31:53 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2de/ende-baseline/last8.ensemble.pt
2023-09-05 21:31:53 | INFO | root | share the sematic adapter and textual encoder
2023-09-05 21:31:53 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-05 21:31:53 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-05 21:31:53 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-05 21:31:53 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-05 21:31:53 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-05 21:31:53 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-05 21:31:53 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 21:31:53 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 21:31:53 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 21:31:53 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1418, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 21:32:08 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-05 21:32:08 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-05 21:32:08 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-05 21:32:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 21:32:09 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 21:32:09 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 21:32:09 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 21:32:09 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 21:32:09 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 21:32:09 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 21:32:09 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 21:32:09 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 21:32:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 21:32:09 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-05 21:32:09 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-05 21:32:09 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt
2023-09-05 21:32:11 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
2023-09-05 21:32:28 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-05 21:32:29 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt (epoch 22 @ 32000 updates)
2023-09-05 21:32:30 | INFO | fairseq.trainer | loading train data for epoch 22
2023-09-05 21:32:30 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 21:32:30 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 21:32:30 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_ende_lcrm/sentencepiece.bpe.model'}
2023-09-05 21:32:33 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 21:32:35 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=225277, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
2023-09-05 21:33:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
2023-09-05 21:33:16 | INFO | fairseq.trainer | begin training epoch 22
2023-09-05 21:33:16 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
2023-09-05 21:34:31 | INFO | train_inner | epoch 022:   1161 / 1474 loss=1.955, trans_loss=4.881, nll_loss=2.115, w2v_ctc_loss=0.688, task_loss=0, task_loss_gen=0, contrastive_loss=0.081, total=4099.59, n_correct=2738.33, ppl=4.33, accuracy=66.795, wps=5244.3, ups=0.64, wpb=8199.2, bsz=296.2, num_updates=32100, lr=7.89337e-05, gnorm=0.539, clip=0, loss_scale=16, train_wall=67, gb_free=14.4, wall=0
2023-09-05 21:35:34 | INFO | train_inner | epoch 022:   1261 / 1474 loss=1.945, trans_loss=4.876, nll_loss=2.108, w2v_ctc_loss=0.68, task_loss=0, task_loss_gen=0, contrastive_loss=0.068, total=4182.05, n_correct=2797.84, ppl=4.31, accuracy=66.901, wps=13129.2, ups=1.57, wpb=8364.1, bsz=323, num_updates=32200, lr=7.8811e-05, gnorm=0.525, clip=0, loss_scale=16, train_wall=63, gb_free=15.1, wall=0
2023-09-05 21:36:39 | INFO | train_inner | epoch 022:   1361 / 1474 loss=1.94, trans_loss=4.861, nll_loss=2.088, w2v_ctc_loss=0.671, task_loss=0, task_loss_gen=0, contrastive_loss=0.098, total=4062.31, n_correct=2732.28, ppl=4.25, accuracy=67.259, wps=12634.8, ups=1.56, wpb=8124.6, bsz=299, num_updates=32300, lr=7.86889e-05, gnorm=0.524, clip=0, loss_scale=16, train_wall=64, gb_free=14.6, wall=0
2023-09-05 21:37:42 | INFO | train_inner | epoch 022:   1461 / 1474 loss=1.948, trans_loss=4.875, nll_loss=2.107, w2v_ctc_loss=0.688, task_loss=0, task_loss_gen=0, contrastive_loss=0.043, total=4081.88, n_correct=2733.35, ppl=4.31, accuracy=66.963, wps=12852.5, ups=1.57, wpb=8163.8, bsz=288.9, num_updates=32400, lr=7.85674e-05, gnorm=0.523, clip=0, loss_scale=16, train_wall=63, gb_free=16.9, wall=0
2023-09-05 21:37:50 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
2023-09-05 21:38:23 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.92 | trans_loss 5.17 | nll_loss 2.43 | w2v_ctc_loss 1.273 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.236 | total 4003.4 | n_correct 2662 | ppl 5.39 | accuracy 66.493 | uer 17.827 | wer 19.858 | raw_wer 19.858 | bleu 22.13 | wps 1631.8 | wpb 4003.4 | bsz 141.8 | num_updates 32413 | best_bleu 22.3
2023-09-05 21:38:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 32413 updates
2023-09-05 21:38:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.1301.pt
2023-09-05 21:38:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.1301.pt
2023-09-05 21:38:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.1301.pt (epoch 22 @ 32413 updates, score 22.13) (writing took 9.211419537081383 seconds)
2023-09-05 21:38:32 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-09-05 21:38:32 | INFO | train | epoch 022 | loss 1.945 | trans_loss 4.866 | nll_loss 2.095 | w2v_ctc_loss 0.678 | task_loss 2.894 | task_loss_gen 4.617 | contrastive_loss 0.086 | total 4138.65 | n_correct 2776.96 | ppl 4.27 | accuracy 67.098 | wps 11349.6 | ups 1.37 | wpb 8277.3 | bsz 305.7 | num_updates 32413 | lr 7.85517e-05 | gnorm 0.686 | clip 0 | loss_scale 16 | train_wall 925 | gb_free 11.2 | wall 0
2023-09-05 21:38:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 21:38:32 | INFO | fairseq.trainer | begin training epoch 23
2023-09-05 21:38:32 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 21:39:35 | INFO | train_inner | epoch 023:     87 / 1474 loss=1.931, trans_loss=4.847, nll_loss=2.07, w2v_ctc_loss=0.679, task_loss=0, task_loss_gen=0, contrastive_loss=0.033, total=4096.09, n_correct=2765.87, ppl=4.2, accuracy=67.525, wps=7263.7, ups=0.89, wpb=8192.2, bsz=301.2, num_updates=32500, lr=7.84465e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=63, gb_free=15.9, wall=0
2023-09-05 21:40:37 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-05 21:40:39 | INFO | train_inner | epoch 023:    188 / 1474 loss=1.929, trans_loss=4.841, nll_loss=2.063, w2v_ctc_loss=0.672, task_loss=0, task_loss_gen=0, contrastive_loss=0.028, total=4103.52, n_correct=2775.05, ppl=4.18, accuracy=67.626, wps=12743.6, ups=1.55, wpb=8207, bsz=291.8, num_updates=32600, lr=7.8326e-05, gnorm=0.518, clip=0, loss_scale=8, train_wall=64, gb_free=16.6, wall=0
2023-09-05 21:41:44 | INFO | train_inner | epoch 023:    288 / 1474 loss=1.937, trans_loss=4.854, nll_loss=2.079, w2v_ctc_loss=0.665, task_loss=0, task_loss_gen=0, contrastive_loss=0.109, total=4148.03, n_correct=2793.78, ppl=4.23, accuracy=67.352, wps=12865.8, ups=1.55, wpb=8296.1, bsz=305.7, num_updates=32700, lr=7.82062e-05, gnorm=0.519, clip=0, loss_scale=8, train_wall=64, gb_free=17, wall=0
2023-09-05 21:42:49 | INFO | train_inner | epoch 023:    388 / 1474 loss=1.92, trans_loss=4.837, nll_loss=2.057, w2v_ctc_loss=0.659, task_loss=0, task_loss_gen=0, contrastive_loss=0.026, total=4115.99, n_correct=2787.93, ppl=4.16, accuracy=67.734, wps=12819.2, ups=1.56, wpb=8232, bsz=294.1, num_updates=32800, lr=7.80869e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=63, gb_free=15.6, wall=0
2023-09-05 21:43:52 | INFO | train_inner | epoch 023:    488 / 1474 loss=1.934, trans_loss=4.851, nll_loss=2.075, w2v_ctc_loss=0.673, task_loss=0, task_loss_gen=0, contrastive_loss=0.079, total=4156.5, n_correct=2803.56, ppl=4.21, accuracy=67.45, wps=13122.7, ups=1.58, wpb=8313, bsz=312.2, num_updates=32900, lr=7.79681e-05, gnorm=0.535, clip=0, loss_scale=8, train_wall=63, gb_free=17, wall=0
2023-09-05 21:44:55 | INFO | train_inner | epoch 023:    588 / 1474 loss=1.923, trans_loss=4.843, nll_loss=2.066, w2v_ctc_loss=0.666, task_loss=0, task_loss_gen=0, contrastive_loss=0.028, total=4174.84, n_correct=2821.17, ppl=4.19, accuracy=67.576, wps=13225.1, ups=1.58, wpb=8349.7, bsz=316.3, num_updates=33000, lr=7.78499e-05, gnorm=0.513, clip=0, loss_scale=8, train_wall=62, gb_free=16.9, wall=0
2023-09-05 21:45:59 | INFO | train_inner | epoch 023:    688 / 1474 loss=1.934, trans_loss=4.85, nll_loss=2.074, w2v_ctc_loss=0.674, task_loss=0, task_loss_gen=0, contrastive_loss=0.067, total=4139.68, n_correct=2793.06, ppl=4.21, accuracy=67.47, wps=13054, ups=1.58, wpb=8279.4, bsz=302.2, num_updates=33100, lr=7.77322e-05, gnorm=0.525, clip=0, loss_scale=8, train_wall=63, gb_free=16.3, wall=0
2023-09-05 21:47:03 | INFO | train_inner | epoch 023:    788 / 1474 loss=1.933, trans_loss=4.853, nll_loss=2.078, w2v_ctc_loss=0.675, task_loss=0, task_loss_gen=0, contrastive_loss=0.044, total=4147.97, n_correct=2795.95, ppl=4.22, accuracy=67.405, wps=12972.9, ups=1.56, wpb=8295.9, bsz=305.8, num_updates=33200, lr=7.76151e-05, gnorm=0.517, clip=0, loss_scale=8, train_wall=63, gb_free=16.6, wall=0
2023-09-05 21:48:07 | INFO | train_inner | epoch 023:    888 / 1474 loss=1.933, trans_loss=4.845, nll_loss=2.068, w2v_ctc_loss=0.669, task_loss=0, task_loss_gen=0, contrastive_loss=0.123, total=4182.69, n_correct=2825.87, ppl=4.19, accuracy=67.561, wps=13068.4, ups=1.56, wpb=8365.4, bsz=325, num_updates=33300, lr=7.74984e-05, gnorm=0.527, clip=0, loss_scale=8, train_wall=63, gb_free=16.4, wall=0
2023-09-05 21:49:12 | INFO | train_inner | epoch 023:    988 / 1474 loss=1.949, trans_loss=4.847, nll_loss=2.07, w2v_ctc_loss=0.665, task_loss=0, task_loss_gen=0, contrastive_loss=0.289, total=4165.01, n_correct=2809.76, ppl=4.2, accuracy=67.461, wps=12819.3, ups=1.54, wpb=8330, bsz=309.6, num_updates=33400, lr=7.73823e-05, gnorm=0.522, clip=0, loss_scale=8, train_wall=64, gb_free=17.4, wall=0
2023-09-05 21:50:16 | INFO | train_inner | epoch 023:   1088 / 1474 loss=1.937, trans_loss=4.856, nll_loss=2.081, w2v_ctc_loss=0.681, task_loss=0, task_loss_gen=0, contrastive_loss=0.035, total=4092.37, n_correct=2753.7, ppl=4.23, accuracy=67.289, wps=12824.2, ups=1.57, wpb=8184.7, bsz=290.8, num_updates=33500, lr=7.72667e-05, gnorm=0.523, clip=0, loss_scale=8, train_wall=63, gb_free=16.9, wall=0
2023-09-05 21:51:20 | INFO | train_inner | epoch 023:   1188 / 1474 loss=1.929, trans_loss=4.854, nll_loss=2.08, w2v_ctc_loss=0.672, task_loss=0, task_loss_gen=0, contrastive_loss=0.029, total=4164.9, n_correct=2802.3, ppl=4.23, accuracy=67.284, wps=12925.6, ups=1.55, wpb=8329.8, bsz=309.2, num_updates=33600, lr=7.71517e-05, gnorm=0.516, clip=0, loss_scale=8, train_wall=64, gb_free=13.3, wall=0
2023-09-05 21:52:23 | INFO | train_inner | epoch 023:   1288 / 1474 loss=1.923, trans_loss=4.847, nll_loss=2.07, w2v_ctc_loss=0.662, task_loss=0, task_loss_gen=0, contrastive_loss=0.037, total=4136.96, n_correct=2794.97, ppl=4.2, accuracy=67.561, wps=13145.1, ups=1.59, wpb=8273.9, bsz=309.8, num_updates=33700, lr=7.70371e-05, gnorm=0.512, clip=0, loss_scale=8, train_wall=62, gb_free=16.3, wall=0
2023-09-05 21:53:27 | INFO | train_inner | epoch 023:   1388 / 1474 loss=1.945, trans_loss=4.863, nll_loss=2.092, w2v_ctc_loss=0.679, task_loss=0, task_loss_gen=0, contrastive_loss=0.094, total=4142.84, n_correct=2781.35, ppl=4.26, accuracy=67.136, wps=12966, ups=1.56, wpb=8285.7, bsz=304.8, num_updates=33800, lr=7.69231e-05, gnorm=0.536, clip=0, loss_scale=8, train_wall=63, gb_free=15.6, wall=0
2023-09-05 21:54:24 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 21:54:58 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 3.938 | trans_loss 5.173 | nll_loss 2.435 | w2v_ctc_loss 1.329 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.232 | total 4003.4 | n_correct 2662.7 | ppl 5.41 | accuracy 66.511 | uer 17.538 | wer 19.429 | raw_wer 19.429 | bleu 22.38 | wps 1522.2 | wpb 4003.4 | bsz 141.8 | num_updates 33886 | best_bleu 22.38
2023-09-05 21:54:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 33886 updates
2023-09-05 21:54:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 21:55:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 21:55:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 23 @ 33886 updates, score 22.38) (writing took 11.799615663010627 seconds)
2023-09-05 21:55:10 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-09-05 21:55:10 | INFO | train | epoch 023 | loss 1.934 | trans_loss 4.85 | nll_loss 2.074 | w2v_ctc_loss 0.671 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.081 | total 4138.1 | n_correct 2790.61 | ppl 4.21 | accuracy 67.437 | wps 12221.8 | ups 1.48 | wpb 8276.2 | bsz 305.5 | num_updates 33886 | lr 7.68254e-05 | gnorm 0.523 | clip 0 | loss_scale 8 | train_wall 933 | gb_free 13.2 | wall 0
2023-09-05 21:55:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 21:55:10 | INFO | fairseq.trainer | begin training epoch 24
2023-09-05 21:55:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 21:55:26 | INFO | train_inner | epoch 024:     14 / 1474 loss=1.95, trans_loss=4.861, nll_loss=2.09, w2v_ctc_loss=0.671, task_loss=0, task_loss_gen=0, contrastive_loss=0.173, total=4084.21, n_correct=2742.62, ppl=4.26, accuracy=67.152, wps=6891.6, ups=0.84, wpb=8168.4, bsz=303.7, num_updates=33900, lr=7.68095e-05, gnorm=0.526, clip=0, loss_scale=8, train_wall=65, gb_free=15, wall=0
2023-09-05 21:56:29 | INFO | train_inner | epoch 024:    114 / 1474 loss=1.931, trans_loss=4.831, nll_loss=2.049, w2v_ctc_loss=0.656, task_loss=0, task_loss_gen=0, contrastive_loss=0.198, total=4168.61, n_correct=2826.37, ppl=4.14, accuracy=67.801, wps=13116.3, ups=1.57, wpb=8337.2, bsz=324.6, num_updates=34000, lr=7.66965e-05, gnorm=0.53, clip=0, loss_scale=8, train_wall=63, gb_free=10.9, wall=0
2023-09-05 21:56:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 21:57:04 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.929 | trans_loss 5.172 | nll_loss 2.431 | w2v_ctc_loss 1.298 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.237 | total 4003.4 | n_correct 2661.3 | ppl 5.39 | accuracy 66.476 | uer 17.41 | wer 19.365 | raw_wer 19.365 | bleu 22.17 | wps 1504.8 | wpb 4003.4 | bsz 141.8 | num_updates 34000 | best_bleu 22.38
2023-09-05 21:57:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 34000 updates
2023-09-05 21:57:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_24_34000.pt
2023-09-05 21:57:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_24_34000.pt
2023-09-05 21:57:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_24_34000.pt (epoch 24 @ 34000 updates, score 22.17) (writing took 7.699610989075154 seconds)
2023-09-05 21:58:15 | INFO | train_inner | epoch 024:    214 / 1474 loss=1.931, trans_loss=4.833, nll_loss=2.054, w2v_ctc_loss=0.647, task_loss=0, task_loss_gen=0, contrastive_loss=0.241, total=4252.53, n_correct=2885.87, ppl=4.15, accuracy=67.862, wps=8055.5, ups=0.95, wpb=8505.1, bsz=341.3, num_updates=34100, lr=7.6584e-05, gnorm=0.507, clip=0, loss_scale=8, train_wall=63, gb_free=16.4, wall=0
2023-09-05 21:59:18 | INFO | train_inner | epoch 024:    314 / 1474 loss=1.915, trans_loss=4.831, nll_loss=2.05, w2v_ctc_loss=0.659, task_loss=0, task_loss_gen=0, contrastive_loss=0.026, total=4138.44, n_correct=2808.85, ppl=4.14, accuracy=67.872, wps=13053, ups=1.58, wpb=8276.9, bsz=307.1, num_updates=34200, lr=7.64719e-05, gnorm=0.511, clip=0, loss_scale=8, train_wall=63, gb_free=15.2, wall=0
2023-09-05 22:00:23 | INFO | train_inner | epoch 024:    414 / 1474 loss=1.941, trans_loss=4.835, nll_loss=2.055, w2v_ctc_loss=0.668, task_loss=0, task_loss_gen=0, contrastive_loss=0.165, total=4153.83, n_correct=2805.53, ppl=4.15, accuracy=67.541, wps=12904.7, ups=1.55, wpb=8307.7, bsz=298.4, num_updates=34300, lr=7.63604e-05, gnorm=0.52, clip=0, loss_scale=8, train_wall=64, gb_free=15.8, wall=0
2023-09-05 22:01:27 | INFO | train_inner | epoch 024:    514 / 1474 loss=1.929, trans_loss=4.836, nll_loss=2.055, w2v_ctc_loss=0.669, task_loss=0, task_loss_gen=0, contrastive_loss=0.095, total=4141.88, n_correct=2805.7, ppl=4.16, accuracy=67.74, wps=12807.1, ups=1.55, wpb=8283.8, bsz=302.6, num_updates=34400, lr=7.62493e-05, gnorm=0.513, clip=0, loss_scale=8, train_wall=64, gb_free=14.7, wall=0
2023-09-05 22:02:31 | INFO | train_inner | epoch 024:    614 / 1474 loss=1.922, trans_loss=4.835, nll_loss=2.056, w2v_ctc_loss=0.658, task_loss=0, task_loss_gen=0, contrastive_loss=0.055, total=4162.06, n_correct=2819.98, ppl=4.16, accuracy=67.754, wps=13172.2, ups=1.58, wpb=8324.1, bsz=308.1, num_updates=34500, lr=7.61387e-05, gnorm=0.516, clip=0, loss_scale=8, train_wall=62, gb_free=16.4, wall=0
2023-09-05 22:03:34 | INFO | train_inner | epoch 024:    714 / 1474 loss=1.927, trans_loss=4.84, nll_loss=2.061, w2v_ctc_loss=0.662, task_loss=0, task_loss_gen=0, contrastive_loss=0.072, total=4097.35, n_correct=2771.48, ppl=4.17, accuracy=67.641, wps=12980.4, ups=1.58, wpb=8194.7, bsz=293.8, num_updates=34600, lr=7.60286e-05, gnorm=0.524, clip=0, loss_scale=8, train_wall=62, gb_free=16.9, wall=0
2023-09-05 22:04:37 | INFO | train_inner | epoch 024:    814 / 1474 loss=1.921, trans_loss=4.842, nll_loss=2.065, w2v_ctc_loss=0.661, task_loss=0, task_loss_gen=0, contrastive_loss=0.043, total=4124.25, n_correct=2791.44, ppl=4.18, accuracy=67.684, wps=12939, ups=1.57, wpb=8248.5, bsz=308.2, num_updates=34700, lr=7.5919e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=63, gb_free=15.6, wall=0
2023-09-05 22:05:40 | INFO | train_inner | epoch 024:    914 / 1474 loss=1.931, trans_loss=4.843, nll_loss=2.064, w2v_ctc_loss=0.677, task_loss=0, task_loss_gen=0, contrastive_loss=0.025, total=4041.44, n_correct=2726.22, ppl=4.18, accuracy=67.457, wps=12880.2, ups=1.59, wpb=8082.9, bsz=280.5, num_updates=34800, lr=7.58098e-05, gnorm=0.537, clip=0, loss_scale=16, train_wall=62, gb_free=15.8, wall=0
2023-09-05 22:06:44 | INFO | train_inner | epoch 024:   1014 / 1474 loss=1.921, trans_loss=4.842, nll_loss=2.064, w2v_ctc_loss=0.659, task_loss=0, task_loss_gen=0, contrastive_loss=0.026, total=4128.8, n_correct=2792.81, ppl=4.18, accuracy=67.642, wps=12940.5, ups=1.57, wpb=8257.6, bsz=296.5, num_updates=34900, lr=7.57011e-05, gnorm=0.519, clip=0, loss_scale=16, train_wall=63, gb_free=14.6, wall=0
2023-09-05 22:07:48 | INFO | train_inner | epoch 024:   1114 / 1474 loss=1.922, trans_loss=4.829, nll_loss=2.048, w2v_ctc_loss=0.666, task_loss=0, task_loss_gen=0, contrastive_loss=0.065, total=4130.49, n_correct=2801.45, ppl=4.14, accuracy=67.824, wps=12924.8, ups=1.56, wpb=8261, bsz=308.9, num_updates=35000, lr=7.55929e-05, gnorm=0.52, clip=0, loss_scale=16, train_wall=63, gb_free=15.6, wall=0
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
2023-09-05 22:08:52 | INFO | train_inner | epoch 024:   1214 / 1474 loss=1.924, trans_loss=4.84, nll_loss=2.062, w2v_ctc_loss=0.663, task_loss=0, task_loss_gen=0, contrastive_loss=0.053, total=4157.47, n_correct=2811.01, ppl=4.18, accuracy=67.613, wps=13074.9, ups=1.57, wpb=8314.9, bsz=311.2, num_updates=35100, lr=7.54851e-05, gnorm=0.545, clip=0, loss_scale=16, train_wall=63, gb_free=12.3, wall=0
2023-09-05 22:09:55 | INFO | train_inner | epoch 024:   1314 / 1474 loss=1.93, trans_loss=4.845, nll_loss=2.068, w2v_ctc_loss=0.675, task_loss=0, task_loss_gen=0, contrastive_loss=0.031, total=4107.23, n_correct=2773.25, ppl=4.19, accuracy=67.521, wps=12947.3, ups=1.58, wpb=8214.5, bsz=294.3, num_updates=35200, lr=7.53778e-05, gnorm=0.518, clip=0, loss_scale=16, train_wall=63, gb_free=17.2, wall=0
2023-09-05 22:10:58 | INFO | train_inner | epoch 024:   1414 / 1474 loss=1.925, trans_loss=4.842, nll_loss=2.064, w2v_ctc_loss=0.67, task_loss=0, task_loss_gen=0, contrastive_loss=0.029, total=4094.39, n_correct=2772.06, ppl=4.18, accuracy=67.704, wps=12994.4, ups=1.59, wpb=8188.8, bsz=292.9, num_updates=35300, lr=7.5271e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=62, gb_free=15.7, wall=0
2023-09-05 22:11:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
mt_weight tensor(0.5000)
asr_weight tensor(0.2109)
2023-09-05 22:12:09 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.927 | trans_loss 5.161 | nll_loss 2.423 | w2v_ctc_loss 1.324 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.223 | total 4003.4 | n_correct 2671 | ppl 5.36 | accuracy 66.718 | uer 17.434 | wer 19.149 | raw_wer 19.149 | bleu 22.45 | wps 1629.7 | wpb 4003.4 | bsz 141.8 | num_updates 35360 | best_bleu 22.45
2023-09-05 22:12:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 35360 updates
2023-09-05 22:12:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 22:12:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt
2023-09-05 22:12:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_best.pt (epoch 24 @ 35360 updates, score 22.45) (writing took 12.160766156972386 seconds)
2023-09-05 22:12:21 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-09-05 22:12:21 | INFO | train | epoch 024 | loss 1.926 | trans_loss 4.837 | nll_loss 2.058 | w2v_ctc_loss 0.663 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.08 | total 4138.65 | n_correct 2801.92 | ppl 4.16 | accuracy 67.701 | wps 11828.7 | ups 1.43 | wpb 8277.3 | bsz 305.7 | num_updates 35360 | lr 7.52071e-05 | gnorm 0.522 | clip 0 | loss_scale 16 | train_wall 926 | gb_free 15.6 | wall 0
2023-09-05 22:12:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 22:12:21 | INFO | fairseq.trainer | begin training epoch 25
2023-09-05 22:12:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 22:12:53 | INFO | train_inner | epoch 025:     40 / 1474 loss=1.913, trans_loss=4.828, nll_loss=2.046, w2v_ctc_loss=0.66, task_loss=0, task_loss_gen=0, contrastive_loss=0.034, total=4165.57, n_correct=2833.83, ppl=4.13, accuracy=68.03, wps=7219.9, ups=0.87, wpb=8331.1, bsz=311.2, num_updates=35400, lr=7.51646e-05, gnorm=0.517, clip=0, loss_scale=16, train_wall=62, gb_free=12.7, wall=0
2023-09-05 22:13:57 | INFO | train_inner | epoch 025:    140 / 1474 loss=1.903, trans_loss=4.813, nll_loss=2.027, w2v_ctc_loss=0.644, task_loss=0, task_loss_gen=0, contrastive_loss=0.032, total=4135.43, n_correct=2823.89, ppl=4.08, accuracy=68.285, wps=12936.9, ups=1.56, wpb=8270.9, bsz=308.9, num_updates=35500, lr=7.50587e-05, gnorm=0.509, clip=0, loss_scale=16, train_wall=63, gb_free=17.4, wall=0
2023-09-05 22:15:01 | INFO | train_inner | epoch 025:    240 / 1474 loss=1.912, trans_loss=4.821, nll_loss=2.037, w2v_ctc_loss=0.655, task_loss=0, task_loss_gen=0, contrastive_loss=0.036, total=4116.13, n_correct=2800.1, ppl=4.1, accuracy=68.027, wps=12956.9, ups=1.57, wpb=8232.3, bsz=303, num_updates=35600, lr=7.49532e-05, gnorm=0.515, clip=0, loss_scale=16, train_wall=63, gb_free=16.9, wall=0
2023-09-05 22:16:04 | INFO | train_inner | epoch 025:    340 / 1474 loss=1.918, trans_loss=4.822, nll_loss=2.037, w2v_ctc_loss=0.653, task_loss=0, task_loss_gen=0, contrastive_loss=0.069, total=4141.49, n_correct=2814.89, ppl=4.1, accuracy=67.968, wps=13100.3, ups=1.58, wpb=8283, bsz=294.2, num_updates=35700, lr=7.48481e-05, gnorm=0.54, clip=0, loss_scale=16, train_wall=63, gb_free=14.8, wall=0
2023-09-05 22:17:07 | INFO | train_inner | epoch 025:    440 / 1474 loss=1.933, trans_loss=4.822, nll_loss=2.038, w2v_ctc_loss=0.669, task_loss=0, task_loss_gen=0, contrastive_loss=0.147, total=4167.4, n_correct=2827.47, ppl=4.11, accuracy=67.847, wps=13160.3, ups=1.58, wpb=8334.8, bsz=297.7, num_updates=35800, lr=7.47435e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=63, gb_free=15.4, wall=0
2023-09-05 22:18:10 | INFO | train_inner | epoch 025:    540 / 1474 loss=1.917, trans_loss=4.833, nll_loss=2.053, w2v_ctc_loss=0.657, task_loss=0, task_loss_gen=0, contrastive_loss=0.035, total=4160.61, n_correct=2824.69, ppl=4.15, accuracy=67.891, wps=13249.9, ups=1.59, wpb=8321.2, bsz=313.9, num_updates=35900, lr=7.46393e-05, gnorm=0.514, clip=0, loss_scale=16, train_wall=62, gb_free=17.2, wall=0
2023-09-05 22:19:14 | INFO | train_inner | epoch 025:    640 / 1474 loss=1.919, trans_loss=4.82, nll_loss=2.037, w2v_ctc_loss=0.658, task_loss=0, task_loss_gen=0, contrastive_loss=0.101, total=4153.68, n_correct=2824.77, ppl=4.1, accuracy=68.006, wps=13078.9, ups=1.57, wpb=8307.4, bsz=309.6, num_updates=36000, lr=7.45356e-05, gnorm=0.526, clip=0, loss_scale=16, train_wall=63, gb_free=16.1, wall=0
2023-09-05 22:19:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 22:19:46 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.928 | trans_loss 5.171 | nll_loss 2.431 | w2v_ctc_loss 1.302 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.229 | total 4003.4 | n_correct 2663.2 | ppl 5.39 | accuracy 66.523 | uer 17.437 | wer 19.287 | raw_wer 19.287 | bleu 22.44 | wps 1637.3 | wpb 4003.4 | bsz 141.8 | num_updates 36000 | best_bleu 22.45
2023-09-05 22:19:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36000 updates
2023-09-05 22:19:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_25_36000.pt
2023-09-05 22:19:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_25_36000.pt
2023-09-05 22:19:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_25_36000.pt (epoch 25 @ 36000 updates, score 22.44) (writing took 6.964498813031241 seconds)
--Backword ST Loss tensor(1421.4897, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(818.9403, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 22:20:56 | INFO | train_inner | epoch 025:    740 / 1474 loss=1.921, trans_loss=4.82, nll_loss=2.035, w2v_ctc_loss=0.656, task_loss=0, task_loss_gen=0, contrastive_loss=0.098, total=4128.34, n_correct=2804.99, ppl=4.1, accuracy=67.945, wps=8039.9, ups=0.97, wpb=8256.7, bsz=301.3, num_updates=36100, lr=7.44323e-05, gnorm=0.521, clip=0, loss_scale=16, train_wall=62, gb_free=16.7, wall=0
2023-09-05 22:21:59 | INFO | train_inner | epoch 025:    840 / 1474 loss=1.911, trans_loss=4.827, nll_loss=2.045, w2v_ctc_loss=0.652, task_loss=0, task_loss_gen=0, contrastive_loss=0.042, total=4182.4, n_correct=2842.92, ppl=4.13, accuracy=67.973, wps=13319.4, ups=1.59, wpb=8364.8, bsz=326, num_updates=36200, lr=7.43294e-05, gnorm=0.511, clip=0, loss_scale=16, train_wall=62, gb_free=17.3, wall=0
2023-09-05 22:23:02 | INFO | train_inner | epoch 025:    940 / 1474 loss=1.92, trans_loss=4.828, nll_loss=2.047, w2v_ctc_loss=0.657, task_loss=0, task_loss_gen=0, contrastive_loss=0.101, total=4155.21, n_correct=2823.54, ppl=4.13, accuracy=67.952, wps=13199.2, ups=1.59, wpb=8310.4, bsz=317, num_updates=36300, lr=7.4227e-05, gnorm=0.53, clip=0, loss_scale=16, train_wall=62, gb_free=13.5, wall=0
2023-09-05 22:24:06 | INFO | train_inner | epoch 025:   1040 / 1474 loss=1.935, trans_loss=4.836, nll_loss=2.057, w2v_ctc_loss=0.657, task_loss=0, task_loss_gen=0, contrastive_loss=0.214, total=4177.7, n_correct=2829.5, ppl=4.16, accuracy=67.729, wps=13175.4, ups=1.58, wpb=8355.4, bsz=309.8, num_updates=36400, lr=7.41249e-05, gnorm=0.527, clip=0, loss_scale=16, train_wall=63, gb_free=15.3, wall=0
2023-09-05 22:25:09 | INFO | train_inner | epoch 025:   1140 / 1474 loss=1.914, trans_loss=4.827, nll_loss=2.044, w2v_ctc_loss=0.653, task_loss=0, task_loss_gen=0, contrastive_loss=0.024, total=4039.24, n_correct=2742.88, ppl=4.12, accuracy=67.906, wps=12726.5, ups=1.58, wpb=8078.5, bsz=285.2, num_updates=36500, lr=7.40233e-05, gnorm=0.534, clip=0, loss_scale=16, train_wall=63, gb_free=16, wall=0
2023-09-05 22:26:12 | INFO | train_inner | epoch 025:   1240 / 1474 loss=1.915, trans_loss=4.832, nll_loss=2.052, w2v_ctc_loss=0.653, task_loss=0, task_loss_gen=0, contrastive_loss=0.031, total=4090.59, n_correct=2774.53, ppl=4.15, accuracy=67.827, wps=13079.3, ups=1.6, wpb=8181.2, bsz=295.7, num_updates=36600, lr=7.39221e-05, gnorm=0.522, clip=0, loss_scale=16, train_wall=62, gb_free=16.8, wall=0
2023-09-05 22:27:15 | INFO | train_inner | epoch 025:   1340 / 1474 loss=1.922, trans_loss=4.825, nll_loss=2.043, w2v_ctc_loss=0.657, task_loss=0, task_loss_gen=0, contrastive_loss=0.124, total=4164.34, n_correct=2832.45, ppl=4.12, accuracy=68.017, wps=13200.8, ups=1.58, wpb=8328.7, bsz=310.1, num_updates=36700, lr=7.38213e-05, gnorm=0.514, clip=0, loss_scale=32, train_wall=63, gb_free=16.3, wall=0
2023-09-05 22:28:18 | INFO | train_inner | epoch 025:   1440 / 1474 loss=1.928, trans_loss=4.841, nll_loss=2.063, w2v_ctc_loss=0.663, task_loss=0, task_loss_gen=0, contrastive_loss=0.071, total=4099.11, n_correct=2771.83, ppl=4.18, accuracy=67.62, wps=12875.1, ups=1.57, wpb=8198.2, bsz=299.3, num_updates=36800, lr=7.3721e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=63, gb_free=11.8, wall=0
2023-09-05 22:28:40 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1264.6503, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(729.4581, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1572.1830, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(923.5114, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1215.0098, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(702.8101, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(812.1111, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(489.6702, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1944.3472, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(1169.2739, device='cuda:3', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1779.5894, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(1079.8857, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(936.9154, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(522.1154, device='cuda:2', grad_fn=<MulBackward0>)
2023-09-05 22:29:13 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.935 | trans_loss 5.172 | nll_loss 2.435 | w2v_ctc_loss 1.319 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.237 | total 4003.4 | n_correct 2668.5 | ppl 5.41 | accuracy 66.656 | uer 17.418 | wer 19.436 | raw_wer 19.436 | bleu 22.16 | wps 1645.4 | wpb 4003.4 | bsz 141.8 | num_updates 36834 | best_bleu 22.45
2023-09-05 22:29:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 36834 updates
2023-09-05 22:29:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.1601.pt
2023-09-05 22:29:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.1601.pt
2023-09-05 22:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint.best_bleu_22.1601.pt (epoch 25 @ 36834 updates, score 22.16) (writing took 7.456194850965403 seconds)
2023-09-05 22:29:21 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-09-05 22:29:21 | INFO | train | epoch 025 | loss 1.919 | trans_loss 4.826 | nll_loss 2.044 | w2v_ctc_loss 0.656 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.079 | total 4138.65 | n_correct 2811.13 | ppl 4.12 | accuracy 67.924 | wps 11969 | ups 1.45 | wpb 8277.3 | bsz 305.7 | num_updates 36834 | lr 7.36869e-05 | gnorm 0.524 | clip 0 | loss_scale 32 | train_wall 922 | gb_free 13.8 | wall 0
2023-09-05 22:29:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 22:29:21 | INFO | fairseq.trainer | begin training epoch 26
2023-09-05 22:29:21 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 22:30:09 | INFO | train_inner | epoch 026:     66 / 1474 loss=1.907, trans_loss=4.814, nll_loss=2.028, w2v_ctc_loss=0.647, task_loss=0, task_loss_gen=0, contrastive_loss=0.055, total=4180.21, n_correct=2848.93, ppl=4.08, accuracy=68.153, wps=7540.5, ups=0.9, wpb=8360.4, bsz=318.3, num_updates=36900, lr=7.3621e-05, gnorm=0.523, clip=0, loss_scale=32, train_wall=62, gb_free=16.6, wall=0
2023-09-05 22:31:12 | INFO | train_inner | epoch 026:    166 / 1474 loss=1.917, trans_loss=4.812, nll_loss=2.026, w2v_ctc_loss=0.631, task_loss=0, task_loss_gen=0, contrastive_loss=0.241, total=4270.78, n_correct=2916.16, ppl=4.07, accuracy=68.282, wps=13528.1, ups=1.58, wpb=8541.6, bsz=340.4, num_updates=37000, lr=7.35215e-05, gnorm=0.504, clip=0, loss_scale=32, train_wall=63, gb_free=14.8, wall=0
2023-09-05 22:32:17 | INFO | train_inner | epoch 026:    266 / 1474 loss=1.912, trans_loss=4.806, nll_loss=2.018, w2v_ctc_loss=0.652, task_loss=0, task_loss_gen=0, contrastive_loss=0.114, total=4125.04, n_correct=2817.25, ppl=4.05, accuracy=68.296, wps=12871.1, ups=1.56, wpb=8250.1, bsz=307.1, num_updates=37100, lr=7.34223e-05, gnorm=0.52, clip=0, loss_scale=32, train_wall=64, gb_free=14.8, wall=0
2023-09-05 22:33:19 | INFO | train_inner | epoch 026:    366 / 1474 loss=1.906, trans_loss=4.809, nll_loss=2.022, w2v_ctc_loss=0.645, task_loss=0, task_loss_gen=0, contrastive_loss=0.071, total=4165.74, n_correct=2844.62, ppl=4.06, accuracy=68.286, wps=13359.2, ups=1.6, wpb=8331.5, bsz=314.7, num_updates=37200, lr=7.33236e-05, gnorm=0.518, clip=0, loss_scale=32, train_wall=62, gb_free=16.4, wall=0
2023-09-05 22:34:21 | INFO | train_inner | epoch 026:    466 / 1474 loss=1.909, trans_loss=4.802, nll_loss=2.013, w2v_ctc_loss=0.644, task_loss=0, task_loss_gen=0, contrastive_loss=0.121, total=4170.23, n_correct=2852.04, ppl=4.04, accuracy=68.39, wps=13344.7, ups=1.6, wpb=8340.5, bsz=315.4, num_updates=37300, lr=7.32252e-05, gnorm=0.508, clip=0, loss_scale=32, train_wall=62, gb_free=17.5, wall=0
2023-09-05 22:35:24 | INFO | train_inner | epoch 026:    566 / 1474 loss=1.915, trans_loss=4.817, nll_loss=2.032, w2v_ctc_loss=0.666, task_loss=0, task_loss_gen=0, contrastive_loss=0.041, total=4155.02, n_correct=2827.94, ppl=4.09, accuracy=68.061, wps=13233.8, ups=1.59, wpb=8310, bsz=303.9, num_updates=37400, lr=7.31272e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=62, gb_free=17.4, wall=0
2023-09-05 22:36:27 | INFO | train_inner | epoch 026:    666 / 1474 loss=1.906, trans_loss=4.814, nll_loss=2.028, w2v_ctc_loss=0.646, task_loss=0, task_loss_gen=0, contrastive_loss=0.029, total=4136.96, n_correct=2818.72, ppl=4.08, accuracy=68.135, wps=13145, ups=1.59, wpb=8273.9, bsz=299.2, num_updates=37500, lr=7.30297e-05, gnorm=0.515, clip=0, loss_scale=32, train_wall=62, gb_free=15, wall=0
2023-09-05 22:37:30 | INFO | train_inner | epoch 026:    766 / 1474 loss=1.92, trans_loss=4.817, nll_loss=2.032, w2v_ctc_loss=0.647, task_loss=0, task_loss_gen=0, contrastive_loss=0.141, total=4086.28, n_correct=2781.83, ppl=4.09, accuracy=68.077, wps=12969.3, ups=1.59, wpb=8172.6, bsz=298.5, num_updates=37600, lr=7.29325e-05, gnorm=0.519, clip=0, loss_scale=32, train_wall=62, gb_free=14.7, wall=0
2023-09-05 22:38:34 | INFO | train_inner | epoch 026:    866 / 1474 loss=1.911, trans_loss=4.815, nll_loss=2.029, w2v_ctc_loss=0.655, task_loss=0, task_loss_gen=0, contrastive_loss=0.039, total=4183.26, n_correct=2845.61, ppl=4.08, accuracy=68.024, wps=13169, ups=1.57, wpb=8366.5, bsz=308.1, num_updates=37700, lr=7.28357e-05, gnorm=0.511, clip=0, loss_scale=32, train_wall=63, gb_free=17, wall=0
2023-09-05 22:39:37 | INFO | train_inner | epoch 026:    966 / 1474 loss=1.916, trans_loss=4.822, nll_loss=2.038, w2v_ctc_loss=0.644, task_loss=0, task_loss_gen=0, contrastive_loss=0.096, total=4137.96, n_correct=2813.86, ppl=4.11, accuracy=68.001, wps=13156.4, ups=1.59, wpb=8275.9, bsz=299, num_updates=37800, lr=7.27393e-05, gnorm=0.53, clip=0, loss_scale=32, train_wall=62, gb_free=16.5, wall=0
2023-09-05 22:40:39 | INFO | train_inner | epoch 026:   1066 / 1474 loss=1.911, trans_loss=4.817, nll_loss=2.032, w2v_ctc_loss=0.656, task_loss=0, task_loss_gen=0, contrastive_loss=0.028, total=4120.53, n_correct=2807.6, ppl=4.09, accuracy=68.137, wps=13127.8, ups=1.59, wpb=8241.1, bsz=294.3, num_updates=37900, lr=7.26433e-05, gnorm=0.517, clip=0, loss_scale=32, train_wall=62, gb_free=16.3, wall=0
2023-09-05 22:41:42 | INFO | train_inner | epoch 026:   1166 / 1474 loss=1.919, trans_loss=4.827, nll_loss=2.044, w2v_ctc_loss=0.657, task_loss=0, task_loss_gen=0, contrastive_loss=0.069, total=4113.86, n_correct=2793.88, ppl=4.12, accuracy=67.914, wps=13074.3, ups=1.59, wpb=8227.7, bsz=298.5, num_updates=38000, lr=7.25476e-05, gnorm=0.521, clip=0, loss_scale=32, train_wall=62, gb_free=16.2, wall=0
2023-09-05 22:41:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
2023-09-05 22:42:16 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.943 | trans_loss 5.168 | nll_loss 2.428 | w2v_ctc_loss 1.363 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.221 | total 4003.4 | n_correct 2666.9 | ppl 5.38 | accuracy 66.616 | uer 17.535 | wer 19.377 | raw_wer 19.377 | bleu 21.97 | wps 1595.4 | wpb 4003.4 | bsz 141.8 | num_updates 38000 | best_bleu 22.45
2023-09-05 22:42:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38000 updates
2023-09-05 22:42:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_26_38000.pt
2023-09-05 22:42:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_26_38000.pt
2023-09-05 22:42:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_26_38000.pt (epoch 26 @ 38000 updates, score 21.97) (writing took 8.431722638080828 seconds)
--Backword ST Loss tensor(1204.3889, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(706.6835, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 22:43:27 | INFO | train_inner | epoch 026:   1266 / 1474 loss=1.923, trans_loss=4.833, nll_loss=2.051, w2v_ctc_loss=0.669, task_loss=0, task_loss_gen=0, contrastive_loss=0.03, total=3996.19, n_correct=2708.87, ppl=4.15, accuracy=67.786, wps=7619, ups=0.95, wpb=7992.4, bsz=279.3, num_updates=38100, lr=7.24524e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=62, gb_free=17.4, wall=0
2023-09-05 22:44:32 | INFO | train_inner | epoch 026:   1366 / 1474 loss=1.909, trans_loss=4.824, nll_loss=2.042, w2v_ctc_loss=0.647, task_loss=0, task_loss_gen=0, contrastive_loss=0.04, total=4159.74, n_correct=2831.63, ppl=4.12, accuracy=68.072, wps=12875.1, ups=1.55, wpb=8319.5, bsz=311.4, num_updates=38200, lr=7.23575e-05, gnorm=0.513, clip=0, loss_scale=32, train_wall=64, gb_free=16.8, wall=0
2023-09-05 22:45:34 | INFO | train_inner | epoch 026:   1466 / 1474 loss=1.903, trans_loss=4.817, nll_loss=2.033, w2v_ctc_loss=0.642, task_loss=0, task_loss_gen=0, contrastive_loss=0.034, total=4165.66, n_correct=2840.2, ppl=4.09, accuracy=68.181, wps=13315.3, ups=1.6, wpb=8331.3, bsz=317.5, num_updates=38300, lr=7.22629e-05, gnorm=0.516, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=0
2023-09-05 22:45:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1513.4753, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(864.8799, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(887.3766, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(492.9058, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1647.3812, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(957.0490, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1114.2668, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(674.3657, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1529.5918, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(852.2833, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1737.0039, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(1052.7532, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1340.9521, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(643.7924, device='cuda:3', grad_fn=<MulBackward0>)
2023-09-05 22:46:13 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.93 | trans_loss 5.165 | nll_loss 2.423 | w2v_ctc_loss 1.326 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.223 | total 4003.4 | n_correct 2669.6 | ppl 5.36 | accuracy 66.683 | uer 17.461 | wer 19.328 | raw_wer 19.328 | bleu 21.89 | wps 1580.7 | wpb 4003.4 | bsz 141.8 | num_updates 38308 | best_bleu 22.45
2023-09-05 22:46:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 38308 updates
2023-09-05 22:46:13 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt
2023-09-05 22:46:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt
2023-09-05 22:46:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/ende_v4_merge_wmt_0905_shrink_both_topCL_AT_sentence_mixup_changeid_scale3.5_alpha1.5_mt0.5_nofz_decrease/checkpoint_last.pt (epoch 26 @ 38308 updates, score 21.89) (writing took 6.370679095038213 seconds)
2023-09-05 22:46:19 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-09-05 22:46:19 | INFO | train | epoch 026 | loss 1.912 | trans_loss 4.816 | nll_loss 2.03 | w2v_ctc_loss 0.65 | task_loss 0 | task_loss_gen 0 | contrastive_loss 0.078 | total 4138.65 | n_correct 2819.92 | ppl 4.08 | accuracy 68.136 | wps 11975.1 | ups 1.45 | wpb 8277.3 | bsz 305.7 | num_updates 38308 | lr 7.22554e-05 | gnorm 0.518 | clip 0 | loss_scale 32 | train_wall 920 | gb_free 15.6 | wall 0
2023-09-05 22:46:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1474
2023-09-05 22:46:20 | INFO | fairseq.trainer | begin training epoch 27
2023-09-05 22:46:20 | INFO | fairseq_cli.train | Start iterating over samples
Traceback (most recent call last):
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 564, in <module>
    cli_main()
  File "/mnt/zhangyh/fairseq-AT/fairseq_cli/train.py", line 557, in cli_main
    distributed_utils.call_main(cfg, main)
  File "/mnt/zhangyh/fairseq-AT/fairseq/distributed/utils.py", line 344, in call_main
    torch.multiprocessing.spawn(
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 239, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 197, in start_processes
    while not context.join():
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/multiprocessing/spawn.py", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 4 terminated with signal SIGKILL
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 336 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
