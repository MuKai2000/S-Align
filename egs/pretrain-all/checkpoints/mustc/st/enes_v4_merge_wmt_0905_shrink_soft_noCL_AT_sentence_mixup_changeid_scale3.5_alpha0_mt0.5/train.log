2023-09-05 10:43:58 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:18103
2023-09-05 10:43:58 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:18103
2023-09-05 10:43:58 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:18103
2023-09-05 10:43:58 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:18103
2023-09-05 10:43:58 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:18103
2023-09-05 10:43:58 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:18103
2023-09-05 10:43:58 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:18103
2023-09-05 10:43:58 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:18103
2023-09-05 10:43:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-05 10:43:58 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 10:43:59 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 10:43:59 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 10:43:59 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 10:43:59 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 10:43:59 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 10:43:59 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 10:43:59 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-05 10:43:59 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 10:43:59 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-05 10:44:02 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:18103', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 50000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=50000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-05 10:44:02 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-05 10:44:02 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-05 10:44:02 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-05 10:44:02 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-05 10:44:02 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline
2023-09-05 10:44:07 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-05 10:44:07 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_enes_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-05 10:44:07 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-05 10:44:09 | INFO | root | load pretrained hubert
2023-09-05 10:44:16 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline
2023-09-05 10:44:20 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt
2023-09-05 10:44:27 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt
2023-09-05 10:44:27 | INFO | root | share the sematic adapter and textual encoder
2023-09-05 10:44:27 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-05 10:44:27 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-05 10:44:27 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-05 10:44:27 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-05 10:44:27 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-05 10:44:27 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-05 10:44:27 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 10:44:27 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-05 10:44:27 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-05 10:44:27 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1312, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 10:44:42 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-05 10:44:42 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-05 10:44:42 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-05 10:44:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 10:44:43 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 10:44:43 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 10:44:43 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 10:44:43 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 10:44:43 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 10:44:43 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 10:44:43 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 10:44:43 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 10:44:43 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 10:44:43 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-05 10:44:43 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-05 10:44:43 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt
2023-09-05 10:44:43 | INFO | fairseq.trainer | No existing checkpoint found ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt
2023-09-05 10:44:43 | INFO | fairseq.trainer | loading train data for epoch 1
2023-09-05 10:44:43 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 10:44:43 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-05 10:44:43 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-05 10:44:45 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=260049, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 10:44:47 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=260049, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 10:45:35 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-05 10:45:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 10:45:35 | INFO | fairseq.trainer | begin training epoch 1
2023-09-05 10:45:35 | INFO | fairseq_cli.train | Start iterating over samples
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-05 10:45:47 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-05 10:45:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
2023-09-05 10:46:47 | INFO | train_inner | epoch 001:    102 / 1826 loss=17.954, trans_loss=5.722, nll_loss=4.564, w2v_ctc_loss=23.234, task_loss=10.578, task_loss_gen=6.21, contrastive_loss=0, total=3940, n_correct=64.47, ppl=23.65, accuracy=1.636, wps=20134, ups=1.7, wpb=11837.1, bsz=400.3, num_updates=100, lr=4.098e-06, gnorm=3.665, clip=0, loss_scale=32, train_wall=64, gb_free=19, wall=124
2023-09-05 10:47:45 | INFO | train_inner | epoch 001:    202 / 1826 loss=12.974, trans_loss=5.71, nll_loss=4.574, w2v_ctc_loss=15.592, task_loss=7.429, task_loss_gen=5.18, contrastive_loss=0, total=4000.63, n_correct=69.91, ppl=23.81, accuracy=1.747, wps=20533.2, ups=1.7, wpb=12045.3, bsz=454.7, num_updates=200, lr=8.096e-06, gnorm=8.411, clip=32, loss_scale=32, train_wall=58, gb_free=19.6, wall=183
2023-09-05 10:48:45 | INFO | train_inner | epoch 001:    302 / 1826 loss=7.139, trans_loss=5.608, nll_loss=4.471, w2v_ctc_loss=6.714, task_loss=5.309, task_loss_gen=5.78, contrastive_loss=0, total=3981.48, n_correct=82.68, ppl=22.18, accuracy=2.077, wps=20018, ups=1.67, wpb=11963.3, bsz=445.6, num_updates=300, lr=1.2094e-05, gnorm=1.333, clip=0, loss_scale=32, train_wall=59, gb_free=18.5, wall=242
2023-09-05 10:49:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 10:49:44 | INFO | train_inner | epoch 001:    403 / 1826 loss=6.705, trans_loss=5.556, nll_loss=4.434, w2v_ctc_loss=6.092, task_loss=3.188, task_loss_gen=7.722, contrastive_loss=0, total=4007.21, n_correct=87.83, ppl=21.61, accuracy=2.192, wps=20353.4, ups=1.69, wpb=12059.5, bsz=431.7, num_updates=400, lr=1.6092e-05, gnorm=0.617, clip=0, loss_scale=16, train_wall=59, gb_free=19.6, wall=302
2023-09-05 10:50:43 | INFO | train_inner | epoch 001:    503 / 1826 loss=6.524, trans_loss=5.588, nll_loss=4.473, w2v_ctc_loss=5.778, task_loss=1.146, task_loss_gen=10.72, contrastive_loss=0, total=3994.6, n_correct=72.8, ppl=22.21, accuracy=1.822, wps=20510.5, ups=1.71, wpb=12013, bsz=442.9, num_updates=500, lr=2.009e-05, gnorm=0.435, clip=0, loss_scale=16, train_wall=58, gb_free=19.1, wall=360
2023-09-05 10:51:41 | INFO | train_inner | epoch 001:    603 / 1826 loss=6.378, trans_loss=5.573, nll_loss=4.463, w2v_ctc_loss=5.569, task_loss=0.369, task_loss_gen=16.156, contrastive_loss=0, total=3946.86, n_correct=71.8, ppl=22.05, accuracy=1.819, wps=20388.9, ups=1.72, wpb=11872, bsz=424.9, num_updates=600, lr=2.4088e-05, gnorm=0.423, clip=0, loss_scale=16, train_wall=57, gb_free=18.5, wall=418
2023-09-05 10:52:39 | INFO | train_inner | epoch 001:    703 / 1826 loss=6.067, trans_loss=5.555, nll_loss=4.441, w2v_ctc_loss=5.112, task_loss=0.096, task_loss_gen=20.503, contrastive_loss=0, total=3953.79, n_correct=87.63, ppl=21.72, accuracy=2.216, wps=20423.5, ups=1.72, wpb=11877.7, bsz=431.2, num_updates=700, lr=2.8086e-05, gnorm=0.569, clip=0, loss_scale=16, train_wall=58, gb_free=18.8, wall=477
2023-09-05 10:53:38 | INFO | train_inner | epoch 001:    803 / 1826 loss=5.807, trans_loss=5.544, nll_loss=4.432, w2v_ctc_loss=4.723, task_loss=0.025, task_loss_gen=27.016, contrastive_loss=0, total=3973.13, n_correct=84.7, ppl=21.58, accuracy=2.132, wps=20509.3, ups=1.72, wpb=11951.9, bsz=428.7, num_updates=800, lr=3.2084e-05, gnorm=0.687, clip=0, loss_scale=16, train_wall=58, gb_free=18.7, wall=535
2023-09-05 10:54:37 | INFO | train_inner | epoch 001:    903 / 1826 loss=5.589, trans_loss=5.539, nll_loss=4.427, w2v_ctc_loss=4.392, task_loss=0.006, task_loss_gen=32.991, contrastive_loss=0, total=3998.85, n_correct=93.57, ppl=21.51, accuracy=2.34, wps=20287.2, ups=1.69, wpb=12033.4, bsz=439.4, num_updates=900, lr=3.6082e-05, gnorm=0.867, clip=0, loss_scale=16, train_wall=58, gb_free=19, wall=594
2023-09-05 10:55:35 | INFO | train_inner | epoch 001:   1003 / 1826 loss=5.454, trans_loss=5.551, nll_loss=4.437, w2v_ctc_loss=4.172, task_loss=0.002, task_loss_gen=36.173, contrastive_loss=0, total=3988.93, n_correct=96.56, ppl=21.66, accuracy=2.421, wps=20760.5, ups=1.73, wpb=12000.5, bsz=449.7, num_updates=1000, lr=4.008e-05, gnorm=0.899, clip=0, loss_scale=16, train_wall=57, gb_free=18.8, wall=652
2023-09-05 10:56:33 | INFO | train_inner | epoch 001:   1103 / 1826 loss=5.49, trans_loss=5.735, nll_loss=4.664, w2v_ctc_loss=4.028, task_loss=4.866, task_loss_gen=16.841, contrastive_loss=0, total=3947.12, n_correct=52.67, ppl=25.36, accuracy=1.334, wps=20258.9, ups=1.71, wpb=11874.1, bsz=420.8, num_updates=1100, lr=4.4078e-05, gnorm=1.003, clip=0, loss_scale=16, train_wall=58, gb_free=18.7, wall=711
2023-09-05 10:57:32 | INFO | train_inner | epoch 001:   1203 / 1826 loss=5.465, trans_loss=5.837, nll_loss=4.774, w2v_ctc_loss=3.881, task_loss=7.866, task_loss_gen=6.325, contrastive_loss=0, total=3914.85, n_correct=33.88, ppl=27.36, accuracy=0.865, wps=20263.5, ups=1.72, wpb=11775.7, bsz=401.2, num_updates=1200, lr=4.8076e-05, gnorm=0.988, clip=0, loss_scale=16, train_wall=57, gb_free=18.5, wall=769
2023-09-05 10:58:30 | INFO | train_inner | epoch 001:   1303 / 1826 loss=5.544, trans_loss=6.084, nll_loss=5.111, w2v_ctc_loss=3.744, task_loss=6.448, task_loss_gen=6.087, contrastive_loss=0, total=3925.76, n_correct=31.37, ppl=34.55, accuracy=0.799, wps=20246.7, ups=1.71, wpb=11815, bsz=424.1, num_updates=1300, lr=5.2074e-05, gnorm=0.95, clip=0, loss_scale=16, train_wall=58, gb_free=19.2, wall=827
2023-09-05 10:59:28 | INFO | train_inner | epoch 001:   1403 / 1826 loss=5.471, trans_loss=6.072, nll_loss=5.094, w2v_ctc_loss=3.644, task_loss=5.882, task_loss_gen=6.254, contrastive_loss=0, total=3918.07, n_correct=17.76, ppl=34.14, accuracy=0.453, wps=20303.5, ups=1.72, wpb=11779.6, bsz=410.4, num_updates=1400, lr=5.6072e-05, gnorm=1.092, clip=0, loss_scale=16, train_wall=57, gb_free=18.7, wall=885
2023-09-05 11:00:28 | INFO | train_inner | epoch 001:   1503 / 1826 loss=5.255, trans_loss=5.877, nll_loss=4.829, w2v_ctc_loss=3.519, task_loss=5.216, task_loss_gen=6.501, contrastive_loss=0, total=3995.02, n_correct=22.51, ppl=28.43, accuracy=0.563, wps=20124.1, ups=1.68, wpb=12005.7, bsz=425.9, num_updates=1500, lr=6.007e-05, gnorm=0.968, clip=0, loss_scale=16, train_wall=59, gb_free=19.6, wall=945
2023-09-05 11:01:25 | INFO | train_inner | epoch 001:   1603 / 1826 loss=5.235, trans_loss=5.912, nll_loss=4.873, w2v_ctc_loss=3.447, task_loss=3.448, task_loss_gen=7.49, contrastive_loss=0, total=3877.45, n_correct=43.47, ppl=29.29, accuracy=1.121, wps=20245.9, ups=1.74, wpb=11667.5, bsz=409.1, num_updates=1600, lr=6.4068e-05, gnorm=1.054, clip=0, loss_scale=16, train_wall=57, gb_free=19.6, wall=1003
2023-09-05 11:02:23 | INFO | train_inner | epoch 001:   1703 / 1826 loss=5.076, trans_loss=5.773, nll_loss=4.727, w2v_ctc_loss=3.352, task_loss=2.429, task_loss_gen=7.952, contrastive_loss=0, total=3947.06, n_correct=62.75, ppl=26.49, accuracy=1.59, wps=20559.8, ups=1.73, wpb=11879.5, bsz=425.7, num_updates=1700, lr=6.8066e-05, gnorm=1.165, clip=0, loss_scale=16, train_wall=57, gb_free=18.6, wall=1060
2023-09-05 11:03:20 | INFO | train_inner | epoch 001:   1803 / 1826 loss=5.08, trans_loss=5.854, nll_loss=4.817, w2v_ctc_loss=3.276, task_loss=1.721, task_loss_gen=8.871, contrastive_loss=0, total=3948.71, n_correct=34.72, ppl=28.19, accuracy=0.879, wps=20690.1, ups=1.74, wpb=11885.2, bsz=430.7, num_updates=1800, lr=7.2064e-05, gnorm=1.024, clip=0, loss_scale=16, train_wall=57, gb_free=18.8, wall=1118
2023-09-05 11:03:34 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
mix_tag: 0.5 at_nopad: True at_low_pos: False mixup_change_id: True
tokens_add_noise: None 
lengths_add_noise: None 
ids: None
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([-0.8813], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.8813], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 11:04:28 | INFO | dev_st | epoch 001 | valid on 'dev_st' subset | loss 10.148 | trans_loss 13.116 | nll_loss 12.82 | w2v_ctc_loss 4.23 | task_loss 30.168 | task_loss_gen 34.575 | contrastive_loss 0 | total 3505.91 | n_correct 20.6364 | ppl 7233 | accuracy 0.589 | uer 56.251 | wer 55.772 | raw_wer 55.772 | bleu 0 | wps 817.1 | wpb 3505.9 | bsz 119.3 | num_updates 1823
2023-09-05 11:04:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 1823 updates
2023-09-05 11:04:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 11:04:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 11:04:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 1 @ 1823 updates, score 0.0) (writing took 4.9806877200026065 seconds)
2023-09-05 11:04:33 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)
2023-09-05 11:04:33 | INFO | train | epoch 001 | loss 6.826 | trans_loss 5.728 | nll_loss 4.645 | w2v_ctc_loss 6.096 | task_loss 3.601 | task_loss_gen 13.108 | contrastive_loss 0 | total 3956.53 | n_correct 61.2633 | ppl 25.02 | accuracy 1.548 | wps 19287.3 | ups 1.62 | wpb 11900.6 | bsz 427.4 | num_updates 1823 | lr 7.29835e-05 | gnorm 1.445 | clip 1.8 | loss_scale 16 | train_wall 1058 | gb_free 19 | wall 1190
2023-09-05 11:04:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 11:04:33 | INFO | fairseq.trainer | begin training epoch 2
2023-09-05 11:04:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 11:05:25 | INFO | train_inner | epoch 002:     77 / 1826 loss=5.11, trans_loss=5.948, nll_loss=4.94, w2v_ctc_loss=3.218, task_loss=1.755, task_loss_gen=9.293, contrastive_loss=0, total=3877.76, n_correct=15.71, ppl=30.71, accuracy=0.405, wps=9386.6, ups=0.8, wpb=11670.1, bsz=418.5, num_updates=1900, lr=7.6062e-05, gnorm=0.843, clip=0, loss_scale=16, train_wall=57, gb_free=19.4, wall=1242
2023-09-05 11:06:23 | INFO | train_inner | epoch 002:    177 / 1826 loss=4.986, trans_loss=5.831, nll_loss=4.796, w2v_ctc_loss=3.146, task_loss=1.109, task_loss_gen=10.948, contrastive_loss=0, total=3945.55, n_correct=64.8, ppl=27.78, accuracy=1.642, wps=20306.5, ups=1.71, wpb=11878.2, bsz=423.1, num_updates=2000, lr=8.006e-05, gnorm=0.926, clip=0, loss_scale=16, train_wall=58, gb_free=19.4, wall=1301
2023-09-05 11:06:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-1.8379], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.8379], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 11:07:18 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 9.691 | trans_loss 12.522 | nll_loss 12.091 | w2v_ctc_loss 4.045 | task_loss 13.278 | task_loss_gen 44.484 | contrastive_loss 0 | total 3505.91 | n_correct 51.8182 | ppl 4363.6 | accuracy 1.478 | uer 54.767 | wer 54.132 | raw_wer 54.132 | bleu 0 | wps 813.3 | wpb 3505.9 | bsz 119.3 | num_updates 2000 | best_bleu 0
2023-09-05 11:07:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 2000 updates
2023-09-05 11:07:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_2_2000.pt
2023-09-05 11:07:20 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_2_2000.pt
2023-09-05 11:07:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_2_2000.pt (epoch 2 @ 2000 updates, score 0.0) (writing took 14.22097915300401 seconds)
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.2817, -0.6582,  0.1119, -0.5391, -0.4248], device='cuda:0',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0898,  0.1257, -0.0886,  ...,  0.0183,  0.1263,  0.1807],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0913,  0.1067,  0.4792,  ...,  0.0160, -0.0898,  0.0682]],
       device='cuda:0', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.2817, -0.6582,  0.1119, -0.5391, -0.4248], device='cuda:0',
       dtype=torch.float16)
--------------------
2023-09-05 11:08:30 | INFO | train_inner | epoch 002:    277 / 1826 loss=4.971, trans_loss=5.89, nll_loss=4.86, w2v_ctc_loss=3.064, task_loss=0.986, task_loss_gen=11.592, contrastive_loss=0, total=3980.57, n_correct=39.14, ppl=29.05, accuracy=0.983, wps=9438.8, ups=0.79, wpb=11976.8, bsz=426.2, num_updates=2100, lr=8.4058e-05, gnorm=0.854, clip=0, loss_scale=16, train_wall=58, gb_free=18.6, wall=1427
2023-09-05 11:09:28 | INFO | train_inner | epoch 002:    377 / 1826 loss=4.945, trans_loss=5.892, nll_loss=4.867, w2v_ctc_loss=3.027, task_loss=0.816, task_loss_gen=11.924, contrastive_loss=0, total=3951.89, n_correct=42.36, ppl=29.18, accuracy=1.072, wps=20607, ups=1.73, wpb=11884.4, bsz=426.5, num_updates=2200, lr=8.8056e-05, gnorm=0.813, clip=0, loss_scale=16, train_wall=57, gb_free=19.3, wall=1485
2023-09-05 11:10:26 | INFO | train_inner | epoch 002:    477 / 1826 loss=4.932, trans_loss=5.968, nll_loss=4.95, w2v_ctc_loss=2.931, task_loss=0.487, task_loss_gen=12.36, contrastive_loss=0, total=3989.48, n_correct=32.01, ppl=30.9, accuracy=0.802, wps=20753.6, ups=1.73, wpb=11999.9, bsz=452.2, num_updates=2300, lr=9.2054e-05, gnorm=0.849, clip=0, loss_scale=16, train_wall=57, gb_free=19.4, wall=1543
2023-09-05 11:11:23 | INFO | train_inner | epoch 002:    577 / 1826 loss=4.803, trans_loss=5.81, nll_loss=4.764, w2v_ctc_loss=2.897, task_loss=0.29, task_loss_gen=14.637, contrastive_loss=0, total=4010.91, n_correct=23.34, ppl=27.16, accuracy=0.582, wps=20893.3, ups=1.73, wpb=12062.1, bsz=444.2, num_updates=2400, lr=9.6052e-05, gnorm=0.724, clip=0, loss_scale=32, train_wall=57, gb_free=19.2, wall=1601
2023-09-05 11:12:21 | INFO | train_inner | epoch 002:    677 / 1826 loss=4.88, trans_loss=5.925, nll_loss=4.927, w2v_ctc_loss=2.875, task_loss=0.2, task_loss_gen=18.616, contrastive_loss=0, total=3863.36, n_correct=32.34, ppl=30.42, accuracy=0.837, wps=20377.2, ups=1.75, wpb=11638.9, bsz=397.9, num_updates=2500, lr=0.00010005, gnorm=0.797, clip=0, loss_scale=32, train_wall=56, gb_free=19.4, wall=1658
2023-09-05 11:13:19 | INFO | train_inner | epoch 002:    777 / 1826 loss=4.785, trans_loss=5.85, nll_loss=4.805, w2v_ctc_loss=2.823, task_loss=0.115, task_loss_gen=18.913, contrastive_loss=0, total=3968.04, n_correct=30.29, ppl=27.96, accuracy=0.763, wps=20286.1, ups=1.7, wpb=11939.6, bsz=431.9, num_updates=2600, lr=0.000104048, gnorm=0.778, clip=0, loss_scale=32, train_wall=58, gb_free=19.3, wall=1717
2023-09-05 11:14:17 | INFO | train_inner | epoch 002:    877 / 1826 loss=4.823, trans_loss=5.932, nll_loss=4.921, w2v_ctc_loss=2.795, task_loss=0.051, task_loss_gen=21.637, contrastive_loss=0, total=3980.03, n_correct=54.45, ppl=30.29, accuracy=1.368, wps=20627.7, ups=1.72, wpb=11977.6, bsz=438.8, num_updates=2700, lr=0.000108046, gnorm=0.831, clip=0, loss_scale=32, train_wall=57, gb_free=18.8, wall=1775
2023-09-05 11:15:16 | INFO | train_inner | epoch 002:    977 / 1826 loss=4.693, trans_loss=5.77, nll_loss=4.711, w2v_ctc_loss=2.765, task_loss=0.033, task_loss_gen=24.969, contrastive_loss=0, total=3945.98, n_correct=50.16, ppl=26.19, accuracy=1.271, wps=20445.5, ups=1.72, wpb=11874, bsz=425.1, num_updates=2800, lr=0.000112044, gnorm=0.713, clip=0, loss_scale=32, train_wall=57, gb_free=19.4, wall=1833
2023-09-05 11:16:13 | INFO | train_inner | epoch 002:   1077 / 1826 loss=4.87, trans_loss=6.102, nll_loss=5.138, w2v_ctc_loss=2.697, task_loss=0.014, task_loss_gen=26.39, contrastive_loss=0, total=3979.79, n_correct=5.67, ppl=35.22, accuracy=0.142, wps=20685.7, ups=1.73, wpb=11956.7, bsz=440.4, num_updates=2900, lr=0.000116042, gnorm=0.732, clip=0, loss_scale=32, train_wall=57, gb_free=19.2, wall=1891
2023-09-05 11:17:11 | INFO | train_inner | epoch 002:   1177 / 1826 loss=4.688, trans_loss=5.816, nll_loss=4.766, w2v_ctc_loss=2.705, task_loss=0.01, task_loss_gen=32.371, contrastive_loss=0, total=3887.16, n_correct=31.71, ppl=27.21, accuracy=0.816, wps=20416.9, ups=1.75, wpb=11682, bsz=391.1, num_updates=3000, lr=0.00012004, gnorm=0.738, clip=0, loss_scale=32, train_wall=57, gb_free=18.5, wall=1948
2023-09-05 11:17:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 11:17:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0
2023-09-05 11:17:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 4.0
2023-09-05 11:18:39 | INFO | train_inner | epoch 002:   1280 / 1826 loss=3.988, trans_loss=5.179, nll_loss=3.95, w2v_ctc_loss=2.315, task_loss=5.04, task_loss_gen=12.203, contrastive_loss=0, total=3929.13, n_correct=192.93, ppl=15.46, accuracy=4.91, wps=13296.6, ups=1.13, wpb=11812.1, bsz=419.9, num_updates=3100, lr=0.000124038, gnorm=1.614, clip=1, loss_scale=4, train_wall=88, gb_free=15.6, wall=2037
2023-09-05 11:20:06 | INFO | train_inner | epoch 002:   1380 / 1826 loss=3.598, trans_loss=4.869, nll_loss=3.532, w2v_ctc_loss=2.065, task_loss=2.507, task_loss_gen=7.4, contrastive_loss=0, total=4012.66, n_correct=490.32, ppl=11.57, accuracy=12.219, wps=13915.5, ups=1.15, wpb=12065.4, bsz=448.5, num_updates=3200, lr=0.000128036, gnorm=1.241, clip=0, loss_scale=4, train_wall=86, gb_free=16.2, wall=2123
2023-09-05 11:21:33 | INFO | train_inner | epoch 002:   1480 / 1826 loss=3.18, trans_loss=4.362, nll_loss=2.854, w2v_ctc_loss=1.969, task_loss=1.83, task_loss_gen=8.604, contrastive_loss=0, total=4011.89, n_correct=962.19, ppl=7.23, accuracy=23.983, wps=13944.6, ups=1.16, wpb=12062.1, bsz=451.9, num_updates=3300, lr=0.000132034, gnorm=1.258, clip=0, loss_scale=4, train_wall=86, gb_free=16.9, wall=2210
2023-09-05 11:22:59 | INFO | train_inner | epoch 002:   1580 / 1826 loss=3.096, trans_loss=4.297, nll_loss=2.772, w2v_ctc_loss=1.912, task_loss=1.862, task_loss_gen=9.037, contrastive_loss=0, total=4002.24, n_correct=1040.47, ppl=6.83, accuracy=25.997, wps=13920.1, ups=1.16, wpb=12045.9, bsz=431.3, num_updates=3400, lr=0.000136032, gnorm=1.143, clip=0, loss_scale=4, train_wall=86, gb_free=16.3, wall=2296
2023-09-05 11:24:25 | INFO | train_inner | epoch 002:   1680 / 1826 loss=3.032, trans_loss=4.292, nll_loss=2.761, w2v_ctc_loss=1.832, task_loss=1.615, task_loss_gen=9.335, contrastive_loss=0, total=3944.6, n_correct=1051.78, ppl=6.78, accuracy=26.664, wps=13900.6, ups=1.17, wpb=11859.3, bsz=423.6, num_updates=3500, lr=0.00014003, gnorm=1.062, clip=0, loss_scale=4, train_wall=85, gb_free=16, wall=2382
2023-09-05 11:25:51 | INFO | train_inner | epoch 002:   1780 / 1826 loss=3.012, trans_loss=4.306, nll_loss=2.778, w2v_ctc_loss=1.801, task_loss=1.791, task_loss_gen=10.107, contrastive_loss=0, total=3964.45, n_correct=1056.65, ppl=6.86, accuracy=26.653, wps=13816.6, ups=1.16, wpb=11912.3, bsz=413.8, num_updates=3600, lr=0.000144028, gnorm=1.098, clip=0, loss_scale=4, train_wall=86, gb_free=15.8, wall=2468
2023-09-05 11:26:30 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.5830, -1.7051,  0.3301, -1.1465, -0.7354], device='cuda:7',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2206,  0.3472,  0.2566,  ...,  0.0617,  0.1396,  0.2686],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2991, -0.2742,  1.0215,  ...,  0.0634, -0.1221,  0.0629]],
       device='cuda:7', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.5830, -1.7051,  0.3301, -1.1465, -0.7354], device='cuda:7',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.4148, -0.8447,  0.2510, -0.6587, -0.3787], device='cuda:5',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0103,  0.3284, -0.1404,  ..., -0.0675,  0.0879,  0.1647],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1166,  0.3730,  0.3394,  ...,  0.0925, -0.1360, -0.0066]],
       device='cuda:5', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.4148, -0.8447,  0.2510, -0.6587, -0.3787], device='cuda:5',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.3362, -1.3154,  0.3367, -0.9990, -0.4197], device='cuda:4',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1025,  0.3140, -0.0565,  ...,  0.0320,  0.1869,  0.2185],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1345, -0.0565,  0.4548,  ...,  0.1141, -0.1501, -0.0043]],
       device='cuda:4', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.3362, -1.3154,  0.3367, -0.9990, -0.4197], device='cuda:4',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.2030, -1.6895,  0.3892, -1.0117, -0.4380], device='cuda:3',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1031, -0.0856,  0.1504,  ...,  0.0818,  0.0939,  0.2074],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0815,  0.0692,  0.1870,  ...,  0.0306, -0.1935,  0.0534]],
       device='cuda:3', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.2030, -1.6895,  0.3892, -1.0117, -0.4380], device='cuda:3',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.3716, -1.3682,  0.2177, -0.8779, -0.5063], device='cuda:1',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.2900,  0.2325, -0.1816,  ...,  0.0522,  0.1487,  0.1770],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0263,  0.5513,  0.7363,  ...,  0.0224, -0.0313,  0.0524]],
       device='cuda:1', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.3716, -1.3682,  0.2177, -0.8779, -0.5063], device='cuda:1',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.2722, -0.5000,  0.0618, -0.3735, -0.2179], device='cuda:2',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1315,  0.0664, -0.0646,  ...,  0.0754,  0.0835,  0.1700],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0021,  0.2043,  0.1750,  ...,  0.0231, -0.0510, -0.0015]],
       device='cuda:2', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.2722, -0.5000,  0.0618, -0.3735, -0.2179], device='cuda:2',
       dtype=torch.float16)
--------------------
--------------------
Before gen at loss:
task_net layer_norm.weight True tensor([-0.1807, -0.5884,  0.0798, -0.4414, -0.2910], device='cuda:6',
       dtype=torch.float16)
After gen at loss:
textual_encoder embed_tokens.weight True tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.1083,  0.1112, -0.0758,  ...,  0.0643,  0.1177,  0.1976],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0403,  0.1139,  0.0224,  ...,  0.0055, -0.0301,  0.0059]],
       device='cuda:6', dtype=torch.float16)
task_net layer_norm.weight True tensor([-0.1807, -0.5884,  0.0798, -0.4414, -0.2910], device='cuda:6',
       dtype=torch.float16)
--------------------
tensor([-2.5469], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-2.5469], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 11:27:10 | INFO | dev_st | epoch 002 | valid on 'dev_st' subset | loss 6.008 | trans_loss 7.931 | nll_loss 6.024 | w2v_ctc_loss 2.128 | task_loss 2.882 | task_loss_gen 35 | contrastive_loss 0 | total 3505.91 | n_correct 982.273 | ppl 65.07 | accuracy 28.018 | uer 31.027 | wer 31.69 | raw_wer 31.69 | bleu 0.18 | wps 1126 | wpb 3505.9 | bsz 119.3 | num_updates 3646 | best_bleu 0.18
2023-09-05 11:27:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 3646 updates
2023-09-05 11:27:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 11:27:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 11:27:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 2 @ 3646 updates, score 0.18) (writing took 11.326314773003105 seconds)
2023-09-05 11:27:22 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)
2023-09-05 11:27:22 | INFO | train | epoch 002 | loss 4.311 | trans_loss 5.411 | nll_loss 4.237 | w2v_ctc_loss 2.572 | task_loss 1.148 | task_loss_gen 14.881 | contrastive_loss 0 | total 3956.3 | n_correct 311.643 | ppl 18.85 | accuracy 7.877 | wps 15845.4 | ups 1.33 | wpb 11899.9 | bsz 427.3 | num_updates 3646 | lr 0.000145867 | gnorm 0.949 | clip 0.1 | loss_scale 4 | train_wall 1228 | gb_free 16.3 | wall 2559
2023-09-05 11:27:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 11:27:22 | INFO | fairseq.trainer | begin training epoch 3
2023-09-05 11:27:22 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 11:28:16 | INFO | train_inner | epoch 003:     54 / 1826 loss=2.965, trans_loss=4.282, nll_loss=2.747, w2v_ctc_loss=1.752, task_loss=1.951, task_loss_gen=9.814, contrastive_loss=0, total=3866.85, n_correct=1053.21, ppl=6.71, accuracy=27.237, wps=8014.5, ups=0.69, wpb=11625.8, bsz=401.1, num_updates=3700, lr=0.000148026, gnorm=1.054, clip=0, loss_scale=4, train_wall=85, gb_free=15.3, wall=2613
2023-09-05 11:29:41 | INFO | train_inner | epoch 003:    154 / 1826 loss=2.899, trans_loss=4.256, nll_loss=2.717, w2v_ctc_loss=1.684, task_loss=1.751, task_loss_gen=9.711, contrastive_loss=0, total=3969.28, n_correct=1118.02, ppl=6.57, accuracy=28.167, wps=13947.3, ups=1.17, wpb=11948.4, bsz=442.1, num_updates=3800, lr=0.000152024, gnorm=0.964, clip=0, loss_scale=4, train_wall=85, gb_free=16, wall=2699
2023-09-05 11:31:08 | INFO | train_inner | epoch 003:    254 / 1826 loss=2.886, trans_loss=4.253, nll_loss=2.712, w2v_ctc_loss=1.678, task_loss=1.561, task_loss_gen=10.251, contrastive_loss=0, total=3997.68, n_correct=1146.31, ppl=6.55, accuracy=28.674, wps=13959.5, ups=1.16, wpb=12022.8, bsz=439.1, num_updates=3900, lr=0.000156022, gnorm=1.024, clip=0, loss_scale=4, train_wall=85, gb_free=14.6, wall=2785
2023-09-05 11:32:33 | INFO | train_inner | epoch 003:    354 / 1826 loss=2.855, trans_loss=4.258, nll_loss=2.717, w2v_ctc_loss=1.627, task_loss=1.69, task_loss_gen=9.908, contrastive_loss=0, total=4043.01, n_correct=1153.2, ppl=6.57, accuracy=28.523, wps=14181.2, ups=1.17, wpb=12155.1, bsz=462.1, num_updates=4000, lr=0.00016002, gnorm=1.091, clip=0, loss_scale=4, train_wall=85, gb_free=12, wall=2871
2023-09-05 11:32:33 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-3.1758], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-3.1758], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 11:33:14 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.905 | trans_loss 7.802 | nll_loss 5.86 | w2v_ctc_loss 2.079 | task_loss 1.714 | task_loss_gen 44.179 | contrastive_loss 0 | total 3505.91 | n_correct 1034 | ppl 58.09 | accuracy 29.493 | uer 29.189 | wer 30.128 | raw_wer 30.128 | bleu 0.19 | wps 1117.8 | wpb 3505.9 | bsz 119.3 | num_updates 4000 | best_bleu 0.19
2023-09-05 11:33:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 4000 updates
2023-09-05 11:33:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_3_4000.pt
2023-09-05 11:33:17 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_3_4000.pt
2023-09-05 11:33:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_3_4000.pt (epoch 3 @ 4000 updates, score 0.19) (writing took 12.619501570996363 seconds)
2023-09-05 11:34:52 | INFO | train_inner | epoch 003:    454 / 1826 loss=2.883, trans_loss=4.292, nll_loss=2.761, w2v_ctc_loss=1.632, task_loss=1.884, task_loss_gen=11.25, contrastive_loss=0, total=3929.75, n_correct=1072.68, ppl=6.78, accuracy=27.296, wps=8521.1, ups=0.72, wpb=11819.9, bsz=420.9, num_updates=4100, lr=0.000164018, gnorm=1.101, clip=0, loss_scale=4, train_wall=84, gb_free=15.7, wall=3009
2023-09-05 11:36:19 | INFO | train_inner | epoch 003:    554 / 1826 loss=2.855, trans_loss=4.269, nll_loss=2.732, w2v_ctc_loss=1.621, task_loss=2.239, task_loss_gen=9.5, contrastive_loss=0, total=3965.34, n_correct=1118.17, ppl=6.64, accuracy=28.199, wps=13776.1, ups=1.15, wpb=11936.1, bsz=439.3, num_updates=4200, lr=0.000168016, gnorm=1.152, clip=0, loss_scale=4, train_wall=86, gb_free=16.5, wall=3096
2023-09-05 11:37:44 | INFO | train_inner | epoch 003:    654 / 1826 loss=2.838, trans_loss=4.278, nll_loss=2.741, w2v_ctc_loss=1.581, task_loss=1.863, task_loss_gen=9.629, contrastive_loss=0, total=4008.32, n_correct=1110.74, ppl=6.69, accuracy=27.711, wps=14175.9, ups=1.17, wpb=12065.1, bsz=445.4, num_updates=4300, lr=0.000172014, gnorm=1.122, clip=0, loss_scale=4, train_wall=84, gb_free=15.8, wall=3181
2023-09-05 11:39:10 | INFO | train_inner | epoch 003:    754 / 1826 loss=2.858, trans_loss=4.301, nll_loss=2.769, w2v_ctc_loss=1.592, task_loss=2.556, task_loss_gen=9.318, contrastive_loss=0, total=3974.64, n_correct=1087.69, ppl=6.82, accuracy=27.366, wps=13899, ups=1.16, wpb=11956.7, bsz=427.8, num_updates=4400, lr=0.000176012, gnorm=1.416, clip=0, loss_scale=4, train_wall=85, gb_free=15.8, wall=3267
2023-09-05 11:40:35 | INFO | train_inner | epoch 003:    854 / 1826 loss=2.859, trans_loss=4.345, nll_loss=2.826, w2v_ctc_loss=1.559, task_loss=2.899, task_loss_gen=9.856, contrastive_loss=0, total=3916.61, n_correct=1037.08, ppl=7.09, accuracy=26.479, wps=13748.8, ups=1.17, wpb=11772.9, bsz=405.5, num_updates=4500, lr=0.00018001, gnorm=1.426, clip=0, loss_scale=4, train_wall=85, gb_free=13.5, wall=3353
2023-09-05 11:42:01 | INFO | train_inner | epoch 003:    954 / 1826 loss=2.863, trans_loss=4.368, nll_loss=2.854, w2v_ctc_loss=1.543, task_loss=4.168, task_loss_gen=8.541, contrastive_loss=0, total=3978.17, n_correct=1022.02, ppl=7.23, accuracy=25.691, wps=14009.5, ups=1.17, wpb=11971.8, bsz=430.3, num_updates=4600, lr=0.000184008, gnorm=1.742, clip=0, loss_scale=4, train_wall=85, gb_free=12.7, wall=3438
2023-09-05 11:42:18 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 2.0
2023-09-05 11:43:27 | INFO | train_inner | epoch 003:   1055 / 1826 loss=2.88, trans_loss=4.403, nll_loss=2.9, w2v_ctc_loss=1.54, task_loss=7.763, task_loss_gen=8.91, contrastive_loss=0, total=3975.46, n_correct=989.18, ppl=7.46, accuracy=24.882, wps=13822.5, ups=1.16, wpb=11955.1, bsz=435.8, num_updates=4700, lr=0.000188006, gnorm=4.371, clip=6, loss_scale=2, train_wall=86, gb_free=15.9, wall=3525
2023-09-05 11:44:55 | INFO | train_inner | epoch 003:   1155 / 1826 loss=2.83, trans_loss=4.33, nll_loss=2.809, w2v_ctc_loss=1.509, task_loss=9.613, task_loss_gen=7.917, contrastive_loss=0, total=3900.75, n_correct=1018.89, ppl=7.01, accuracy=26.12, wps=13438.5, ups=1.15, wpb=11734.7, bsz=406.8, num_updates=4800, lr=0.000192004, gnorm=5.058, clip=7, loss_scale=2, train_wall=87, gb_free=16.8, wall=3612
2023-09-05 11:46:21 | INFO | train_inner | epoch 003:   1255 / 1826 loss=2.806, trans_loss=4.28, nll_loss=2.746, w2v_ctc_loss=1.51, task_loss=10.035, task_loss_gen=7.522, contrastive_loss=0, total=3998.99, n_correct=1084.11, ppl=6.71, accuracy=27.11, wps=14005, ups=1.16, wpb=12025, bsz=426.9, num_updates=4900, lr=0.000196002, gnorm=3.71, clip=1, loss_scale=2, train_wall=85, gb_free=17.3, wall=3698
2023-09-05 11:47:46 | INFO | train_inner | epoch 003:   1355 / 1826 loss=2.813, trans_loss=4.28, nll_loss=2.746, w2v_ctc_loss=1.501, task_loss=6.841, task_loss_gen=6.533, contrastive_loss=0, total=3937.64, n_correct=1050.22, ppl=6.71, accuracy=26.671, wps=13901.3, ups=1.17, wpb=11840.7, bsz=404.7, num_updates=5000, lr=0.0002, gnorm=2.94, clip=0, loss_scale=2, train_wall=85, gb_free=17.3, wall=3783
mt_weight tensor(0.5000)
asr_weight tensor(0.3020, device='cuda:0')
2023-09-05 11:49:11 | INFO | train_inner | epoch 003:   1455 / 1826 loss=2.784, trans_loss=4.269, nll_loss=2.731, w2v_ctc_loss=1.465, task_loss=6.868, task_loss_gen=6.118, contrastive_loss=0, total=3858.61, n_correct=1038.13, ppl=6.64, accuracy=26.904, wps=13658.3, ups=1.18, wpb=11605.5, bsz=414.8, num_updates=5100, lr=0.00019803, gnorm=2.803, clip=0, loss_scale=2, train_wall=84, gb_free=12, wall=3868
2023-09-05 11:50:36 | INFO | train_inner | epoch 003:   1555 / 1826 loss=2.757, trans_loss=4.224, nll_loss=2.679, w2v_ctc_loss=1.47, task_loss=7.392, task_loss_gen=6.093, contrastive_loss=0, total=3915.09, n_correct=1101.2, ppl=6.41, accuracy=28.127, wps=13828.4, ups=1.17, wpb=11777.3, bsz=409.1, num_updates=5200, lr=0.000196116, gnorm=2.153, clip=0, loss_scale=2, train_wall=84, gb_free=16.4, wall=3953
2023-09-05 11:52:02 | INFO | train_inner | epoch 003:   1655 / 1826 loss=2.739, trans_loss=4.216, nll_loss=2.666, w2v_ctc_loss=1.452, task_loss=4.59, task_loss_gen=4.837, contrastive_loss=0, total=3943.82, n_correct=1119.51, ppl=6.35, accuracy=28.386, wps=13815.9, ups=1.16, wpb=11862.9, bsz=431.4, num_updates=5300, lr=0.000194257, gnorm=1.556, clip=0, loss_scale=2, train_wall=85, gb_free=16.6, wall=4039
2023-09-05 11:53:27 | INFO | train_inner | epoch 003:   1755 / 1826 loss=2.716, trans_loss=4.201, nll_loss=2.648, w2v_ctc_loss=1.432, task_loss=4.695, task_loss_gen=4.914, contrastive_loss=0, total=3976.28, n_correct=1152.6, ppl=6.27, accuracy=28.987, wps=14024.7, ups=1.17, wpb=11954.2, bsz=443.5, num_updates=5400, lr=0.00019245, gnorm=1.546, clip=0, loss_scale=2, train_wall=85, gb_free=17.1, wall=4124
2023-09-05 11:54:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.3020, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.3020, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.3020, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.3020, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.3020, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.3020, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.3020, device='cuda:7')
tensor([-0.3223], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.3223], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 11:55:11 | INFO | dev_st | epoch 003 | valid on 'dev_st' subset | loss 5.81 | trans_loss 7.875 | nll_loss 5.982 | w2v_ctc_loss 1.596 | task_loss 22.338 | task_loss_gen 19.636 | contrastive_loss 0 | total 3505.91 | n_correct 1000.45 | ppl 63.23 | accuracy 28.536 | uer 24.076 | wer 25.831 | raw_wer 25.831 | bleu 0.13 | wps 1032.3 | wpb 3505.9 | bsz 119.3 | num_updates 5471 | best_bleu 0.19
2023-09-05 11:55:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 5471 updates
2023-09-05 11:55:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_0.1303.pt
Traceback (most recent call last):
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/queues.py", line 245, in _feed
    send_bytes(obj)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 200, in send_bytes
    self._send_bytes(m[offset:offset + size])
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 411, in _send_bytes
    self._send(header + buf)
  File "/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/connection.py", line 368, in _send
    n = write(self._handle, buf)
BrokenPipeError: [Errno 32] Broken pipe
2023-09-05 11:55:13 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_0.1303.pt
2023-09-05 11:55:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_0.1303.pt (epoch 3 @ 5471 updates, score 0.13) (writing took 7.163685363018885 seconds)
2023-09-05 11:55:18 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)
2023-09-05 11:55:18 | INFO | train | epoch 003 | loss 2.83 | trans_loss 4.28 | nll_loss 2.746 | w2v_ctc_loss 1.554 | task_loss 4.485 | task_loss_gen 8.23 | contrastive_loss 0 | total 3956.29 | n_correct 1085.3 | ppl 6.71 | accuracy 27.432 | wps 12957.2 | ups 1.09 | wpb 11899.9 | bsz 427.3 | num_updates 5471 | lr 0.000191197 | gnorm 2.016 | clip 0.8 | loss_scale 2 | train_wall 1552 | gb_free 17 | wall 4235
2023-09-05 11:55:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 11:55:18 | INFO | fairseq.trainer | begin training epoch 4
2023-09-05 11:55:18 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 11:55:50 | INFO | train_inner | epoch 004:     29 / 1826 loss=2.711, trans_loss=4.192, nll_loss=2.633, w2v_ctc_loss=1.429, task_loss=4.555, task_loss_gen=4.929, contrastive_loss=0, total=3985.85, n_correct=1157.07, ppl=6.21, accuracy=29.029, wps=8372.7, ups=0.7, wpb=11983.7, bsz=427.7, num_updates=5500, lr=0.000190693, gnorm=1.449, clip=0, loss_scale=2, train_wall=84, gb_free=16.4, wall=4267
2023-09-05 11:57:16 | INFO | train_inner | epoch 004:    129 / 1826 loss=2.671, trans_loss=4.184, nll_loss=2.629, w2v_ctc_loss=1.369, task_loss=4.627, task_loss_gen=5.006, contrastive_loss=0, total=3968.21, n_correct=1149.23, ppl=6.19, accuracy=28.961, wps=13955.1, ups=1.17, wpb=11944.7, bsz=429.4, num_updates=5600, lr=0.000188982, gnorm=1.55, clip=0, loss_scale=2, train_wall=85, gb_free=15.9, wall=4353
2023-09-05 11:57:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 1.0
2023-09-05 11:58:41 | INFO | train_inner | epoch 004:    230 / 1826 loss=2.757, trans_loss=4.233, nll_loss=2.686, w2v_ctc_loss=1.46, task_loss=5.147, task_loss_gen=4.928, contrastive_loss=0, total=3987.21, n_correct=1107.85, ppl=6.44, accuracy=27.785, wps=13986.8, ups=1.17, wpb=11988.4, bsz=436.4, num_updates=5700, lr=0.000187317, gnorm=3.554, clip=1, loss_scale=1, train_wall=85, gb_free=16.9, wall=4439
2023-09-05 12:00:07 | INFO | train_inner | epoch 004:    330 / 1826 loss=2.673, trans_loss=4.207, nll_loss=2.652, w2v_ctc_loss=1.346, task_loss=4.835, task_loss_gen=4.466, contrastive_loss=0, total=3978.09, n_correct=1126.5, ppl=6.29, accuracy=28.318, wps=13983.3, ups=1.17, wpb=11959.8, bsz=444.7, num_updates=5800, lr=0.000185695, gnorm=2.883, clip=0, loss_scale=1, train_wall=85, gb_free=16.1, wall=4524
2023-09-05 12:01:34 | INFO | train_inner | epoch 004:    430 / 1826 loss=2.7, trans_loss=4.226, nll_loss=2.678, w2v_ctc_loss=1.358, task_loss=5.292, task_loss_gen=4.843, contrastive_loss=0, total=3976.78, n_correct=1090.81, ppl=6.4, accuracy=27.429, wps=13797, ups=1.15, wpb=11957.6, bsz=433.3, num_updates=5900, lr=0.000184115, gnorm=3.094, clip=0, loss_scale=1, train_wall=86, gb_free=16.3, wall=4611
2023-09-05 12:03:00 | INFO | train_inner | epoch 004:    530 / 1826 loss=2.666, trans_loss=4.197, nll_loss=2.642, w2v_ctc_loss=1.345, task_loss=5.738, task_loss_gen=5.084, contrastive_loss=0, total=3925.45, n_correct=1116.71, ppl=6.24, accuracy=28.448, wps=13679.8, ups=1.16, wpb=11802.8, bsz=421.8, num_updates=6000, lr=0.000182574, gnorm=2.751, clip=0, loss_scale=1, train_wall=86, gb_free=16, wall=4697
2023-09-05 12:03:00 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.2340], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.2340], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 12:03:43 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 5.782 | trans_loss 7.828 | nll_loss 5.926 | w2v_ctc_loss 1.609 | task_loss 28.212 | task_loss_gen 19.492 | contrastive_loss 0 | total 3505.91 | n_correct 1018.55 | ppl 60.79 | accuracy 29.052 | uer 23.153 | wer 24.642 | raw_wer 24.642 | bleu 0.27 | wps 1061.8 | wpb 3505.9 | bsz 119.3 | num_updates 6000 | best_bleu 0.27
2023-09-05 12:03:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 6000 updates
2023-09-05 12:03:43 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_4_6000.pt
2023-09-05 12:03:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_4_6000.pt
2023-09-05 12:03:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_4_6000.pt (epoch 4 @ 6000 updates, score 0.27) (writing took 12.734226534957998 seconds)
2023-09-05 12:05:20 | INFO | train_inner | epoch 004:    630 / 1826 loss=2.677, trans_loss=4.206, nll_loss=2.65, w2v_ctc_loss=1.354, task_loss=4.939, task_loss_gen=4.735, contrastive_loss=0, total=4007.91, n_correct=1138.98, ppl=6.28, accuracy=28.418, wps=8607.6, ups=0.71, wpb=12044, bsz=437.6, num_updates=6100, lr=0.000181071, gnorm=2.328, clip=0, loss_scale=1, train_wall=84, gb_free=16.8, wall=4837
2023-09-05 12:06:45 | INFO | train_inner | epoch 004:    730 / 1826 loss=2.671, trans_loss=4.199, nll_loss=2.648, w2v_ctc_loss=1.344, task_loss=5.364, task_loss_gen=5.131, contrastive_loss=0, total=3919.08, n_correct=1104.66, ppl=6.27, accuracy=28.187, wps=13865.7, ups=1.17, wpb=11800.9, bsz=406.4, num_updates=6200, lr=0.000179605, gnorm=2.366, clip=0, loss_scale=1, train_wall=84, gb_free=15.1, wall=4922
2023-09-05 12:08:11 | INFO | train_inner | epoch 004:    830 / 1826 loss=2.62, trans_loss=4.144, nll_loss=2.574, w2v_ctc_loss=1.324, task_loss=4.685, task_loss_gen=4.331, contrastive_loss=0, total=4014.17, n_correct=1205, ppl=5.96, accuracy=30.019, wps=13975.3, ups=1.16, wpb=12071.2, bsz=451.3, num_updates=6300, lr=0.000178174, gnorm=2.246, clip=0, loss_scale=1, train_wall=86, gb_free=16.4, wall=5009
2023-09-05 12:09:39 | INFO | train_inner | epoch 004:    930 / 1826 loss=2.641, trans_loss=4.162, nll_loss=2.599, w2v_ctc_loss=1.342, task_loss=4.983, task_loss_gen=4.659, contrastive_loss=0, total=3970.82, n_correct=1175.67, ppl=6.06, accuracy=29.608, wps=13599.3, ups=1.14, wpb=11940, bsz=432.2, num_updates=6400, lr=0.000176777, gnorm=2.495, clip=0, loss_scale=1, train_wall=87, gb_free=16.6, wall=5096
2023-09-05 12:11:05 | INFO | train_inner | epoch 004:   1030 / 1826 loss=2.668, trans_loss=4.16, nll_loss=2.597, w2v_ctc_loss=1.376, task_loss=5.07, task_loss_gen=4.823, contrastive_loss=0, total=3921.72, n_correct=1145.85, ppl=6.05, accuracy=29.218, wps=13787.4, ups=1.17, wpb=11796.6, bsz=415.2, num_updates=6500, lr=0.000175412, gnorm=2.369, clip=1, loss_scale=1, train_wall=85, gb_free=12.5, wall=5182
2023-09-05 12:12:30 | INFO | train_inner | epoch 004:   1130 / 1826 loss=2.639, trans_loss=4.161, nll_loss=2.597, w2v_ctc_loss=1.326, task_loss=5.709, task_loss_gen=5.181, contrastive_loss=0, total=3904.68, n_correct=1141.29, ppl=6.05, accuracy=29.229, wps=13706.3, ups=1.17, wpb=11745.2, bsz=395.8, num_updates=6600, lr=0.000174078, gnorm=2.27, clip=0, loss_scale=1, train_wall=85, gb_free=16, wall=5268
2023-09-05 12:13:55 | INFO | train_inner | epoch 004:   1230 / 1826 loss=2.607, trans_loss=4.145, nll_loss=2.573, w2v_ctc_loss=1.299, task_loss=5.155, task_loss_gen=4.824, contrastive_loss=0, total=3980.69, n_correct=1193.24, ppl=5.95, accuracy=29.976, wps=14076.3, ups=1.18, wpb=11965.5, bsz=423.8, num_updates=6700, lr=0.000172774, gnorm=2.261, clip=0, loss_scale=1, train_wall=84, gb_free=14.7, wall=5353
2023-09-05 12:15:21 | INFO | train_inner | epoch 004:   1330 / 1826 loss=2.639, trans_loss=4.17, nll_loss=2.608, w2v_ctc_loss=1.322, task_loss=5.394, task_loss_gen=5.26, contrastive_loss=0, total=3850.3, n_correct=1115.35, ppl=6.1, accuracy=28.968, wps=13603.6, ups=1.17, wpb=11578.4, bsz=397.6, num_updates=6800, lr=0.000171499, gnorm=2.541, clip=0, loss_scale=1, train_wall=84, gb_free=9.2, wall=5438
2023-09-05 12:16:46 | INFO | train_inner | epoch 004:   1430 / 1826 loss=2.603, trans_loss=4.149, nll_loss=2.58, w2v_ctc_loss=1.303, task_loss=5.849, task_loss_gen=4.727, contrastive_loss=0, total=3980.52, n_correct=1199.75, ppl=5.98, accuracy=30.141, wps=13952.3, ups=1.17, wpb=11970.9, bsz=451.1, num_updates=6900, lr=0.000170251, gnorm=2.494, clip=0, loss_scale=1, train_wall=85, gb_free=16.6, wall=5524
2023-09-05 12:18:13 | INFO | train_inner | epoch 004:   1530 / 1826 loss=2.566, trans_loss=4.093, nll_loss=2.513, w2v_ctc_loss=1.29, task_loss=5.044, task_loss_gen=4.169, contrastive_loss=0, total=4014.75, n_correct=1266.18, ppl=5.71, accuracy=31.538, wps=13969, ups=1.16, wpb=12084.9, bsz=454.1, num_updates=7000, lr=0.000169031, gnorm=2.037, clip=0, loss_scale=1, train_wall=86, gb_free=17.3, wall=5610
2023-09-05 12:19:39 | INFO | train_inner | epoch 004:   1630 / 1826 loss=2.596, trans_loss=4.112, nll_loss=2.54, w2v_ctc_loss=1.306, task_loss=5.861, task_loss_gen=4.636, contrastive_loss=0, total=3950.61, n_correct=1202.55, ppl=5.82, accuracy=30.44, wps=13833.6, ups=1.16, wpb=11889.1, bsz=425.6, num_updates=7100, lr=0.000167836, gnorm=2.045, clip=0, loss_scale=1, train_wall=85, gb_free=15.6, wall=5696
2023-09-05 12:21:04 | INFO | train_inner | epoch 004:   1730 / 1826 loss=2.643, trans_loss=4.111, nll_loss=2.539, w2v_ctc_loss=1.385, task_loss=8.335, task_loss_gen=5.686, contrastive_loss=0, total=3877.11, n_correct=1186.2, ppl=5.81, accuracy=30.595, wps=13733.7, ups=1.18, wpb=11672.1, bsz=399.8, num_updates=7200, lr=0.000166667, gnorm=3.243, clip=0, loss_scale=1, train_wall=84, gb_free=16.2, wall=5781
2023-09-05 12:22:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.2644], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.2644], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 12:23:10 | INFO | dev_st | epoch 004 | valid on 'dev_st' subset | loss 5.973 | trans_loss 8.104 | nll_loss 6.286 | w2v_ctc_loss 1.622 | task_loss 25.562 | task_loss_gen 20.624 | contrastive_loss 0 | total 3505.91 | n_correct 955.727 | ppl 78.04 | accuracy 27.26 | uer 23.156 | wer 25.178 | raw_wer 25.178 | bleu 0.31 | wps 991.1 | wpb 3505.9 | bsz 119.3 | num_updates 7296 | best_bleu 0.31
2023-09-05 12:23:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 7296 updates
2023-09-05 12:23:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 12:23:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 12:23:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 4 @ 7296 updates, score 0.31) (writing took 12.551374627975747 seconds)
2023-09-05 12:23:23 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)
2023-09-05 12:23:23 | INFO | train | epoch 004 | loss 2.647 | trans_loss 4.164 | nll_loss 2.601 | w2v_ctc_loss 1.346 | task_loss 5.435 | task_loss_gen 4.824 | contrastive_loss 0 | total 3956.06 | n_correct 1162.37 | ppl 6.07 | accuracy 29.382 | wps 12889.5 | ups 1.08 | wpb 11899.2 | bsz 427.2 | num_updates 7296 | lr 0.000165567 | gnorm 2.481 | clip 0.1 | loss_scale 1 | train_wall 1552 | gb_free 16.8 | wall 5920
2023-09-05 12:23:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 12:23:23 | INFO | fairseq.trainer | begin training epoch 5
2023-09-05 12:23:23 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 12:23:33 | INFO | train_inner | epoch 005:      4 / 1826 loss=2.606, trans_loss=4.079, nll_loss=2.497, w2v_ctc_loss=1.353, task_loss=6.578, task_loss_gen=4.652, contrastive_loss=0, total=3927.9, n_correct=1243.41, ppl=5.64, accuracy=31.656, wps=7899.7, ups=0.67, wpb=11812.8, bsz=419.6, num_updates=7300, lr=0.000165521, gnorm=2.462, clip=0, loss_scale=1, train_wall=84, gb_free=16.8, wall=5931
2023-09-05 12:24:58 | INFO | train_inner | epoch 005:    104 / 1826 loss=2.546, trans_loss=4.06, nll_loss=2.472, w2v_ctc_loss=1.273, task_loss=6.201, task_loss_gen=4.439, contrastive_loss=0, total=3935.9, n_correct=1260.26, ppl=5.55, accuracy=32.02, wps=14009.3, ups=1.18, wpb=11832.5, bsz=439.9, num_updates=7400, lr=0.000164399, gnorm=2.246, clip=0, loss_scale=1, train_wall=84, gb_free=15.8, wall=6015
2023-09-05 12:26:23 | INFO | train_inner | epoch 005:    204 / 1826 loss=2.537, trans_loss=4.041, nll_loss=2.451, w2v_ctc_loss=1.28, task_loss=5.371, task_loss_gen=4.179, contrastive_loss=0, total=3985.53, n_correct=1296.79, ppl=5.47, accuracy=32.537, wps=14077.4, ups=1.17, wpb=11999.9, bsz=449.7, num_updates=7500, lr=0.000163299, gnorm=1.787, clip=0, loss_scale=1, train_wall=85, gb_free=15.9, wall=6100
2023-09-05 12:27:48 | INFO | train_inner | epoch 005:    304 / 1826 loss=2.565, trans_loss=4.048, nll_loss=2.457, w2v_ctc_loss=1.309, task_loss=5.72, task_loss_gen=4.559, contrastive_loss=0, total=3957.96, n_correct=1274.37, ppl=5.49, accuracy=32.198, wps=13934.3, ups=1.17, wpb=11899, bsz=422.9, num_updates=7600, lr=0.000162221, gnorm=1.875, clip=1, loss_scale=1, train_wall=85, gb_free=16.5, wall=6186
2023-09-05 12:29:13 | INFO | train_inner | epoch 005:    404 / 1826 loss=2.562, trans_loss=4.04, nll_loss=2.447, w2v_ctc_loss=1.314, task_loss=5.945, task_loss_gen=4.795, contrastive_loss=0, total=3937.75, n_correct=1282.74, ppl=5.45, accuracy=32.575, wps=13942.8, ups=1.18, wpb=11843.6, bsz=413.3, num_updates=7700, lr=0.000161165, gnorm=1.312, clip=0, loss_scale=2, train_wall=84, gb_free=14.9, wall=6271
2023-09-05 12:30:39 | INFO | train_inner | epoch 005:    504 / 1826 loss=2.525, trans_loss=4.018, nll_loss=2.422, w2v_ctc_loss=1.275, task_loss=5.109, task_loss_gen=4.428, contrastive_loss=0, total=3982.86, n_correct=1310.38, ppl=5.36, accuracy=32.9, wps=14025.5, ups=1.17, wpb=11992.6, bsz=446.7, num_updates=7800, lr=0.000160128, gnorm=1.036, clip=0, loss_scale=2, train_wall=85, gb_free=16, wall=6356
2023-09-05 12:32:04 | INFO | train_inner | epoch 005:    604 / 1826 loss=2.56, trans_loss=4.03, nll_loss=2.434, w2v_ctc_loss=1.32, task_loss=5.083, task_loss_gen=4.618, contrastive_loss=0, total=3964.35, n_correct=1297.44, ppl=5.4, accuracy=32.728, wps=14043.4, ups=1.18, wpb=11924.1, bsz=421.7, num_updates=7900, lr=0.000159111, gnorm=0.963, clip=0, loss_scale=2, train_wall=84, gb_free=16.3, wall=6441
2023-09-05 12:33:29 | INFO | train_inner | epoch 005:    704 / 1826 loss=2.529, trans_loss=4.02, nll_loss=2.423, w2v_ctc_loss=1.278, task_loss=5.539, task_loss_gen=4.887, contrastive_loss=0, total=3884.31, n_correct=1284.67, ppl=5.36, accuracy=33.073, wps=13651.7, ups=1.17, wpb=11686.5, bsz=407.4, num_updates=8000, lr=0.000158114, gnorm=0.929, clip=0, loss_scale=2, train_wall=85, gb_free=16.7, wall=6527
2023-09-05 12:33:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.7534], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.7534], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 12:34:11 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 5.749 | trans_loss 7.767 | nll_loss 5.852 | w2v_ctc_loss 1.636 | task_loss 17.901 | task_loss_gen 21.311 | contrastive_loss 0 | total 3505.91 | n_correct 1032.36 | ppl 57.77 | accuracy 29.446 | uer 22.976 | wer 24.886 | raw_wer 24.886 | bleu 0.34 | wps 1099.1 | wpb 3505.9 | bsz 119.3 | num_updates 8000 | best_bleu 0.34
2023-09-05 12:34:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 8000 updates
2023-09-05 12:34:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_5_8000.pt
2023-09-05 12:34:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_5_8000.pt
2023-09-05 12:34:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_5_8000.pt (epoch 5 @ 8000 updates, score 0.34) (writing took 12.888839141989592 seconds)
2023-09-05 12:35:49 | INFO | train_inner | epoch 005:    804 / 1826 loss=2.527, trans_loss=4.02, nll_loss=2.422, w2v_ctc_loss=1.271, task_loss=5.499, task_loss_gen=4.886, contrastive_loss=0, total=3876.66, n_correct=1281.51, ppl=5.36, accuracy=33.057, wps=8351.8, ups=0.72, wpb=11660.1, bsz=403.9, num_updates=8100, lr=0.000157135, gnorm=0.754, clip=0, loss_scale=2, train_wall=85, gb_free=16.1, wall=6666
2023-09-05 12:37:14 | INFO | train_inner | epoch 005:    904 / 1826 loss=2.502, trans_loss=4.007, nll_loss=2.408, w2v_ctc_loss=1.254, task_loss=4.705, task_loss_gen=4.395, contrastive_loss=0, total=4004.14, n_correct=1341.5, ppl=5.31, accuracy=33.503, wps=14120.7, ups=1.17, wpb=12054.4, bsz=445.8, num_updates=8200, lr=0.000156174, gnorm=0.695, clip=0, loss_scale=2, train_wall=85, gb_free=17.2, wall=6752
2023-09-05 12:38:40 | INFO | train_inner | epoch 005:   1004 / 1826 loss=2.515, trans_loss=4.007, nll_loss=2.405, w2v_ctc_loss=1.271, task_loss=4.975, task_loss_gen=4.763, contrastive_loss=0, total=3995.32, n_correct=1343.63, ppl=5.3, accuracy=33.63, wps=14114.9, ups=1.17, wpb=12020.1, bsz=425.6, num_updates=8300, lr=0.00015523, gnorm=0.735, clip=0, loss_scale=2, train_wall=84, gb_free=16.2, wall=6837
2023-09-05 12:40:05 | INFO | train_inner | epoch 005:   1104 / 1826 loss=2.527, trans_loss=4.002, nll_loss=2.399, w2v_ctc_loss=1.298, task_loss=4.21, task_loss_gen=4.481, contrastive_loss=0, total=3962.38, n_correct=1341.34, ppl=5.27, accuracy=33.852, wps=13943.3, ups=1.17, wpb=11917.5, bsz=447.3, num_updates=8400, lr=0.000154303, gnorm=0.803, clip=0, loss_scale=2, train_wall=85, gb_free=16.1, wall=6922
2023-09-05 12:41:32 | INFO | train_inner | epoch 005:   1204 / 1826 loss=2.525, trans_loss=4.023, nll_loss=2.424, w2v_ctc_loss=1.272, task_loss=4.708, task_loss_gen=5.022, contrastive_loss=0, total=3916.75, n_correct=1292.66, ppl=5.37, accuracy=33.003, wps=13532.2, ups=1.15, wpb=11776.4, bsz=406, num_updates=8500, lr=0.000153393, gnorm=0.73, clip=0, loss_scale=2, train_wall=86, gb_free=15.6, wall=7009
2023-09-05 12:42:58 | INFO | train_inner | epoch 005:   1304 / 1826 loss=2.48, trans_loss=4.002, nll_loss=2.396, w2v_ctc_loss=1.224, task_loss=4.198, task_loss_gen=4.444, contrastive_loss=0, total=3978.72, n_correct=1342.16, ppl=5.26, accuracy=33.733, wps=13940.2, ups=1.17, wpb=11965.4, bsz=433.5, num_updates=8600, lr=0.000152499, gnorm=0.494, clip=0, loss_scale=2, train_wall=85, gb_free=15, wall=7095
2023-09-05 12:44:23 | INFO | train_inner | epoch 005:   1404 / 1826 loss=2.48, trans_loss=4.005, nll_loss=2.399, w2v_ctc_loss=1.225, task_loss=4.448, task_loss_gen=4.416, contrastive_loss=0, total=3981.04, n_correct=1352.98, ppl=5.28, accuracy=33.986, wps=14083, ups=1.18, wpb=11963.1, bsz=429.3, num_updates=8700, lr=0.00015162, gnorm=0.57, clip=0, loss_scale=2, train_wall=84, gb_free=16.6, wall=7180
2023-09-05 12:45:48 | INFO | train_inner | epoch 005:   1504 / 1826 loss=2.476, trans_loss=3.989, nll_loss=2.381, w2v_ctc_loss=1.23, task_loss=5.011, task_loss_gen=4.736, contrastive_loss=0, total=3937.22, n_correct=1344.48, ppl=5.21, accuracy=34.148, wps=13954.4, ups=1.18, wpb=11847.9, bsz=414, num_updates=8800, lr=0.000150756, gnorm=0.547, clip=0, loss_scale=2, train_wall=84, gb_free=15.3, wall=7265
2023-09-05 12:47:14 | INFO | train_inner | epoch 005:   1604 / 1826 loss=2.468, trans_loss=3.986, nll_loss=2.376, w2v_ctc_loss=1.223, task_loss=4.457, task_loss_gen=4.486, contrastive_loss=0, total=4001.21, n_correct=1376.6, ppl=5.19, accuracy=34.405, wps=14012.1, ups=1.16, wpb=12033.7, bsz=430, num_updates=8900, lr=0.000149906, gnorm=0.562, clip=0, loss_scale=2, train_wall=85, gb_free=16.5, wall=7351
2023-09-05 12:48:39 | INFO | train_inner | epoch 005:   1704 / 1826 loss=2.457, trans_loss=3.982, nll_loss=2.368, w2v_ctc_loss=1.215, task_loss=4.944, task_loss_gen=4.377, contrastive_loss=0, total=3999.89, n_correct=1393.72, ppl=5.16, accuracy=34.844, wps=14160.3, ups=1.18, wpb=12016.4, bsz=431.6, num_updates=9000, lr=0.000149071, gnorm=0.639, clip=0, loss_scale=2, train_wall=84, gb_free=15.5, wall=7436
2023-09-05 12:50:05 | INFO | train_inner | epoch 005:   1804 / 1826 loss=2.429, trans_loss=3.954, nll_loss=2.334, w2v_ctc_loss=1.199, task_loss=4.657, task_loss_gen=4.257, contrastive_loss=0, total=3997.09, n_correct=1420, ppl=5.04, accuracy=35.526, wps=13971.5, ups=1.16, wpb=12018.6, bsz=442.5, num_updates=9100, lr=0.00014825, gnorm=0.602, clip=0, loss_scale=2, train_wall=85, gb_free=16, wall=7522
2023-09-05 12:50:23 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.4880], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.4880], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 12:51:04 | INFO | dev_st | epoch 005 | valid on 'dev_st' subset | loss 5.342 | trans_loss 7.247 | nll_loss 5.154 | w2v_ctc_loss 1.451 | task_loss 17.82 | task_loss_gen 20.03 | contrastive_loss 0 | total 3505.91 | n_correct 1252.55 | ppl 35.6 | accuracy 35.727 | uer 22.254 | wer 24.18 | raw_wer 24.18 | bleu 1.39 | wps 1109.6 | wpb 3505.9 | bsz 119.3 | num_updates 9122 | best_bleu 1.39
2023-09-05 12:51:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 9122 updates
2023-09-05 12:51:04 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 12:51:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 12:51:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 5 @ 9122 updates, score 1.39) (writing took 14.20231226598844 seconds)
2023-09-05 12:51:18 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)
2023-09-05 12:51:18 | INFO | train | epoch 005 | loss 2.511 | trans_loss 4.012 | nll_loss 2.411 | w2v_ctc_loss 1.262 | task_loss 5.056 | task_loss_gen 4.566 | contrastive_loss 0 | total 3956.37 | n_correct 1323.24 | ppl 5.32 | accuracy 33.446 | wps 12967.3 | ups 1.09 | wpb 11900.1 | bsz 427.2 | num_updates 9122 | lr 0.000148071 | gnorm 0.961 | clip 0.1 | loss_scale 2 | train_wall 1546 | gb_free 17.1 | wall 7596
2023-09-05 12:51:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 12:51:19 | INFO | fairseq.trainer | begin training epoch 6
2023-09-05 12:51:19 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 12:52:32 | INFO | train_inner | epoch 006:     78 / 1826 loss=2.399, trans_loss=3.935, nll_loss=2.31, w2v_ctc_loss=1.165, task_loss=5.195, task_loss_gen=4.541, contrastive_loss=0, total=3898.11, n_correct=1404.93, ppl=4.96, accuracy=36.041, wps=7954.7, ups=0.68, wpb=11726.7, bsz=413, num_updates=9200, lr=0.000147442, gnorm=0.608, clip=0, loss_scale=2, train_wall=84, gb_free=15.7, wall=7669
2023-09-05 12:53:59 | INFO | train_inner | epoch 006:    178 / 1826 loss=2.402, trans_loss=3.92, nll_loss=2.289, w2v_ctc_loss=1.184, task_loss=5.758, task_loss_gen=4.76, contrastive_loss=0, total=3953.57, n_correct=1456.8, ppl=4.89, accuracy=36.848, wps=13753, ups=1.16, wpb=11890.2, bsz=415, num_updates=9300, lr=0.000146647, gnorm=0.698, clip=0, loss_scale=2, train_wall=86, gb_free=17.2, wall=7756
2023-09-05 12:55:24 | INFO | train_inner | epoch 006:    278 / 1826 loss=2.396, trans_loss=3.896, nll_loss=2.26, w2v_ctc_loss=1.2, task_loss=5.231, task_loss_gen=4.691, contrastive_loss=0, total=3921.02, n_correct=1478.56, ppl=4.79, accuracy=37.709, wps=13880.1, ups=1.18, wpb=11798.2, bsz=410.9, num_updates=9400, lr=0.000145865, gnorm=0.765, clip=1, loss_scale=2, train_wall=84, gb_free=16.8, wall=7841
2023-09-05 12:56:50 | INFO | train_inner | epoch 006:    378 / 1826 loss=2.359, trans_loss=3.852, nll_loss=2.203, w2v_ctc_loss=1.181, task_loss=4.997, task_loss_gen=4.44, contrastive_loss=0, total=3977.69, n_correct=1561.78, ppl=4.61, accuracy=39.263, wps=13877.6, ups=1.16, wpb=11972.5, bsz=437.6, num_updates=9500, lr=0.000145095, gnorm=0.611, clip=0, loss_scale=2, train_wall=86, gb_free=14.4, wall=7927
2023-09-05 12:58:15 | INFO | train_inner | epoch 006:    478 / 1826 loss=2.337, trans_loss=3.82, nll_loss=2.161, w2v_ctc_loss=1.187, task_loss=5.538, task_loss_gen=4.674, contrastive_loss=0, total=3925.99, n_correct=1603.65, ppl=4.47, accuracy=40.847, wps=13790, ups=1.17, wpb=11811.9, bsz=412.5, num_updates=9600, lr=0.000144338, gnorm=0.654, clip=0, loss_scale=2, train_wall=85, gb_free=17.4, wall=8013
2023-09-05 12:59:42 | INFO | train_inner | epoch 006:    578 / 1826 loss=2.312, trans_loss=3.778, nll_loss=2.104, w2v_ctc_loss=1.197, task_loss=5.235, task_loss_gen=4.457, contrastive_loss=0, total=3961.06, n_correct=1706.48, ppl=4.3, accuracy=43.081, wps=13794.4, ups=1.16, wpb=11910, bsz=424.8, num_updates=9700, lr=0.000143592, gnorm=0.614, clip=0, loss_scale=2, train_wall=86, gb_free=17.6, wall=8099
2023-09-05 13:01:07 | INFO | train_inner | epoch 006:    678 / 1826 loss=2.261, trans_loss=3.7, nll_loss=2.004, w2v_ctc_loss=1.197, task_loss=4.605, task_loss_gen=4.433, contrastive_loss=0, total=3962.11, n_correct=1835.29, ppl=4.01, accuracy=46.321, wps=14040.7, ups=1.18, wpb=11922.3, bsz=431.9, num_updates=9800, lr=0.000142857, gnorm=0.472, clip=0, loss_scale=4, train_wall=84, gb_free=16.9, wall=8184
2023-09-05 13:02:32 | INFO | train_inner | epoch 006:    778 / 1826 loss=2.23, trans_loss=3.651, nll_loss=1.939, w2v_ctc_loss=1.205, task_loss=4.519, task_loss_gen=4.624, contrastive_loss=0, total=3994.23, n_correct=1945.26, ppl=3.83, accuracy=48.702, wps=14032.2, ups=1.17, wpb=12010.9, bsz=435.9, num_updates=9900, lr=0.000142134, gnorm=0.502, clip=0, loss_scale=4, train_wall=85, gb_free=15, wall=8270
2023-09-05 13:03:57 | INFO | train_inner | epoch 006:    878 / 1826 loss=2.214, trans_loss=3.61, nll_loss=1.887, w2v_ctc_loss=1.222, task_loss=4.274, task_loss_gen=4.552, contrastive_loss=0, total=3963.99, n_correct=1994.58, ppl=3.7, accuracy=50.317, wps=14017.5, ups=1.18, wpb=11928, bsz=434.2, num_updates=10000, lr=0.000141421, gnorm=0.618, clip=0, loss_scale=4, train_wall=84, gb_free=17.1, wall=8355
2023-09-05 13:03:57 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-1.7559], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.7559], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 13:04:39 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.483 | trans_loss 5.899 | nll_loss 3.39 | w2v_ctc_loss 1.632 | task_loss 8.887 | task_loss_gen 25.92 | contrastive_loss 0 | total 3505.91 | n_correct 1951.55 | ppl 10.48 | accuracy 55.664 | uer 21.079 | wer 22.855 | raw_wer 22.855 | bleu 17.07 | wps 1082 | wpb 3505.9 | bsz 119.3 | num_updates 10000 | best_bleu 17.07
2023-09-05 13:04:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10000 updates
2023-09-05 13:04:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_6_10000.pt
2023-09-05 13:04:41 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_6_10000.pt
2023-09-05 13:04:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_6_10000.pt (epoch 6 @ 10000 updates, score 17.07) (writing took 13.640487420023419 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:0')
2023-09-05 13:06:18 | INFO | train_inner | epoch 006:    978 / 1826 loss=2.187, trans_loss=3.577, nll_loss=1.841, w2v_ctc_loss=1.22, task_loss=4.212, task_loss_gen=4.392, contrastive_loss=0, total=4004.17, n_correct=2084.09, ppl=3.58, accuracy=52.048, wps=8548.8, ups=0.71, wpb=12032, bsz=444.8, num_updates=10100, lr=0.00014072, gnorm=0.477, clip=0, loss_scale=4, train_wall=85, gb_free=16.3, wall=8495
2023-09-05 13:07:43 | INFO | train_inner | epoch 006:   1078 / 1826 loss=2.163, trans_loss=3.538, nll_loss=1.794, w2v_ctc_loss=1.22, task_loss=4.42, task_loss_gen=4.712, contrastive_loss=0, total=3952.95, n_correct=2110.88, ppl=3.47, accuracy=53.4, wps=13955, ups=1.17, wpb=11893.5, bsz=422.9, num_updates=10200, lr=0.000140028, gnorm=0.488, clip=0, loss_scale=4, train_wall=85, gb_free=16.1, wall=8581
2023-09-05 13:09:08 | INFO | train_inner | epoch 006:   1178 / 1826 loss=2.144, trans_loss=3.518, nll_loss=1.769, w2v_ctc_loss=1.213, task_loss=4.516, task_loss_gen=4.745, contrastive_loss=0, total=3919.76, n_correct=2127.36, ppl=3.41, accuracy=54.273, wps=13884.1, ups=1.18, wpb=11794.6, bsz=423.3, num_updates=10300, lr=0.000139347, gnorm=0.498, clip=0, loss_scale=4, train_wall=84, gb_free=15.3, wall=8666
2023-09-05 13:10:33 | INFO | train_inner | epoch 006:   1278 / 1826 loss=2.131, trans_loss=3.489, nll_loss=1.731, w2v_ctc_loss=1.218, task_loss=4.26, task_loss_gen=4.572, contrastive_loss=0, total=3957.79, n_correct=2189.98, ppl=3.32, accuracy=55.333, wps=14135.6, ups=1.19, wpb=11907.5, bsz=441.4, num_updates=10400, lr=0.000138675, gnorm=0.475, clip=0, loss_scale=4, train_wall=84, gb_free=17.4, wall=8750
2023-09-05 13:11:59 | INFO | train_inner | epoch 006:   1378 / 1826 loss=2.13, trans_loss=3.487, nll_loss=1.726, w2v_ctc_loss=1.22, task_loss=4.829, task_loss_gen=5.062, contrastive_loss=0, total=3937.9, n_correct=2192.82, ppl=3.31, accuracy=55.685, wps=13772.5, ups=1.16, wpb=11838.1, bsz=409.6, num_updates=10500, lr=0.000138013, gnorm=0.494, clip=0, loss_scale=4, train_wall=85, gb_free=16.4, wall=8836
2023-09-05 13:13:24 | INFO | train_inner | epoch 006:   1478 / 1826 loss=2.114, trans_loss=3.473, nll_loss=1.708, w2v_ctc_loss=1.213, task_loss=4.423, task_loss_gen=4.79, contrastive_loss=0, total=3946.83, n_correct=2228.73, ppl=3.27, accuracy=56.469, wps=13849.6, ups=1.17, wpb=11859.6, bsz=426.1, num_updates=10600, lr=0.000137361, gnorm=0.481, clip=0, loss_scale=4, train_wall=85, gb_free=16.1, wall=8921
2023-09-05 13:14:51 | INFO | train_inner | epoch 006:   1578 / 1826 loss=2.108, trans_loss=3.466, nll_loss=1.701, w2v_ctc_loss=1.211, task_loss=4.565, task_loss_gen=4.955, contrastive_loss=0, total=3944.52, n_correct=2241.71, ppl=3.25, accuracy=56.831, wps=13720.7, ups=1.16, wpb=11859.3, bsz=421.2, num_updates=10700, lr=0.000136717, gnorm=0.495, clip=0, loss_scale=4, train_wall=86, gb_free=15.5, wall=9008
2023-09-05 13:16:16 | INFO | train_inner | epoch 006:   1678 / 1826 loss=2.075, trans_loss=3.441, nll_loss=1.669, w2v_ctc_loss=1.187, task_loss=4.323, task_loss_gen=4.591, contrastive_loss=0, total=3973.29, n_correct=2297.9, ppl=3.18, accuracy=57.834, wps=14048.5, ups=1.18, wpb=11948.7, bsz=433.7, num_updates=10800, lr=0.000136083, gnorm=0.469, clip=0, loss_scale=4, train_wall=84, gb_free=17.1, wall=9093
2023-09-05 13:17:40 | INFO | train_inner | epoch 006:   1778 / 1826 loss=2.09, trans_loss=3.441, nll_loss=1.672, w2v_ctc_loss=1.21, task_loss=4.21, task_loss_gen=4.767, contrastive_loss=0, total=3956.46, n_correct=2285.59, ppl=3.19, accuracy=57.769, wps=14171.1, ups=1.19, wpb=11909.1, bsz=429.1, num_updates=10900, lr=0.000135457, gnorm=0.473, clip=0, loss_scale=4, train_wall=83, gb_free=15.4, wall=9177
2023-09-05 13:18:20 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:2')
tensor([-0.4250], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.4250], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 13:19:00 | INFO | dev_st | epoch 006 | valid on 'dev_st' subset | loss 4.1 | trans_loss 5.408 | nll_loss 2.76 | w2v_ctc_loss 1.462 | task_loss 29.58 | task_loss_gen 19.975 | contrastive_loss 0 | total 3505.91 | n_correct 2205.82 | ppl 6.77 | accuracy 62.917 | uer 20.325 | wer 22.097 | raw_wer 22.097 | bleu 24.96 | wps 1178.1 | wpb 3505.9 | bsz 119.3 | num_updates 10948 | best_bleu 24.96
2023-09-05 13:19:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 10948 updates
2023-09-05 13:19:00 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 13:19:06 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 13:19:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 6 @ 10948 updates, score 24.96) (writing took 10.223671305982862 seconds)
2023-09-05 13:19:10 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)
2023-09-05 13:19:10 | INFO | train | epoch 006 | loss 2.218 | trans_loss 3.635 | nll_loss 1.919 | w2v_ctc_loss 1.203 | task_loss 4.696 | task_loss_gen 4.648 | contrastive_loss 0 | total 3956.37 | n_correct 1947.79 | ppl 3.78 | accuracy 49.232 | wps 12997.8 | ups 1.09 | wpb 11900.1 | bsz 427.2 | num_updates 10948 | lr 0.00013516 | gnorm 0.547 | clip 0.1 | loss_scale 4 | train_wall 1547 | gb_free 16.6 | wall 9267
2023-09-05 13:19:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 13:19:10 | INFO | fairseq.trainer | begin training epoch 7
2023-09-05 13:19:10 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 13:20:02 | INFO | train_inner | epoch 007:     52 / 1826 loss=2.07, trans_loss=3.42, nll_loss=1.645, w2v_ctc_loss=1.193, task_loss=4.453, task_loss_gen=4.825, contrastive_loss=0, total=3931.13, n_correct=2295.39, ppl=3.13, accuracy=58.39, wps=8320.1, ups=0.7, wpb=11831.4, bsz=427.9, num_updates=11000, lr=0.00013484, gnorm=0.498, clip=0, loss_scale=4, train_wall=84, gb_free=16.2, wall=9319
2023-09-05 13:21:27 | INFO | train_inner | epoch 007:    152 / 1826 loss=2.037, trans_loss=3.409, nll_loss=1.628, w2v_ctc_loss=1.157, task_loss=4.332, task_loss_gen=4.668, contrastive_loss=0, total=3953.01, n_correct=2325.47, ppl=3.09, accuracy=58.828, wps=13987.4, ups=1.18, wpb=11890.5, bsz=431.4, num_updates=11100, lr=0.000134231, gnorm=0.515, clip=0, loss_scale=4, train_wall=84, gb_free=12.7, wall=9404
2023-09-05 13:22:53 | INFO | train_inner | epoch 007:    252 / 1826 loss=2.056, trans_loss=3.412, nll_loss=1.63, w2v_ctc_loss=1.18, task_loss=4.952, task_loss_gen=5.339, contrastive_loss=0, total=3898.38, n_correct=2293.82, ppl=3.1, accuracy=58.84, wps=13672.6, ups=1.17, wpb=11720.9, bsz=401.6, num_updates=11200, lr=0.000133631, gnorm=0.501, clip=0, loss_scale=4, train_wall=85, gb_free=15.6, wall=9490
2023-09-05 13:24:17 | INFO | train_inner | epoch 007:    352 / 1826 loss=2.026, trans_loss=3.4, nll_loss=1.619, w2v_ctc_loss=1.149, task_loss=3.982, task_loss_gen=4.389, contrastive_loss=0, total=4034.29, n_correct=2398.68, ppl=3.07, accuracy=59.457, wps=14339.4, ups=1.18, wpb=12139.1, bsz=466.5, num_updates=11300, lr=0.000133038, gnorm=0.459, clip=0, loss_scale=4, train_wall=84, gb_free=16.9, wall=9575
2023-09-05 13:25:41 | INFO | train_inner | epoch 007:    452 / 1826 loss=2.027, trans_loss=3.394, nll_loss=1.611, w2v_ctc_loss=1.158, task_loss=3.981, task_loss_gen=4.643, contrastive_loss=0, total=3989.92, n_correct=2374.42, ppl=3.06, accuracy=59.51, wps=14305.5, ups=1.19, wpb=12006.9, bsz=444.5, num_updates=11400, lr=0.000132453, gnorm=0.483, clip=0, loss_scale=4, train_wall=83, gb_free=15.2, wall=9659
2023-09-05 13:27:06 | INFO | train_inner | epoch 007:    552 / 1826 loss=2.031, trans_loss=3.385, nll_loss=1.598, w2v_ctc_loss=1.168, task_loss=4.116, task_loss_gen=4.712, contrastive_loss=0, total=3952.57, n_correct=2369.53, ppl=3.03, accuracy=59.949, wps=14055.4, ups=1.18, wpb=11886.2, bsz=433.2, num_updates=11500, lr=0.000131876, gnorm=0.478, clip=0, loss_scale=4, train_wall=84, gb_free=16.8, wall=9743
2023-09-05 13:28:31 | INFO | train_inner | epoch 007:    652 / 1826 loss=2.02, trans_loss=3.384, nll_loss=1.597, w2v_ctc_loss=1.153, task_loss=4.28, task_loss_gen=4.965, contrastive_loss=0, total=3942.35, n_correct=2362.64, ppl=3.03, accuracy=59.93, wps=13847.6, ups=1.17, wpb=11857.5, bsz=423.7, num_updates=11600, lr=0.000131306, gnorm=0.479, clip=0, loss_scale=4, train_wall=85, gb_free=16.8, wall=9829
2023-09-05 13:29:56 | INFO | train_inner | epoch 007:    752 / 1826 loss=2.011, trans_loss=3.383, nll_loss=1.596, w2v_ctc_loss=1.143, task_loss=4.078, task_loss_gen=4.865, contrastive_loss=0, total=3949.88, n_correct=2379.43, ppl=3.02, accuracy=60.241, wps=14044.4, ups=1.18, wpb=11876.3, bsz=431.9, num_updates=11700, lr=0.000130744, gnorm=0.464, clip=0, loss_scale=4, train_wall=84, gb_free=15.5, wall=9913
2023-09-05 13:31:22 | INFO | train_inner | epoch 007:    852 / 1826 loss=2.022, trans_loss=3.379, nll_loss=1.59, w2v_ctc_loss=1.163, task_loss=4.497, task_loss_gen=5.349, contrastive_loss=0, total=3930.38, n_correct=2364.88, ppl=3.01, accuracy=60.169, wps=13761.6, ups=1.16, wpb=11820.2, bsz=404.7, num_updates=11800, lr=0.000130189, gnorm=0.457, clip=0, loss_scale=8, train_wall=85, gb_free=11.3, wall=9999
2023-09-05 13:32:47 | INFO | train_inner | epoch 007:    952 / 1826 loss=2.007, trans_loss=3.366, nll_loss=1.575, w2v_ctc_loss=1.15, task_loss=3.455, task_loss_gen=4.943, contrastive_loss=0, total=4048.82, n_correct=2459.11, ppl=2.98, accuracy=60.736, wps=14318.4, ups=1.18, wpb=12177.2, bsz=443.4, num_updates=11900, lr=0.000129641, gnorm=0.414, clip=0, loss_scale=8, train_wall=84, gb_free=15.9, wall=10084
2023-09-05 13:34:12 | INFO | train_inner | epoch 007:   1052 / 1826 loss=2, trans_loss=3.358, nll_loss=1.564, w2v_ctc_loss=1.147, task_loss=3.546, task_loss_gen=5.209, contrastive_loss=0, total=3928.57, n_correct=2395.21, ppl=2.96, accuracy=60.969, wps=13893.8, ups=1.18, wpb=11816.1, bsz=424.5, num_updates=12000, lr=0.000129099, gnorm=0.421, clip=0, loss_scale=8, train_wall=84, gb_free=13.8, wall=10169
2023-09-05 13:34:12 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-1.0107], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.0107], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 13:34:52 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 3.986 | trans_loss 5.272 | nll_loss 2.584 | w2v_ctc_loss 1.391 | task_loss 15.294 | task_loss_gen 21.817 | contrastive_loss 0 | total 3505.91 | n_correct 2275.18 | ppl 6 | accuracy 64.896 | uer 19.496 | wer 21.208 | raw_wer 21.208 | bleu 27.14 | wps 1179.8 | wpb 3505.9 | bsz 119.3 | num_updates 12000 | best_bleu 27.14
2023-09-05 13:34:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12000 updates
2023-09-05 13:34:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_7_12000.pt
2023-09-05 13:34:54 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_7_12000.pt
2023-09-05 13:35:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_7_12000.pt (epoch 7 @ 12000 updates, score 27.14) (writing took 11.782499342982192 seconds)
2023-09-05 13:36:30 | INFO | train_inner | epoch 007:   1152 / 1826 loss=2.021, trans_loss=3.366, nll_loss=1.576, w2v_ctc_loss=1.174, task_loss=3.798, task_loss_gen=5.728, contrastive_loss=0, total=3932.31, n_correct=2387.89, ppl=2.98, accuracy=60.725, wps=8573.9, ups=0.72, wpb=11834.7, bsz=408.4, num_updates=12100, lr=0.000128565, gnorm=0.424, clip=0, loss_scale=8, train_wall=85, gb_free=15.6, wall=10307
2023-09-05 13:37:56 | INFO | train_inner | epoch 007:   1252 / 1826 loss=2, trans_loss=3.361, nll_loss=1.57, w2v_ctc_loss=1.145, task_loss=3.502, task_loss_gen=5.221, contrastive_loss=0, total=3998.13, n_correct=2438.07, ppl=2.97, accuracy=60.98, wps=14056.4, ups=1.17, wpb=12030.8, bsz=433.4, num_updates=12200, lr=0.000128037, gnorm=0.413, clip=0, loss_scale=8, train_wall=85, gb_free=15.9, wall=10393
2023-09-05 13:39:21 | INFO | train_inner | epoch 007:   1352 / 1826 loss=2.002, trans_loss=3.358, nll_loss=1.565, w2v_ctc_loss=1.153, task_loss=3.658, task_loss_gen=5.51, contrastive_loss=0, total=3909.99, n_correct=2384.28, ppl=2.96, accuracy=60.979, wps=13829.2, ups=1.18, wpb=11765, bsz=411.2, num_updates=12300, lr=0.000127515, gnorm=0.421, clip=0, loss_scale=8, train_wall=84, gb_free=15.8, wall=10478
2023-09-05 13:40:46 | INFO | train_inner | epoch 007:   1452 / 1826 loss=2, trans_loss=3.361, nll_loss=1.566, w2v_ctc_loss=1.15, task_loss=3.475, task_loss_gen=5.099, contrastive_loss=0, total=3964.82, n_correct=2421.61, ppl=2.96, accuracy=61.077, wps=13902.5, ups=1.17, wpb=11915.6, bsz=435.9, num_updates=12400, lr=0.000127, gnorm=0.421, clip=0, loss_scale=8, train_wall=85, gb_free=17.5, wall=10564
2023-09-05 13:42:12 | INFO | train_inner | epoch 007:   1552 / 1826 loss=1.993, trans_loss=3.354, nll_loss=1.561, w2v_ctc_loss=1.142, task_loss=3.657, task_loss_gen=5.502, contrastive_loss=0, total=3929.01, n_correct=2406.37, ppl=2.95, accuracy=61.246, wps=13893.2, ups=1.18, wpb=11818.4, bsz=413.4, num_updates=12500, lr=0.000126491, gnorm=0.427, clip=0, loss_scale=8, train_wall=84, gb_free=11.7, wall=10649
2023-09-05 13:43:37 | INFO | train_inner | epoch 007:   1652 / 1826 loss=1.975, trans_loss=3.352, nll_loss=1.556, w2v_ctc_loss=1.119, task_loss=3.276, task_loss_gen=5.072, contrastive_loss=0, total=3986.3, n_correct=2447.78, ppl=2.94, accuracy=61.405, wps=14001.5, ups=1.17, wpb=11985.1, bsz=449.1, num_updates=12600, lr=0.000125988, gnorm=0.415, clip=0, loss_scale=8, train_wall=85, gb_free=16, wall=10734
2023-09-05 13:45:03 | INFO | train_inner | epoch 007:   1752 / 1826 loss=1.995, trans_loss=3.355, nll_loss=1.56, w2v_ctc_loss=1.146, task_loss=3.566, task_loss_gen=5.249, contrastive_loss=0, total=3979.89, n_correct=2439.39, ppl=2.95, accuracy=61.293, wps=13988.2, ups=1.17, wpb=11965.7, bsz=429.8, num_updates=12700, lr=0.000125491, gnorm=0.422, clip=0, loss_scale=8, train_wall=85, gb_free=17.4, wall=10820
2023-09-05 13:46:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.5596], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.5596], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 13:46:45 | INFO | dev_st | epoch 007 | valid on 'dev_st' subset | loss 3.96 | trans_loss 5.231 | nll_loss 2.532 | w2v_ctc_loss 1.397 | task_loss 23.552 | task_loss_gen 20.024 | contrastive_loss 0 | total 3505.91 | n_correct 2299.73 | ppl 5.79 | accuracy 65.596 | uer 19.214 | wer 20.885 | raw_wer 20.885 | bleu 27.75 | wps 1165.8 | wpb 3505.9 | bsz 119.3 | num_updates 12774 | best_bleu 27.75
2023-09-05 13:46:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 12774 updates
2023-09-05 13:46:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 13:46:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 13:46:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 7 @ 12774 updates, score 27.75) (writing took 12.691783497983124 seconds)
2023-09-05 13:46:58 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)
2023-09-05 13:46:58 | INFO | train | epoch 007 | loss 2.014 | trans_loss 3.375 | nll_loss 1.586 | w2v_ctc_loss 1.153 | task_loss 3.912 | task_loss_gen 5.092 | contrastive_loss 0 | total 3956.37 | n_correct 2388.31 | ppl 3 | accuracy 60.366 | wps 13032.1 | ups 1.1 | wpb 11900.1 | bsz 427.2 | num_updates 12774 | lr 0.000125127 | gnorm 0.448 | clip 0 | loss_scale 8 | train_wall 1543 | gb_free 16.5 | wall 10935
2023-09-05 13:46:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 13:46:58 | INFO | fairseq.trainer | begin training epoch 8
2023-09-05 13:46:58 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 13:47:28 | INFO | train_inner | epoch 008:     26 / 1826 loss=1.99, trans_loss=3.343, nll_loss=1.547, w2v_ctc_loss=1.144, task_loss=4.115, task_loss_gen=5.703, contrastive_loss=0, total=3885.49, n_correct=2391.08, ppl=2.92, accuracy=61.539, wps=8030, ups=0.69, wpb=11693.8, bsz=401.5, num_updates=12800, lr=0.000125, gnorm=0.439, clip=0, loss_scale=8, train_wall=84, gb_free=9.1, wall=10966
2023-09-05 13:48:54 | INFO | train_inner | epoch 008:    126 / 1826 loss=1.959, trans_loss=3.323, nll_loss=1.522, w2v_ctc_loss=1.114, task_loss=4.182, task_loss_gen=5.177, contrastive_loss=0, total=3955.57, n_correct=2462.96, ppl=2.87, accuracy=62.266, wps=13967.1, ups=1.17, wpb=11904.6, bsz=420.5, num_updates=12900, lr=0.000124515, gnorm=0.425, clip=0, loss_scale=8, train_wall=85, gb_free=16.9, wall=11051
2023-09-05 13:50:19 | INFO | train_inner | epoch 008:    226 / 1826 loss=1.945, trans_loss=3.318, nll_loss=1.516, w2v_ctc_loss=1.097, task_loss=3.761, task_loss_gen=5.322, contrastive_loss=0, total=3921.96, n_correct=2447.04, ppl=2.86, accuracy=62.393, wps=13795.8, ups=1.17, wpb=11805, bsz=428, num_updates=13000, lr=0.000124035, gnorm=0.423, clip=0, loss_scale=8, train_wall=85, gb_free=15.9, wall=11136
2023-09-05 13:51:44 | INFO | train_inner | epoch 008:    326 / 1826 loss=1.953, trans_loss=3.324, nll_loss=1.522, w2v_ctc_loss=1.103, task_loss=4.072, task_loss_gen=5.519, contrastive_loss=0, total=3887.36, n_correct=2419.44, ppl=2.87, accuracy=62.239, wps=13772.1, ups=1.18, wpb=11693.5, bsz=406.4, num_updates=13100, lr=0.00012356, gnorm=0.426, clip=0, loss_scale=8, train_wall=84, gb_free=11.6, wall=11221
2023-09-05 13:53:10 | INFO | train_inner | epoch 008:    426 / 1826 loss=1.954, trans_loss=3.322, nll_loss=1.519, w2v_ctc_loss=1.104, task_loss=3.997, task_loss_gen=5.432, contrastive_loss=0, total=3929.14, n_correct=2451.41, ppl=2.87, accuracy=62.39, wps=13787.6, ups=1.17, wpb=11818.2, bsz=411.9, num_updates=13200, lr=0.000123091, gnorm=0.421, clip=0, loss_scale=8, train_wall=85, gb_free=15.6, wall=11307
2023-09-05 13:54:37 | INFO | train_inner | epoch 008:    526 / 1826 loss=1.959, trans_loss=3.321, nll_loss=1.519, w2v_ctc_loss=1.12, task_loss=3.81, task_loss_gen=5.357, contrastive_loss=0, total=3997.46, n_correct=2499.33, ppl=2.87, accuracy=62.523, wps=13850.5, ups=1.15, wpb=12027.4, bsz=435.5, num_updates=13300, lr=0.000122628, gnorm=0.424, clip=0, loss_scale=8, train_wall=86, gb_free=15.5, wall=11394
2023-09-05 13:56:02 | INFO | train_inner | epoch 008:    626 / 1826 loss=1.946, trans_loss=3.321, nll_loss=1.518, w2v_ctc_loss=1.101, task_loss=3.392, task_loss_gen=4.793, contrastive_loss=0, total=4026.89, n_correct=2516.59, ppl=2.86, accuracy=62.495, wps=14109, ups=1.16, wpb=12112.3, bsz=460.2, num_updates=13400, lr=0.000122169, gnorm=0.415, clip=0, loss_scale=8, train_wall=85, gb_free=16.5, wall=11480
2023-09-05 13:57:27 | INFO | train_inner | epoch 008:    726 / 1826 loss=1.945, trans_loss=3.324, nll_loss=1.521, w2v_ctc_loss=1.101, task_loss=3.709, task_loss_gen=5.358, contrastive_loss=0, total=3935.44, n_correct=2462.1, ppl=2.87, accuracy=62.562, wps=14003.9, ups=1.18, wpb=11831.7, bsz=421.4, num_updates=13500, lr=0.000121716, gnorm=0.419, clip=0, loss_scale=8, train_wall=84, gb_free=15.3, wall=11564
2023-09-05 13:58:52 | INFO | train_inner | epoch 008:    826 / 1826 loss=1.948, trans_loss=3.32, nll_loss=1.514, w2v_ctc_loss=1.101, task_loss=3.539, task_loss_gen=5.178, contrastive_loss=0, total=3996.46, n_correct=2506.58, ppl=2.86, accuracy=62.72, wps=14069.1, ups=1.17, wpb=12007.6, bsz=432.5, num_updates=13600, lr=0.000121268, gnorm=0.413, clip=0, loss_scale=8, train_wall=85, gb_free=16.5, wall=11650
2023-09-05 14:00:17 | INFO | train_inner | epoch 008:    926 / 1826 loss=1.95, trans_loss=3.312, nll_loss=1.506, w2v_ctc_loss=1.11, task_loss=3.676, task_loss_gen=5.667, contrastive_loss=0, total=3885.41, n_correct=2439.8, ppl=2.84, accuracy=62.794, wps=13731, ups=1.17, wpb=11686, bsz=406.3, num_updates=13700, lr=0.000120824, gnorm=0.416, clip=0, loss_scale=8, train_wall=85, gb_free=11.5, wall=11735
2023-09-05 14:01:43 | INFO | train_inner | epoch 008:   1026 / 1826 loss=1.954, trans_loss=3.309, nll_loss=1.504, w2v_ctc_loss=1.119, task_loss=3.651, task_loss_gen=5.58, contrastive_loss=0, total=3950.23, n_correct=2484.34, ppl=2.84, accuracy=62.891, wps=13919.8, ups=1.17, wpb=11884.1, bsz=413.4, num_updates=13800, lr=0.000120386, gnorm=0.419, clip=0, loss_scale=16, train_wall=85, gb_free=15.7, wall=11820
2023-09-05 14:03:08 | INFO | train_inner | epoch 008:   1126 / 1826 loss=1.931, trans_loss=3.306, nll_loss=1.499, w2v_ctc_loss=1.089, task_loss=3.192, task_loss_gen=5.359, contrastive_loss=0, total=3967.4, n_correct=2503.71, ppl=2.83, accuracy=63.107, wps=13996.3, ups=1.17, wpb=11931.2, bsz=436.8, num_updates=13900, lr=0.000119952, gnorm=0.393, clip=0, loss_scale=16, train_wall=84, gb_free=15.1, wall=11905
2023-09-05 14:04:35 | INFO | train_inner | epoch 008:   1226 / 1826 loss=1.923, trans_loss=3.305, nll_loss=1.498, w2v_ctc_loss=1.08, task_loss=2.869, task_loss_gen=5.474, contrastive_loss=0, total=4000.75, n_correct=2530.01, ppl=2.82, accuracy=63.238, wps=13843.2, ups=1.15, wpb=12030.1, bsz=442.5, num_updates=14000, lr=0.000119523, gnorm=0.396, clip=0, loss_scale=16, train_wall=86, gb_free=16.5, wall=11992
2023-09-05 14:04:35 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.4622], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.4622], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 14:05:14 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 3.916 | trans_loss 5.175 | nll_loss 2.466 | w2v_ctc_loss 1.376 | task_loss 15.963 | task_loss_gen 21.917 | contrastive_loss 0 | total 3505.91 | n_correct 2328.18 | ppl 5.53 | accuracy 66.407 | uer 19.035 | wer 20.712 | raw_wer 20.712 | bleu 28.42 | wps 1199.1 | wpb 3505.9 | bsz 119.3 | num_updates 14000 | best_bleu 28.42
2023-09-05 14:05:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14000 updates
2023-09-05 14:05:14 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_8_14000.pt
2023-09-05 14:05:16 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_8_14000.pt
2023-09-05 14:05:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_8_14000.pt (epoch 8 @ 14000 updates, score 28.42) (writing took 14.37124888697872 seconds)
2023-09-05 14:06:53 | INFO | train_inner | epoch 008:   1326 / 1826 loss=1.942, trans_loss=3.313, nll_loss=1.509, w2v_ctc_loss=1.101, task_loss=3.147, task_loss_gen=6.274, contrastive_loss=0, total=3927.23, n_correct=2470.79, ppl=2.85, accuracy=62.914, wps=8548, ups=0.72, wpb=11813, bsz=411.5, num_updates=14100, lr=0.000119098, gnorm=0.405, clip=0, loss_scale=16, train_wall=84, gb_free=16.1, wall=12130
2023-09-05 14:08:18 | INFO | train_inner | epoch 008:   1426 / 1826 loss=1.913, trans_loss=3.302, nll_loss=1.494, w2v_ctc_loss=1.067, task_loss=2.744, task_loss_gen=5.643, contrastive_loss=0, total=3987.61, n_correct=2520.23, ppl=2.82, accuracy=63.202, wps=14081.8, ups=1.17, wpb=11989.8, bsz=441.2, num_updates=14200, lr=0.000118678, gnorm=0.396, clip=0, loss_scale=16, train_wall=84, gb_free=16.2, wall=12216
2023-09-05 14:09:43 | INFO | train_inner | epoch 008:   1526 / 1826 loss=1.934, trans_loss=3.305, nll_loss=1.5, w2v_ctc_loss=1.094, task_loss=2.391, task_loss_gen=5.863, contrastive_loss=0, total=3997.99, n_correct=2523.78, ppl=2.83, accuracy=63.126, wps=14173.6, ups=1.18, wpb=12027.4, bsz=444.4, num_updates=14300, lr=0.000118262, gnorm=0.471, clip=0, loss_scale=16, train_wall=84, gb_free=16, wall=12300
2023-09-05 14:11:08 | INFO | train_inner | epoch 008:   1626 / 1826 loss=1.944, trans_loss=3.306, nll_loss=1.501, w2v_ctc_loss=1.109, task_loss=2.93, task_loss_gen=6.569, contrastive_loss=0, total=3947.23, n_correct=2486.48, ppl=2.83, accuracy=62.993, wps=14059.3, ups=1.18, wpb=11878.9, bsz=415.7, num_updates=14400, lr=0.000117851, gnorm=0.405, clip=0, loss_scale=16, train_wall=84, gb_free=17, wall=12385
2023-09-05 14:12:33 | INFO | train_inner | epoch 008:   1726 / 1826 loss=1.941, trans_loss=3.312, nll_loss=1.507, w2v_ctc_loss=1.101, task_loss=2.687, task_loss_gen=6.262, contrastive_loss=0, total=3957.64, n_correct=2489.3, ppl=2.84, accuracy=62.899, wps=13900.8, ups=1.17, wpb=11901.8, bsz=433.4, num_updates=14500, lr=0.000117444, gnorm=0.397, clip=0, loss_scale=16, train_wall=85, gb_free=15.1, wall=12470
2023-09-05 14:13:58 | INFO | train_inner | epoch 008:   1826 / 1826 loss=1.936, trans_loss=3.305, nll_loss=1.498, w2v_ctc_loss=1.1, task_loss=2.714, task_loss_gen=5.933, contrastive_loss=0, total=3953.77, n_correct=2500.27, ppl=2.82, accuracy=63.238, wps=14102.2, ups=1.19, wpb=11888.9, bsz=433.6, num_updates=14600, lr=0.000117041, gnorm=0.405, clip=0, loss_scale=16, train_wall=84, gb_free=16.6, wall=12555
2023-09-05 14:13:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.0252], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.0252], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 14:14:37 | INFO | dev_st | epoch 008 | valid on 'dev_st' subset | loss 3.907 | trans_loss 5.161 | nll_loss 2.446 | w2v_ctc_loss 1.376 | task_loss 26.124 | task_loss_gen 20.767 | contrastive_loss 0 | total 3505.91 | n_correct 2332.82 | ppl 5.45 | accuracy 66.54 | uer 18.71 | wer 20.649 | raw_wer 20.649 | bleu 28.6 | wps 1169.2 | wpb 3505.9 | bsz 119.3 | num_updates 14600 | best_bleu 28.6
2023-09-05 14:14:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 14600 updates
2023-09-05 14:14:37 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 14:14:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 14:14:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 8 @ 14600 updates, score 28.6) (writing took 12.279452710004989 seconds)
2023-09-05 14:14:50 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)
2023-09-05 14:14:50 | INFO | train | epoch 008 | loss 1.944 | trans_loss 3.314 | nll_loss 1.51 | w2v_ctc_loss 1.101 | task_loss 3.358 | task_loss_gen 5.594 | contrastive_loss 0 | total 3956.37 | n_correct 2483.13 | ppl 2.85 | accuracy 62.763 | wps 12994.7 | ups 1.09 | wpb 11900.1 | bsz 427.2 | num_updates 14600 | lr 0.000117041 | gnorm 0.415 | clip 0 | loss_scale 16 | train_wall 1545 | gb_free 16.6 | wall 12607
2023-09-05 14:14:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 14:14:50 | INFO | fairseq.trainer | begin training epoch 9
2023-09-05 14:14:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 14:16:23 | INFO | train_inner | epoch 009:    100 / 1826 loss=1.896, trans_loss=3.28, nll_loss=1.465, w2v_ctc_loss=1.052, task_loss=3.1, task_loss_gen=6.122, contrastive_loss=0, total=3922.53, n_correct=2510.19, ppl=2.76, accuracy=63.994, wps=8129.2, ups=0.69, wpb=11792.7, bsz=419.4, num_updates=14700, lr=0.000116642, gnorm=0.4, clip=0, loss_scale=16, train_wall=84, gb_free=17, wall=12700
2023-09-05 14:17:47 | INFO | train_inner | epoch 009:    200 / 1826 loss=1.896, trans_loss=3.282, nll_loss=1.465, w2v_ctc_loss=1.055, task_loss=3.111, task_loss_gen=5.929, contrastive_loss=0, total=3983.09, n_correct=2553.63, ppl=2.76, accuracy=64.112, wps=14145.4, ups=1.18, wpb=11962.5, bsz=426.6, num_updates=14800, lr=0.000116248, gnorm=0.398, clip=0, loss_scale=16, train_wall=84, gb_free=15.6, wall=12784
2023-09-05 14:19:13 | INFO | train_inner | epoch 009:    300 / 1826 loss=1.899, trans_loss=3.282, nll_loss=1.469, w2v_ctc_loss=1.062, task_loss=3.179, task_loss_gen=5.687, contrastive_loss=0, total=4021.21, n_correct=2568.2, ppl=2.77, accuracy=63.866, wps=14094.8, ups=1.17, wpb=12092.6, bsz=445.2, num_updates=14900, lr=0.000115857, gnorm=0.402, clip=0, loss_scale=16, train_wall=85, gb_free=15.9, wall=12870
2023-09-05 14:20:39 | INFO | train_inner | epoch 009:    400 / 1826 loss=1.894, trans_loss=3.277, nll_loss=1.463, w2v_ctc_loss=1.056, task_loss=2.892, task_loss_gen=5.961, contrastive_loss=0, total=3972.29, n_correct=2547.91, ppl=2.76, accuracy=64.142, wps=13946.6, ups=1.17, wpb=11949.2, bsz=436.3, num_updates=15000, lr=0.00011547, gnorm=0.393, clip=0, loss_scale=16, train_wall=85, gb_free=17.3, wall=12956
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:0')
2023-09-05 14:21:41 | INFO | train_inner | epoch 009:    500 / 1826 loss=2.048, trans_loss=4.922, nll_loss=2.213, w2v_ctc_loss=0.816, task_loss=4.959, task_loss_gen=9.256, contrastive_loss=0, total=3930.64, n_correct=2505.76, ppl=4.64, accuracy=63.749, wps=12692.5, ups=1.61, wpb=7902.3, bsz=275.5, num_updates=15100, lr=0.000115087, gnorm=0.558, clip=0, loss_scale=16, train_wall=62, gb_free=17, wall=13018
2023-09-05 14:22:43 | INFO | train_inner | epoch 009:    600 / 1826 loss=2.036, trans_loss=4.941, nll_loss=2.216, w2v_ctc_loss=0.789, task_loss=4.572, task_loss_gen=8.809, contrastive_loss=0, total=3996.71, n_correct=2549.48, ppl=4.65, accuracy=63.789, wps=12953.2, ups=1.62, wpb=7993.4, bsz=293.4, num_updates=15200, lr=0.000114708, gnorm=0.549, clip=0, loss_scale=16, train_wall=61, gb_free=16.2, wall=13080
2023-09-05 14:23:45 | INFO | train_inner | epoch 009:    700 / 1826 loss=2.042, trans_loss=4.936, nll_loss=2.211, w2v_ctc_loss=0.81, task_loss=4.46, task_loss_gen=9.191, contrastive_loss=0, total=3922.45, n_correct=2506.96, ppl=4.63, accuracy=63.913, wps=12642.9, ups=1.61, wpb=7844.9, bsz=279.5, num_updates=15300, lr=0.000114332, gnorm=0.55, clip=0, loss_scale=16, train_wall=61, gb_free=15.7, wall=13142
2023-09-05 14:24:47 | INFO | train_inner | epoch 009:    800 / 1826 loss=2.047, trans_loss=4.951, nll_loss=2.229, w2v_ctc_loss=0.809, task_loss=4.246, task_loss_gen=9.552, contrastive_loss=0, total=3927.21, n_correct=2498.19, ppl=4.69, accuracy=63.612, wps=12658.5, ups=1.61, wpb=7854.4, bsz=282.8, num_updates=15400, lr=0.000113961, gnorm=0.559, clip=0, loss_scale=16, train_wall=61, gb_free=10.7, wall=13204
2023-09-05 14:25:48 | INFO | train_inner | epoch 009:    900 / 1826 loss=2.044, trans_loss=4.941, nll_loss=2.217, w2v_ctc_loss=0.811, task_loss=4.692, task_loss_gen=9.645, contrastive_loss=0, total=3925.04, n_correct=2505.1, ppl=4.65, accuracy=63.824, wps=12772.3, ups=1.63, wpb=7850.1, bsz=284.6, num_updates=15500, lr=0.000113592, gnorm=0.569, clip=0, loss_scale=16, train_wall=61, gb_free=15.3, wall=13265
2023-09-05 14:26:51 | INFO | train_inner | epoch 009:   1000 / 1826 loss=2.048, trans_loss=4.939, nll_loss=2.216, w2v_ctc_loss=0.821, task_loss=4.312, task_loss_gen=9.009, contrastive_loss=0, total=3978.13, n_correct=2535.05, ppl=4.64, accuracy=63.725, wps=12635.1, ups=1.59, wpb=7956.3, bsz=293.5, num_updates=15600, lr=0.000113228, gnorm=0.549, clip=0, loss_scale=16, train_wall=62, gb_free=16.5, wall=13328
2023-09-05 14:27:53 | INFO | train_inner | epoch 009:   1100 / 1826 loss=2.039, trans_loss=4.934, nll_loss=2.208, w2v_ctc_loss=0.807, task_loss=4.263, task_loss_gen=9.46, contrastive_loss=0, total=3936.56, n_correct=2518.24, ppl=4.62, accuracy=63.971, wps=12786.8, ups=1.62, wpb=7873.1, bsz=282.6, num_updates=15700, lr=0.000112867, gnorm=0.556, clip=0, loss_scale=16, train_wall=61, gb_free=10.9, wall=13390
2023-09-05 14:28:55 | INFO | train_inner | epoch 009:   1200 / 1826 loss=2.038, trans_loss=4.927, nll_loss=2.199, w2v_ctc_loss=0.814, task_loss=4.417, task_loss_gen=9.16, contrastive_loss=0, total=4012.94, n_correct=2573.86, ppl=4.59, accuracy=64.139, wps=12936.1, ups=1.61, wpb=8025.9, bsz=288.5, num_updates=15800, lr=0.000112509, gnorm=0.551, clip=0, loss_scale=16, train_wall=61, gb_free=16.4, wall=13452
2023-09-05 14:29:57 | INFO | train_inner | epoch 009:   1300 / 1826 loss=2.049, trans_loss=4.943, nll_loss=2.221, w2v_ctc_loss=0.821, task_loss=4.238, task_loss_gen=10.245, contrastive_loss=0, total=3903.69, n_correct=2488.78, ppl=4.66, accuracy=63.755, wps=12618.2, ups=1.62, wpb=7807.4, bsz=270.3, num_updates=15900, lr=0.000112154, gnorm=0.553, clip=0, loss_scale=32, train_wall=61, gb_free=14.1, wall=13514
2023-09-05 14:30:58 | INFO | train_inner | epoch 009:   1400 / 1826 loss=2.032, trans_loss=4.933, nll_loss=2.207, w2v_ctc_loss=0.797, task_loss=3.885, task_loss_gen=9.614, contrastive_loss=0, total=3983.29, n_correct=2553.38, ppl=4.62, accuracy=64.102, wps=12920.4, ups=1.62, wpb=7966.6, bsz=293.5, num_updates=16000, lr=0.000111803, gnorm=0.536, clip=0, loss_scale=32, train_wall=61, gb_free=16.6, wall=13576
2023-09-05 14:30:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:7')
tensor([0.6040], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.6040], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 14:31:38 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 3.881 | trans_loss 5.131 | nll_loss 2.41 | w2v_ctc_loss 1.359 | task_loss 28.716 | task_loss_gen 21.194 | contrastive_loss 0 | total 3505.91 | n_correct 2344 | ppl 5.32 | accuracy 66.859 | uer 18.828 | wer 20.686 | raw_wer 20.686 | bleu 29.01 | wps 1181.1 | wpb 3505.9 | bsz 119.3 | num_updates 16000 | best_bleu 29.01
2023-09-05 14:31:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16000 updates
2023-09-05 14:31:38 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_9_16000.pt
2023-09-05 14:31:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_9_16000.pt
2023-09-05 14:31:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_9_16000.pt (epoch 9 @ 16000 updates, score 29.01) (writing took 12.91684008500306 seconds)
2023-09-05 14:32:53 | INFO | train_inner | epoch 009:   1500 / 1826 loss=2.039, trans_loss=4.927, nll_loss=2.2, w2v_ctc_loss=0.819, task_loss=3.695, task_loss_gen=10.672, contrastive_loss=0, total=3939.43, n_correct=2531.82, ppl=4.6, accuracy=64.269, wps=6852.6, ups=0.87, wpb=7878.9, bsz=281.4, num_updates=16100, lr=0.000111456, gnorm=0.543, clip=0, loss_scale=32, train_wall=62, gb_free=16.5, wall=13691
2023-09-05 14:33:55 | INFO | train_inner | epoch 009:   1600 / 1826 loss=2.025, trans_loss=4.916, nll_loss=2.186, w2v_ctc_loss=0.8, task_loss=2.812, task_loss_gen=10.068, contrastive_loss=0, total=4013.71, n_correct=2586.9, ppl=4.55, accuracy=64.452, wps=13091.6, ups=1.63, wpb=8027.4, bsz=304.7, num_updates=16200, lr=0.000111111, gnorm=0.532, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=13752
2023-09-05 14:34:56 | INFO | train_inner | epoch 009:   1700 / 1826 loss=2.037, trans_loss=4.925, nll_loss=2.197, w2v_ctc_loss=0.815, task_loss=3.599, task_loss_gen=11.513, contrastive_loss=0, total=3918.49, n_correct=2516.57, ppl=4.59, accuracy=64.223, wps=12733.8, ups=1.62, wpb=7837, bsz=273.5, num_updates=16300, lr=0.00011077, gnorm=0.544, clip=0, loss_scale=32, train_wall=61, gb_free=16.6, wall=13813
2023-09-05 14:35:58 | INFO | train_inner | epoch 009:   1800 / 1826 loss=2.044, trans_loss=4.933, nll_loss=2.207, w2v_ctc_loss=0.821, task_loss=3.835, task_loss_gen=11.273, contrastive_loss=0, total=3940.79, n_correct=2528.19, ppl=4.62, accuracy=64.154, wps=12802.6, ups=1.62, wpb=7881.6, bsz=271.3, num_updates=16400, lr=0.000110432, gnorm=0.545, clip=0, loss_scale=32, train_wall=61, gb_free=17.6, wall=13875
2023-09-05 14:36:14 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.3730], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.3730], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 14:36:53 | INFO | dev_st | epoch 009 | valid on 'dev_st' subset | loss 3.887 | trans_loss 5.118 | nll_loss 2.392 | w2v_ctc_loss 1.409 | task_loss 13.631 | task_loss_gen 24.698 | contrastive_loss 0 | total 3505.91 | n_correct 2361.27 | ppl 5.25 | accuracy 67.351 | uer 19.043 | wer 20.765 | raw_wer 20.765 | bleu 29.28 | wps 1173.1 | wpb 3505.9 | bsz 119.3 | num_updates 16426 | best_bleu 29.28
2023-09-05 14:36:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 16426 updates
2023-09-05 14:36:53 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 14:37:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 14:37:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 9 @ 16426 updates, score 29.28) (writing took 11.588542720011901 seconds)
2023-09-05 14:37:05 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)
2023-09-05 14:37:05 | INFO | train | epoch 009 | loss 1.998 | trans_loss 4.441 | nll_loss 1.987 | w2v_ctc_loss 0.883 | task_loss 3.806 | task_loss_gen 8.644 | contrastive_loss 0 | total 3956.37 | n_correct 2531.95 | ppl 3.97 | accuracy 63.997 | wps 12018.8 | ups 1.37 | wpb 8791.1 | bsz 316.4 | num_updates 16426 | lr 0.000110344 | gnorm 0.517 | clip 0 | loss_scale 32 | train_wall 1211 | gb_free 17.5 | wall 13943
2023-09-05 14:37:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 14:37:06 | INFO | fairseq.trainer | begin training epoch 10
2023-09-05 14:37:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 14:37:59 | INFO | train_inner | epoch 010:     74 / 1826 loss=2.023, trans_loss=4.906, nll_loss=2.174, w2v_ctc_loss=0.802, task_loss=3.386, task_loss_gen=10.77, contrastive_loss=0, total=3937.23, n_correct=2550.54, ppl=4.51, accuracy=64.78, wps=6497.1, ups=0.83, wpb=7874.5, bsz=284.1, num_updates=16500, lr=0.000110096, gnorm=0.55, clip=0, loss_scale=32, train_wall=61, gb_free=11, wall=13996
2023-09-05 14:39:01 | INFO | train_inner | epoch 010:    174 / 1826 loss=2.021, trans_loss=4.906, nll_loss=2.172, w2v_ctc_loss=0.795, task_loss=4.055, task_loss_gen=11.435, contrastive_loss=0, total=3947.54, n_correct=2553.73, ppl=4.51, accuracy=64.692, wps=12726.3, ups=1.61, wpb=7895.1, bsz=278.6, num_updates=16600, lr=0.000109764, gnorm=0.548, clip=0, loss_scale=32, train_wall=61, gb_free=16.3, wall=14058
2023-09-05 14:40:03 | INFO | train_inner | epoch 010:    274 / 1826 loss=2.004, trans_loss=4.884, nll_loss=2.145, w2v_ctc_loss=0.778, task_loss=3.337, task_loss_gen=11.19, contrastive_loss=0, total=3941.33, n_correct=2565.65, ppl=4.42, accuracy=65.096, wps=12819.9, ups=1.63, wpb=7882.7, bsz=283, num_updates=16700, lr=0.000109435, gnorm=0.535, clip=0, loss_scale=32, train_wall=61, gb_free=15.8, wall=14120
2023-09-05 14:41:05 | INFO | train_inner | epoch 010:    374 / 1826 loss=2.018, trans_loss=4.909, nll_loss=2.177, w2v_ctc_loss=0.788, task_loss=3.471, task_loss_gen=9.881, contrastive_loss=0, total=3992.9, n_correct=2577.7, ppl=4.52, accuracy=64.557, wps=12865.1, ups=1.61, wpb=7985.8, bsz=302.7, num_updates=16800, lr=0.000109109, gnorm=0.543, clip=0, loss_scale=32, train_wall=61, gb_free=15.5, wall=14182
2023-09-05 14:42:07 | INFO | train_inner | epoch 010:    474 / 1826 loss=2.016, trans_loss=4.899, nll_loss=2.164, w2v_ctc_loss=0.799, task_loss=3.432, task_loss_gen=10.603, contrastive_loss=0, total=3983.02, n_correct=2586.59, ppl=4.48, accuracy=64.94, wps=12823.5, ups=1.61, wpb=7966, bsz=290.2, num_updates=16900, lr=0.000108786, gnorm=0.533, clip=0, loss_scale=32, train_wall=61, gb_free=16.5, wall=14244
2023-09-05 14:43:08 | INFO | train_inner | epoch 010:    574 / 1826 loss=2.01, trans_loss=4.892, nll_loss=2.156, w2v_ctc_loss=0.788, task_loss=3.515, task_loss_gen=10.513, contrastive_loss=0, total=3961.82, n_correct=2575.37, ppl=4.46, accuracy=65.005, wps=12942.9, ups=1.63, wpb=7923.6, bsz=291.1, num_updates=17000, lr=0.000108465, gnorm=0.546, clip=0, loss_scale=32, train_wall=60, gb_free=16.8, wall=14305
2023-09-05 14:44:10 | INFO | train_inner | epoch 010:    674 / 1826 loss=2.012, trans_loss=4.897, nll_loss=2.162, w2v_ctc_loss=0.781, task_loss=3.822, task_loss_gen=10.891, contrastive_loss=0, total=3914.87, n_correct=2538.59, ppl=4.47, accuracy=64.845, wps=12641, ups=1.61, wpb=7829.7, bsz=282, num_updates=17100, lr=0.000108148, gnorm=0.548, clip=0, loss_scale=32, train_wall=61, gb_free=12, wall=14367
2023-09-05 14:45:12 | INFO | train_inner | epoch 010:    774 / 1826 loss=2.012, trans_loss=4.89, nll_loss=2.153, w2v_ctc_loss=0.793, task_loss=3.579, task_loss_gen=11.085, contrastive_loss=0, total=3949.94, n_correct=2567.58, ppl=4.45, accuracy=65.003, wps=12693.5, ups=1.61, wpb=7899.9, bsz=283.6, num_updates=17200, lr=0.000107833, gnorm=0.543, clip=0, loss_scale=32, train_wall=62, gb_free=11.9, wall=14429
2023-09-05 14:46:15 | INFO | train_inner | epoch 010:    874 / 1826 loss=2.017, trans_loss=4.895, nll_loss=2.159, w2v_ctc_loss=0.796, task_loss=4.077, task_loss_gen=11.472, contrastive_loss=0, total=3945.73, n_correct=2563.1, ppl=4.47, accuracy=64.959, wps=12628.5, ups=1.6, wpb=7891.5, bsz=272.9, num_updates=17300, lr=0.000107521, gnorm=0.544, clip=0, loss_scale=32, train_wall=62, gb_free=10.6, wall=14492
2023-09-05 14:47:16 | INFO | train_inner | epoch 010:    974 / 1826 loss=2.018, trans_loss=4.903, nll_loss=2.17, w2v_ctc_loss=0.796, task_loss=3.355, task_loss_gen=10.359, contrastive_loss=0, total=4061.61, n_correct=2629.85, ppl=4.5, accuracy=64.749, wps=13271.8, ups=1.63, wpb=8123.2, bsz=301.5, num_updates=17400, lr=0.000107211, gnorm=0.537, clip=0, loss_scale=32, train_wall=60, gb_free=16, wall=14553
2023-09-05 14:48:18 | INFO | train_inner | epoch 010:   1074 / 1826 loss=2.024, trans_loss=4.911, nll_loss=2.179, w2v_ctc_loss=0.804, task_loss=3.908, task_loss_gen=11.135, contrastive_loss=0, total=3915.91, n_correct=2531.53, ppl=4.53, accuracy=64.647, wps=12680.6, ups=1.62, wpb=7831.8, bsz=277.1, num_updates=17500, lr=0.000106904, gnorm=0.547, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=14615
2023-09-05 14:49:19 | INFO | train_inner | epoch 010:   1174 / 1826 loss=2.01, trans_loss=4.885, nll_loss=2.147, w2v_ctc_loss=0.792, task_loss=3.561, task_loss_gen=11.603, contrastive_loss=0, total=3941.08, n_correct=2567.6, ppl=4.43, accuracy=65.15, wps=12762.1, ups=1.62, wpb=7882.2, bsz=282.6, num_updates=17600, lr=0.0001066, gnorm=0.543, clip=0, loss_scale=32, train_wall=61, gb_free=13.9, wall=14677
2023-09-05 14:50:22 | INFO | train_inner | epoch 010:   1274 / 1826 loss=2.014, trans_loss=4.895, nll_loss=2.159, w2v_ctc_loss=0.801, task_loss=3.095, task_loss_gen=10.846, contrastive_loss=0, total=4014.67, n_correct=2603.91, ppl=4.47, accuracy=64.86, wps=12906.8, ups=1.61, wpb=8029.3, bsz=299.4, num_updates=17700, lr=0.000106299, gnorm=0.535, clip=0, loss_scale=32, train_wall=61, gb_free=15.1, wall=14739
2023-09-05 14:51:24 | INFO | train_inner | epoch 010:   1374 / 1826 loss=2.02, trans_loss=4.894, nll_loss=2.157, w2v_ctc_loss=0.805, task_loss=4.113, task_loss_gen=11.934, contrastive_loss=0, total=3914.68, n_correct=2542.84, ppl=4.46, accuracy=64.957, wps=12605.2, ups=1.61, wpb=7829.4, bsz=264.5, num_updates=17800, lr=0.000106, gnorm=0.541, clip=0, loss_scale=32, train_wall=61, gb_free=16.1, wall=14801
2023-09-05 14:52:25 | INFO | train_inner | epoch 010:   1474 / 1826 loss=2.006, trans_loss=4.878, nll_loss=2.139, w2v_ctc_loss=0.783, task_loss=4.185, task_loss_gen=11.684, contrastive_loss=0, total=3907.93, n_correct=2551.8, ppl=4.4, accuracy=65.298, wps=12731.7, ups=1.63, wpb=7815.9, bsz=271.3, num_updates=17900, lr=0.000105703, gnorm=0.548, clip=0, loss_scale=64, train_wall=61, gb_free=16.3, wall=14862
2023-09-05 14:53:27 | INFO | train_inner | epoch 010:   1574 / 1826 loss=1.999, trans_loss=4.872, nll_loss=2.131, w2v_ctc_loss=0.782, task_loss=3.083, task_loss_gen=11.698, contrastive_loss=0, total=3964.37, n_correct=2595.54, ppl=4.38, accuracy=65.472, wps=12880.9, ups=1.62, wpb=7928.7, bsz=287.3, num_updates=18000, lr=0.000105409, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=15.7, wall=14924
2023-09-05 14:53:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.2477], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.2477], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 14:54:06 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 3.863 | trans_loss 5.098 | nll_loss 2.369 | w2v_ctc_loss 1.371 | task_loss 15.693 | task_loss_gen 25.213 | contrastive_loss 0 | total 3505.91 | n_correct 2368.82 | ppl 5.17 | accuracy 67.566 | uer 18.769 | wer 20.543 | raw_wer 20.543 | bleu 29.56 | wps 1186.1 | wpb 3505.9 | bsz 119.3 | num_updates 18000 | best_bleu 29.56
2023-09-05 14:54:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18000 updates
2023-09-05 14:54:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_10_18000.pt
2023-09-05 14:54:08 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_10_18000.pt
2023-09-05 14:54:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_10_18000.pt (epoch 10 @ 18000 updates, score 29.56) (writing took 12.90895991603611 seconds)
2023-09-05 14:55:21 | INFO | train_inner | epoch 010:   1674 / 1826 loss=2.011, trans_loss=4.887, nll_loss=2.15, w2v_ctc_loss=0.794, task_loss=3.313, task_loss_gen=12.774, contrastive_loss=0, total=3937.03, n_correct=2568.38, ppl=4.44, accuracy=65.236, wps=6884.8, ups=0.87, wpb=7874.1, bsz=276.5, num_updates=18100, lr=0.000105118, gnorm=0.538, clip=0, loss_scale=64, train_wall=61, gb_free=15.3, wall=15038
2023-09-05 14:56:23 | INFO | train_inner | epoch 010:   1774 / 1826 loss=2.008, trans_loss=4.894, nll_loss=2.159, w2v_ctc_loss=0.783, task_loss=2.422, task_loss_gen=12.117, contrastive_loss=0, total=4002.26, n_correct=2598.57, ppl=4.47, accuracy=64.928, wps=12935.5, ups=1.62, wpb=8004.5, bsz=303.5, num_updates=18200, lr=0.000104828, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=14.6, wall=15100
2023-09-05 14:56:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 14:56:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-1.1475], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.1475], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 14:57:35 | INFO | dev_st | epoch 010 | valid on 'dev_st' subset | loss 3.851 | trans_loss 5.097 | nll_loss 2.371 | w2v_ctc_loss 1.334 | task_loss 7.337 | task_loss_gen 32.161 | contrastive_loss 0 | total 3505.91 | n_correct 2370.18 | ppl 5.17 | accuracy 67.605 | uer 18.345 | wer 20.191 | raw_wer 20.191 | bleu 29.31 | wps 1179.3 | wpb 3505.9 | bsz 119.3 | num_updates 18251 | best_bleu 29.56
2023-09-05 14:57:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 18251 updates
2023-09-05 14:57:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_29.3107.pt
2023-09-05 14:57:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_29.3107.pt
2023-09-05 14:57:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_29.3107.pt (epoch 10 @ 18251 updates, score 29.31) (writing took 7.120409836003091 seconds)
2023-09-05 14:57:42 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)
2023-09-05 14:57:42 | INFO | train | epoch 010 | loss 2.013 | trans_loss 4.894 | nll_loss 2.158 | w2v_ctc_loss 0.792 | task_loss 3.509 | task_loss_gen 11.283 | contrastive_loss 0 | total 3956.15 | n_correct 2569.98 | ppl 4.46 | accuracy 64.962 | wps 11675.3 | ups 1.48 | wpb 7912.3 | bsz 284.8 | num_updates 18251 | lr 0.000104682 | gnorm 0.542 | clip 0 | loss_scale 32 | train_wall 1115 | gb_free 17 | wall 15179
2023-09-05 14:57:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 14:57:42 | INFO | fairseq.trainer | begin training epoch 11
2023-09-05 14:57:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 14:58:21 | INFO | train_inner | epoch 011:     49 / 1826 loss=2.004, trans_loss=4.878, nll_loss=2.138, w2v_ctc_loss=0.788, task_loss=2.937, task_loss_gen=12.791, contrastive_loss=0, total=3896.15, n_correct=2544.74, ppl=4.4, accuracy=65.314, wps=6623.8, ups=0.85, wpb=7792.3, bsz=279, num_updates=18300, lr=0.000104542, gnorm=0.547, clip=0, loss_scale=32, train_wall=62, gb_free=14.8, wall=15218
2023-09-05 14:59:23 | INFO | train_inner | epoch 011:    149 / 1826 loss=1.995, trans_loss=4.868, nll_loss=2.125, w2v_ctc_loss=0.773, task_loss=3.616, task_loss_gen=11.571, contrastive_loss=0, total=3987.55, n_correct=2610.57, ppl=4.36, accuracy=65.468, wps=12818.3, ups=1.61, wpb=7975.1, bsz=288.5, num_updates=18400, lr=0.000104257, gnorm=0.539, clip=0, loss_scale=32, train_wall=61, gb_free=15.4, wall=15280
2023-09-05 15:00:24 | INFO | train_inner | epoch 011:    249 / 1826 loss=1.999, trans_loss=4.868, nll_loss=2.126, w2v_ctc_loss=0.78, task_loss=4.244, task_loss_gen=11.924, contrastive_loss=0, total=3914.16, n_correct=2563.61, ppl=4.36, accuracy=65.496, wps=12736.3, ups=1.63, wpb=7828.3, bsz=278.6, num_updates=18500, lr=0.000103975, gnorm=0.55, clip=0, loss_scale=32, train_wall=61, gb_free=17.3, wall=15341
2023-09-05 15:01:26 | INFO | train_inner | epoch 011:    349 / 1826 loss=1.992, trans_loss=4.86, nll_loss=2.116, w2v_ctc_loss=0.773, task_loss=3.644, task_loss_gen=11.494, contrastive_loss=0, total=3936.35, n_correct=2588.08, ppl=4.33, accuracy=65.748, wps=12780.7, ups=1.62, wpb=7872.7, bsz=276.7, num_updates=18600, lr=0.000103695, gnorm=0.537, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=15403
2023-09-05 15:02:28 | INFO | train_inner | epoch 011:    449 / 1826 loss=1.997, trans_loss=4.861, nll_loss=2.116, w2v_ctc_loss=0.787, task_loss=4.035, task_loss_gen=11.343, contrastive_loss=0, total=3953.38, n_correct=2594.33, ppl=4.34, accuracy=65.623, wps=12697.4, ups=1.61, wpb=7906.8, bsz=281, num_updates=18700, lr=0.000103418, gnorm=0.551, clip=0, loss_scale=32, train_wall=61, gb_free=16.3, wall=15465
2023-09-05 15:03:04 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0
2023-09-05 15:03:31 | INFO | train_inner | epoch 011:    550 / 1826 loss=1.993, trans_loss=4.862, nll_loss=2.118, w2v_ctc_loss=0.776, task_loss=4.28, task_loss_gen=11.133, contrastive_loss=0, total=3947.87, n_correct=2599.61, ppl=4.34, accuracy=65.848, wps=12554.7, ups=1.59, wpb=7895.7, bsz=277.9, num_updates=18800, lr=0.000103142, gnorm=0.558, clip=0, loss_scale=16, train_wall=62, gb_free=11.5, wall=15528
2023-09-05 15:04:32 | INFO | train_inner | epoch 011:    650 / 1826 loss=1.999, trans_loss=4.871, nll_loss=2.13, w2v_ctc_loss=0.785, task_loss=4.997, task_loss_gen=10.32, contrastive_loss=0, total=3910.12, n_correct=2561.8, ppl=4.38, accuracy=65.517, wps=12720.8, ups=1.63, wpb=7820.2, bsz=277, num_updates=18900, lr=0.000102869, gnorm=0.586, clip=0, loss_scale=16, train_wall=61, gb_free=15.7, wall=15590
2023-09-05 15:05:34 | INFO | train_inner | epoch 011:    750 / 1826 loss=1.997, trans_loss=4.875, nll_loss=2.134, w2v_ctc_loss=0.779, task_loss=4.822, task_loss_gen=9.744, contrastive_loss=0, total=3946.22, n_correct=2585.69, ppl=4.39, accuracy=65.523, wps=12807.6, ups=1.62, wpb=7892.4, bsz=282.7, num_updates=19000, lr=0.000102598, gnorm=0.575, clip=0, loss_scale=16, train_wall=61, gb_free=10.9, wall=15651
2023-09-05 15:06:35 | INFO | train_inner | epoch 011:    850 / 1826 loss=1.999, trans_loss=4.877, nll_loss=2.137, w2v_ctc_loss=0.774, task_loss=5.054, task_loss_gen=9.843, contrastive_loss=0, total=3920.77, n_correct=2562.33, ppl=4.4, accuracy=65.353, wps=12786.7, ups=1.63, wpb=7841.5, bsz=278.5, num_updates=19100, lr=0.000102329, gnorm=0.577, clip=0, loss_scale=16, train_wall=60, gb_free=16.7, wall=15713
2023-09-05 15:07:38 | INFO | train_inner | epoch 011:    950 / 1826 loss=2.001, trans_loss=4.875, nll_loss=2.135, w2v_ctc_loss=0.781, task_loss=5.101, task_loss_gen=9.589, contrastive_loss=0, total=4000.2, n_correct=2613.69, ppl=4.39, accuracy=65.339, wps=12853.2, ups=1.61, wpb=8000.4, bsz=289.2, num_updates=19200, lr=0.000102062, gnorm=0.567, clip=0, loss_scale=16, train_wall=62, gb_free=15.8, wall=15775
2023-09-05 15:08:40 | INFO | train_inner | epoch 011:   1050 / 1826 loss=1.999, trans_loss=4.866, nll_loss=2.123, w2v_ctc_loss=0.789, task_loss=5.359, task_loss_gen=9.806, contrastive_loss=0, total=3962.25, n_correct=2598.41, ppl=4.36, accuracy=65.579, wps=12725.7, ups=1.61, wpb=7924.5, bsz=277.4, num_updates=19300, lr=0.000101797, gnorm=0.575, clip=0, loss_scale=16, train_wall=62, gb_free=16.2, wall=15837
2023-09-05 15:09:42 | INFO | train_inner | epoch 011:   1150 / 1826 loss=2.003, trans_loss=4.876, nll_loss=2.135, w2v_ctc_loss=0.789, task_loss=4.874, task_loss_gen=9.627, contrastive_loss=0, total=3942.9, n_correct=2577.62, ppl=4.39, accuracy=65.374, wps=12779.7, ups=1.62, wpb=7885.8, bsz=275.9, num_updates=19400, lr=0.000101535, gnorm=0.551, clip=0, loss_scale=16, train_wall=61, gb_free=15.8, wall=15899
2023-09-05 15:10:43 | INFO | train_inner | epoch 011:   1250 / 1826 loss=2.002, trans_loss=4.871, nll_loss=2.128, w2v_ctc_loss=0.784, task_loss=5.533, task_loss_gen=10.386, contrastive_loss=0, total=3869.34, n_correct=2530.44, ppl=4.37, accuracy=65.397, wps=12625.6, ups=1.63, wpb=7738.7, bsz=265.7, num_updates=19500, lr=0.000101274, gnorm=0.577, clip=0, loss_scale=16, train_wall=60, gb_free=12.1, wall=15960
2023-09-05 15:11:44 | INFO | train_inner | epoch 011:   1350 / 1826 loss=1.995, trans_loss=4.873, nll_loss=2.132, w2v_ctc_loss=0.779, task_loss=4.779, task_loss_gen=8.992, contrastive_loss=0, total=3983.96, n_correct=2610.76, ppl=4.38, accuracy=65.532, wps=12975.7, ups=1.63, wpb=7967.9, bsz=297.1, num_updates=19600, lr=0.000101015, gnorm=0.564, clip=0, loss_scale=16, train_wall=61, gb_free=17.1, wall=16022
2023-09-05 15:12:47 | INFO | train_inner | epoch 011:   1450 / 1826 loss=1.996, trans_loss=4.874, nll_loss=2.134, w2v_ctc_loss=0.781, task_loss=4.845, task_loss_gen=9.368, contrastive_loss=0, total=4000.92, n_correct=2625.02, ppl=4.39, accuracy=65.61, wps=12748.9, ups=1.59, wpb=8001.8, bsz=293.5, num_updates=19700, lr=0.000100759, gnorm=0.56, clip=0, loss_scale=16, train_wall=62, gb_free=15.7, wall=16084
2023-09-05 15:13:49 | INFO | train_inner | epoch 011:   1550 / 1826 loss=1.997, trans_loss=4.877, nll_loss=2.137, w2v_ctc_loss=0.779, task_loss=4.737, task_loss_gen=8.796, contrastive_loss=0, total=4013.12, n_correct=2630.88, ppl=4.4, accuracy=65.557, wps=12952.2, ups=1.61, wpb=8026.2, bsz=304.4, num_updates=19800, lr=0.000100504, gnorm=0.566, clip=0, loss_scale=16, train_wall=61, gb_free=15.5, wall=16146
2023-09-05 15:14:51 | INFO | train_inner | epoch 011:   1650 / 1826 loss=1.994, trans_loss=4.871, nll_loss=2.129, w2v_ctc_loss=0.776, task_loss=4.797, task_loss_gen=9.105, contrastive_loss=0, total=3975.76, n_correct=2609.95, ppl=4.37, accuracy=65.647, wps=12822.2, ups=1.61, wpb=7951.5, bsz=290.5, num_updates=19900, lr=0.000100251, gnorm=0.561, clip=0, loss_scale=16, train_wall=61, gb_free=16, wall=16208
2023-09-05 15:15:54 | INFO | train_inner | epoch 011:   1750 / 1826 loss=1.989, trans_loss=4.874, nll_loss=2.134, w2v_ctc_loss=0.766, task_loss=4.84, task_loss_gen=8.88, contrastive_loss=0, total=4033.09, n_correct=2642.4, ppl=4.39, accuracy=65.518, wps=12915.8, ups=1.6, wpb=8066.2, bsz=306.4, num_updates=20000, lr=0.0001, gnorm=0.558, clip=0, loss_scale=16, train_wall=62, gb_free=15.5, wall=16271
2023-09-05 15:15:54 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.7393], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.7393], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 15:16:33 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 3.87 | trans_loss 5.079 | nll_loss 2.346 | w2v_ctc_loss 1.438 | task_loss 40.214 | task_loss_gen 22.13 | contrastive_loss 0 | total 3505.91 | n_correct 2379.55 | ppl 5.09 | accuracy 67.872 | uer 18.49 | wer 20.311 | raw_wer 20.311 | bleu 30.03 | wps 1174.7 | wpb 3505.9 | bsz 119.3 | num_updates 20000 | best_bleu 30.03
2023-09-05 15:16:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 20000 updates
2023-09-05 15:16:33 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_11_20000.pt
2023-09-05 15:16:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_11_20000.pt
2023-09-05 15:16:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_11_20000.pt (epoch 11 @ 20000 updates, score 30.03) (writing took 12.53581040899735 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:0')
2023-09-05 15:17:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:5')
tensor([0.3970], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.3970], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 15:18:12 | INFO | dev_st | epoch 011 | valid on 'dev_st' subset | loss 3.836 | trans_loss 5.075 | nll_loss 2.343 | w2v_ctc_loss 1.333 | task_loss 20.546 | task_loss_gen 22.867 | contrastive_loss 0 | total 3505.91 | n_correct 2380.55 | ppl 5.07 | accuracy 67.901 | uer 18.324 | wer 20.011 | raw_wer 20.011 | bleu 29.91 | wps 1172.3 | wpb 3505.9 | bsz 119.3 | num_updates 20076 | best_bleu 30.03
2023-09-05 15:18:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 20076 updates
2023-09-05 15:18:12 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_29.9104.pt
2023-09-05 15:18:15 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_29.9104.pt
2023-09-05 15:18:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_29.9104.pt (epoch 11 @ 20076 updates, score 29.91) (writing took 7.197066826978698 seconds)
2023-09-05 15:18:20 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)
2023-09-05 15:18:20 | INFO | train | epoch 011 | loss 1.996 | trans_loss 4.87 | nll_loss 2.128 | w2v_ctc_loss 0.779 | task_loss 4.62 | task_loss_gen 10.121 | contrastive_loss 0 | total 3956.53 | n_correct 2593.5 | ppl 4.37 | accuracy 65.55 | wps 11669.1 | ups 1.47 | wpb 7913.1 | bsz 284.9 | num_updates 20076 | lr 9.98105e-05 | gnorm 0.561 | clip 0 | loss_scale 16 | train_wall 1116 | gb_free 15.6 | wall 16417
2023-09-05 15:18:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 15:18:20 | INFO | fairseq.trainer | begin training epoch 12
2023-09-05 15:18:20 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 15:18:43 | INFO | train_inner | epoch 012:     24 / 1826 loss=1.983, trans_loss=4.857, nll_loss=2.111, w2v_ctc_loss=0.763, task_loss=4.252, task_loss_gen=9.456, contrastive_loss=0, total=3952.69, n_correct=2603.65, ppl=4.32, accuracy=65.87, wps=4675.1, ups=0.59, wpb=7905.4, bsz=289, num_updates=20100, lr=9.97509e-05, gnorm=0.557, clip=0, loss_scale=16, train_wall=60, gb_free=16.8, wall=16440
2023-09-05 15:19:44 | INFO | train_inner | epoch 012:    124 / 1826 loss=1.992, trans_loss=4.86, nll_loss=2.113, w2v_ctc_loss=0.777, task_loss=5.243, task_loss_gen=10.722, contrastive_loss=0, total=3862.92, n_correct=2542.42, ppl=4.33, accuracy=65.816, wps=12574.5, ups=1.63, wpb=7725.8, bsz=267, num_updates=20200, lr=9.95037e-05, gnorm=0.576, clip=0, loss_scale=16, train_wall=61, gb_free=15.1, wall=16501
2023-09-05 15:20:46 | INFO | train_inner | epoch 012:    224 / 1826 loss=1.978, trans_loss=4.851, nll_loss=2.103, w2v_ctc_loss=0.757, task_loss=4.817, task_loss_gen=9.051, contrastive_loss=0, total=3993.93, n_correct=2639.78, ppl=4.3, accuracy=66.095, wps=12951.7, ups=1.62, wpb=7987.9, bsz=292.7, num_updates=20300, lr=9.92583e-05, gnorm=0.55, clip=0, loss_scale=16, train_wall=61, gb_free=11.8, wall=16563
2023-09-05 15:21:48 | INFO | train_inner | epoch 012:    324 / 1826 loss=1.979, trans_loss=4.847, nll_loss=2.097, w2v_ctc_loss=0.761, task_loss=4.354, task_loss_gen=9.596, contrastive_loss=0, total=3964.38, n_correct=2621.36, ppl=4.28, accuracy=66.123, wps=12777.3, ups=1.61, wpb=7928.8, bsz=290.2, num_updates=20400, lr=9.90148e-05, gnorm=0.555, clip=0, loss_scale=16, train_wall=61, gb_free=16.2, wall=16625
2023-09-05 15:22:50 | INFO | train_inner | epoch 012:    424 / 1826 loss=1.983, trans_loss=4.852, nll_loss=2.104, w2v_ctc_loss=0.77, task_loss=4.58, task_loss_gen=9.604, contrastive_loss=0, total=3984.39, n_correct=2631.27, ppl=4.3, accuracy=66.039, wps=12836.8, ups=1.61, wpb=7968.8, bsz=293.8, num_updates=20500, lr=9.8773e-05, gnorm=0.554, clip=0, loss_scale=16, train_wall=61, gb_free=16.4, wall=16687
2023-09-05 15:23:52 | INFO | train_inner | epoch 012:    524 / 1826 loss=1.977, trans_loss=4.841, nll_loss=2.09, w2v_ctc_loss=0.762, task_loss=4.682, task_loss_gen=9.807, contrastive_loss=0, total=3956.92, n_correct=2617.01, ppl=4.26, accuracy=66.138, wps=12842.6, ups=1.62, wpb=7913.8, bsz=283.7, num_updates=20600, lr=9.85329e-05, gnorm=0.56, clip=0, loss_scale=16, train_wall=61, gb_free=16, wall=16749
2023-09-05 15:24:53 | INFO | train_inner | epoch 012:    624 / 1826 loss=1.976, trans_loss=4.843, nll_loss=2.093, w2v_ctc_loss=0.757, task_loss=4.579, task_loss_gen=9.972, contrastive_loss=0, total=3953.03, n_correct=2617.19, ppl=4.27, accuracy=66.207, wps=12847.1, ups=1.62, wpb=7906.1, bsz=280.1, num_updates=20700, lr=9.82946e-05, gnorm=0.558, clip=0, loss_scale=16, train_wall=61, gb_free=14.5, wall=16810
2023-09-05 15:25:55 | INFO | train_inner | epoch 012:    724 / 1826 loss=1.989, trans_loss=4.856, nll_loss=2.109, w2v_ctc_loss=0.773, task_loss=3.9, task_loss_gen=10.383, contrastive_loss=0, total=3952.49, n_correct=2600.49, ppl=4.31, accuracy=65.794, wps=12800.7, ups=1.62, wpb=7905, bsz=282.3, num_updates=20800, lr=9.80581e-05, gnorm=0.549, clip=0, loss_scale=16, train_wall=61, gb_free=17.1, wall=16872
2023-09-05 15:26:57 | INFO | train_inner | epoch 012:    824 / 1826 loss=1.985, trans_loss=4.847, nll_loss=2.098, w2v_ctc_loss=0.77, task_loss=4.374, task_loss_gen=11.453, contrastive_loss=0, total=3920.99, n_correct=2590.51, ppl=4.28, accuracy=66.068, wps=12556.9, ups=1.6, wpb=7842, bsz=265.2, num_updates=20900, lr=9.78232e-05, gnorm=0.542, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=16935
2023-09-05 15:28:00 | INFO | train_inner | epoch 012:    924 / 1826 loss=1.966, trans_loss=4.832, nll_loss=2.081, w2v_ctc_loss=0.753, task_loss=2.797, task_loss_gen=9.878, contrastive_loss=0, total=4064.59, n_correct=2698.31, ppl=4.23, accuracy=66.386, wps=13049.6, ups=1.61, wpb=8129.2, bsz=315.7, num_updates=21000, lr=9.759e-05, gnorm=0.525, clip=0, loss_scale=32, train_wall=62, gb_free=17.2, wall=16997
2023-09-05 15:29:01 | INFO | train_inner | epoch 012:   1024 / 1826 loss=1.961, trans_loss=4.826, nll_loss=2.073, w2v_ctc_loss=0.747, task_loss=2.379, task_loss_gen=10.951, contrastive_loss=0, total=4027.9, n_correct=2681.7, ppl=4.21, accuracy=66.578, wps=13176.5, ups=1.64, wpb=8055.8, bsz=307.1, num_updates=21100, lr=9.73585e-05, gnorm=0.528, clip=0, loss_scale=32, train_wall=60, gb_free=16, wall=17058
2023-09-05 15:30:03 | INFO | train_inner | epoch 012:   1124 / 1826 loss=1.976, trans_loss=4.837, nll_loss=2.086, w2v_ctc_loss=0.766, task_loss=3.077, task_loss_gen=11.595, contrastive_loss=0, total=3982.26, n_correct=2639.67, ppl=4.24, accuracy=66.286, wps=12867.8, ups=1.62, wpb=7964.5, bsz=286.6, num_updates=21200, lr=9.71286e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=61, gb_free=15.8, wall=17120
2023-09-05 15:31:04 | INFO | train_inner | epoch 012:   1224 / 1826 loss=1.98, trans_loss=4.85, nll_loss=2.104, w2v_ctc_loss=0.763, task_loss=3.4, task_loss_gen=11.223, contrastive_loss=0, total=3949.09, n_correct=2611.44, ppl=4.3, accuracy=66.128, wps=12823.1, ups=1.62, wpb=7898.2, bsz=285.4, num_updates=21300, lr=9.69003e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=61, gb_free=12.3, wall=17182
2023-09-05 15:32:06 | INFO | train_inner | epoch 012:   1324 / 1826 loss=1.974, trans_loss=4.845, nll_loss=2.096, w2v_ctc_loss=0.755, task_loss=2.753, task_loss_gen=11.696, contrastive_loss=0, total=3963.34, n_correct=2620.26, ppl=4.27, accuracy=66.112, wps=12866.8, ups=1.62, wpb=7926.7, bsz=290.9, num_updates=21400, lr=9.66736e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=61, gb_free=12.9, wall=17243
2023-09-05 15:33:09 | INFO | train_inner | epoch 012:   1424 / 1826 loss=1.985, trans_loss=4.844, nll_loss=2.095, w2v_ctc_loss=0.776, task_loss=3.848, task_loss_gen=12.481, contrastive_loss=0, total=3926.15, n_correct=2593.24, ppl=4.27, accuracy=66.05, wps=12514.2, ups=1.59, wpb=7852.3, bsz=267.9, num_updates=21500, lr=9.64486e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=17306
2023-09-05 15:34:11 | INFO | train_inner | epoch 012:   1524 / 1826 loss=1.989, trans_loss=4.869, nll_loss=2.128, w2v_ctc_loss=0.772, task_loss=3.13, task_loss_gen=11.219, contrastive_loss=0, total=4009.37, n_correct=2637.99, ppl=4.37, accuracy=65.796, wps=12818.5, ups=1.6, wpb=8018.7, bsz=300.2, num_updates=21600, lr=9.6225e-05, gnorm=0.532, clip=0, loss_scale=32, train_wall=62, gb_free=15.4, wall=17368
2023-09-05 15:35:13 | INFO | train_inner | epoch 012:   1624 / 1826 loss=1.995, trans_loss=4.857, nll_loss=2.112, w2v_ctc_loss=0.787, task_loss=4.181, task_loss_gen=12.884, contrastive_loss=0, total=3883.94, n_correct=2558.29, ppl=4.32, accuracy=65.868, wps=12509.5, ups=1.61, wpb=7767.9, bsz=262.9, num_updates=21700, lr=9.60031e-05, gnorm=0.55, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=17431
2023-09-05 15:36:15 | INFO | train_inner | epoch 012:   1724 / 1826 loss=1.985, trans_loss=4.851, nll_loss=2.105, w2v_ctc_loss=0.78, task_loss=3.17, task_loss_gen=11.706, contrastive_loss=0, total=3919.88, n_correct=2589.34, ppl=4.3, accuracy=66.057, wps=12802.8, ups=1.63, wpb=7839.8, bsz=283, num_updates=21800, lr=9.57826e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=61, gb_free=15, wall=17492
2023-09-05 15:37:17 | INFO | train_inner | epoch 012:   1824 / 1826 loss=1.971, trans_loss=4.829, nll_loss=2.077, w2v_ctc_loss=0.754, task_loss=3.355, task_loss_gen=13.049, contrastive_loss=0, total=3908.91, n_correct=2603.61, ppl=4.22, accuracy=66.607, wps=12525.7, ups=1.6, wpb=7817.8, bsz=269, num_updates=21900, lr=9.55637e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=62, gb_free=16.1, wall=17554
2023-09-05 15:37:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.4990], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.4990], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 15:37:58 | INFO | dev_st | epoch 012 | valid on 'dev_st' subset | loss 3.838 | trans_loss 5.058 | nll_loss 2.318 | w2v_ctc_loss 1.379 | task_loss 20.179 | task_loss_gen 23.207 | contrastive_loss 0 | total 3505.91 | n_correct 2388.64 | ppl 4.98 | accuracy 68.132 | uer 18.447 | wer 20.202 | raw_wer 20.202 | bleu 30.16 | wps 1168.4 | wpb 3505.9 | bsz 119.3 | num_updates 21902 | best_bleu 30.16
2023-09-05 15:37:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 21902 updates
2023-09-05 15:37:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 15:38:05 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 15:38:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 12 @ 21902 updates, score 30.16) (writing took 12.623381908982992 seconds)
2023-09-05 15:38:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)
2023-09-05 15:38:11 | INFO | train | epoch 012 | loss 1.98 | trans_loss 4.846 | nll_loss 2.098 | w2v_ctc_loss 0.765 | task_loss 3.795 | task_loss_gen 10.902 | contrastive_loss 0 | total 3956.37 | n_correct 2615.99 | ppl 4.28 | accuracy 66.121 | wps 12131 | ups 1.53 | wpb 7912.7 | bsz 284.8 | num_updates 21902 | lr 9.55593e-05 | gnorm 0.545 | clip 0 | loss_scale 32 | train_wall 1117 | gb_free 10 | wall 17608
2023-09-05 15:38:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 15:38:11 | INFO | fairseq.trainer | begin training epoch 13
2023-09-05 15:38:11 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 15:39:19 | INFO | train_inner | epoch 013:     98 / 1826 loss=1.956, trans_loss=4.817, nll_loss=2.061, w2v_ctc_loss=0.742, task_loss=2.651, task_loss_gen=11.977, contrastive_loss=0, total=3964.32, n_correct=2649.6, ppl=4.17, accuracy=66.836, wps=6482.6, ups=0.82, wpb=7928.6, bsz=298.2, num_updates=22000, lr=9.53463e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=61, gb_free=17.4, wall=17677
2023-09-05 15:39:19 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.9883], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.9883], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 15:39:59 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.834 | trans_loss 5.051 | nll_loss 2.312 | w2v_ctc_loss 1.382 | task_loss 32.707 | task_loss_gen 21.417 | contrastive_loss 0 | total 3505.91 | n_correct 2385.64 | ppl 4.97 | accuracy 68.046 | uer 17.994 | wer 19.879 | raw_wer 19.879 | bleu 29.94 | wps 1183.5 | wpb 3505.9 | bsz 119.3 | num_updates 22000 | best_bleu 30.16
2023-09-05 15:39:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 22000 updates
2023-09-05 15:39:59 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_13_22000.pt
2023-09-05 15:40:01 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_13_22000.pt
2023-09-05 15:40:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_13_22000.pt (epoch 13 @ 22000 updates, score 29.94) (writing took 8.534820596105419 seconds)
2023-09-05 15:41:10 | INFO | train_inner | epoch 013:    198 / 1826 loss=1.965, trans_loss=4.824, nll_loss=2.07, w2v_ctc_loss=0.752, task_loss=3.483, task_loss_gen=12.282, contrastive_loss=0, total=3968.9, n_correct=2645.48, ppl=4.2, accuracy=66.655, wps=7180.8, ups=0.9, wpb=7937.8, bsz=284.2, num_updates=22100, lr=9.51303e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=61, gb_free=15.5, wall=17787
2023-09-05 15:42:11 | INFO | train_inner | epoch 013:    298 / 1826 loss=1.96, trans_loss=4.822, nll_loss=2.067, w2v_ctc_loss=0.743, task_loss=2.806, task_loss_gen=12.335, contrastive_loss=0, total=3986, n_correct=2663.16, ppl=4.19, accuracy=66.813, wps=13133.2, ups=1.65, wpb=7972, bsz=284.1, num_updates=22200, lr=9.49158e-05, gnorm=0.534, clip=0, loss_scale=32, train_wall=60, gb_free=10.5, wall=17848
2023-09-05 15:43:12 | INFO | train_inner | epoch 013:    398 / 1826 loss=1.96, trans_loss=4.825, nll_loss=2.071, w2v_ctc_loss=0.74, task_loss=2.83, task_loss_gen=11.848, contrastive_loss=0, total=3998.65, n_correct=2660.49, ppl=4.2, accuracy=66.535, wps=12934.8, ups=1.62, wpb=7997.3, bsz=304.3, num_updates=22300, lr=9.47027e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=17910
2023-09-05 15:44:14 | INFO | train_inner | epoch 013:    498 / 1826 loss=1.964, trans_loss=4.819, nll_loss=2.064, w2v_ctc_loss=0.753, task_loss=3.135, task_loss_gen=12.1, contrastive_loss=0, total=3899.81, n_correct=2602.09, ppl=4.18, accuracy=66.724, wps=12584.8, ups=1.61, wpb=7799.6, bsz=284.4, num_updates=22400, lr=9.44911e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=61, gb_free=17.3, wall=17972
2023-09-05 15:45:16 | INFO | train_inner | epoch 013:    598 / 1826 loss=1.968, trans_loss=4.824, nll_loss=2.07, w2v_ctc_loss=0.76, task_loss=2.958, task_loss_gen=13.281, contrastive_loss=0, total=3901.11, n_correct=2596.7, ppl=4.2, accuracy=66.563, wps=12587.4, ups=1.61, wpb=7802.2, bsz=275.9, num_updates=22500, lr=9.42809e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=61, gb_free=15.8, wall=18034
2023-09-05 15:46:19 | INFO | train_inner | epoch 013:    698 / 1826 loss=1.974, trans_loss=4.826, nll_loss=2.073, w2v_ctc_loss=0.771, task_loss=4.209, task_loss_gen=12.326, contrastive_loss=0, total=3939.37, n_correct=2623.04, ppl=4.21, accuracy=66.585, wps=12602, ups=1.6, wpb=7878.7, bsz=273.3, num_updates=22600, lr=9.40721e-05, gnorm=0.553, clip=0, loss_scale=32, train_wall=62, gb_free=15.5, wall=18096
2023-09-05 15:47:20 | INFO | train_inner | epoch 013:    798 / 1826 loss=1.972, trans_loss=4.832, nll_loss=2.08, w2v_ctc_loss=0.76, task_loss=3.5, task_loss_gen=12.269, contrastive_loss=0, total=3978.46, n_correct=2640.22, ppl=4.23, accuracy=66.363, wps=12932.3, ups=1.63, wpb=7956.9, bsz=284.7, num_updates=22700, lr=9.38647e-05, gnorm=0.544, clip=0, loss_scale=32, train_wall=61, gb_free=15.8, wall=18158
2023-09-05 15:48:22 | INFO | train_inner | epoch 013:    898 / 1826 loss=1.962, trans_loss=4.818, nll_loss=2.062, w2v_ctc_loss=0.754, task_loss=3.442, task_loss_gen=12.356, contrastive_loss=0, total=3963.66, n_correct=2648.29, ppl=4.17, accuracy=66.814, wps=12849.4, ups=1.62, wpb=7927.3, bsz=283.7, num_updates=22800, lr=9.36586e-05, gnorm=0.54, clip=0, loss_scale=32, train_wall=61, gb_free=11.4, wall=18219
2023-09-05 15:49:23 | INFO | train_inner | epoch 013:    998 / 1826 loss=1.954, trans_loss=4.81, nll_loss=2.052, w2v_ctc_loss=0.737, task_loss=3.268, task_loss_gen=12.623, contrastive_loss=0, total=3903.96, n_correct=2610.91, ppl=4.15, accuracy=66.879, wps=12750.4, ups=1.63, wpb=7807.9, bsz=277.4, num_updates=22900, lr=9.34539e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=60, gb_free=15.2, wall=18281
2023-09-05 15:50:26 | INFO | train_inner | epoch 013:   1098 / 1826 loss=1.965, trans_loss=4.821, nll_loss=2.066, w2v_ctc_loss=0.751, task_loss=2.983, task_loss_gen=12.974, contrastive_loss=0, total=3984.36, n_correct=2654.59, ppl=4.19, accuracy=66.625, wps=12785, ups=1.6, wpb=7968.7, bsz=283.6, num_updates=23000, lr=9.32505e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=62, gb_free=16.7, wall=18343
2023-09-05 15:51:28 | INFO | train_inner | epoch 013:   1198 / 1826 loss=1.966, trans_loss=4.825, nll_loss=2.072, w2v_ctc_loss=0.759, task_loss=2.288, task_loss_gen=14.024, contrastive_loss=0, total=4004.3, n_correct=2664.01, ppl=4.21, accuracy=66.529, wps=12882.8, ups=1.61, wpb=8008.6, bsz=292, num_updates=23100, lr=9.30484e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=16.2, wall=18405
2023-09-05 15:52:30 | INFO | train_inner | epoch 013:   1298 / 1826 loss=1.965, trans_loss=4.825, nll_loss=2.072, w2v_ctc_loss=0.752, task_loss=2.786, task_loss_gen=13.883, contrastive_loss=0, total=3983.14, n_correct=2655.79, ppl=4.2, accuracy=66.676, wps=12836, ups=1.61, wpb=7966.3, bsz=285, num_updates=23200, lr=9.28477e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=16.7, wall=18467
2023-09-05 15:53:31 | INFO | train_inner | epoch 013:   1398 / 1826 loss=1.956, trans_loss=4.813, nll_loss=2.057, w2v_ctc_loss=0.746, task_loss=2.592, task_loss_gen=13.585, contrastive_loss=0, total=3950.68, n_correct=2645.11, ppl=4.16, accuracy=66.953, wps=12906.7, ups=1.63, wpb=7901.4, bsz=286.7, num_updates=23300, lr=9.26482e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=61, gb_free=14.5, wall=18528
2023-09-05 15:54:33 | INFO | train_inner | epoch 013:   1498 / 1826 loss=1.959, trans_loss=4.817, nll_loss=2.063, w2v_ctc_loss=0.742, task_loss=2.654, task_loss_gen=14.413, contrastive_loss=0, total=3943.21, n_correct=2632.36, ppl=4.18, accuracy=66.757, wps=12773.5, ups=1.62, wpb=7886.4, bsz=280.1, num_updates=23400, lr=9.245e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=61, gb_free=16.5, wall=18590
2023-09-05 15:55:35 | INFO | train_inner | epoch 013:   1598 / 1826 loss=1.965, trans_loss=4.814, nll_loss=2.059, w2v_ctc_loss=0.756, task_loss=2.795, task_loss_gen=14.865, contrastive_loss=0, total=3883.43, n_correct=2590.97, ppl=4.17, accuracy=66.719, wps=12496.8, ups=1.61, wpb=7766.9, bsz=272.4, num_updates=23500, lr=9.22531e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=61, gb_free=17, wall=18652
2023-09-05 15:56:37 | INFO | train_inner | epoch 013:   1698 / 1826 loss=1.962, trans_loss=4.817, nll_loss=2.063, w2v_ctc_loss=0.755, task_loss=2.784, task_loss_gen=14.518, contrastive_loss=0, total=3971.09, n_correct=2654.08, ppl=4.18, accuracy=66.835, wps=12770.9, ups=1.61, wpb=7942.2, bsz=286.3, num_updates=23600, lr=9.20575e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=16.2, wall=18714
2023-09-05 15:57:39 | INFO | train_inner | epoch 013:   1798 / 1826 loss=1.957, trans_loss=4.826, nll_loss=2.074, w2v_ctc_loss=0.739, task_loss=1.864, task_loss_gen=13.853, contrastive_loss=0, total=4015.37, n_correct=2677.53, ppl=4.21, accuracy=66.682, wps=12991.7, ups=1.62, wpb=8030.7, bsz=300.8, num_updates=23700, lr=9.1863e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=61, gb_free=15.9, wall=18776
2023-09-05 15:57:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.6274], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.6274], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 15:58:36 | INFO | dev_st | epoch 013 | valid on 'dev_st' subset | loss 3.833 | trans_loss 5.05 | nll_loss 2.31 | w2v_ctc_loss 1.379 | task_loss 12.239 | task_loss_gen 29.112 | contrastive_loss 0 | total 3505.91 | n_correct 2392.91 | ppl 4.96 | accuracy 68.254 | uer 18.52 | wer 20.206 | raw_wer 20.206 | bleu 30.21 | wps 1193.4 | wpb 3505.9 | bsz 119.3 | num_updates 23728 | best_bleu 30.21
2023-09-05 15:58:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 23728 updates
2023-09-05 15:58:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 15:58:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 15:58:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 13 @ 23728 updates, score 30.21) (writing took 12.884198268991895 seconds)
2023-09-05 15:58:49 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)
2023-09-05 15:58:49 | INFO | train | epoch 013 | loss 1.963 | trans_loss 4.821 | nll_loss 2.066 | w2v_ctc_loss 0.751 | task_loss 2.94 | task_loss_gen 13.126 | contrastive_loss 0 | total 3956.37 | n_correct 2638.74 | ppl 4.19 | accuracy 66.696 | wps 11671.7 | ups 1.48 | wpb 7912.7 | bsz 284.8 | num_updates 23728 | lr 9.18088e-05 | gnorm 0.539 | clip 0 | loss_scale 64 | train_wall 1114 | gb_free 11.7 | wall 18846
2023-09-05 15:58:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 15:58:49 | INFO | fairseq.trainer | begin training epoch 14
2023-09-05 15:58:49 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 15:59:41 | INFO | train_inner | epoch 014:     72 / 1826 loss=1.955, trans_loss=4.803, nll_loss=2.043, w2v_ctc_loss=0.748, task_loss=2.584, task_loss_gen=15.251, contrastive_loss=0, total=3902.2, n_correct=2615.1, ppl=4.12, accuracy=67.016, wps=6410.7, ups=0.82, wpb=7804.4, bsz=272.6, num_updates=23800, lr=9.16698e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=60, gb_free=16, wall=18898
2023-09-05 16:00:43 | INFO | train_inner | epoch 014:    172 / 1826 loss=1.945, trans_loss=4.795, nll_loss=2.033, w2v_ctc_loss=0.736, task_loss=2.712, task_loss_gen=14.227, contrastive_loss=0, total=3932.76, n_correct=2645.26, ppl=4.09, accuracy=67.262, wps=12702.9, ups=1.62, wpb=7865.5, bsz=287.3, num_updates=23900, lr=9.14779e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=61, gb_free=11.2, wall=18960
2023-09-05 16:01:44 | INFO | train_inner | epoch 014:    272 / 1826 loss=1.945, trans_loss=4.798, nll_loss=2.037, w2v_ctc_loss=0.737, task_loss=2.538, task_loss_gen=14.32, contrastive_loss=0, total=3979.01, n_correct=2676.31, ppl=4.1, accuracy=67.261, wps=12920.2, ups=1.62, wpb=7958, bsz=296.7, num_updates=24000, lr=9.12871e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=17.4, wall=19022
2023-09-05 16:01:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.8970], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.8970], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 16:02:23 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.837 | trans_loss 5.047 | nll_loss 2.306 | w2v_ctc_loss 1.403 | task_loss 10.946 | task_loss_gen 30.614 | contrastive_loss 0 | total 3505.91 | n_correct 2392.45 | ppl 4.94 | accuracy 68.241 | uer 18.34 | wer 20.172 | raw_wer 20.172 | bleu 30.1 | wps 1191.4 | wpb 3505.9 | bsz 119.3 | num_updates 24000 | best_bleu 30.21
2023-09-05 16:02:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 24000 updates
2023-09-05 16:02:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_14_24000.pt
2023-09-05 16:02:26 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_14_24000.pt
2023-09-05 16:02:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_14_24000.pt (epoch 14 @ 24000 updates, score 30.1) (writing took 9.408261951990426 seconds)
2023-09-05 16:03:35 | INFO | train_inner | epoch 014:    372 / 1826 loss=1.949, trans_loss=4.803, nll_loss=2.044, w2v_ctc_loss=0.735, task_loss=2.487, task_loss_gen=15.29, contrastive_loss=0, total=3919.85, n_correct=2630.46, ppl=4.12, accuracy=67.106, wps=7081.7, ups=0.9, wpb=7839.7, bsz=281.2, num_updates=24100, lr=9.10975e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=16, wall=19132
2023-09-05 16:04:36 | INFO | train_inner | epoch 014:    472 / 1826 loss=1.949, trans_loss=4.798, nll_loss=2.037, w2v_ctc_loss=0.737, task_loss=2.88, task_loss_gen=15.284, contrastive_loss=0, total=3900.8, n_correct=2620.52, ppl=4.1, accuracy=67.179, wps=12707.3, ups=1.63, wpb=7801.6, bsz=272.9, num_updates=24200, lr=9.09091e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=16.1, wall=19194
2023-09-05 16:05:38 | INFO | train_inner | epoch 014:    572 / 1826 loss=1.952, trans_loss=4.805, nll_loss=2.046, w2v_ctc_loss=0.74, task_loss=2.633, task_loss_gen=14.881, contrastive_loss=0, total=3931.83, n_correct=2637.98, ppl=4.13, accuracy=67.093, wps=12799.5, ups=1.63, wpb=7863.7, bsz=278.6, num_updates=24300, lr=9.07218e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=61, gb_free=16.1, wall=19255
2023-09-05 16:06:39 | INFO | train_inner | epoch 014:    672 / 1826 loss=1.952, trans_loss=4.802, nll_loss=2.043, w2v_ctc_loss=0.746, task_loss=2.229, task_loss_gen=14.617, contrastive_loss=0, total=3948.45, n_correct=2646.92, ppl=4.12, accuracy=67.037, wps=12847, ups=1.63, wpb=7896.9, bsz=292.8, num_updates=24400, lr=9.05357e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=61, gb_free=14.4, wall=19317
2023-09-05 16:07:41 | INFO | train_inner | epoch 014:    772 / 1826 loss=1.941, trans_loss=4.794, nll_loss=2.033, w2v_ctc_loss=0.727, task_loss=2.305, task_loss_gen=14.96, contrastive_loss=0, total=4028.47, n_correct=2711.78, ppl=4.09, accuracy=67.315, wps=13110.9, ups=1.63, wpb=8056.9, bsz=294.5, num_updates=24500, lr=9.03508e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=61, gb_free=15.8, wall=19378
2023-09-05 16:08:43 | INFO | train_inner | epoch 014:    872 / 1826 loss=1.956, trans_loss=4.8, nll_loss=2.04, w2v_ctc_loss=0.749, task_loss=2.562, task_loss_gen=16.245, contrastive_loss=0, total=3996.44, n_correct=2681.57, ppl=4.11, accuracy=67.099, wps=12880.1, ups=1.61, wpb=7992.9, bsz=271.3, num_updates=24600, lr=9.0167e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=61, gb_free=15.5, wall=19440
2023-09-05 16:09:45 | INFO | train_inner | epoch 014:    972 / 1826 loss=1.943, trans_loss=4.799, nll_loss=2.039, w2v_ctc_loss=0.729, task_loss=2.423, task_loss_gen=14.247, contrastive_loss=0, total=4038.54, n_correct=2716.86, ppl=4.11, accuracy=67.273, wps=12893.1, ups=1.6, wpb=8077.1, bsz=300.8, num_updates=24700, lr=8.99843e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=62, gb_free=17.3, wall=19503
2023-09-05 16:10:48 | INFO | train_inner | epoch 014:   1072 / 1826 loss=1.945, trans_loss=4.79, nll_loss=2.027, w2v_ctc_loss=0.74, task_loss=2.674, task_loss_gen=15.399, contrastive_loss=0, total=3945.93, n_correct=2658.23, ppl=4.08, accuracy=67.366, wps=12667.4, ups=1.61, wpb=7891.9, bsz=283.7, num_updates=24800, lr=8.98027e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=17.5, wall=19565
2023-09-05 16:11:50 | INFO | train_inner | epoch 014:   1172 / 1826 loss=1.952, trans_loss=4.804, nll_loss=2.045, w2v_ctc_loss=0.739, task_loss=2.14, task_loss_gen=15.316, contrastive_loss=0, total=3973.95, n_correct=2662.9, ppl=4.13, accuracy=67.009, wps=12785.1, ups=1.61, wpb=7947.9, bsz=285.2, num_updates=24900, lr=8.96221e-05, gnorm=0.534, clip=0, loss_scale=128, train_wall=61, gb_free=16.8, wall=19627
2023-09-05 16:12:51 | INFO | train_inner | epoch 014:   1272 / 1826 loss=1.946, trans_loss=4.798, nll_loss=2.038, w2v_ctc_loss=0.734, task_loss=2.079, task_loss_gen=15.779, contrastive_loss=0, total=3954.66, n_correct=2655.25, ppl=4.11, accuracy=67.142, wps=12875.3, ups=1.63, wpb=7909.3, bsz=288.9, num_updates=25000, lr=8.94427e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=61, gb_free=15.7, wall=19689
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:0')
2023-09-05 16:13:59 | INFO | train_inner | epoch 014:   1372 / 1826 loss=1.958, trans_loss=4.81, nll_loss=2.054, w2v_ctc_loss=0.749, task_loss=2.603, task_loss_gen=16.225, contrastive_loss=0, total=3923.64, n_correct=2624.02, ppl=4.15, accuracy=66.877, wps=11650.5, ups=1.48, wpb=7847.3, bsz=286.8, num_updates=25100, lr=8.92644e-05, gnorm=0.539, clip=0, loss_scale=128, train_wall=67, gb_free=16.2, wall=19756
2023-09-05 16:14:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-09-05 16:15:05 | INFO | train_inner | epoch 014:   1473 / 1826 loss=1.949, trans_loss=4.803, nll_loss=2.045, w2v_ctc_loss=0.736, task_loss=2.063, task_loss_gen=17.722, contrastive_loss=0, total=3960.68, n_correct=2661.49, ppl=4.13, accuracy=67.198, wps=11944.6, ups=1.51, wpb=7921.4, bsz=281.6, num_updates=25200, lr=8.90871e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=65, gb_free=15.5, wall=19822
2023-09-05 16:16:11 | INFO | train_inner | epoch 014:   1573 / 1826 loss=1.954, trans_loss=4.8, nll_loss=2.039, w2v_ctc_loss=0.746, task_loss=2.184, task_loss_gen=17.991, contrastive_loss=0, total=3882.9, n_correct=2605.15, ppl=4.11, accuracy=67.093, wps=11843.2, ups=1.53, wpb=7765.8, bsz=261.1, num_updates=25300, lr=8.89108e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=65, gb_free=16.3, wall=19888
2023-09-05 16:17:13 | INFO | train_inner | epoch 014:   1673 / 1826 loss=1.948, trans_loss=4.807, nll_loss=2.049, w2v_ctc_loss=0.739, task_loss=2.096, task_loss_gen=15.205, contrastive_loss=0, total=4028.71, n_correct=2703.61, ppl=4.14, accuracy=67.109, wps=13017.3, ups=1.62, wpb=8057.4, bsz=301.2, num_updates=25400, lr=8.87357e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=61, gb_free=15.4, wall=19950
2023-09-05 16:18:14 | INFO | train_inner | epoch 014:   1773 / 1826 loss=1.945, trans_loss=4.794, nll_loss=2.033, w2v_ctc_loss=0.734, task_loss=2.416, task_loss_gen=15.806, contrastive_loss=0, total=3970.96, n_correct=2672.97, ppl=4.09, accuracy=67.313, wps=12961.9, ups=1.63, wpb=7941.9, bsz=286.9, num_updates=25500, lr=8.85615e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=61, gb_free=17, wall=20011
2023-09-05 16:18:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:1')
tensor([0.7124], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.7124], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 16:19:26 | INFO | dev_st | epoch 014 | valid on 'dev_st' subset | loss 3.808 | trans_loss 5.03 | nll_loss 2.285 | w2v_ctc_loss 1.341 | task_loss 40.052 | task_loss_gen 25.261 | contrastive_loss 0 | total 3505.91 | n_correct 2408.09 | ppl 4.87 | accuracy 68.687 | uer 17.862 | wer 19.767 | raw_wer 19.767 | bleu 30.61 | wps 1177 | wpb 3505.9 | bsz 119.3 | num_updates 25553 | best_bleu 30.61
2023-09-05 16:19:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 25553 updates
2023-09-05 16:19:26 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 16:19:35 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 16:19:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 14 @ 25553 updates, score 30.61) (writing took 15.047120813978836 seconds)
2023-09-05 16:19:42 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)
2023-09-05 16:19:42 | INFO | train | epoch 014 | loss 1.949 | trans_loss 4.8 | nll_loss 2.041 | w2v_ctc_loss 0.739 | task_loss 2.407 | task_loss_gen 15.487 | contrastive_loss 0 | total 3956.42 | n_correct 2656.98 | ppl 4.11 | accuracy 67.156 | wps 11525.9 | ups 1.46 | wpb 7912.8 | bsz 284.8 | num_updates 25553 | lr 8.84696e-05 | gnorm 0.533 | clip 0 | loss_scale 64 | train_wall 1126 | gb_free 17 | wall 20099
2023-09-05 16:19:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 16:19:42 | INFO | fairseq.trainer | begin training epoch 15
2023-09-05 16:19:42 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 16:20:19 | INFO | train_inner | epoch 015:     47 / 1826 loss=1.943, trans_loss=4.797, nll_loss=2.037, w2v_ctc_loss=0.734, task_loss=2.384, task_loss_gen=15.868, contrastive_loss=0, total=3892.38, n_correct=2622.91, ppl=4.1, accuracy=67.386, wps=6233.9, ups=0.8, wpb=7784.8, bsz=281.3, num_updates=25600, lr=8.83883e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=61, gb_free=14.7, wall=20136
2023-09-05 16:21:20 | INFO | train_inner | epoch 015:    147 / 1826 loss=1.939, trans_loss=4.783, nll_loss=2.018, w2v_ctc_loss=0.731, task_loss=2.728, task_loss_gen=15.475, contrastive_loss=0, total=3933.74, n_correct=2656.66, ppl=4.05, accuracy=67.535, wps=12783.9, ups=1.62, wpb=7867.5, bsz=280.2, num_updates=25700, lr=8.82162e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=16.9, wall=20197
2023-09-05 16:22:22 | INFO | train_inner | epoch 015:    247 / 1826 loss=1.936, trans_loss=4.781, nll_loss=2.016, w2v_ctc_loss=0.725, task_loss=2.871, task_loss_gen=14.791, contrastive_loss=0, total=3951.59, n_correct=2673.8, ppl=4.05, accuracy=67.664, wps=12728.6, ups=1.61, wpb=7903.2, bsz=281.2, num_updates=25800, lr=8.80451e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=15.4, wall=20260
2023-09-05 16:23:24 | INFO | train_inner | epoch 015:    347 / 1826 loss=1.932, trans_loss=4.772, nll_loss=2.004, w2v_ctc_loss=0.723, task_loss=2.68, task_loss_gen=14.865, contrastive_loss=0, total=3976.75, n_correct=2693.18, ppl=4.01, accuracy=67.723, wps=12896.8, ups=1.62, wpb=7953.5, bsz=284.7, num_updates=25900, lr=8.7875e-05, gnorm=0.53, clip=0, loss_scale=64, train_wall=61, gb_free=14.8, wall=20321
2023-09-05 16:24:25 | INFO | train_inner | epoch 015:    447 / 1826 loss=1.938, trans_loss=4.782, nll_loss=2.017, w2v_ctc_loss=0.726, task_loss=2.865, task_loss_gen=15.726, contrastive_loss=0, total=3878.95, n_correct=2624.03, ppl=4.05, accuracy=67.648, wps=12621.7, ups=1.63, wpb=7757.9, bsz=263.5, num_updates=26000, lr=8.77058e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=61, gb_free=16.5, wall=20383
2023-09-05 16:24:25 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.7471], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.7471], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 16:25:05 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.826 | trans_loss 5.034 | nll_loss 2.286 | w2v_ctc_loss 1.394 | task_loss 38.687 | task_loss_gen 24.925 | contrastive_loss 0 | total 3505.91 | n_correct 2409.55 | ppl 4.88 | accuracy 68.728 | uer 17.956 | wer 19.778 | raw_wer 19.778 | bleu 30.6 | wps 1194 | wpb 3505.9 | bsz 119.3 | num_updates 26000 | best_bleu 30.61
2023-09-05 16:25:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 26000 updates
2023-09-05 16:25:05 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_15_26000.pt
2023-09-05 16:25:07 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_15_26000.pt
2023-09-05 16:25:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_15_26000.pt (epoch 15 @ 26000 updates, score 30.6) (writing took 10.356251964927651 seconds)
2023-09-05 16:26:16 | INFO | train_inner | epoch 015:    547 / 1826 loss=1.93, trans_loss=4.777, nll_loss=2.012, w2v_ctc_loss=0.717, task_loss=2.61, task_loss_gen=13.961, contrastive_loss=0, total=3934.71, n_correct=2661.06, ppl=4.03, accuracy=67.63, wps=7092.4, ups=0.9, wpb=7869.4, bsz=295.5, num_updates=26100, lr=8.75376e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=60, gb_free=13.5, wall=20494
2023-09-05 16:27:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 16:27:19 | INFO | train_inner | epoch 015:    648 / 1826 loss=1.932, trans_loss=4.778, nll_loss=2.012, w2v_ctc_loss=0.726, task_loss=2.445, task_loss_gen=14.048, contrastive_loss=0, total=3969.28, n_correct=2686.79, ppl=4.03, accuracy=67.69, wps=12746.9, ups=1.61, wpb=7938.6, bsz=293.5, num_updates=26200, lr=8.73704e-05, gnorm=0.537, clip=0, loss_scale=32, train_wall=61, gb_free=16.3, wall=20556
2023-09-05 16:28:20 | INFO | train_inner | epoch 015:    748 / 1826 loss=1.942, trans_loss=4.788, nll_loss=2.025, w2v_ctc_loss=0.731, task_loss=3.705, task_loss_gen=13.663, contrastive_loss=0, total=3889.93, n_correct=2622.32, ppl=4.07, accuracy=67.413, wps=12653.6, ups=1.63, wpb=7779.9, bsz=271.3, num_updates=26300, lr=8.72041e-05, gnorm=0.562, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=20617
2023-09-05 16:29:22 | INFO | train_inner | epoch 015:    848 / 1826 loss=1.942, trans_loss=4.784, nll_loss=2.02, w2v_ctc_loss=0.733, task_loss=4.844, task_loss_gen=12.718, contrastive_loss=0, total=3912.34, n_correct=2639.49, ppl=4.05, accuracy=67.466, wps=12690.4, ups=1.62, wpb=7824.7, bsz=270.8, num_updates=26400, lr=8.70388e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=61, gb_free=16.8, wall=20679
2023-09-05 16:30:24 | INFO | train_inner | epoch 015:    948 / 1826 loss=1.946, trans_loss=4.803, nll_loss=2.044, w2v_ctc_loss=0.73, task_loss=4.925, task_loss_gen=11.25, contrastive_loss=0, total=3982.81, n_correct=2674.97, ppl=4.12, accuracy=67.163, wps=12871, ups=1.62, wpb=7965.6, bsz=286.4, num_updates=26500, lr=8.68744e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=61, gb_free=14.9, wall=20741
2023-09-05 16:31:25 | INFO | train_inner | epoch 015:   1048 / 1826 loss=1.939, trans_loss=4.794, nll_loss=2.033, w2v_ctc_loss=0.725, task_loss=4.524, task_loss_gen=10.769, contrastive_loss=0, total=3987.14, n_correct=2686.41, ppl=4.09, accuracy=67.377, wps=12920.4, ups=1.62, wpb=7974.3, bsz=295.8, num_updates=26600, lr=8.6711e-05, gnorm=0.545, clip=0, loss_scale=32, train_wall=61, gb_free=16.5, wall=20803
2023-09-05 16:32:28 | INFO | train_inner | epoch 015:   1148 / 1826 loss=1.945, trans_loss=4.798, nll_loss=2.037, w2v_ctc_loss=0.732, task_loss=4.569, task_loss_gen=11.903, contrastive_loss=0, total=3978.61, n_correct=2680.51, ppl=4.1, accuracy=67.373, wps=12770.2, ups=1.6, wpb=7957.2, bsz=278.1, num_updates=26700, lr=8.65485e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=61, gb_free=16.6, wall=20865
2023-09-05 16:33:30 | INFO | train_inner | epoch 015:   1248 / 1826 loss=1.942, trans_loss=4.791, nll_loss=2.029, w2v_ctc_loss=0.729, task_loss=4.764, task_loss_gen=11.615, contrastive_loss=0, total=3971.9, n_correct=2678.15, ppl=4.08, accuracy=67.427, wps=12867.1, ups=1.62, wpb=7943.8, bsz=285.3, num_updates=26800, lr=8.63868e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=61, gb_free=10.3, wall=20927
2023-09-05 16:34:32 | INFO | train_inner | epoch 015:   1348 / 1826 loss=1.933, trans_loss=4.788, nll_loss=2.025, w2v_ctc_loss=0.712, task_loss=4.505, task_loss_gen=10.526, contrastive_loss=0, total=3978.09, n_correct=2684.49, ppl=4.07, accuracy=67.482, wps=12741, ups=1.6, wpb=7956.2, bsz=298.6, num_updates=26900, lr=8.62261e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=62, gb_free=16.5, wall=20989
2023-09-05 16:35:33 | INFO | train_inner | epoch 015:   1448 / 1826 loss=1.932, trans_loss=4.788, nll_loss=2.026, w2v_ctc_loss=0.716, task_loss=3.677, task_loss_gen=10.378, contrastive_loss=0, total=4025.06, n_correct=2715.76, ppl=4.07, accuracy=67.471, wps=13089.6, ups=1.63, wpb=8050.1, bsz=308.1, num_updates=27000, lr=8.60663e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=61, gb_free=16.1, wall=21051
2023-09-05 16:36:35 | INFO | train_inner | epoch 015:   1548 / 1826 loss=1.95, trans_loss=4.795, nll_loss=2.034, w2v_ctc_loss=0.746, task_loss=4.527, task_loss_gen=11.699, contrastive_loss=0, total=3914.86, n_correct=2633.01, ppl=4.09, accuracy=67.257, wps=12674.3, ups=1.62, wpb=7829.7, bsz=275.5, num_updates=27100, lr=8.59074e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=61, gb_free=17.2, wall=21113
2023-09-05 16:37:37 | INFO | train_inner | epoch 015:   1648 / 1826 loss=1.943, trans_loss=4.79, nll_loss=2.027, w2v_ctc_loss=0.736, task_loss=4.173, task_loss_gen=11.666, contrastive_loss=0, total=3982.78, n_correct=2686.06, ppl=4.08, accuracy=67.442, wps=12891, ups=1.62, wpb=7965.6, bsz=280, num_updates=27200, lr=8.57493e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=61, gb_free=16.3, wall=21174
2023-09-05 16:38:39 | INFO | train_inner | epoch 015:   1748 / 1826 loss=1.949, trans_loss=4.797, nll_loss=2.037, w2v_ctc_loss=0.742, task_loss=3.935, task_loss_gen=12.506, contrastive_loss=0, total=3983.24, n_correct=2681.43, ppl=4.1, accuracy=67.318, wps=12817.9, ups=1.61, wpb=7966.5, bsz=282.1, num_updates=27300, lr=8.55921e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=61, gb_free=15.1, wall=21236
2023-09-05 16:39:28 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.1311], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.1311], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 16:40:07 | INFO | dev_st | epoch 015 | valid on 'dev_st' subset | loss 3.82 | trans_loss 5.03 | nll_loss 2.284 | w2v_ctc_loss 1.382 | task_loss 24.875 | task_loss_gen 26.03 | contrastive_loss 0 | total 3505.91 | n_correct 2401 | ppl 4.87 | accuracy 68.484 | uer 17.986 | wer 19.902 | raw_wer 19.902 | bleu 30.55 | wps 1182.7 | wpb 3505.9 | bsz 119.3 | num_updates 27378 | best_bleu 30.61
2023-09-05 16:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 27378 updates
2023-09-05 16:40:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.5505.pt
2023-09-05 16:40:10 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.5505.pt
2023-09-05 16:40:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.5505.pt (epoch 15 @ 27378 updates, score 30.55) (writing took 7.999423361965455 seconds)
2023-09-05 16:40:16 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)
2023-09-05 16:40:16 | INFO | train | epoch 015 | loss 1.939 | trans_loss 4.788 | nll_loss 2.025 | w2v_ctc_loss 0.728 | task_loss 3.759 | task_loss_gen 12.762 | contrastive_loss 0 | total 3956.46 | n_correct 2669.3 | ppl 4.07 | accuracy 67.467 | wps 11704.1 | ups 1.48 | wpb 7912.9 | bsz 284.8 | num_updates 27378 | lr 8.54701e-05 | gnorm 0.544 | clip 0 | loss_scale 32 | train_wall 1114 | gb_free 15.6 | wall 21333
2023-09-05 16:40:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 16:40:16 | INFO | fairseq.trainer | begin training epoch 16
2023-09-05 16:40:16 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 16:40:37 | INFO | train_inner | epoch 016:     22 / 1826 loss=1.942, trans_loss=4.795, nll_loss=2.035, w2v_ctc_loss=0.73, task_loss=4.133, task_loss_gen=11.469, contrastive_loss=0, total=3968.39, n_correct=2670.82, ppl=4.1, accuracy=67.302, wps=6717, ups=0.85, wpb=7936.8, bsz=290.5, num_updates=27400, lr=8.54358e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=61, gb_free=12.9, wall=21355
2023-09-05 16:41:39 | INFO | train_inner | epoch 016:    122 / 1826 loss=1.919, trans_loss=4.771, nll_loss=2.003, w2v_ctc_loss=0.701, task_loss=4.244, task_loss_gen=11.066, contrastive_loss=0, total=4003.43, n_correct=2719.96, ppl=4.01, accuracy=67.941, wps=12960.1, ups=1.62, wpb=8006.9, bsz=297.6, num_updates=27500, lr=8.52803e-05, gnorm=0.536, clip=0, loss_scale=32, train_wall=61, gb_free=16.7, wall=21416
2023-09-05 16:42:41 | INFO | train_inner | epoch 016:    222 / 1826 loss=1.93, trans_loss=4.778, nll_loss=2.012, w2v_ctc_loss=0.72, task_loss=4.559, task_loss_gen=10.768, contrastive_loss=0, total=3973, n_correct=2691.6, ppl=4.03, accuracy=67.747, wps=12894.4, ups=1.62, wpb=7946, bsz=293.2, num_updates=27600, lr=8.51257e-05, gnorm=0.546, clip=0, loss_scale=32, train_wall=61, gb_free=15.2, wall=21478
2023-09-05 16:43:42 | INFO | train_inner | epoch 016:    322 / 1826 loss=1.93, trans_loss=4.776, nll_loss=2.009, w2v_ctc_loss=0.713, task_loss=3.779, task_loss_gen=12.129, contrastive_loss=0, total=3907.57, n_correct=2646.27, ppl=4.03, accuracy=67.722, wps=12740.9, ups=1.63, wpb=7815.1, bsz=277.3, num_updates=27700, lr=8.49719e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=21539
2023-09-05 16:44:44 | INFO | train_inner | epoch 016:    422 / 1826 loss=1.928, trans_loss=4.777, nll_loss=2.011, w2v_ctc_loss=0.719, task_loss=3.76, task_loss_gen=11.061, contrastive_loss=0, total=3983.88, n_correct=2703.05, ppl=4.03, accuracy=67.85, wps=12902, ups=1.62, wpb=7967.8, bsz=298.2, num_updates=27800, lr=8.48189e-05, gnorm=0.535, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=21601
2023-09-05 16:45:46 | INFO | train_inner | epoch 016:    522 / 1826 loss=1.935, trans_loss=4.781, nll_loss=2.015, w2v_ctc_loss=0.72, task_loss=3.929, task_loss_gen=11.959, contrastive_loss=0, total=3942.7, n_correct=2666.91, ppl=4.04, accuracy=67.642, wps=12728.9, ups=1.61, wpb=7885.4, bsz=276.6, num_updates=27900, lr=8.46668e-05, gnorm=0.538, clip=0, loss_scale=32, train_wall=61, gb_free=17, wall=21663
2023-09-05 16:46:48 | INFO | train_inner | epoch 016:    622 / 1826 loss=1.934, trans_loss=4.776, nll_loss=2.009, w2v_ctc_loss=0.722, task_loss=4.475, task_loss_gen=11.444, contrastive_loss=0, total=3940.29, n_correct=2664.09, ppl=4.03, accuracy=67.612, wps=12601.8, ups=1.6, wpb=7880.6, bsz=283.8, num_updates=28000, lr=8.45154e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=62, gb_free=14.8, wall=21726
2023-09-05 16:46:48 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-1.2666], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.2666], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 16:47:28 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 3.82 | trans_loss 5.019 | nll_loss 2.269 | w2v_ctc_loss 1.407 | task_loss 16.804 | task_loss_gen 25.579 | contrastive_loss 0 | total 3505.91 | n_correct 2410.55 | ppl 4.82 | accuracy 68.757 | uer 17.927 | wer 19.725 | raw_wer 19.725 | bleu 30.87 | wps 1187.3 | wpb 3505.9 | bsz 119.3 | num_updates 28000 | best_bleu 30.87
2023-09-05 16:47:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 28000 updates
2023-09-05 16:47:28 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_16_28000.pt
2023-09-05 16:47:30 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_16_28000.pt
2023-09-05 16:47:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_16_28000.pt (epoch 16 @ 28000 updates, score 30.87) (writing took 12.924962316988967 seconds)
2023-09-05 16:48:43 | INFO | train_inner | epoch 016:    722 / 1826 loss=1.925, trans_loss=4.771, nll_loss=2.004, w2v_ctc_loss=0.714, task_loss=4.425, task_loss_gen=11.329, contrastive_loss=0, total=3964.31, n_correct=2694.21, ppl=4.01, accuracy=67.962, wps=6909.1, ups=0.87, wpb=7928.6, bsz=288.5, num_updates=28100, lr=8.43649e-05, gnorm=0.543, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=21840
2023-09-05 16:49:45 | INFO | train_inner | epoch 016:    822 / 1826 loss=1.931, trans_loss=4.771, nll_loss=2.003, w2v_ctc_loss=0.723, task_loss=4.107, task_loss_gen=11.987, contrastive_loss=0, total=3928.49, n_correct=2665.72, ppl=4.01, accuracy=67.856, wps=12740, ups=1.62, wpb=7857, bsz=274.1, num_updates=28200, lr=8.42152e-05, gnorm=0.541, clip=0, loss_scale=32, train_wall=61, gb_free=16.7, wall=21902
2023-09-05 16:50:47 | INFO | train_inner | epoch 016:    922 / 1826 loss=1.931, trans_loss=4.777, nll_loss=2.011, w2v_ctc_loss=0.718, task_loss=3.554, task_loss_gen=12.663, contrastive_loss=0, total=3936.55, n_correct=2666.94, ppl=4.03, accuracy=67.748, wps=12719.4, ups=1.62, wpb=7873.1, bsz=272.9, num_updates=28300, lr=8.40663e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=61, gb_free=16.5, wall=21964
2023-09-05 16:51:48 | INFO | train_inner | epoch 016:   1022 / 1826 loss=1.93, trans_loss=4.773, nll_loss=2.005, w2v_ctc_loss=0.723, task_loss=3.724, task_loss_gen=12.304, contrastive_loss=0, total=3931.11, n_correct=2666.38, ppl=4.01, accuracy=67.828, wps=12734.6, ups=1.62, wpb=7862.2, bsz=281.6, num_updates=28400, lr=8.39181e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=61, gb_free=17, wall=22026
2023-09-05 16:52:50 | INFO | train_inner | epoch 016:   1122 / 1826 loss=1.928, trans_loss=4.771, nll_loss=2.004, w2v_ctc_loss=0.716, task_loss=3.427, task_loss_gen=12.79, contrastive_loss=0, total=3983.75, n_correct=2704.4, ppl=4.01, accuracy=67.886, wps=12965.2, ups=1.63, wpb=7967.5, bsz=280.7, num_updates=28500, lr=8.37708e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=16.7, wall=22087
2023-09-05 16:53:52 | INFO | train_inner | epoch 016:   1222 / 1826 loss=1.918, trans_loss=4.761, nll_loss=1.992, w2v_ctc_loss=0.705, task_loss=2.614, task_loss_gen=13.317, contrastive_loss=0, total=3959.67, n_correct=2695.06, ppl=3.98, accuracy=68.063, wps=12750, ups=1.61, wpb=7919.3, bsz=289.5, num_updates=28600, lr=8.36242e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=61, gb_free=16.8, wall=22149
2023-09-05 16:54:54 | INFO | train_inner | epoch 016:   1322 / 1826 loss=1.939, trans_loss=4.773, nll_loss=2.006, w2v_ctc_loss=0.735, task_loss=3.155, task_loss_gen=14.977, contrastive_loss=0, total=3904.89, n_correct=2645.16, ppl=4.02, accuracy=67.74, wps=12574.6, ups=1.61, wpb=7809.8, bsz=267.1, num_updates=28700, lr=8.34784e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=61, gb_free=16, wall=22211
2023-09-05 16:55:56 | INFO | train_inner | epoch 016:   1422 / 1826 loss=1.925, trans_loss=4.767, nll_loss=2, w2v_ctc_loss=0.712, task_loss=2.822, task_loss_gen=14.033, contrastive_loss=0, total=3969.02, n_correct=2694.86, ppl=4, accuracy=67.897, wps=12789.4, ups=1.61, wpb=7938, bsz=288.3, num_updates=28800, lr=8.33333e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=16.9, wall=22273
2023-09-05 16:56:58 | INFO | train_inner | epoch 016:   1522 / 1826 loss=1.936, trans_loss=4.778, nll_loss=2.013, w2v_ctc_loss=0.734, task_loss=2.552, task_loss_gen=14.63, contrastive_loss=0, total=3950.42, n_correct=2672.48, ppl=4.04, accuracy=67.651, wps=12768.5, ups=1.62, wpb=7900.8, bsz=283.2, num_updates=28900, lr=8.3189e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=61, gb_free=16.4, wall=22335
2023-09-05 16:58:01 | INFO | train_inner | epoch 016:   1622 / 1826 loss=1.926, trans_loss=4.773, nll_loss=2.007, w2v_ctc_loss=0.713, task_loss=3.08, task_loss_gen=13.931, contrastive_loss=0, total=3987.85, n_correct=2704.33, ppl=4.02, accuracy=67.814, wps=12734.2, ups=1.6, wpb=7975.7, bsz=292.6, num_updates=29000, lr=8.30455e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=62, gb_free=16.4, wall=22398
2023-09-05 16:59:03 | INFO | train_inner | epoch 016:   1722 / 1826 loss=1.921, trans_loss=4.769, nll_loss=2.001, w2v_ctc_loss=0.711, task_loss=2.54, task_loss_gen=14.035, contrastive_loss=0, total=4019.17, n_correct=2728.41, ppl=4, accuracy=67.885, wps=12951.2, ups=1.61, wpb=8038.3, bsz=297.7, num_updates=29100, lr=8.29027e-05, gnorm=0.527, clip=0, loss_scale=64, train_wall=61, gb_free=17.5, wall=22460
2023-09-05 17:00:04 | INFO | train_inner | epoch 016:   1822 / 1826 loss=1.926, trans_loss=4.769, nll_loss=2.002, w2v_ctc_loss=0.72, task_loss=2.576, task_loss_gen=13.652, contrastive_loss=0, total=3975.07, n_correct=2701.25, ppl=4.01, accuracy=67.955, wps=12958.1, ups=1.63, wpb=7950.1, bsz=290.9, num_updates=29200, lr=8.27606e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=16.2, wall=22521
2023-09-05 17:00:07 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-1.2695], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.2695], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 17:00:46 | INFO | dev_st | epoch 016 | valid on 'dev_st' subset | loss 3.792 | trans_loss 5.022 | nll_loss 2.272 | w2v_ctc_loss 1.309 | task_loss 11.115 | task_loss_gen 29.982 | contrastive_loss 0 | total 3505.91 | n_correct 2407.82 | ppl 4.83 | accuracy 68.679 | uer 17.594 | wer 19.568 | raw_wer 19.568 | bleu 30.69 | wps 1171 | wpb 3505.9 | bsz 119.3 | num_updates 29204 | best_bleu 30.87
2023-09-05 17:00:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 29204 updates
2023-09-05 17:00:46 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.6901.pt
2023-09-05 17:00:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.6901.pt
2023-09-05 17:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.6901.pt (epoch 16 @ 29204 updates, score 30.69) (writing took 7.302915895008482 seconds)
2023-09-05 17:00:54 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)
2023-09-05 17:00:54 | INFO | train | epoch 016 | loss 1.929 | trans_loss 4.773 | nll_loss 2.006 | w2v_ctc_loss 0.718 | task_loss 3.53 | task_loss_gen 12.664 | contrastive_loss 0 | total 3956.37 | n_correct 2683.36 | ppl 4.02 | accuracy 67.824 | wps 11665.3 | ups 1.47 | wpb 7912.7 | bsz 284.8 | num_updates 29204 | lr 8.27549e-05 | gnorm 0.538 | clip 0 | loss_scale 64 | train_wall 1116 | gb_free 15.7 | wall 22571
2023-09-05 17:00:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 17:00:54 | INFO | fairseq.trainer | begin training epoch 17
2023-09-05 17:00:54 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 17:02:01 | INFO | train_inner | epoch 017:     96 / 1826 loss=1.917, trans_loss=4.749, nll_loss=1.975, w2v_ctc_loss=0.709, task_loss=2.917, task_loss_gen=13.791, contrastive_loss=0, total=3925.48, n_correct=2679.57, ppl=3.93, accuracy=68.261, wps=6713.2, ups=0.86, wpb=7851, bsz=283.3, num_updates=29300, lr=8.26192e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=60, gb_free=16.8, wall=22638
2023-09-05 17:03:02 | INFO | train_inner | epoch 017:    196 / 1826 loss=1.903, trans_loss=4.734, nll_loss=1.957, w2v_ctc_loss=0.698, task_loss=2.506, task_loss_gen=13.655, contrastive_loss=0, total=3981.77, n_correct=2731.53, ppl=3.88, accuracy=68.601, wps=13024.8, ups=1.64, wpb=7963.5, bsz=300.5, num_updates=29400, lr=8.24786e-05, gnorm=0.526, clip=0, loss_scale=64, train_wall=60, gb_free=15.9, wall=22700
2023-09-05 17:04:04 | INFO | train_inner | epoch 017:    296 / 1826 loss=1.921, trans_loss=4.755, nll_loss=1.983, w2v_ctc_loss=0.713, task_loss=2.811, task_loss_gen=16.294, contrastive_loss=0, total=3872.12, n_correct=2640.97, ppl=3.95, accuracy=68.205, wps=12528.6, ups=1.62, wpb=7744.2, bsz=262.8, num_updates=29500, lr=8.23387e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=16.8, wall=22761
2023-09-05 17:05:06 | INFO | train_inner | epoch 017:    396 / 1826 loss=1.917, trans_loss=4.749, nll_loss=1.976, w2v_ctc_loss=0.71, task_loss=2.485, task_loss_gen=15.344, contrastive_loss=0, total=3939.85, n_correct=2689.29, ppl=3.93, accuracy=68.259, wps=12642.5, ups=1.6, wpb=7879.7, bsz=283.5, num_updates=29600, lr=8.21995e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=62, gb_free=14.5, wall=22824
2023-09-05 17:06:08 | INFO | train_inner | epoch 017:    496 / 1826 loss=1.913, trans_loss=4.758, nll_loss=1.988, w2v_ctc_loss=0.702, task_loss=2.471, task_loss_gen=13.919, contrastive_loss=0, total=4011.56, n_correct=2737.13, ppl=3.97, accuracy=68.231, wps=13098.2, ups=1.63, wpb=8023.1, bsz=304.5, num_updates=29700, lr=8.2061e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=60, gb_free=16.6, wall=22885
2023-09-05 17:07:10 | INFO | train_inner | epoch 017:    596 / 1826 loss=1.919, trans_loss=4.76, nll_loss=1.989, w2v_ctc_loss=0.706, task_loss=2.221, task_loss_gen=15.349, contrastive_loss=0, total=3978.63, n_correct=2709.56, ppl=3.97, accuracy=68.103, wps=12820.3, ups=1.61, wpb=7957.3, bsz=287.7, num_updates=29800, lr=8.19232e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=61, gb_free=17.4, wall=22947
2023-09-05 17:08:11 | INFO | train_inner | epoch 017:    696 / 1826 loss=1.926, trans_loss=4.765, nll_loss=1.996, w2v_ctc_loss=0.719, task_loss=2.637, task_loss_gen=16.737, contrastive_loss=0, total=3916.31, n_correct=2663.32, ppl=3.99, accuracy=68.006, wps=12767.7, ups=1.63, wpb=7832.6, bsz=276.9, num_updates=29900, lr=8.17861e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=61, gb_free=17, wall=23008
2023-09-05 17:09:13 | INFO | train_inner | epoch 017:    796 / 1826 loss=1.922, trans_loss=4.763, nll_loss=1.994, w2v_ctc_loss=0.714, task_loss=2.447, task_loss_gen=16.034, contrastive_loss=0, total=3903, n_correct=2660.41, ppl=3.98, accuracy=68.163, wps=12642.2, ups=1.62, wpb=7806, bsz=276.5, num_updates=30000, lr=8.16497e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=61, gb_free=16, wall=23070
2023-09-05 17:09:13 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-2.1738], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-2.1738], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 17:09:52 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.804 | trans_loss 5.017 | nll_loss 2.267 | w2v_ctc_loss 1.357 | task_loss 9.076 | task_loss_gen 32 | contrastive_loss 0 | total 3505.91 | n_correct 2407.09 | ppl 4.81 | accuracy 68.658 | uer 17.793 | wer 19.782 | raw_wer 19.782 | bleu 30.66 | wps 1186.1 | wpb 3505.9 | bsz 119.3 | num_updates 30000 | best_bleu 30.87
2023-09-05 17:09:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 30000 updates
2023-09-05 17:09:52 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_17_30000.pt
2023-09-05 17:09:55 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_17_30000.pt
2023-09-05 17:10:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_17_30000.pt (epoch 17 @ 30000 updates, score 30.66) (writing took 9.420794153935276 seconds)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:0')
2023-09-05 17:11:05 | INFO | train_inner | epoch 017:    896 / 1826 loss=1.923, trans_loss=4.759, nll_loss=1.989, w2v_ctc_loss=0.717, task_loss=2.769, task_loss_gen=15.665, contrastive_loss=0, total=3983.4, n_correct=2714.3, ppl=3.97, accuracy=68.14, wps=7108.5, ups=0.89, wpb=7966.8, bsz=278.3, num_updates=30100, lr=8.15139e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=12.6, wall=23182
2023-09-05 17:12:07 | INFO | train_inner | epoch 017:    996 / 1826 loss=1.917, trans_loss=4.758, nll_loss=1.987, w2v_ctc_loss=0.702, task_loss=2.192, task_loss_gen=15.49, contrastive_loss=0, total=4001.1, n_correct=2724.89, ppl=3.96, accuracy=68.104, wps=12905, ups=1.61, wpb=8002.2, bsz=288.7, num_updates=30200, lr=8.13788e-05, gnorm=0.523, clip=0, loss_scale=64, train_wall=61, gb_free=14.2, wall=23244
2023-09-05 17:13:08 | INFO | train_inner | epoch 017:   1096 / 1826 loss=1.921, trans_loss=4.757, nll_loss=1.986, w2v_ctc_loss=0.71, task_loss=2.409, task_loss_gen=16.683, contrastive_loss=0, total=3928.39, n_correct=2677.87, ppl=3.96, accuracy=68.167, wps=12815.3, ups=1.63, wpb=7856.8, bsz=271.7, num_updates=30300, lr=8.12444e-05, gnorm=0.539, clip=0, loss_scale=128, train_wall=61, gb_free=16.2, wall=23305
2023-09-05 17:13:27 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-09-05 17:14:10 | INFO | train_inner | epoch 017:   1197 / 1826 loss=1.916, trans_loss=4.756, nll_loss=1.984, w2v_ctc_loss=0.706, task_loss=2.395, task_loss_gen=15.71, contrastive_loss=0, total=3953.01, n_correct=2695.64, ppl=3.96, accuracy=68.192, wps=12801.4, ups=1.62, wpb=7906, bsz=284.6, num_updates=30400, lr=8.11107e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=61, gb_free=12.1, wall=23367
2023-09-05 17:15:11 | INFO | train_inner | epoch 017:   1297 / 1826 loss=1.914, trans_loss=4.75, nll_loss=1.977, w2v_ctc_loss=0.709, task_loss=2.247, task_loss_gen=15.846, contrastive_loss=0, total=3980.83, n_correct=2718.11, ppl=3.94, accuracy=68.28, wps=13069.7, ups=1.64, wpb=7961.7, bsz=287.4, num_updates=30500, lr=8.09776e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=60, gb_free=16, wall=23428
2023-09-05 17:16:13 | INFO | train_inner | epoch 017:   1397 / 1826 loss=1.916, trans_loss=4.749, nll_loss=1.975, w2v_ctc_loss=0.709, task_loss=2.405, task_loss_gen=17.185, contrastive_loss=0, total=3908.86, n_correct=2673.82, ppl=3.93, accuracy=68.404, wps=12610.2, ups=1.61, wpb=7817.7, bsz=272.8, num_updates=30600, lr=8.08452e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=61, gb_free=17, wall=23490
2023-09-05 17:17:16 | INFO | train_inner | epoch 017:   1497 / 1826 loss=1.918, trans_loss=4.767, nll_loss=2, w2v_ctc_loss=0.702, task_loss=2.217, task_loss_gen=14.924, contrastive_loss=0, total=3975.82, n_correct=2700.27, ppl=4, accuracy=67.917, wps=12588.8, ups=1.58, wpb=7951.6, bsz=304, num_updates=30700, lr=8.07134e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=62, gb_free=16.4, wall=23553
2023-09-05 17:18:18 | INFO | train_inner | epoch 017:   1597 / 1826 loss=1.917, trans_loss=4.762, nll_loss=1.993, w2v_ctc_loss=0.704, task_loss=2.345, task_loss_gen=15.494, contrastive_loss=0, total=3984.61, n_correct=2712.35, ppl=3.98, accuracy=68.071, wps=12933.5, ups=1.62, wpb=7969.2, bsz=295.4, num_updates=30800, lr=8.05823e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=61, gb_free=16.5, wall=23615
2023-09-05 17:19:20 | INFO | train_inner | epoch 017:   1697 / 1826 loss=1.916, trans_loss=4.758, nll_loss=1.988, w2v_ctc_loss=0.704, task_loss=2.147, task_loss_gen=16.149, contrastive_loss=0, total=4003.28, n_correct=2728.81, ppl=3.97, accuracy=68.164, wps=12929.1, ups=1.61, wpb=8006.6, bsz=286.3, num_updates=30900, lr=8.04518e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=61, gb_free=15.6, wall=23677
2023-09-05 17:20:22 | INFO | train_inner | epoch 017:   1797 / 1826 loss=1.922, trans_loss=4.756, nll_loss=1.984, w2v_ctc_loss=0.718, task_loss=2.354, task_loss_gen=16.536, contrastive_loss=0, total=3945.39, n_correct=2687.35, ppl=3.96, accuracy=68.114, wps=12735.3, ups=1.61, wpb=7890.8, bsz=279.4, num_updates=31000, lr=8.03219e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=61, gb_free=16.5, wall=23739
2023-09-05 17:20:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:7')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:1')
tensor([-0.0938], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.0938], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 17:21:18 | INFO | dev_st | epoch 017 | valid on 'dev_st' subset | loss 3.804 | trans_loss 5.008 | nll_loss 2.26 | w2v_ctc_loss 1.38 | task_loss 19.246 | task_loss_gen 27.946 | contrastive_loss 0 | total 3505.91 | n_correct 2413.64 | ppl 4.79 | accuracy 68.845 | uer 17.514 | wer 19.354 | raw_wer 19.354 | bleu 30.81 | wps 1188.7 | wpb 3505.9 | bsz 119.3 | num_updates 31029 | best_bleu 30.87
2023-09-05 17:21:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 31029 updates
2023-09-05 17:21:18 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.8106.pt
2023-09-05 17:21:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.8106.pt
2023-09-05 17:21:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.8106.pt (epoch 17 @ 31029 updates, score 30.81) (writing took 8.00179777503945 seconds)
2023-09-05 17:21:27 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)
2023-09-05 17:21:27 | INFO | train | epoch 017 | loss 1.917 | trans_loss 4.756 | nll_loss 1.984 | w2v_ctc_loss 0.708 | task_loss 2.423 | task_loss_gen 15.584 | contrastive_loss 0 | total 3956.51 | n_correct 2697.94 | ppl 3.96 | accuracy 68.19 | wps 11711.8 | ups 1.48 | wpb 7913 | bsz 284.9 | num_updates 31029 | lr 8.02844e-05 | gnorm 0.534 | clip 0 | loss_scale 64 | train_wall 1113 | gb_free 13.5 | wall 23804
2023-09-05 17:21:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 17:21:27 | INFO | fairseq.trainer | begin training epoch 18
2023-09-05 17:21:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 17:22:20 | INFO | train_inner | epoch 018:     71 / 1826 loss=1.903, trans_loss=4.739, nll_loss=1.963, w2v_ctc_loss=0.693, task_loss=2.252, task_loss_gen=14.907, contrastive_loss=0, total=4002.34, n_correct=2746.06, ppl=3.9, accuracy=68.611, wps=6771.3, ups=0.85, wpb=8004.7, bsz=299.3, num_updates=31100, lr=8.01927e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=61, gb_free=15.9, wall=23857
2023-09-05 17:23:22 | INFO | train_inner | epoch 018:    171 / 1826 loss=1.897, trans_loss=4.733, nll_loss=1.956, w2v_ctc_loss=0.686, task_loss=2.456, task_loss_gen=14.177, contrastive_loss=0, total=4003.69, n_correct=2751.55, ppl=3.88, accuracy=68.725, wps=12814.4, ups=1.6, wpb=8007.4, bsz=300.3, num_updates=31200, lr=8.00641e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=62, gb_free=15.4, wall=23920
2023-09-05 17:24:24 | INFO | train_inner | epoch 018:    271 / 1826 loss=1.898, trans_loss=4.734, nll_loss=1.956, w2v_ctc_loss=0.684, task_loss=2.841, task_loss_gen=14.224, contrastive_loss=0, total=4005.4, n_correct=2751.84, ppl=3.88, accuracy=68.703, wps=12966.9, ups=1.62, wpb=8010.8, bsz=291.1, num_updates=31300, lr=7.99361e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=61, gb_free=14.7, wall=23981
2023-09-05 17:25:26 | INFO | train_inner | epoch 018:    371 / 1826 loss=1.905, trans_loss=4.732, nll_loss=1.953, w2v_ctc_loss=0.696, task_loss=2.799, task_loss_gen=14.992, contrastive_loss=0, total=3955.75, n_correct=2712.91, ppl=3.87, accuracy=68.581, wps=12858.4, ups=1.63, wpb=7911.5, bsz=280, num_updates=31400, lr=7.98087e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=61, gb_free=16.5, wall=24043
2023-09-05 17:26:27 | INFO | train_inner | epoch 018:    471 / 1826 loss=1.91, trans_loss=4.741, nll_loss=1.965, w2v_ctc_loss=0.704, task_loss=2.683, task_loss_gen=15.01, contrastive_loss=0, total=3959.59, n_correct=2710.65, ppl=3.91, accuracy=68.458, wps=12887.6, ups=1.63, wpb=7919.2, bsz=286.2, num_updates=31500, lr=7.96819e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=61, gb_free=16.3, wall=24104
2023-09-05 17:27:29 | INFO | train_inner | epoch 018:    571 / 1826 loss=1.909, trans_loss=4.746, nll_loss=1.971, w2v_ctc_loss=0.701, task_loss=2.355, task_loss_gen=14.572, contrastive_loss=0, total=3999.98, n_correct=2739.27, ppl=3.92, accuracy=68.482, wps=12921.4, ups=1.62, wpb=8000, bsz=295.9, num_updates=31600, lr=7.95557e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=16.6, wall=24166
2023-09-05 17:28:32 | INFO | train_inner | epoch 018:    671 / 1826 loss=1.914, trans_loss=4.747, nll_loss=1.973, w2v_ctc_loss=0.705, task_loss=3.105, task_loss_gen=15.511, contrastive_loss=0, total=3915.65, n_correct=2674.57, ppl=3.93, accuracy=68.305, wps=12458.5, ups=1.59, wpb=7831.3, bsz=272.4, num_updates=31700, lr=7.94301e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=62, gb_free=16.5, wall=24229
2023-09-05 17:29:34 | INFO | train_inner | epoch 018:    771 / 1826 loss=1.911, trans_loss=4.749, nll_loss=1.975, w2v_ctc_loss=0.694, task_loss=3.284, task_loss_gen=14.953, contrastive_loss=0, total=3949.83, n_correct=2696.01, ppl=3.93, accuracy=68.256, wps=12724.9, ups=1.61, wpb=7899.7, bsz=277.9, num_updates=31800, lr=7.93052e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=61, gb_free=15.3, wall=24291
2023-09-05 17:30:35 | INFO | train_inner | epoch 018:    871 / 1826 loss=1.907, trans_loss=4.742, nll_loss=1.967, w2v_ctc_loss=0.696, task_loss=3.52, task_loss_gen=14.427, contrastive_loss=0, total=3969.22, n_correct=2722.54, ppl=3.91, accuracy=68.591, wps=12894.6, ups=1.62, wpb=7938.4, bsz=281, num_updates=31900, lr=7.91808e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=15, wall=24353
2023-09-05 17:31:37 | INFO | train_inner | epoch 018:    971 / 1826 loss=1.909, trans_loss=4.742, nll_loss=1.966, w2v_ctc_loss=0.706, task_loss=3.258, task_loss_gen=13.988, contrastive_loss=0, total=3933.47, n_correct=2698.44, ppl=3.91, accuracy=68.602, wps=12876.3, ups=1.64, wpb=7866.9, bsz=287.2, num_updates=32000, lr=7.90569e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=60, gb_free=17.3, wall=24414
2023-09-05 17:31:37 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.3013], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.3013], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 17:32:16 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.807 | trans_loss 5.007 | nll_loss 2.256 | w2v_ctc_loss 1.389 | task_loss 29.626 | task_loss_gen 24.79 | contrastive_loss 0 | total 3505.91 | n_correct 2411.45 | ppl 4.78 | accuracy 68.783 | uer 17.481 | wer 19.433 | raw_wer 19.433 | bleu 30.98 | wps 1190.5 | wpb 3505.9 | bsz 119.3 | num_updates 32000 | best_bleu 30.98
2023-09-05 17:32:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 32000 updates
2023-09-05 17:32:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_18_32000.pt
2023-09-05 17:32:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_18_32000.pt
2023-09-05 17:32:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_18_32000.pt (epoch 18 @ 32000 updates, score 30.98) (writing took 12.034064452978782 seconds)
2023-09-05 17:33:30 | INFO | train_inner | epoch 018:   1071 / 1826 loss=1.905, trans_loss=4.745, nll_loss=1.971, w2v_ctc_loss=0.695, task_loss=2.563, task_loss_gen=14.114, contrastive_loss=0, total=4025.24, n_correct=2754.99, ppl=3.92, accuracy=68.443, wps=7078.2, ups=0.88, wpb=8050.5, bsz=306, num_updates=32100, lr=7.89337e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=61, gb_free=17.1, wall=24528
2023-09-05 17:34:33 | INFO | train_inner | epoch 018:   1171 / 1826 loss=1.905, trans_loss=4.737, nll_loss=1.96, w2v_ctc_loss=0.696, task_loss=3.345, task_loss_gen=14.719, contrastive_loss=0, total=3946.97, n_correct=2710.24, ppl=3.89, accuracy=68.666, wps=12670.2, ups=1.61, wpb=7893.9, bsz=280.1, num_updates=32200, lr=7.8811e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=62, gb_free=15.2, wall=24590
2023-09-05 17:35:35 | INFO | train_inner | epoch 018:   1271 / 1826 loss=1.91, trans_loss=4.742, nll_loss=1.968, w2v_ctc_loss=0.705, task_loss=3.641, task_loss_gen=13.64, contrastive_loss=0, total=3982.71, n_correct=2729.19, ppl=3.91, accuracy=68.526, wps=12805.5, ups=1.61, wpb=7965.4, bsz=280.8, num_updates=32300, lr=7.86889e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=15.6, wall=24652
2023-09-05 17:36:37 | INFO | train_inner | epoch 018:   1371 / 1826 loss=1.908, trans_loss=4.754, nll_loss=1.982, w2v_ctc_loss=0.694, task_loss=2.959, task_loss_gen=14.674, contrastive_loss=0, total=3965.84, n_correct=2709.85, ppl=3.95, accuracy=68.33, wps=12808.6, ups=1.61, wpb=7931.7, bsz=290.3, num_updates=32400, lr=7.85674e-05, gnorm=0.538, clip=0, loss_scale=128, train_wall=61, gb_free=16.6, wall=24714
2023-09-05 17:37:38 | INFO | train_inner | epoch 018:   1471 / 1826 loss=1.905, trans_loss=4.74, nll_loss=1.964, w2v_ctc_loss=0.695, task_loss=2.535, task_loss_gen=16.183, contrastive_loss=0, total=3953.27, n_correct=2707.98, ppl=3.9, accuracy=68.5, wps=12839.1, ups=1.62, wpb=7906.5, bsz=285.3, num_updates=32500, lr=7.84465e-05, gnorm=0.531, clip=0, loss_scale=128, train_wall=61, gb_free=17.1, wall=24776
2023-09-05 17:38:40 | INFO | train_inner | epoch 018:   1571 / 1826 loss=1.914, trans_loss=4.744, nll_loss=1.969, w2v_ctc_loss=0.706, task_loss=2.584, task_loss_gen=18.33, contrastive_loss=0, total=3825.85, n_correct=2618.32, ppl=3.92, accuracy=68.438, wps=12460.2, ups=1.63, wpb=7651.7, bsz=260.6, num_updates=32600, lr=7.8326e-05, gnorm=0.546, clip=0, loss_scale=128, train_wall=60, gb_free=14.6, wall=24837
2023-09-05 17:39:42 | INFO | train_inner | epoch 018:   1671 / 1826 loss=1.917, trans_loss=4.749, nll_loss=1.977, w2v_ctc_loss=0.713, task_loss=2.76, task_loss_gen=17.367, contrastive_loss=0, total=3922.44, n_correct=2679.26, ppl=3.94, accuracy=68.306, wps=12675.6, ups=1.62, wpb=7844.9, bsz=277.8, num_updates=32700, lr=7.82062e-05, gnorm=0.54, clip=0, loss_scale=128, train_wall=61, gb_free=17.3, wall=24899
2023-09-05 17:40:44 | INFO | train_inner | epoch 018:   1771 / 1826 loss=1.916, trans_loss=4.758, nll_loss=1.987, w2v_ctc_loss=0.704, task_loss=2.365, task_loss_gen=17.988, contrastive_loss=0, total=3899.6, n_correct=2660.41, ppl=3.96, accuracy=68.223, wps=12472.1, ups=1.6, wpb=7799.2, bsz=274.1, num_updates=32800, lr=7.80869e-05, gnorm=0.544, clip=0, loss_scale=128, train_wall=62, gb_free=13.9, wall=24961
2023-09-05 17:41:18 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.0873], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.0873], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 17:41:58 | INFO | dev_st | epoch 018 | valid on 'dev_st' subset | loss 3.779 | trans_loss 5.005 | nll_loss 2.253 | w2v_ctc_loss 1.303 | task_loss 15.102 | task_loss_gen 29.249 | contrastive_loss 0 | total 3505.91 | n_correct 2416.91 | ppl 4.77 | accuracy 68.938 | uer 17.253 | wer 19.245 | raw_wer 19.245 | bleu 30.85 | wps 1175.4 | wpb 3505.9 | bsz 119.3 | num_updates 32855 | best_bleu 30.98
2023-09-05 17:41:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 32855 updates
2023-09-05 17:41:58 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.8505.pt
2023-09-05 17:42:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.8505.pt
2023-09-05 17:42:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.8505.pt (epoch 18 @ 32855 updates, score 30.85) (writing took 7.610905182082206 seconds)
2023-09-05 17:42:06 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)
2023-09-05 17:42:06 | INFO | train | epoch 018 | loss 1.908 | trans_loss 4.743 | nll_loss 1.968 | w2v_ctc_loss 0.698 | task_loss 2.838 | task_loss_gen 15.223 | contrastive_loss 0 | total 3956.37 | n_correct 2709.5 | ppl 3.91 | accuracy 68.485 | wps 11668 | ups 1.47 | wpb 7912.7 | bsz 284.8 | num_updates 32855 | lr 7.80215e-05 | gnorm 0.535 | clip 0 | loss_scale 128 | train_wall 1116 | gb_free 15.6 | wall 25043
2023-09-05 17:42:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 17:42:06 | INFO | fairseq.trainer | begin training epoch 19
2023-09-05 17:42:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 17:42:42 | INFO | train_inner | epoch 019:     45 / 1826 loss=1.903, trans_loss=4.741, nll_loss=1.966, w2v_ctc_loss=0.688, task_loss=1.898, task_loss_gen=17.841, contrastive_loss=0, total=3920.8, n_correct=2688.72, ppl=3.91, accuracy=68.576, wps=6669.2, ups=0.85, wpb=7841.6, bsz=281.7, num_updates=32900, lr=7.79681e-05, gnorm=0.531, clip=0, loss_scale=128, train_wall=61, gb_free=16.5, wall=25079
2023-09-05 17:43:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-09-05 17:43:44 | INFO | train_inner | epoch 019:    146 / 1826 loss=1.894, trans_loss=4.725, nll_loss=1.945, w2v_ctc_loss=0.685, task_loss=2.526, task_loss_gen=16.791, contrastive_loss=0, total=4014.97, n_correct=2767.27, ppl=3.85, accuracy=68.924, wps=12887.7, ups=1.6, wpb=8029.9, bsz=295.2, num_updates=33000, lr=7.78499e-05, gnorm=0.529, clip=0, loss_scale=64, train_wall=61, gb_free=16.1, wall=25141
2023-09-05 17:44:46 | INFO | train_inner | epoch 019:    246 / 1826 loss=1.895, trans_loss=4.72, nll_loss=1.939, w2v_ctc_loss=0.683, task_loss=2.552, task_loss_gen=16.375, contrastive_loss=0, total=3946.3, n_correct=2717.95, ppl=3.83, accuracy=68.873, wps=12812.4, ups=1.62, wpb=7892.6, bsz=276.9, num_updates=33100, lr=7.77322e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=61, gb_free=14.9, wall=25203
2023-09-05 17:45:47 | INFO | train_inner | epoch 019:    346 / 1826 loss=1.896, trans_loss=4.73, nll_loss=1.951, w2v_ctc_loss=0.684, task_loss=3.273, task_loss_gen=15.054, contrastive_loss=0, total=3951.26, n_correct=2718.97, ppl=3.87, accuracy=68.813, wps=12794.3, ups=1.62, wpb=7902.5, bsz=285, num_updates=33200, lr=7.76151e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=61, gb_free=14.4, wall=25265
2023-09-05 17:46:50 | INFO | train_inner | epoch 019:    446 / 1826 loss=1.9, trans_loss=4.726, nll_loss=1.947, w2v_ctc_loss=0.694, task_loss=4.549, task_loss_gen=13.64, contrastive_loss=0, total=3979.41, n_correct=2734.15, ppl=3.86, accuracy=68.707, wps=12781.1, ups=1.61, wpb=7958.8, bsz=287, num_updates=33300, lr=7.74984e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=62, gb_free=15.9, wall=25327
2023-09-05 17:47:52 | INFO | train_inner | epoch 019:    546 / 1826 loss=1.901, trans_loss=4.732, nll_loss=1.954, w2v_ctc_loss=0.69, task_loss=4.972, task_loss_gen=12.88, contrastive_loss=0, total=3972.47, n_correct=2729.77, ppl=3.87, accuracy=68.717, wps=12670.5, ups=1.59, wpb=7944.9, bsz=284.1, num_updates=33400, lr=7.73823e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=62, gb_free=16.1, wall=25390
2023-09-05 17:48:54 | INFO | train_inner | epoch 019:    646 / 1826 loss=1.9, trans_loss=4.739, nll_loss=1.964, w2v_ctc_loss=0.681, task_loss=4.314, task_loss_gen=12.789, contrastive_loss=0, total=3976.86, n_correct=2730.47, ppl=3.9, accuracy=68.659, wps=12857.1, ups=1.62, wpb=7953.7, bsz=288.8, num_updates=33500, lr=7.72667e-05, gnorm=0.533, clip=0, loss_scale=64, train_wall=61, gb_free=16.7, wall=25452
2023-09-05 17:49:56 | INFO | train_inner | epoch 019:    746 / 1826 loss=1.903, trans_loss=4.737, nll_loss=1.96, w2v_ctc_loss=0.692, task_loss=4.388, task_loss_gen=13.512, contrastive_loss=0, total=3997.74, n_correct=2745.23, ppl=3.89, accuracy=68.67, wps=12995.3, ups=1.63, wpb=7995.5, bsz=290, num_updates=33600, lr=7.71517e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=61, gb_free=15.9, wall=25513
2023-09-05 17:50:57 | INFO | train_inner | epoch 019:    846 / 1826 loss=1.909, trans_loss=4.741, nll_loss=1.966, w2v_ctc_loss=0.704, task_loss=4.365, task_loss_gen=13.495, contrastive_loss=0, total=3907.8, n_correct=2676.47, ppl=3.91, accuracy=68.49, wps=12764.4, ups=1.63, wpb=7815.6, bsz=279, num_updates=33700, lr=7.70371e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=61, gb_free=17.1, wall=25574
2023-09-05 17:51:59 | INFO | train_inner | epoch 019:    946 / 1826 loss=1.894, trans_loss=4.724, nll_loss=1.944, w2v_ctc_loss=0.685, task_loss=3.998, task_loss_gen=13.855, contrastive_loss=0, total=3941.03, n_correct=2716.67, ppl=3.85, accuracy=68.933, wps=12825.9, ups=1.63, wpb=7882.1, bsz=286, num_updates=33800, lr=7.69231e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=61, gb_free=15.9, wall=25636
2023-09-05 17:53:00 | INFO | train_inner | epoch 019:   1046 / 1826 loss=1.899, trans_loss=4.73, nll_loss=1.951, w2v_ctc_loss=0.688, task_loss=4.444, task_loss_gen=12.664, contrastive_loss=0, total=4014.67, n_correct=2759.07, ppl=3.87, accuracy=68.725, wps=12991.6, ups=1.62, wpb=8029.3, bsz=289.6, num_updates=33900, lr=7.68095e-05, gnorm=0.528, clip=0, loss_scale=64, train_wall=61, gb_free=14.7, wall=25698
2023-09-05 17:54:03 | INFO | train_inner | epoch 019:   1146 / 1826 loss=1.9, trans_loss=4.733, nll_loss=1.955, w2v_ctc_loss=0.689, task_loss=4.827, task_loss_gen=13.215, contrastive_loss=0, total=3886.95, n_correct=2673.71, ppl=3.88, accuracy=68.787, wps=12497.7, ups=1.61, wpb=7773.9, bsz=276.1, num_updates=34000, lr=7.66965e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=62, gb_free=16.3, wall=25760
2023-09-05 17:54:03 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-1.1689], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.1689], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 17:54:42 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.797 | trans_loss 5 | nll_loss 2.247 | w2v_ctc_loss 1.373 | task_loss 48.514 | task_loss_gen 28.378 | contrastive_loss 0 | total 3505.91 | n_correct 2417.64 | ppl 4.75 | accuracy 68.959 | uer 17.505 | wer 19.245 | raw_wer 19.245 | bleu 30.94 | wps 1176.8 | wpb 3505.9 | bsz 119.3 | num_updates 34000 | best_bleu 30.98
2023-09-05 17:54:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 34000 updates
2023-09-05 17:54:42 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_19_34000.pt
2023-09-05 17:54:45 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_19_34000.pt
2023-09-05 17:54:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_19_34000.pt (epoch 19 @ 34000 updates, score 30.94) (writing took 8.004294697078876 seconds)
2023-09-05 17:55:53 | INFO | train_inner | epoch 019:   1246 / 1826 loss=1.904, trans_loss=4.739, nll_loss=1.963, w2v_ctc_loss=0.693, task_loss=4.472, task_loss_gen=12.233, contrastive_loss=0, total=3993.74, n_correct=2738.51, ppl=3.9, accuracy=68.57, wps=7254.8, ups=0.91, wpb=7987.5, bsz=297.4, num_updates=34100, lr=7.6584e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=61, gb_free=16.7, wall=25870
2023-09-05 17:56:54 | INFO | train_inner | epoch 019:   1346 / 1826 loss=1.906, trans_loss=4.735, nll_loss=1.958, w2v_ctc_loss=0.698, task_loss=4.861, task_loss_gen=13.572, contrastive_loss=0, total=3885.74, n_correct=2662.9, ppl=3.89, accuracy=68.53, wps=12610.4, ups=1.62, wpb=7771.5, bsz=277, num_updates=34200, lr=7.64719e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=61, gb_free=12.6, wall=25932
2023-09-05 17:57:56 | INFO | train_inner | epoch 019:   1446 / 1826 loss=1.902, trans_loss=4.739, nll_loss=1.963, w2v_ctc_loss=0.69, task_loss=4.489, task_loss_gen=13.17, contrastive_loss=0, total=3996.54, n_correct=2746.33, ppl=3.9, accuracy=68.718, wps=12915.1, ups=1.62, wpb=7993.1, bsz=291.8, num_updates=34300, lr=7.63604e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=61, gb_free=15.8, wall=25993
2023-09-05 17:58:58 | INFO | train_inner | epoch 019:   1546 / 1826 loss=1.908, trans_loss=4.744, nll_loss=1.97, w2v_ctc_loss=0.699, task_loss=5.322, task_loss_gen=13.139, contrastive_loss=0, total=3912.47, n_correct=2684.17, ppl=3.92, accuracy=68.606, wps=12625, ups=1.61, wpb=7824.9, bsz=277.7, num_updates=34400, lr=7.62493e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=61, gb_free=16.6, wall=26055
2023-09-05 18:00:00 | INFO | train_inner | epoch 019:   1646 / 1826 loss=1.898, trans_loss=4.732, nll_loss=1.954, w2v_ctc_loss=0.689, task_loss=4.613, task_loss_gen=13.172, contrastive_loss=0, total=3959.43, n_correct=2727.68, ppl=3.88, accuracy=68.891, wps=12819.7, ups=1.62, wpb=7918.9, bsz=280.8, num_updates=34500, lr=7.61387e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=61, gb_free=13.3, wall=26117
2023-09-05 18:01:02 | INFO | train_inner | epoch 019:   1746 / 1826 loss=1.904, trans_loss=4.746, nll_loss=1.972, w2v_ctc_loss=0.69, task_loss=4.17, task_loss_gen=12.933, contrastive_loss=0, total=3978.87, n_correct=2728.92, ppl=3.92, accuracy=68.585, wps=12892.7, ups=1.62, wpb=7957.7, bsz=289.8, num_updates=34600, lr=7.60286e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=14.9, wall=26179
2023-09-05 18:01:51 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([2.9180], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([2.9180], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 18:02:30 | INFO | dev_st | epoch 019 | valid on 'dev_st' subset | loss 3.789 | trans_loss 5.002 | nll_loss 2.25 | w2v_ctc_loss 1.343 | task_loss 73.324 | task_loss_gen 30.184 | contrastive_loss 0 | total 3505.91 | n_correct 2424.18 | ppl 4.76 | accuracy 69.146 | uer 17.409 | wer 19.279 | raw_wer 19.279 | bleu 31.02 | wps 1192.6 | wpb 3505.9 | bsz 119.3 | num_updates 34680 | best_bleu 31.02
2023-09-05 18:02:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 34680 updates
2023-09-05 18:02:30 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 18:02:37 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 18:02:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 19 @ 34680 updates, score 31.02) (writing took 13.509322525002062 seconds)
2023-09-05 18:02:44 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)
2023-09-05 18:02:44 | INFO | train | epoch 019 | loss 1.901 | trans_loss 4.734 | nll_loss 1.956 | w2v_ctc_loss 0.69 | task_loss 4.208 | task_loss_gen 13.764 | contrastive_loss 0 | total 3956.68 | n_correct 2718.88 | ppl 3.88 | accuracy 68.716 | wps 11658.6 | ups 1.47 | wpb 7913.4 | bsz 284.9 | num_updates 34680 | lr 7.59408e-05 | gnorm 0.539 | clip 0 | loss_scale 64 | train_wall 1114 | gb_free 16.8 | wall 26282
2023-09-05 18:02:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 18:02:44 | INFO | fairseq.trainer | begin training epoch 20
2023-09-05 18:02:44 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 18:03:05 | INFO | train_inner | epoch 020:     20 / 1826 loss=1.906, trans_loss=4.74, nll_loss=1.964, w2v_ctc_loss=0.696, task_loss=4.51, task_loss_gen=12.928, contrastive_loss=0, total=3925.72, n_correct=2691.88, ppl=3.9, accuracy=68.57, wps=6385.5, ups=0.81, wpb=7851.4, bsz=280.1, num_updates=34700, lr=7.5919e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=61, gb_free=17.3, wall=26302
2023-09-05 18:04:06 | INFO | train_inner | epoch 020:    120 / 1826 loss=1.885, trans_loss=4.718, nll_loss=1.936, w2v_ctc_loss=0.669, task_loss=4.391, task_loss_gen=11.143, contrastive_loss=0, total=3983.59, n_correct=2755.47, ppl=3.83, accuracy=69.171, wps=12927.7, ups=1.62, wpb=7967.2, bsz=288.1, num_updates=34800, lr=7.58098e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=61, gb_free=14.7, wall=26364
2023-09-05 18:05:08 | INFO | train_inner | epoch 020:    220 / 1826 loss=1.888, trans_loss=4.715, nll_loss=1.93, w2v_ctc_loss=0.676, task_loss=4.587, task_loss_gen=11.34, contrastive_loss=0, total=4018.09, n_correct=2778.8, ppl=3.81, accuracy=69.157, wps=12950, ups=1.61, wpb=8036.2, bsz=284.2, num_updates=34900, lr=7.57011e-05, gnorm=0.532, clip=0, loss_scale=64, train_wall=61, gb_free=15.1, wall=26426
2023-09-05 18:06:10 | INFO | train_inner | epoch 020:    320 / 1826 loss=1.886, trans_loss=4.722, nll_loss=1.942, w2v_ctc_loss=0.676, task_loss=4.283, task_loss_gen=9.995, contrastive_loss=0, total=4029.35, n_correct=2784.37, ppl=3.84, accuracy=69.102, wps=13008.3, ups=1.61, wpb=8058.7, bsz=300.4, num_updates=35000, lr=7.55929e-05, gnorm=0.525, clip=0, loss_scale=64, train_wall=61, gb_free=15.9, wall=26488
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:0')
2023-09-05 18:07:12 | INFO | train_inner | epoch 020:    420 / 1826 loss=1.889, trans_loss=4.715, nll_loss=1.932, w2v_ctc_loss=0.682, task_loss=3.606, task_loss_gen=12.084, contrastive_loss=0, total=3989.36, n_correct=2755.09, ppl=3.82, accuracy=69.061, wps=12895.8, ups=1.62, wpb=7978.7, bsz=288.8, num_updates=35100, lr=7.54851e-05, gnorm=0.528, clip=0, loss_scale=128, train_wall=61, gb_free=15.8, wall=26549
2023-09-05 18:08:13 | INFO | train_inner | epoch 020:    520 / 1826 loss=1.89, trans_loss=4.721, nll_loss=1.939, w2v_ctc_loss=0.677, task_loss=3.084, task_loss_gen=13.819, contrastive_loss=0, total=3933.87, n_correct=2716.56, ppl=3.84, accuracy=69.056, wps=12831.2, ups=1.63, wpb=7867.7, bsz=282.2, num_updates=35200, lr=7.53778e-05, gnorm=0.529, clip=0, loss_scale=128, train_wall=61, gb_free=15.2, wall=26611
2023-09-05 18:09:15 | INFO | train_inner | epoch 020:    620 / 1826 loss=1.895, trans_loss=4.722, nll_loss=1.941, w2v_ctc_loss=0.689, task_loss=2.887, task_loss_gen=14.93, contrastive_loss=0, total=3954.32, n_correct=2724.62, ppl=3.84, accuracy=68.902, wps=12833.5, ups=1.62, wpb=7908.6, bsz=281.4, num_updates=35300, lr=7.5271e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=61, gb_free=11.9, wall=26672
2023-09-05 18:10:17 | INFO | train_inner | epoch 020:    720 / 1826 loss=1.892, trans_loss=4.72, nll_loss=1.938, w2v_ctc_loss=0.681, task_loss=2.726, task_loss_gen=15.699, contrastive_loss=0, total=3927.01, n_correct=2710.47, ppl=3.83, accuracy=69.021, wps=12752.4, ups=1.62, wpb=7854, bsz=275, num_updates=35400, lr=7.51646e-05, gnorm=0.533, clip=0, loss_scale=128, train_wall=61, gb_free=15.7, wall=26734
2023-09-05 18:11:15 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-09-05 18:11:19 | INFO | train_inner | epoch 020:    821 / 1826 loss=1.894, trans_loss=4.728, nll_loss=1.948, w2v_ctc_loss=0.678, task_loss=3.385, task_loss_gen=15.73, contrastive_loss=0, total=3916.04, n_correct=2700.2, ppl=3.86, accuracy=68.952, wps=12563.5, ups=1.6, wpb=7832.1, bsz=276.4, num_updates=35500, lr=7.50587e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=62, gb_free=17.6, wall=26796
2023-09-05 18:12:21 | INFO | train_inner | epoch 020:    921 / 1826 loss=1.894, trans_loss=4.717, nll_loss=1.935, w2v_ctc_loss=0.688, task_loss=3.951, task_loss_gen=12.751, contrastive_loss=0, total=3859.83, n_correct=2663.44, ppl=3.82, accuracy=69.004, wps=12501.1, ups=1.62, wpb=7719.7, bsz=273.6, num_updates=35600, lr=7.49532e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=61, gb_free=11.6, wall=26858
2023-09-05 18:13:22 | INFO | train_inner | epoch 020:   1021 / 1826 loss=1.889, trans_loss=4.723, nll_loss=1.943, w2v_ctc_loss=0.672, task_loss=3.99, task_loss_gen=12.26, contrastive_loss=0, total=3948.02, n_correct=2725.77, ppl=3.84, accuracy=69.041, wps=12877.6, ups=1.63, wpb=7896, bsz=278, num_updates=35700, lr=7.48481e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=60, gb_free=16.5, wall=26919
2023-09-05 18:14:24 | INFO | train_inner | epoch 020:   1121 / 1826 loss=1.888, trans_loss=4.723, nll_loss=1.943, w2v_ctc_loss=0.675, task_loss=3.788, task_loss_gen=11.227, contrastive_loss=0, total=3980.71, n_correct=2747.93, ppl=3.84, accuracy=69.031, wps=12907.3, ups=1.62, wpb=7961.4, bsz=293.2, num_updates=35800, lr=7.47435e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=61, gb_free=16.1, wall=26981
2023-09-05 18:15:26 | INFO | train_inner | epoch 020:   1221 / 1826 loss=1.893, trans_loss=4.736, nll_loss=1.959, w2v_ctc_loss=0.68, task_loss=4.274, task_loss_gen=10.232, contrastive_loss=0, total=4014.48, n_correct=2758.44, ppl=3.89, accuracy=68.712, wps=12883.5, ups=1.6, wpb=8029, bsz=306.2, num_updates=35900, lr=7.46393e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=62, gb_free=14.1, wall=27043
2023-09-05 18:15:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 18:16:29 | INFO | train_inner | epoch 020:   1322 / 1826 loss=1.892, trans_loss=4.729, nll_loss=1.951, w2v_ctc_loss=0.677, task_loss=4.405, task_loss_gen=9.721, contrastive_loss=0, total=4034.52, n_correct=2776.87, ppl=3.87, accuracy=68.828, wps=12773.8, ups=1.58, wpb=8069, bsz=306.5, num_updates=36000, lr=7.45356e-05, gnorm=0.547, clip=0, loss_scale=32, train_wall=62, gb_free=16.4, wall=27107
2023-09-05 18:16:29 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:3')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:4')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:2')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:1')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:5')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:6')
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:7')
tensor([1.7695], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([1.7695], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 18:17:09 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.804 | trans_loss 5.003 | nll_loss 2.255 | w2v_ctc_loss 1.39 | task_loss 80.767 | task_loss_gen 30.415 | contrastive_loss 0 | total 3505.91 | n_correct 2418.91 | ppl 4.77 | accuracy 68.995 | uer 17.844 | wer 19.594 | raw_wer 19.594 | bleu 31.15 | wps 1174.5 | wpb 3505.9 | bsz 119.3 | num_updates 36000 | best_bleu 31.15
2023-09-05 18:17:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 36000 updates
2023-09-05 18:17:09 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_20_36000.pt
2023-09-05 18:17:12 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_20_36000.pt
2023-09-05 18:17:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_20_36000.pt (epoch 20 @ 36000 updates, score 31.15) (writing took 14.293845275067724 seconds)
--Backword ST Loss tensor(901.8505, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(497.0833, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 18:18:26 | INFO | train_inner | epoch 020:   1422 / 1826 loss=1.893, trans_loss=4.716, nll_loss=1.933, w2v_ctc_loss=0.684, task_loss=6.049, task_loss_gen=10.518, contrastive_loss=0, total=3880.04, n_correct=2683.36, ppl=3.82, accuracy=69.158, wps=6666.9, ups=0.86, wpb=7760.1, bsz=273.1, num_updates=36100, lr=7.44323e-05, gnorm=0.571, clip=0, loss_scale=32, train_wall=61, gb_free=16.6, wall=27223
2023-09-05 18:19:28 | INFO | train_inner | epoch 020:   1522 / 1826 loss=1.906, trans_loss=4.727, nll_loss=1.947, w2v_ctc_loss=0.701, task_loss=6.915, task_loss_gen=10.518, contrastive_loss=0, total=3892.6, n_correct=2676.51, ppl=3.86, accuracy=68.759, wps=12508.8, ups=1.61, wpb=7785.2, bsz=260.9, num_updates=36200, lr=7.43294e-05, gnorm=0.568, clip=0, loss_scale=32, train_wall=61, gb_free=13.8, wall=27285
2023-09-05 18:20:30 | INFO | train_inner | epoch 020:   1622 / 1826 loss=1.896, trans_loss=4.725, nll_loss=1.944, w2v_ctc_loss=0.688, task_loss=5.732, task_loss_gen=9.495, contrastive_loss=0, total=3975.65, n_correct=2742.96, ppl=3.85, accuracy=68.994, wps=12878.7, ups=1.62, wpb=7951.3, bsz=283.1, num_updates=36300, lr=7.4227e-05, gnorm=0.552, clip=0, loss_scale=32, train_wall=61, gb_free=17.5, wall=27347
2023-09-05 18:21:32 | INFO | train_inner | epoch 020:   1722 / 1826 loss=1.902, trans_loss=4.73, nll_loss=1.951, w2v_ctc_loss=0.697, task_loss=5.747, task_loss_gen=9.369, contrastive_loss=0, total=3932.02, n_correct=2700.42, ppl=3.87, accuracy=68.678, wps=12710.6, ups=1.62, wpb=7864, bsz=279.2, num_updates=36400, lr=7.41249e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=61, gb_free=13.5, wall=27409
2023-09-05 18:22:33 | INFO | train_inner | epoch 020:   1822 / 1826 loss=1.897, trans_loss=4.738, nll_loss=1.963, w2v_ctc_loss=0.683, task_loss=6.01, task_loss_gen=9.226, contrastive_loss=0, total=3974.02, n_correct=2729.31, ppl=3.9, accuracy=68.679, wps=12846.3, ups=1.62, wpb=7948, bsz=298, num_updates=36500, lr=7.40233e-05, gnorm=0.569, clip=0, loss_scale=32, train_wall=61, gb_free=16.5, wall=27471
2023-09-05 18:22:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1341.2644, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(684.8344, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1063.1294, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(589.0299, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(848.9553, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(392.5574, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1153.5397, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(628.7665, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1609.0671, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(910.7401, device='cuda:3', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1176.0990, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(683.0685, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1224.9464, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(654.0564, device='cuda:5', grad_fn=<MulBackward0>)
tensor([2.0547], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([2.0547], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 18:23:16 | INFO | dev_st | epoch 020 | valid on 'dev_st' subset | loss 3.791 | trans_loss 5.004 | nll_loss 2.251 | w2v_ctc_loss 1.347 | task_loss 59.905 | task_loss_gen 25.967 | contrastive_loss 0 | total 3505.91 | n_correct 2423 | ppl 4.76 | accuracy 69.112 | uer 17.342 | wer 19.039 | raw_wer 19.039 | bleu 30.9 | wps 1167.3 | wpb 3505.9 | bsz 119.3 | num_updates 36504 | best_bleu 31.15
2023-09-05 18:23:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 36504 updates
2023-09-05 18:23:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.9008.pt
2023-09-05 18:23:18 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.9008.pt
2023-09-05 18:23:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_30.9008.pt (epoch 20 @ 36504 updates, score 30.9) (writing took 7.936420848011039 seconds)
2023-09-05 18:23:24 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)
2023-09-05 18:23:24 | INFO | train | epoch 020 | loss 1.893 | trans_loss 4.724 | nll_loss 1.943 | w2v_ctc_loss 0.682 | task_loss 4.417 | task_loss_gen 11.635 | contrastive_loss 0 | total 3956.42 | n_correct 2728.35 | ppl 3.85 | accuracy 68.96 | wps 11643.9 | ups 1.47 | wpb 7912.8 | bsz 284.9 | num_updates 36504 | lr 7.40193e-05 | gnorm 0.543 | clip 0 | loss_scale 32 | train_wall 1115 | gb_free 15.5 | wall 27521
2023-09-05 18:23:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 18:23:24 | INFO | fairseq.trainer | begin training epoch 21
2023-09-05 18:23:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 18:24:32 | INFO | train_inner | epoch 021:     96 / 1826 loss=1.886, trans_loss=4.719, nll_loss=1.936, w2v_ctc_loss=0.674, task_loss=5.687, task_loss_gen=9.197, contrastive_loss=0, total=3941.06, n_correct=2726.69, ppl=3.83, accuracy=69.187, wps=6653.3, ups=0.84, wpb=7882.1, bsz=285.7, num_updates=36600, lr=7.39221e-05, gnorm=0.558, clip=0, loss_scale=32, train_wall=61, gb_free=15.3, wall=27589
2023-09-05 18:25:33 | INFO | train_inner | epoch 021:    196 / 1826 loss=1.878, trans_loss=4.707, nll_loss=1.922, w2v_ctc_loss=0.664, task_loss=5.455, task_loss_gen=8.765, contrastive_loss=0, total=3977.85, n_correct=2758, ppl=3.79, accuracy=69.334, wps=13024.4, ups=1.64, wpb=7955.7, bsz=293.7, num_updates=36700, lr=7.38213e-05, gnorm=0.548, clip=0, loss_scale=32, train_wall=60, gb_free=17.4, wall=27650
2023-09-05 18:26:34 | INFO | train_inner | epoch 021:    296 / 1826 loss=1.887, trans_loss=4.714, nll_loss=1.929, w2v_ctc_loss=0.676, task_loss=6.162, task_loss_gen=10.364, contrastive_loss=0, total=3939.83, n_correct=2729.08, ppl=3.81, accuracy=69.269, wps=12843.7, ups=1.63, wpb=7879.7, bsz=276.1, num_updates=36800, lr=7.3721e-05, gnorm=0.576, clip=0, loss_scale=32, train_wall=61, gb_free=10.2, wall=27712
2023-09-05 18:27:36 | INFO | train_inner | epoch 021:    396 / 1826 loss=1.876, trans_loss=4.705, nll_loss=1.919, w2v_ctc_loss=0.662, task_loss=5.968, task_loss_gen=8.572, contrastive_loss=0, total=4052.19, n_correct=2809.87, ppl=3.78, accuracy=69.342, wps=13186, ups=1.63, wpb=8104.4, bsz=305.5, num_updates=36900, lr=7.3621e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=61, gb_free=14.7, wall=27773
2023-09-05 18:28:38 | INFO | train_inner | epoch 021:    496 / 1826 loss=1.89, trans_loss=4.719, nll_loss=1.936, w2v_ctc_loss=0.68, task_loss=6.582, task_loss_gen=9.101, contrastive_loss=0, total=3975.54, n_correct=2747.39, ppl=3.83, accuracy=69.107, wps=12854.3, ups=1.62, wpb=7951.1, bsz=279.9, num_updates=37000, lr=7.35215e-05, gnorm=0.561, clip=0, loss_scale=32, train_wall=61, gb_free=16.1, wall=27835
2023-09-05 18:29:40 | INFO | train_inner | epoch 021:    596 / 1826 loss=1.885, trans_loss=4.716, nll_loss=1.934, w2v_ctc_loss=0.669, task_loss=7.187, task_loss_gen=9.381, contrastive_loss=0, total=3945.03, n_correct=2725.7, ppl=3.82, accuracy=69.092, wps=12656.3, ups=1.6, wpb=7890.1, bsz=279.5, num_updates=37100, lr=7.34223e-05, gnorm=0.567, clip=0, loss_scale=32, train_wall=62, gb_free=15.3, wall=27897
2023-09-05 18:30:42 | INFO | train_inner | epoch 021:    696 / 1826 loss=1.882, trans_loss=4.71, nll_loss=1.925, w2v_ctc_loss=0.668, task_loss=5.794, task_loss_gen=8.454, contrastive_loss=0, total=3976.27, n_correct=2751.9, ppl=3.8, accuracy=69.208, wps=12867.9, ups=1.62, wpb=7952.5, bsz=295.9, num_updates=37200, lr=7.33236e-05, gnorm=0.559, clip=0, loss_scale=32, train_wall=61, gb_free=11.7, wall=27959
2023-09-05 18:31:43 | INFO | train_inner | epoch 021:    796 / 1826 loss=1.886, trans_loss=4.716, nll_loss=1.933, w2v_ctc_loss=0.671, task_loss=6.365, task_loss_gen=9.497, contrastive_loss=0, total=3943.94, n_correct=2730.24, ppl=3.82, accuracy=69.226, wps=12873.9, ups=1.63, wpb=7887.9, bsz=275, num_updates=37300, lr=7.32252e-05, gnorm=0.563, clip=0, loss_scale=32, train_wall=60, gb_free=13.2, wall=28020
2023-09-05 18:32:45 | INFO | train_inner | epoch 021:    896 / 1826 loss=1.881, trans_loss=4.703, nll_loss=1.917, w2v_ctc_loss=0.67, task_loss=6.333, task_loss_gen=9.474, contrastive_loss=0, total=3933.6, n_correct=2728.92, ppl=3.78, accuracy=69.375, wps=12709.5, ups=1.62, wpb=7867.2, bsz=286.8, num_updates=37400, lr=7.31272e-05, gnorm=0.568, clip=0, loss_scale=32, train_wall=61, gb_free=17.1, wall=28082
2023-09-05 18:33:46 | INFO | train_inner | epoch 021:    996 / 1826 loss=1.89, trans_loss=4.717, nll_loss=1.934, w2v_ctc_loss=0.683, task_loss=6.674, task_loss_gen=9.271, contrastive_loss=0, total=3921.19, n_correct=2709.15, ppl=3.82, accuracy=69.09, wps=12776.4, ups=1.63, wpb=7842.4, bsz=278.5, num_updates=37500, lr=7.30297e-05, gnorm=0.577, clip=0, loss_scale=32, train_wall=60, gb_free=16.8, wall=28144
2023-09-05 18:34:48 | INFO | train_inner | epoch 021:   1096 / 1826 loss=1.882, trans_loss=4.711, nll_loss=1.927, w2v_ctc_loss=0.67, task_loss=6.464, task_loss_gen=8.899, contrastive_loss=0, total=3949.95, n_correct=2732.5, ppl=3.8, accuracy=69.178, wps=12808.5, ups=1.62, wpb=7899.9, bsz=290.9, num_updates=37600, lr=7.29325e-05, gnorm=0.564, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=28205
2023-09-05 18:35:50 | INFO | train_inner | epoch 021:   1196 / 1826 loss=1.894, trans_loss=4.718, nll_loss=1.935, w2v_ctc_loss=0.685, task_loss=6.482, task_loss_gen=9.725, contrastive_loss=0, total=3907.08, n_correct=2699.61, ppl=3.82, accuracy=69.095, wps=12562.8, ups=1.61, wpb=7814.2, bsz=270.2, num_updates=37700, lr=7.28357e-05, gnorm=0.576, clip=0, loss_scale=32, train_wall=61, gb_free=17.5, wall=28268
2023-09-05 18:36:52 | INFO | train_inner | epoch 021:   1296 / 1826 loss=1.895, trans_loss=4.72, nll_loss=1.938, w2v_ctc_loss=0.69, task_loss=5.856, task_loss_gen=9.226, contrastive_loss=0, total=3953.94, n_correct=2731.05, ppl=3.83, accuracy=69.072, wps=12897, ups=1.63, wpb=7907.9, bsz=282.8, num_updates=37800, lr=7.27393e-05, gnorm=0.564, clip=0, loss_scale=32, train_wall=61, gb_free=17.5, wall=28329
2023-09-05 18:37:54 | INFO | train_inner | epoch 021:   1396 / 1826 loss=1.895, trans_loss=4.722, nll_loss=1.94, w2v_ctc_loss=0.684, task_loss=6.369, task_loss_gen=10.034, contrastive_loss=0, total=3944.23, n_correct=2719.32, ppl=3.84, accuracy=68.944, wps=12670.3, ups=1.61, wpb=7888.5, bsz=271.4, num_updates=37900, lr=7.26433e-05, gnorm=0.562, clip=0, loss_scale=32, train_wall=62, gb_free=13.8, wall=28391
2023-09-05 18:38:56 | INFO | train_inner | epoch 021:   1496 / 1826 loss=1.888, trans_loss=4.719, nll_loss=1.937, w2v_ctc_loss=0.673, task_loss=5.572, task_loss_gen=9.133, contrastive_loss=0, total=3962.56, n_correct=2737.71, ppl=3.83, accuracy=69.089, wps=12782.9, ups=1.61, wpb=7925.1, bsz=288.4, num_updates=38000, lr=7.25476e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=61, gb_free=17.2, wall=28453
2023-09-05 18:38:56 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.5420], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.5420], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 18:39:35 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.778 | trans_loss 4.993 | nll_loss 2.243 | w2v_ctc_loss 1.326 | task_loss 61.475 | task_loss_gen 26.61 | contrastive_loss 0 | total 3505.91 | n_correct 2425.27 | ppl 4.73 | accuracy 69.177 | uer 17.28 | wer 19.192 | raw_wer 19.192 | bleu 31.19 | wps 1197.1 | wpb 3505.9 | bsz 119.3 | num_updates 38000 | best_bleu 31.19
2023-09-05 18:39:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 38000 updates
2023-09-05 18:39:35 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_21_38000.pt
2023-09-05 18:39:38 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_21_38000.pt
2023-09-05 18:39:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_21_38000.pt (epoch 21 @ 38000 updates, score 31.19) (writing took 14.545465688919649 seconds)
--Backword ST Loss tensor(690.7707, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(390.4859, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 18:40:52 | INFO | train_inner | epoch 021:   1596 / 1826 loss=1.882, trans_loss=4.71, nll_loss=1.925, w2v_ctc_loss=0.668, task_loss=4.336, task_loss_gen=9.781, contrastive_loss=0, total=3990.69, n_correct=2762.8, ppl=3.8, accuracy=69.231, wps=6868.3, ups=0.86, wpb=7981.4, bsz=293.2, num_updates=38100, lr=7.24524e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=14.3, wall=28569
2023-09-05 18:41:54 | INFO | train_inner | epoch 021:   1696 / 1826 loss=1.886, trans_loss=4.724, nll_loss=1.944, w2v_ctc_loss=0.669, task_loss=4.491, task_loss_gen=10.222, contrastive_loss=0, total=3955.24, n_correct=2732.41, ppl=3.85, accuracy=69.083, wps=12779.5, ups=1.62, wpb=7910.5, bsz=292.2, num_updates=38200, lr=7.23575e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=61, gb_free=16, wall=28631
2023-09-05 18:42:56 | INFO | train_inner | epoch 021:   1796 / 1826 loss=1.887, trans_loss=4.712, nll_loss=1.929, w2v_ctc_loss=0.682, task_loss=4.278, task_loss_gen=10.78, contrastive_loss=0, total=3945.24, n_correct=2731.71, ppl=3.81, accuracy=69.241, wps=12654.5, ups=1.6, wpb=7890.5, bsz=284.9, num_updates=38300, lr=7.22629e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=62, gb_free=15.5, wall=28694
2023-09-05 18:43:15 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1421.6007, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(801.1879, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1467.2981, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(928.0031, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1227.4983, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(691.2371, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1390.5333, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(826.6562, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1381.2086, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(837.7825, device='cuda:3', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1476.0416, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(694.6431, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1638.2454, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(974.8745, device='cuda:4', grad_fn=<MulBackward0>)
tensor([0.7520], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.7520], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 18:43:54 | INFO | dev_st | epoch 021 | valid on 'dev_st' subset | loss 3.778 | trans_loss 4.991 | nll_loss 2.235 | w2v_ctc_loss 1.331 | task_loss 73.343 | task_loss_gen 29.937 | contrastive_loss 0 | total 3505.91 | n_correct 2423.09 | ppl 4.71 | accuracy 69.114 | uer 17.221 | wer 19.23 | raw_wer 19.23 | bleu 31.34 | wps 1181.5 | wpb 3505.9 | bsz 119.3 | num_updates 38330 | best_bleu 31.34
2023-09-05 18:43:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 38330 updates
2023-09-05 18:43:54 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 18:44:00 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 18:44:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 21 @ 38330 updates, score 31.34) (writing took 11.47591470007319 seconds)
2023-09-05 18:44:06 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)
2023-09-05 18:44:06 | INFO | train | epoch 021 | loss 1.886 | trans_loss 4.715 | nll_loss 1.931 | w2v_ctc_loss 0.674 | task_loss 5.854 | task_loss_gen 9.472 | contrastive_loss 0 | total 3956.37 | n_correct 2736.92 | ppl 3.81 | accuracy 69.178 | wps 11630.7 | ups 1.47 | wpb 7912.7 | bsz 284.8 | num_updates 38330 | lr 7.22347e-05 | gnorm 0.559 | clip 0 | loss_scale 64 | train_wall 1114 | gb_free 15.1 | wall 28763
2023-09-05 18:44:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 18:44:06 | INFO | fairseq.trainer | begin training epoch 22
2023-09-05 18:44:06 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 18:44:57 | INFO | train_inner | epoch 022:     70 / 1826 loss=1.872, trans_loss=4.698, nll_loss=1.91, w2v_ctc_loss=0.658, task_loss=4.486, task_loss_gen=10.984, contrastive_loss=0, total=3934.75, n_correct=2738.59, ppl=3.76, accuracy=69.6, wps=6525.5, ups=0.83, wpb=7869.5, bsz=284, num_updates=38400, lr=7.21688e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=60, gb_free=17.5, wall=28814
2023-09-05 18:45:59 | INFO | train_inner | epoch 022:    170 / 1826 loss=1.875, trans_loss=4.704, nll_loss=1.918, w2v_ctc_loss=0.652, task_loss=4.389, task_loss_gen=10.68, contrastive_loss=0, total=3984.09, n_correct=2761.89, ppl=3.78, accuracy=69.323, wps=12831.3, ups=1.61, wpb=7968.2, bsz=294.2, num_updates=38500, lr=7.2075e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=11.7, wall=28876
2023-09-05 18:47:01 | INFO | train_inner | epoch 022:    270 / 1826 loss=1.874, trans_loss=4.694, nll_loss=1.905, w2v_ctc_loss=0.667, task_loss=4.329, task_loss_gen=10.149, contrastive_loss=0, total=3989.53, n_correct=2776.94, ppl=3.75, accuracy=69.606, wps=12921.1, ups=1.62, wpb=7979.1, bsz=291.4, num_updates=38600, lr=7.19816e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=61, gb_free=15.9, wall=28938
2023-09-05 18:48:03 | INFO | train_inner | epoch 022:    370 / 1826 loss=1.875, trans_loss=4.698, nll_loss=1.911, w2v_ctc_loss=0.669, task_loss=4.066, task_loss_gen=10.177, contrastive_loss=0, total=3982.96, n_correct=2769.04, ppl=3.76, accuracy=69.522, wps=12857.2, ups=1.61, wpb=7965.9, bsz=298.1, num_updates=38700, lr=7.18885e-05, gnorm=0.534, clip=0, loss_scale=64, train_wall=61, gb_free=11.9, wall=29000
2023-09-05 18:49:04 | INFO | train_inner | epoch 022:    470 / 1826 loss=1.87, trans_loss=4.694, nll_loss=1.906, w2v_ctc_loss=0.657, task_loss=4.586, task_loss_gen=10.771, contrastive_loss=0, total=3953.44, n_correct=2754.16, ppl=3.75, accuracy=69.665, wps=12843.3, ups=1.62, wpb=7906.9, bsz=286.5, num_updates=38800, lr=7.17958e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=61, gb_free=15.5, wall=29062
2023-09-05 18:50:06 | INFO | train_inner | epoch 022:    570 / 1826 loss=1.875, trans_loss=4.707, nll_loss=1.922, w2v_ctc_loss=0.659, task_loss=4.481, task_loss_gen=10.316, contrastive_loss=0, total=3947.53, n_correct=2742.39, ppl=3.79, accuracy=69.471, wps=12764.1, ups=1.62, wpb=7895.1, bsz=291.2, num_updates=38900, lr=7.17035e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=61, gb_free=16, wall=29123
2023-09-05 18:51:08 | INFO | train_inner | epoch 022:    670 / 1826 loss=1.888, trans_loss=4.714, nll_loss=1.931, w2v_ctc_loss=0.675, task_loss=4.949, task_loss_gen=11.312, contrastive_loss=0, total=3948.76, n_correct=2732.64, ppl=3.81, accuracy=69.202, wps=12738.2, ups=1.61, wpb=7897.5, bsz=275.5, num_updates=39000, lr=7.16115e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=61, gb_free=15.4, wall=29185
2023-09-05 18:52:11 | INFO | train_inner | epoch 022:    770 / 1826 loss=1.886, trans_loss=4.705, nll_loss=1.919, w2v_ctc_loss=0.678, task_loss=4.921, task_loss_gen=11.372, contrastive_loss=0, total=3987.04, n_correct=2763.15, ppl=3.78, accuracy=69.303, wps=12731, ups=1.6, wpb=7974.1, bsz=279.8, num_updates=39100, lr=7.15199e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=62, gb_free=15.4, wall=29248
2023-09-05 18:53:13 | INFO | train_inner | epoch 022:    870 / 1826 loss=1.883, trans_loss=4.708, nll_loss=1.923, w2v_ctc_loss=0.676, task_loss=4.162, task_loss_gen=10.88, contrastive_loss=0, total=3981.09, n_correct=2765.64, ppl=3.79, accuracy=69.469, wps=12785.8, ups=1.61, wpb=7962.2, bsz=284.7, num_updates=39200, lr=7.14286e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=62, gb_free=17.5, wall=29310
2023-09-05 18:54:15 | INFO | train_inner | epoch 022:    970 / 1826 loss=1.88, trans_loss=4.71, nll_loss=1.925, w2v_ctc_loss=0.664, task_loss=3.945, task_loss_gen=10.778, contrastive_loss=0, total=3982.58, n_correct=2761.23, ppl=3.8, accuracy=69.333, wps=12890.4, ups=1.62, wpb=7965.2, bsz=287.4, num_updates=39300, lr=7.13376e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=61, gb_free=16.3, wall=29372
2023-09-05 18:55:17 | INFO | train_inner | epoch 022:   1070 / 1826 loss=1.887, trans_loss=4.713, nll_loss=1.929, w2v_ctc_loss=0.681, task_loss=4.232, task_loss_gen=11.519, contrastive_loss=0, total=3980.94, n_correct=2756.12, ppl=3.81, accuracy=69.233, wps=12863.7, ups=1.62, wpb=7961.9, bsz=283.5, num_updates=39400, lr=7.1247e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=61, gb_free=16.5, wall=29434
2023-09-05 18:56:18 | INFO | train_inner | epoch 022:   1170 / 1826 loss=1.882, trans_loss=4.704, nll_loss=1.918, w2v_ctc_loss=0.674, task_loss=4.687, task_loss_gen=10.975, contrastive_loss=0, total=3950.57, n_correct=2746.12, ppl=3.78, accuracy=69.512, wps=12803.5, ups=1.62, wpb=7901.1, bsz=277.2, num_updates=39500, lr=7.11568e-05, gnorm=0.531, clip=0, loss_scale=64, train_wall=61, gb_free=11.7, wall=29496
2023-09-05 18:57:20 | INFO | train_inner | epoch 022:   1270 / 1826 loss=1.881, trans_loss=4.703, nll_loss=1.916, w2v_ctc_loss=0.671, task_loss=4.082, task_loss_gen=12.075, contrastive_loss=0, total=3898.83, n_correct=2704.83, ppl=3.77, accuracy=69.375, wps=12586.8, ups=1.61, wpb=7797.7, bsz=275.5, num_updates=39600, lr=7.10669e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=61, gb_free=17, wall=29558
2023-09-05 18:58:22 | INFO | train_inner | epoch 022:   1370 / 1826 loss=1.881, trans_loss=4.704, nll_loss=1.918, w2v_ctc_loss=0.669, task_loss=4, task_loss_gen=11.28, contrastive_loss=0, total=3944.47, n_correct=2733.57, ppl=3.78, accuracy=69.301, wps=12842.4, ups=1.63, wpb=7888.9, bsz=283.8, num_updates=39700, lr=7.09773e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=61, gb_free=17, wall=29619
2023-09-05 18:59:24 | INFO | train_inner | epoch 022:   1470 / 1826 loss=1.885, trans_loss=4.708, nll_loss=1.924, w2v_ctc_loss=0.679, task_loss=4.881, task_loss_gen=11.048, contrastive_loss=0, total=3970.3, n_correct=2748.01, ppl=3.8, accuracy=69.214, wps=12710.1, ups=1.6, wpb=7940.6, bsz=295.8, num_updates=39800, lr=7.08881e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=62, gb_free=16.1, wall=29682
2023-09-05 19:00:26 | INFO | train_inner | epoch 022:   1570 / 1826 loss=1.875, trans_loss=4.699, nll_loss=1.912, w2v_ctc_loss=0.668, task_loss=4.627, task_loss_gen=10.311, contrastive_loss=0, total=3948.96, n_correct=2746.62, ppl=3.76, accuracy=69.553, wps=12793.5, ups=1.62, wpb=7897.9, bsz=289.8, num_updates=39900, lr=7.07992e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=15, wall=29743
2023-09-05 19:01:27 | INFO | train_inner | epoch 022:   1670 / 1826 loss=1.881, trans_loss=4.7, nll_loss=1.913, w2v_ctc_loss=0.673, task_loss=3.532, task_loss_gen=11.559, contrastive_loss=0, total=3931.44, n_correct=2733.55, ppl=3.77, accuracy=69.531, wps=12859.5, ups=1.64, wpb=7862.9, bsz=282, num_updates=40000, lr=7.07107e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=60, gb_free=17.4, wall=29804
2023-09-05 19:01:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.1868], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.1868], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 19:02:07 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.776 | trans_loss 4.985 | nll_loss 2.229 | w2v_ctc_loss 1.337 | task_loss 59.182 | task_loss_gen 27.015 | contrastive_loss 0 | total 3505.91 | n_correct 2427.27 | ppl 4.69 | accuracy 69.234 | uer 17.253 | wer 19.174 | raw_wer 19.174 | bleu 31.04 | wps 1182 | wpb 3505.9 | bsz 119.3 | num_updates 40000 | best_bleu 31.34
2023-09-05 19:02:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 40000 updates
2023-09-05 19:02:07 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_22_40000.pt
2023-09-05 19:02:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_22_40000.pt
2023-09-05 19:02:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_22_40000.pt (epoch 22 @ 40000 updates, score 31.04) (writing took 8.349394491058774 seconds)
--Backword ST Loss tensor(1816.7645, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(1035.0715, device='cuda:0', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:0')
2023-09-05 19:02:41 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-09-05 19:02:58 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 19:03:18 | INFO | train_inner | epoch 022:   1772 / 1826 loss=1.884, trans_loss=4.705, nll_loss=1.92, w2v_ctc_loss=0.671, task_loss=5.902, task_loss_gen=12.009, contrastive_loss=0, total=3940.7, n_correct=2732.31, ppl=3.78, accuracy=69.336, wps=7092.2, ups=0.9, wpb=7881.4, bsz=275.6, num_updates=40100, lr=7.06225e-05, gnorm=0.56, clip=0, loss_scale=32, train_wall=62, gb_free=16.2, wall=29916
2023-09-05 19:03:52 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1249.5315, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(732.7437, device='cuda:5', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:5')
--Backword ST Loss tensor(1752.6921, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(1035.6852, device='cuda:4', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:4')
--Backword ST Loss tensor(1106.5236, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(651.1616, device='cuda:6', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:6')
--Backword ST Loss tensor(1164.3236, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(584.3571, device='cuda:3', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:3')
--Backword ST Loss tensor(2000.2621, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(1151.7732, device='cuda:2', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:2')
--Backword ST Loss tensor(1234.5791, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(598.1685, device='cuda:1', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:1')
--Backword ST Loss tensor(853.9266, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(448.6480, device='cuda:7', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:7')
tensor([1.0137], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([1.0137], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 19:04:31 | INFO | dev_st | epoch 022 | valid on 'dev_st' subset | loss 3.788 | trans_loss 4.99 | nll_loss 2.236 | w2v_ctc_loss 1.367 | task_loss 81.13 | task_loss_gen 32.22 | contrastive_loss 0 | total 3505.91 | n_correct 2423.73 | ppl 4.71 | accuracy 69.133 | uer 17.149 | wer 19.117 | raw_wer 19.117 | bleu 31.06 | wps 1190.9 | wpb 3505.9 | bsz 119.3 | num_updates 40154 | best_bleu 31.34
2023-09-05 19:04:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 40154 updates
2023-09-05 19:04:31 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.0605.pt
2023-09-05 19:04:34 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.0605.pt
2023-09-05 19:04:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.0605.pt (epoch 22 @ 40154 updates, score 31.06) (writing took 7.748733763000928 seconds)
2023-09-05 19:04:39 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)
2023-09-05 19:04:39 | INFO | train | epoch 022 | loss 1.88 | trans_loss 4.704 | nll_loss 1.918 | w2v_ctc_loss 0.669 | task_loss 4.56 | task_loss_gen 10.978 | contrastive_loss 0 | total 3956.55 | n_correct 2746.24 | ppl 3.78 | accuracy 69.41 | wps 11703.7 | ups 1.48 | wpb 7913.1 | bsz 284.9 | num_updates 40154 | lr 7.0575e-05 | gnorm 0.544 | clip 0 | loss_scale 32 | train_wall 1115 | gb_free 12.6 | wall 29997
2023-09-05 19:04:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 19:04:40 | INFO | fairseq.trainer | begin training epoch 23
2023-09-05 19:04:40 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 19:05:16 | INFO | train_inner | epoch 023:     46 / 1826 loss=1.886, trans_loss=4.71, nll_loss=1.924, w2v_ctc_loss=0.671, task_loss=7.891, task_loss_gen=10.7, contrastive_loss=0, total=3889.53, n_correct=2692.55, ppl=3.79, accuracy=69.226, wps=6639.3, ups=0.85, wpb=7779.1, bsz=263.8, num_updates=40200, lr=7.05346e-05, gnorm=0.608, clip=0, loss_scale=32, train_wall=60, gb_free=15.4, wall=30033
2023-09-05 19:06:17 | INFO | train_inner | epoch 023:    146 / 1826 loss=1.871, trans_loss=4.698, nll_loss=1.91, w2v_ctc_loss=0.659, task_loss=7.177, task_loss_gen=9.069, contrastive_loss=0, total=3974.32, n_correct=2767.93, ppl=3.76, accuracy=69.645, wps=12895.4, ups=1.62, wpb=7948.6, bsz=294.1, num_updates=40300, lr=7.0447e-05, gnorm=0.589, clip=0, loss_scale=32, train_wall=61, gb_free=15.3, wall=30094
2023-09-05 19:07:19 | INFO | train_inner | epoch 023:    246 / 1826 loss=1.875, trans_loss=4.693, nll_loss=1.903, w2v_ctc_loss=0.665, task_loss=7.327, task_loss_gen=9.923, contrastive_loss=0, total=3934.29, n_correct=2737, ppl=3.74, accuracy=69.568, wps=12782.3, ups=1.62, wpb=7868.6, bsz=282.4, num_updates=40400, lr=7.03598e-05, gnorm=0.605, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=30156
2023-09-05 19:08:20 | INFO | train_inner | epoch 023:    346 / 1826 loss=1.879, trans_loss=4.698, nll_loss=1.91, w2v_ctc_loss=0.668, task_loss=8.054, task_loss_gen=9.656, contrastive_loss=0, total=3906.92, n_correct=2714.7, ppl=3.76, accuracy=69.484, wps=12765.2, ups=1.63, wpb=7813.8, bsz=275, num_updates=40500, lr=7.02728e-05, gnorm=0.607, clip=0, loss_scale=32, train_wall=60, gb_free=14.4, wall=30217
2023-09-05 19:09:22 | INFO | train_inner | epoch 023:    446 / 1826 loss=1.865, trans_loss=4.691, nll_loss=1.901, w2v_ctc_loss=0.65, task_loss=6.904, task_loss_gen=8.646, contrastive_loss=0, total=4007.87, n_correct=2789.82, ppl=3.74, accuracy=69.609, wps=13001.3, ups=1.62, wpb=8015.7, bsz=301.9, num_updates=40600, lr=7.01862e-05, gnorm=0.573, clip=0, loss_scale=32, train_wall=61, gb_free=15.7, wall=30279
2023-09-05 19:10:23 | INFO | train_inner | epoch 023:    546 / 1826 loss=1.874, trans_loss=4.699, nll_loss=1.912, w2v_ctc_loss=0.663, task_loss=7.062, task_loss_gen=9.319, contrastive_loss=0, total=3952.3, n_correct=2749.94, ppl=3.76, accuracy=69.578, wps=12791.6, ups=1.62, wpb=7904.6, bsz=289.4, num_updates=40700, lr=7.01e-05, gnorm=0.583, clip=0, loss_scale=32, train_wall=61, gb_free=16.6, wall=30341
2023-09-05 19:11:25 | INFO | train_inner | epoch 023:    646 / 1826 loss=1.871, trans_loss=4.695, nll_loss=1.906, w2v_ctc_loss=0.656, task_loss=7.98, task_loss_gen=9.444, contrastive_loss=0, total=3963.31, n_correct=2761.28, ppl=3.75, accuracy=69.671, wps=12790.4, ups=1.61, wpb=7926.6, bsz=281.6, num_updates=40800, lr=7.0014e-05, gnorm=0.599, clip=0, loss_scale=32, train_wall=61, gb_free=12.2, wall=30403
2023-09-05 19:12:28 | INFO | train_inner | epoch 023:    746 / 1826 loss=1.873, trans_loss=4.697, nll_loss=1.909, w2v_ctc_loss=0.661, task_loss=8.048, task_loss_gen=9.178, contrastive_loss=0, total=3975.41, n_correct=2770.08, ppl=3.76, accuracy=69.68, wps=12757.5, ups=1.6, wpb=7950.8, bsz=281.8, num_updates=40900, lr=6.99284e-05, gnorm=0.59, clip=0, loss_scale=32, train_wall=62, gb_free=15.2, wall=30465
2023-09-05 19:13:29 | INFO | train_inner | epoch 023:    846 / 1826 loss=1.872, trans_loss=4.694, nll_loss=1.906, w2v_ctc_loss=0.661, task_loss=7.108, task_loss_gen=9.188, contrastive_loss=0, total=3993.74, n_correct=2779.35, ppl=3.75, accuracy=69.593, wps=12996.3, ups=1.63, wpb=7987.5, bsz=291.2, num_updates=41000, lr=6.9843e-05, gnorm=0.579, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=30526
2023-09-05 19:14:31 | INFO | train_inner | epoch 023:    946 / 1826 loss=1.87, trans_loss=4.69, nll_loss=1.901, w2v_ctc_loss=0.658, task_loss=6.912, task_loss_gen=9.275, contrastive_loss=0, total=3954.06, n_correct=2757.01, ppl=3.73, accuracy=69.726, wps=12751, ups=1.61, wpb=7908.1, bsz=285.9, num_updates=41100, lr=6.9758e-05, gnorm=0.581, clip=0, loss_scale=32, train_wall=61, gb_free=17.4, wall=30588
2023-09-05 19:15:33 | INFO | train_inner | epoch 023:   1046 / 1826 loss=1.876, trans_loss=4.696, nll_loss=1.908, w2v_ctc_loss=0.663, task_loss=7.601, task_loss_gen=10.204, contrastive_loss=0, total=3945.08, n_correct=2744.53, ppl=3.75, accuracy=69.568, wps=12766.5, ups=1.62, wpb=7890.2, bsz=276.4, num_updates=41200, lr=6.96733e-05, gnorm=0.595, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=30650
2023-09-05 19:16:34 | INFO | train_inner | epoch 023:   1146 / 1826 loss=1.877, trans_loss=4.696, nll_loss=1.909, w2v_ctc_loss=0.67, task_loss=6.879, task_loss_gen=8.975, contrastive_loss=0, total=3983.71, n_correct=2770.34, ppl=3.76, accuracy=69.542, wps=12972.9, ups=1.63, wpb=7967.4, bsz=293, num_updates=41300, lr=6.95889e-05, gnorm=0.577, clip=0, loss_scale=32, train_wall=61, gb_free=15.3, wall=30712
2023-09-05 19:17:36 | INFO | train_inner | epoch 023:   1246 / 1826 loss=1.871, trans_loss=4.69, nll_loss=1.9, w2v_ctc_loss=0.658, task_loss=7.159, task_loss_gen=9.135, contrastive_loss=0, total=3933.18, n_correct=2738.91, ppl=3.73, accuracy=69.636, wps=12725.9, ups=1.62, wpb=7866.4, bsz=283.3, num_updates=41400, lr=6.95048e-05, gnorm=0.581, clip=0, loss_scale=32, train_wall=61, gb_free=14.8, wall=30774
2023-09-05 19:18:38 | INFO | train_inner | epoch 023:   1346 / 1826 loss=1.889, trans_loss=4.711, nll_loss=1.927, w2v_ctc_loss=0.678, task_loss=7.18, task_loss_gen=9.958, contrastive_loss=0, total=3899.1, n_correct=2695.51, ppl=3.8, accuracy=69.132, wps=12567, ups=1.61, wpb=7798.2, bsz=271.6, num_updates=41500, lr=6.9421e-05, gnorm=0.598, clip=0, loss_scale=32, train_wall=61, gb_free=15.2, wall=30836
2023-09-05 19:19:40 | INFO | train_inner | epoch 023:   1446 / 1826 loss=1.878, trans_loss=4.707, nll_loss=1.923, w2v_ctc_loss=0.669, task_loss=6.479, task_loss_gen=9.345, contrastive_loss=0, total=3992.43, n_correct=2772, ppl=3.79, accuracy=69.431, wps=12886.1, ups=1.61, wpb=7984.9, bsz=294.9, num_updates=41600, lr=6.93375e-05, gnorm=0.579, clip=0, loss_scale=32, train_wall=61, gb_free=15.7, wall=30898
2023-09-05 19:20:42 | INFO | train_inner | epoch 023:   1546 / 1826 loss=1.875, trans_loss=4.697, nll_loss=1.908, w2v_ctc_loss=0.664, task_loss=6.805, task_loss_gen=9.194, contrastive_loss=0, total=3930.04, n_correct=2732.84, ppl=3.75, accuracy=69.537, wps=12736.5, ups=1.62, wpb=7860.1, bsz=282.8, num_updates=41700, lr=6.92543e-05, gnorm=0.581, clip=0, loss_scale=32, train_wall=61, gb_free=15.6, wall=30959
2023-09-05 19:21:44 | INFO | train_inner | epoch 023:   1646 / 1826 loss=1.873, trans_loss=4.698, nll_loss=1.911, w2v_ctc_loss=0.658, task_loss=6.905, task_loss_gen=8.806, contrastive_loss=0, total=3980.48, n_correct=2769.66, ppl=3.76, accuracy=69.581, wps=12906.2, ups=1.62, wpb=7961, bsz=292, num_updates=41800, lr=6.91714e-05, gnorm=0.586, clip=0, loss_scale=32, train_wall=61, gb_free=16.1, wall=31021
2023-09-05 19:22:46 | INFO | train_inner | epoch 023:   1746 / 1826 loss=1.883, trans_loss=4.706, nll_loss=1.92, w2v_ctc_loss=0.671, task_loss=6.24, task_loss_gen=9.623, contrastive_loss=0, total=3964.67, n_correct=2746.32, ppl=3.78, accuracy=69.27, wps=12694, ups=1.6, wpb=7929.3, bsz=281.9, num_updates=41900, lr=6.90889e-05, gnorm=0.575, clip=0, loss_scale=32, train_wall=62, gb_free=16.7, wall=31083
2023-09-05 19:23:36 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([2.8086], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([2.8086], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 19:24:16 | INFO | dev_st | epoch 023 | valid on 'dev_st' subset | loss 3.798 | trans_loss 4.999 | nll_loss 2.244 | w2v_ctc_loss 1.381 | task_loss 103.608 | task_loss_gen 36.891 | contrastive_loss 0 | total 3505.91 | n_correct 2423.36 | ppl 4.74 | accuracy 69.122 | uer 17.261 | wer 19.249 | raw_wer 19.249 | bleu 31.02 | wps 1176.7 | wpb 3505.9 | bsz 119.3 | num_updates 41980 | best_bleu 31.34
2023-09-05 19:24:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 41980 updates
2023-09-05 19:24:16 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.0209.pt
2023-09-05 19:24:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.0209.pt
2023-09-05 19:24:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.0209.pt (epoch 23 @ 41980 updates, score 31.02) (writing took 7.484315206995234 seconds)
2023-09-05 19:24:24 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)
2023-09-05 19:24:24 | INFO | train | epoch 023 | loss 1.875 | trans_loss 4.698 | nll_loss 1.91 | w2v_ctc_loss 0.663 | task_loss 7.115 | task_loss_gen 9.393 | contrastive_loss 0 | total 3956.37 | n_correct 2751.22 | ppl 3.76 | accuracy 69.539 | wps 12193.7 | ups 1.54 | wpb 7912.7 | bsz 284.8 | num_updates 41980 | lr 6.9023e-05 | gnorm 0.587 | clip 0 | loss_scale 32 | train_wall 1115 | gb_free 16.1 | wall 31182
2023-09-05 19:24:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 19:24:24 | INFO | fairseq.trainer | begin training epoch 24
2023-09-05 19:24:24 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 19:24:45 | INFO | train_inner | epoch 024:     20 / 1826 loss=1.875, trans_loss=4.699, nll_loss=1.911, w2v_ctc_loss=0.653, task_loss=5.956, task_loss_gen=9.818, contrastive_loss=0, total=3952.68, n_correct=2747.02, ppl=3.76, accuracy=69.498, wps=6634.9, ups=0.84, wpb=7905.4, bsz=281.4, num_updates=42000, lr=6.90066e-05, gnorm=0.58, clip=0, loss_scale=32, train_wall=62, gb_free=12.5, wall=31203
2023-09-05 19:24:45 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([1.0938], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([1.0938], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 19:25:25 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.796 | trans_loss 4.995 | nll_loss 2.242 | w2v_ctc_loss 1.382 | task_loss 51.724 | task_loss_gen 25.058 | contrastive_loss 0 | total 3505.91 | n_correct 2423.27 | ppl 4.73 | accuracy 69.12 | uer 17.173 | wer 19.185 | raw_wer 19.185 | bleu 31.03 | wps 1172.9 | wpb 3505.9 | bsz 119.3 | num_updates 42000 | best_bleu 31.34
2023-09-05 19:25:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 42000 updates
2023-09-05 19:25:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_24_42000.pt
2023-09-05 19:25:27 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_24_42000.pt
2023-09-05 19:25:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_24_42000.pt (epoch 24 @ 42000 updates, score 31.03) (writing took 8.850464518065564 seconds)
--Backword ST Loss tensor(797.6265, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(433.1602, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 19:26:36 | INFO | train_inner | epoch 024:    120 / 1826 loss=1.861, trans_loss=4.683, nll_loss=1.891, w2v_ctc_loss=0.65, task_loss=6.012, task_loss_gen=8.507, contrastive_loss=0, total=4034.77, n_correct=2819.58, ppl=3.71, accuracy=69.882, wps=7309.9, ups=0.91, wpb=8069.5, bsz=307.2, num_updates=42100, lr=6.89246e-05, gnorm=0.567, clip=0, loss_scale=32, train_wall=61, gb_free=16.9, wall=31313
2023-09-05 19:27:38 | INFO | train_inner | epoch 024:    220 / 1826 loss=1.859, trans_loss=4.689, nll_loss=1.9, w2v_ctc_loss=0.637, task_loss=5.409, task_loss_gen=8.215, contrastive_loss=0, total=4050.07, n_correct=2832.2, ppl=3.73, accuracy=69.93, wps=13081.2, ups=1.61, wpb=8100.1, bsz=314.8, num_updates=42200, lr=6.88428e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=61, gb_free=15.5, wall=31375
2023-09-05 19:28:39 | INFO | train_inner | epoch 024:    320 / 1826 loss=1.858, trans_loss=4.669, nll_loss=1.873, w2v_ctc_loss=0.647, task_loss=5.964, task_loss_gen=9.933, contrastive_loss=0, total=3901.71, n_correct=2737.28, ppl=3.66, accuracy=70.156, wps=12672.6, ups=1.62, wpb=7803.4, bsz=275.4, num_updates=42300, lr=6.87614e-05, gnorm=0.551, clip=0, loss_scale=64, train_wall=61, gb_free=16.4, wall=31436
2023-09-05 19:29:24 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 19:29:42 | INFO | train_inner | epoch 024:    421 / 1826 loss=1.855, trans_loss=4.666, nll_loss=1.869, w2v_ctc_loss=0.643, task_loss=5.594, task_loss_gen=10.113, contrastive_loss=0, total=3973.67, n_correct=2791.21, ppl=3.65, accuracy=70.243, wps=12725.4, ups=1.6, wpb=7947.3, bsz=282.3, num_updates=42400, lr=6.86803e-05, gnorm=0.551, clip=0, loss_scale=32, train_wall=62, gb_free=17.5, wall=31499
2023-09-05 19:30:43 | INFO | train_inner | epoch 024:    521 / 1826 loss=1.877, trans_loss=4.695, nll_loss=1.906, w2v_ctc_loss=0.665, task_loss=7.494, task_loss_gen=10.313, contrastive_loss=0, total=3896.74, n_correct=2709.82, ppl=3.75, accuracy=69.541, wps=12675, ups=1.63, wpb=7793.5, bsz=266, num_updates=42500, lr=6.85994e-05, gnorm=0.604, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=31560
2023-09-05 19:31:45 | INFO | train_inner | epoch 024:    621 / 1826 loss=1.858, trans_loss=4.681, nll_loss=1.889, w2v_ctc_loss=0.643, task_loss=6.43, task_loss_gen=8.641, contrastive_loss=0, total=4006.81, n_correct=2804.56, ppl=3.7, accuracy=69.995, wps=12977.5, ups=1.62, wpb=8013.6, bsz=295.8, num_updates=42600, lr=6.85189e-05, gnorm=0.57, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=31622
2023-09-05 19:32:47 | INFO | train_inner | epoch 024:    721 / 1826 loss=1.869, trans_loss=4.686, nll_loss=1.895, w2v_ctc_loss=0.658, task_loss=7.102, task_loss_gen=9.456, contrastive_loss=0, total=3924.23, n_correct=2736.33, ppl=3.72, accuracy=69.729, wps=12686.2, ups=1.62, wpb=7848.5, bsz=278.2, num_updates=42700, lr=6.84386e-05, gnorm=0.578, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=31684
2023-09-05 19:33:49 | INFO | train_inner | epoch 024:    821 / 1826 loss=1.872, trans_loss=4.694, nll_loss=1.905, w2v_ctc_loss=0.658, task_loss=7.093, task_loss_gen=9.183, contrastive_loss=0, total=3976.98, n_correct=2768.13, ppl=3.74, accuracy=69.604, wps=12826.3, ups=1.61, wpb=7954, bsz=289.8, num_updates=42800, lr=6.83586e-05, gnorm=0.579, clip=0, loss_scale=32, train_wall=61, gb_free=16.7, wall=31746
2023-09-05 19:34:50 | INFO | train_inner | epoch 024:    921 / 1826 loss=1.87, trans_loss=4.695, nll_loss=1.906, w2v_ctc_loss=0.655, task_loss=6.933, task_loss_gen=8.811, contrastive_loss=0, total=3915.33, n_correct=2726.62, ppl=3.75, accuracy=69.64, wps=12804.6, ups=1.64, wpb=7830.7, bsz=281.3, num_updates=42900, lr=6.82789e-05, gnorm=0.573, clip=0, loss_scale=32, train_wall=60, gb_free=16, wall=31807
2023-09-05 19:35:52 | INFO | train_inner | epoch 024:   1021 / 1826 loss=1.873, trans_loss=4.697, nll_loss=1.909, w2v_ctc_loss=0.656, task_loss=7.736, task_loss_gen=9.917, contrastive_loss=0, total=3931.56, n_correct=2733.49, ppl=3.75, accuracy=69.527, wps=12649.4, ups=1.61, wpb=7863.1, bsz=275.5, num_updates=43000, lr=6.81994e-05, gnorm=0.606, clip=0, loss_scale=32, train_wall=61, gb_free=14.7, wall=31869
2023-09-05 19:36:54 | INFO | train_inner | epoch 024:   1121 / 1826 loss=1.868, trans_loss=4.684, nll_loss=1.893, w2v_ctc_loss=0.658, task_loss=6.919, task_loss_gen=8.715, contrastive_loss=0, total=3998.21, n_correct=2787.54, ppl=3.71, accuracy=69.72, wps=12930.2, ups=1.62, wpb=7996.4, bsz=291.9, num_updates=43100, lr=6.81203e-05, gnorm=0.58, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=31931
2023-09-05 19:37:56 | INFO | train_inner | epoch 024:   1221 / 1826 loss=1.881, trans_loss=4.694, nll_loss=1.906, w2v_ctc_loss=0.673, task_loss=7.393, task_loss_gen=9.932, contrastive_loss=0, total=3888.24, n_correct=2703.56, ppl=3.75, accuracy=69.532, wps=12631.5, ups=1.62, wpb=7776.5, bsz=262.3, num_updates=43200, lr=6.80414e-05, gnorm=0.582, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=31993
2023-09-05 19:38:57 | INFO | train_inner | epoch 024:   1321 / 1826 loss=1.864, trans_loss=4.693, nll_loss=1.904, w2v_ctc_loss=0.646, task_loss=6.343, task_loss_gen=8.602, contrastive_loss=0, total=3953.29, n_correct=2758.55, ppl=3.74, accuracy=69.779, wps=12884.1, ups=1.63, wpb=7906.6, bsz=289.1, num_updates=43300, lr=6.79628e-05, gnorm=0.571, clip=0, loss_scale=32, train_wall=61, gb_free=16.8, wall=32054
2023-09-05 19:39:59 | INFO | train_inner | epoch 024:   1421 / 1826 loss=1.872, trans_loss=4.697, nll_loss=1.909, w2v_ctc_loss=0.657, task_loss=7.651, task_loss_gen=9.696, contrastive_loss=0, total=3998.91, n_correct=2788.14, ppl=3.76, accuracy=69.722, wps=12913.8, ups=1.61, wpb=7997.8, bsz=283.3, num_updates=43400, lr=6.78844e-05, gnorm=0.6, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=32116
2023-09-05 19:41:00 | INFO | train_inner | epoch 024:   1521 / 1826 loss=1.87, trans_loss=4.692, nll_loss=1.903, w2v_ctc_loss=0.662, task_loss=6.255, task_loss_gen=8.278, contrastive_loss=0, total=3985.88, n_correct=2775.14, ppl=3.74, accuracy=69.624, wps=12930.7, ups=1.62, wpb=7971.8, bsz=298.2, num_updates=43500, lr=6.78064e-05, gnorm=0.578, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=32178
2023-09-05 19:42:02 | INFO | train_inner | epoch 024:   1621 / 1826 loss=1.871, trans_loss=4.685, nll_loss=1.894, w2v_ctc_loss=0.659, task_loss=6.621, task_loss_gen=9.681, contrastive_loss=0, total=3870.79, n_correct=2700.33, ppl=3.72, accuracy=69.762, wps=12583.4, ups=1.63, wpb=7741.6, bsz=267.2, num_updates=43600, lr=6.77285e-05, gnorm=0.581, clip=0, loss_scale=32, train_wall=61, gb_free=15, wall=32239
2023-09-05 19:43:04 | INFO | train_inner | epoch 024:   1721 / 1826 loss=1.88, trans_loss=4.694, nll_loss=1.905, w2v_ctc_loss=0.677, task_loss=8.31, task_loss_gen=9.899, contrastive_loss=0, total=3920.38, n_correct=2728.31, ppl=3.75, accuracy=69.593, wps=12714, ups=1.62, wpb=7840.8, bsz=272.1, num_updates=43700, lr=6.7651e-05, gnorm=0.611, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=32301
2023-09-05 19:44:05 | INFO | train_inner | epoch 024:   1821 / 1826 loss=1.862, trans_loss=4.683, nll_loss=1.892, w2v_ctc_loss=0.65, task_loss=7.047, task_loss_gen=8.354, contrastive_loss=0, total=3990.53, n_correct=2785.29, ppl=3.71, accuracy=69.797, wps=12981.8, ups=1.63, wpb=7981.1, bsz=294.9, num_updates=43800, lr=6.75737e-05, gnorm=0.578, clip=0, loss_scale=32, train_wall=61, gb_free=16.5, wall=32362
2023-09-05 19:44:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1375.0731, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(792.6952, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1471.0160, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(849.5917, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1386.8442, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(771.0203, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(825.1396, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(466.7276, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1670.3292, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(934.2860, device='cuda:3', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1166.3455, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(653.9489, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1537.4445, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(880.4893, device='cuda:2', grad_fn=<MulBackward0>)
tensor([0.1071], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.1071], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 19:44:48 | INFO | dev_st | epoch 024 | valid on 'dev_st' subset | loss 3.786 | trans_loss 4.992 | nll_loss 2.238 | w2v_ctc_loss 1.354 | task_loss 110.535 | task_loss_gen 39.335 | contrastive_loss 0 | total 3505.91 | n_correct 2425.45 | ppl 4.72 | accuracy 69.182 | uer 17.264 | wer 19.327 | raw_wer 19.327 | bleu 31.1 | wps 1173.4 | wpb 3505.9 | bsz 119.3 | num_updates 43805 | best_bleu 31.34
2023-09-05 19:44:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 43805 updates
2023-09-05 19:44:48 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.1003.pt
2023-09-05 19:44:51 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.1003.pt
2023-09-05 19:44:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.1003.pt (epoch 24 @ 43805 updates, score 31.1) (writing took 7.735112588037737 seconds)
2023-09-05 19:44:56 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)
2023-09-05 19:44:56 | INFO | train | epoch 024 | loss 1.868 | trans_loss 4.688 | nll_loss 1.897 | w2v_ctc_loss 0.655 | task_loss 6.777 | task_loss_gen 9.214 | contrastive_loss 0 | total 3956.38 | n_correct 2760.06 | ppl 3.72 | accuracy 69.762 | wps 11724.4 | ups 1.48 | wpb 7912.8 | bsz 284.8 | num_updates 43805 | lr 6.75699e-05 | gnorm 0.579 | clip 0 | loss_scale 32 | train_wall 1112 | gb_free 17 | wall 32413
2023-09-05 19:44:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 19:44:56 | INFO | fairseq.trainer | begin training epoch 25
2023-09-05 19:44:56 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 19:46:02 | INFO | train_inner | epoch 025:     95 / 1826 loss=1.862, trans_loss=4.685, nll_loss=1.893, w2v_ctc_loss=0.648, task_loss=6.593, task_loss_gen=8.422, contrastive_loss=0, total=3970.23, n_correct=2777.5, ppl=3.72, accuracy=69.958, wps=6768.2, ups=0.85, wpb=7940.5, bsz=291, num_updates=43900, lr=6.74967e-05, gnorm=0.584, clip=0, loss_scale=32, train_wall=60, gb_free=12.1, wall=32480
2023-09-05 19:47:04 | INFO | train_inner | epoch 025:    195 / 1826 loss=1.854, trans_loss=4.674, nll_loss=1.88, w2v_ctc_loss=0.635, task_loss=6.405, task_loss_gen=8.631, contrastive_loss=0, total=3982.1, n_correct=2791.31, ppl=3.68, accuracy=70.096, wps=12881.3, ups=1.62, wpb=7964.2, bsz=294.3, num_updates=44000, lr=6.742e-05, gnorm=0.566, clip=0, loss_scale=32, train_wall=61, gb_free=15.4, wall=32542
2023-09-05 19:47:04 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.3813], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.3813], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 19:47:44 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.798 | trans_loss 4.987 | nll_loss 2.229 | w2v_ctc_loss 1.406 | task_loss 71.149 | task_loss_gen 29.194 | contrastive_loss 0 | total 3505.91 | n_correct 2427.64 | ppl 4.69 | accuracy 69.244 | uer 17.261 | wer 19.282 | raw_wer 19.282 | bleu 31.44 | wps 1197.9 | wpb 3505.9 | bsz 119.3 | num_updates 44000 | best_bleu 31.44
2023-09-05 19:47:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 44000 updates
2023-09-05 19:47:44 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_25_44000.pt
2023-09-05 19:47:46 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_25_44000.pt
2023-09-05 19:47:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_25_44000.pt (epoch 25 @ 44000 updates, score 31.44) (writing took 13.179401997942477 seconds)
--Backword ST Loss tensor(1500.9905, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(871.1525, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 19:48:59 | INFO | train_inner | epoch 025:    295 / 1826 loss=1.869, trans_loss=4.679, nll_loss=1.885, w2v_ctc_loss=0.661, task_loss=7.069, task_loss_gen=9.724, contrastive_loss=0, total=3966.94, n_correct=2771.16, ppl=3.69, accuracy=69.856, wps=6900.1, ups=0.87, wpb=7933.9, bsz=277.6, num_updates=44100, lr=6.73435e-05, gnorm=0.578, clip=0, loss_scale=32, train_wall=61, gb_free=14.5, wall=32657
2023-09-05 19:50:01 | INFO | train_inner | epoch 025:    395 / 1826 loss=1.86, trans_loss=4.677, nll_loss=1.883, w2v_ctc_loss=0.643, task_loss=8.225, task_loss_gen=9.669, contrastive_loss=0, total=3885.05, n_correct=2720.82, ppl=3.69, accuracy=70.033, wps=12688.3, ups=1.63, wpb=7770.1, bsz=268.2, num_updates=44200, lr=6.72673e-05, gnorm=0.599, clip=0, loss_scale=32, train_wall=61, gb_free=16.3, wall=32718
2023-09-05 19:51:03 | INFO | train_inner | epoch 025:    495 / 1826 loss=1.871, trans_loss=4.684, nll_loss=1.892, w2v_ctc_loss=0.658, task_loss=7.379, task_loss_gen=9.692, contrastive_loss=0, total=3898.44, n_correct=2722.07, ppl=3.71, accuracy=69.825, wps=12562, ups=1.61, wpb=7796.9, bsz=265.7, num_updates=44300, lr=6.71913e-05, gnorm=0.568, clip=0, loss_scale=32, train_wall=61, gb_free=16.8, wall=32780
2023-09-05 19:52:04 | INFO | train_inner | epoch 025:    595 / 1826 loss=1.866, trans_loss=4.683, nll_loss=1.891, w2v_ctc_loss=0.652, task_loss=7.293, task_loss_gen=9.212, contrastive_loss=0, total=3928.25, n_correct=2741.63, ppl=3.71, accuracy=69.793, wps=12787.4, ups=1.63, wpb=7856.5, bsz=282.2, num_updates=44400, lr=6.71156e-05, gnorm=0.579, clip=0, loss_scale=32, train_wall=61, gb_free=17.4, wall=32841
2023-09-05 19:53:05 | INFO | train_inner | epoch 025:    695 / 1826 loss=1.863, trans_loss=4.685, nll_loss=1.894, w2v_ctc_loss=0.65, task_loss=6.416, task_loss_gen=8.862, contrastive_loss=0, total=3959.17, n_correct=2765.44, ppl=3.72, accuracy=69.849, wps=12927.9, ups=1.63, wpb=7918.3, bsz=286.4, num_updates=44500, lr=6.70402e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=60, gb_free=13.6, wall=32903
2023-09-05 19:54:08 | INFO | train_inner | epoch 025:    795 / 1826 loss=1.865, trans_loss=4.686, nll_loss=1.895, w2v_ctc_loss=0.656, task_loss=5.883, task_loss_gen=9.441, contrastive_loss=0, total=3981.26, n_correct=2779.08, ppl=3.72, accuracy=69.804, wps=12806.9, ups=1.61, wpb=7962.5, bsz=298.5, num_updates=44600, lr=6.6965e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=61, gb_free=16.3, wall=32965
2023-09-05 19:54:21 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 19:55:10 | INFO | train_inner | epoch 025:    896 / 1826 loss=1.863, trans_loss=4.68, nll_loss=1.888, w2v_ctc_loss=0.645, task_loss=6.568, task_loss_gen=9.173, contrastive_loss=0, total=3996.03, n_correct=2793.97, ppl=3.7, accuracy=69.919, wps=12710.7, ups=1.59, wpb=7992.1, bsz=286.8, num_updates=44700, lr=6.689e-05, gnorm=0.577, clip=0, loss_scale=32, train_wall=62, gb_free=15.6, wall=33028
2023-09-05 19:56:12 | INFO | train_inner | epoch 025:    996 / 1826 loss=1.855, trans_loss=4.671, nll_loss=1.876, w2v_ctc_loss=0.641, task_loss=6.747, task_loss_gen=8.454, contrastive_loss=0, total=4046.08, n_correct=2840.2, ppl=3.67, accuracy=70.196, wps=13138.3, ups=1.62, wpb=8092.2, bsz=295.4, num_updates=44800, lr=6.68153e-05, gnorm=0.567, clip=0, loss_scale=32, train_wall=61, gb_free=17, wall=33089
2023-09-05 19:57:13 | INFO | train_inner | epoch 025:   1096 / 1826 loss=1.861, trans_loss=4.678, nll_loss=1.885, w2v_ctc_loss=0.649, task_loss=6.032, task_loss_gen=8.574, contrastive_loss=0, total=3954.45, n_correct=2767.97, ppl=3.69, accuracy=69.996, wps=12899.2, ups=1.63, wpb=7908.9, bsz=294.1, num_updates=44900, lr=6.67409e-05, gnorm=0.567, clip=0, loss_scale=32, train_wall=61, gb_free=17.4, wall=33151
2023-09-05 19:58:15 | INFO | train_inner | epoch 025:   1196 / 1826 loss=1.857, trans_loss=4.675, nll_loss=1.882, w2v_ctc_loss=0.644, task_loss=6.455, task_loss_gen=8.494, contrastive_loss=0, total=4002.81, n_correct=2808.58, ppl=3.69, accuracy=70.165, wps=13026.4, ups=1.63, wpb=8005.6, bsz=294.1, num_updates=45000, lr=6.66667e-05, gnorm=0.564, clip=0, loss_scale=32, train_wall=61, gb_free=17.5, wall=33212
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:0')
2023-09-05 19:59:17 | INFO | train_inner | epoch 025:   1296 / 1826 loss=1.861, trans_loss=4.676, nll_loss=1.883, w2v_ctc_loss=0.648, task_loss=6.925, task_loss_gen=8.989, contrastive_loss=0, total=3968.78, n_correct=2776.88, ppl=3.69, accuracy=69.968, wps=12682.9, ups=1.6, wpb=7937.6, bsz=289.8, num_updates=45100, lr=6.65927e-05, gnorm=0.589, clip=0, loss_scale=32, train_wall=62, gb_free=15.9, wall=33275
2023-09-05 20:00:19 | INFO | train_inner | epoch 025:   1396 / 1826 loss=1.865, trans_loss=4.672, nll_loss=1.877, w2v_ctc_loss=0.657, task_loss=6.393, task_loss_gen=9.275, contrastive_loss=0, total=3910.18, n_correct=2737.76, ppl=3.67, accuracy=70.016, wps=12730.2, ups=1.63, wpb=7820.4, bsz=271.7, num_updates=45200, lr=6.6519e-05, gnorm=0.571, clip=0, loss_scale=32, train_wall=61, gb_free=14.7, wall=33336
2023-09-05 20:01:21 | INFO | train_inner | epoch 025:   1496 / 1826 loss=1.869, trans_loss=4.684, nll_loss=1.893, w2v_ctc_loss=0.655, task_loss=7.897, task_loss_gen=9.915, contrastive_loss=0, total=3927.96, n_correct=2743.39, ppl=3.71, accuracy=69.843, wps=12688.7, ups=1.62, wpb=7855.9, bsz=273.5, num_updates=45300, lr=6.64455e-05, gnorm=0.616, clip=0, loss_scale=32, train_wall=61, gb_free=17, wall=33398
2023-09-05 20:02:22 | INFO | train_inner | epoch 025:   1596 / 1826 loss=1.865, trans_loss=4.682, nll_loss=1.89, w2v_ctc_loss=0.652, task_loss=6.751, task_loss_gen=9.304, contrastive_loss=0, total=3875.31, n_correct=2711, ppl=3.71, accuracy=69.956, wps=12761.1, ups=1.65, wpb=7750.6, bsz=272.1, num_updates=45400, lr=6.63723e-05, gnorm=0.573, clip=0, loss_scale=32, train_wall=60, gb_free=16, wall=33459
2023-09-05 20:03:23 | INFO | train_inner | epoch 025:   1696 / 1826 loss=1.865, trans_loss=4.683, nll_loss=1.891, w2v_ctc_loss=0.659, task_loss=6.718, task_loss_gen=8.832, contrastive_loss=0, total=3940.67, n_correct=2755.5, ppl=3.71, accuracy=69.925, wps=12828.5, ups=1.63, wpb=7881.3, bsz=286.7, num_updates=45500, lr=6.62994e-05, gnorm=0.579, clip=0, loss_scale=32, train_wall=61, gb_free=16.7, wall=33520
2023-09-05 20:04:25 | INFO | train_inner | epoch 025:   1796 / 1826 loss=1.867, trans_loss=4.69, nll_loss=1.901, w2v_ctc_loss=0.654, task_loss=6.548, task_loss_gen=8.358, contrastive_loss=0, total=4001.31, n_correct=2790.94, ppl=3.74, accuracy=69.751, wps=12923.4, ups=1.61, wpb=8002.6, bsz=297.4, num_updates=45600, lr=6.62266e-05, gnorm=0.571, clip=0, loss_scale=32, train_wall=61, gb_free=15.3, wall=33582
2023-09-05 20:04:44 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1394.1475, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(771.7467, device='cuda:2', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:2')
--Backword ST Loss tensor(1340.2045, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(720.4409, device='cuda:7', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:7')
--Backword ST Loss tensor(1312.8748, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(760.8506, device='cuda:4', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:4')
--Backword ST Loss tensor(1579.6497, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(884.9484, device='cuda:5', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:5')
--Backword ST Loss tensor(1177.6409, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(643.9709, device='cuda:6', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:6')
--Backword ST Loss tensor(1049.4563, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(559.1146, device='cuda:3', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:3')
--Backword ST Loss tensor(769.6882, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(464.5477, device='cuda:1', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090, device='cuda:1')
tensor([-1.3613], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.3613], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 20:05:23 | INFO | dev_st | epoch 025 | valid on 'dev_st' subset | loss 3.787 | trans_loss 4.985 | nll_loss 2.228 | w2v_ctc_loss 1.373 | task_loss 51.927 | task_loss_gen 25.615 | contrastive_loss 0 | total 3505.91 | n_correct 2425.82 | ppl 4.69 | accuracy 69.192 | uer 17.095 | wer 19.11 | raw_wer 19.11 | bleu 31.02 | wps 1192.4 | wpb 3505.9 | bsz 119.3 | num_updates 45630 | best_bleu 31.44
2023-09-05 20:05:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 45630 updates
2023-09-05 20:05:23 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt
2023-09-05 20:05:31 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt
2023-09-05 20:05:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt (epoch 25 @ 45630 updates, score 31.02) (writing took 7.5966013569850475 seconds)
2023-09-05 20:05:31 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)
2023-09-05 20:05:31 | INFO | train | epoch 025 | loss 1.863 | trans_loss 4.68 | nll_loss 1.888 | w2v_ctc_loss 0.65 | task_loss 6.77 | task_loss_gen 9.043 | contrastive_loss 0 | total 3956.51 | n_correct 2767.32 | ppl 3.7 | accuracy 69.944 | wps 11696.3 | ups 1.48 | wpb 7913 | bsz 284.9 | num_updates 45630 | lr 6.62048e-05 | gnorm 0.575 | clip 0 | loss_scale 32 | train_wall 1112 | gb_free 11.8 | wall 33648
2023-09-05 20:05:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 20:05:31 | INFO | fairseq.trainer | begin training epoch 26
2023-09-05 20:05:31 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 20:06:22 | INFO | train_inner | epoch 026:     70 / 1826 loss=1.855, trans_loss=4.66, nll_loss=1.861, w2v_ctc_loss=0.642, task_loss=7.28, task_loss_gen=9.848, contrastive_loss=0, total=3888.8, n_correct=2732.5, ppl=3.63, accuracy=70.266, wps=6636.4, ups=0.85, wpb=7777.6, bsz=269.7, num_updates=45700, lr=6.61541e-05, gnorm=0.601, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=33699
2023-09-05 20:07:24 | INFO | train_inner | epoch 026:    170 / 1826 loss=1.857, trans_loss=4.663, nll_loss=1.866, w2v_ctc_loss=0.644, task_loss=7.745, task_loss_gen=9.365, contrastive_loss=0, total=3911.85, n_correct=2747.81, ppl=3.64, accuracy=70.243, wps=12707, ups=1.62, wpb=7823.7, bsz=269.2, num_updates=45800, lr=6.60819e-05, gnorm=0.585, clip=0, loss_scale=32, train_wall=61, gb_free=17, wall=33761
2023-09-05 20:08:25 | INFO | train_inner | epoch 026:    270 / 1826 loss=1.854, trans_loss=4.67, nll_loss=1.874, w2v_ctc_loss=0.642, task_loss=7.615, task_loss_gen=9.145, contrastive_loss=0, total=3959.89, n_correct=2777.78, ppl=3.67, accuracy=70.148, wps=12882.3, ups=1.63, wpb=7919.8, bsz=291.7, num_updates=45900, lr=6.60098e-05, gnorm=0.605, clip=0, loss_scale=32, train_wall=61, gb_free=13.9, wall=33822
2023-09-05 20:09:27 | INFO | train_inner | epoch 026:    370 / 1826 loss=1.855, trans_loss=4.673, nll_loss=1.878, w2v_ctc_loss=0.642, task_loss=6.777, task_loss_gen=8.901, contrastive_loss=0, total=3976.8, n_correct=2788.18, ppl=3.68, accuracy=70.111, wps=12928.5, ups=1.63, wpb=7953.6, bsz=289.6, num_updates=46000, lr=6.5938e-05, gnorm=0.573, clip=0, loss_scale=32, train_wall=60, gb_free=17.2, wall=33884
2023-09-05 20:09:27 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.2017], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.2017], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 20:10:06 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.791 | trans_loss 4.995 | nll_loss 2.238 | w2v_ctc_loss 1.367 | task_loss 86.226 | task_loss_gen 32.264 | contrastive_loss 0 | total 3505.91 | n_correct 2425.73 | ppl 4.72 | accuracy 69.19 | uer 17.355 | wer 19.282 | raw_wer 19.282 | bleu 31.2 | wps 1179.3 | wpb 3505.9 | bsz 119.3 | num_updates 46000 | best_bleu 31.44
2023-09-05 20:10:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 46000 updates
2023-09-05 20:10:06 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_26_46000.pt
2023-09-05 20:10:09 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_26_46000.pt
2023-09-05 20:10:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_26_46000.pt (epoch 26 @ 46000 updates, score 31.2) (writing took 8.854423103039153 seconds)
--Backword ST Loss tensor(737.1523, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(420.6851, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 20:11:18 | INFO | train_inner | epoch 026:    470 / 1826 loss=1.848, trans_loss=4.672, nll_loss=1.877, w2v_ctc_loss=0.624, task_loss=6.901, task_loss_gen=8.511, contrastive_loss=0, total=3979.19, n_correct=2793.2, ppl=3.67, accuracy=70.195, wps=7179.1, ups=0.9, wpb=7958.4, bsz=300.5, num_updates=46100, lr=6.58665e-05, gnorm=0.583, clip=0, loss_scale=32, train_wall=61, gb_free=17.4, wall=33995
2023-09-05 20:12:19 | INFO | train_inner | epoch 026:    570 / 1826 loss=1.853, trans_loss=4.672, nll_loss=1.877, w2v_ctc_loss=0.634, task_loss=7.201, task_loss_gen=8.487, contrastive_loss=0, total=4025.5, n_correct=2819.98, ppl=3.67, accuracy=70.053, wps=12995.7, ups=1.61, wpb=8051, bsz=306.2, num_updates=46200, lr=6.57952e-05, gnorm=0.581, clip=0, loss_scale=32, train_wall=61, gb_free=16.7, wall=34057
2023-09-05 20:13:21 | INFO | train_inner | epoch 026:    670 / 1826 loss=1.851, trans_loss=4.669, nll_loss=1.874, w2v_ctc_loss=0.633, task_loss=7.001, task_loss_gen=8.516, contrastive_loss=0, total=3972.74, n_correct=2789.63, ppl=3.66, accuracy=70.219, wps=12885.9, ups=1.62, wpb=7945.5, bsz=290.6, num_updates=46300, lr=6.57241e-05, gnorm=0.565, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=34118
2023-09-05 20:14:22 | INFO | train_inner | epoch 026:    770 / 1826 loss=1.862, trans_loss=4.681, nll_loss=1.888, w2v_ctc_loss=0.645, task_loss=7.397, task_loss_gen=9.317, contrastive_loss=0, total=3918.1, n_correct=2740.35, ppl=3.7, accuracy=69.941, wps=12890.8, ups=1.65, wpb=7836.2, bsz=272.1, num_updates=46400, lr=6.56532e-05, gnorm=0.569, clip=0, loss_scale=32, train_wall=60, gb_free=16, wall=34179
2023-09-05 20:15:24 | INFO | train_inner | epoch 026:    870 / 1826 loss=1.851, trans_loss=4.67, nll_loss=1.874, w2v_ctc_loss=0.635, task_loss=6.491, task_loss_gen=8.384, contrastive_loss=0, total=3981.81, n_correct=2795.15, ppl=3.67, accuracy=70.198, wps=12926.8, ups=1.62, wpb=7963.6, bsz=293.5, num_updates=46500, lr=6.55826e-05, gnorm=0.554, clip=0, loss_scale=32, train_wall=61, gb_free=15.2, wall=34241
2023-09-05 20:16:26 | INFO | train_inner | epoch 026:    970 / 1826 loss=1.861, trans_loss=4.676, nll_loss=1.882, w2v_ctc_loss=0.648, task_loss=7.272, task_loss_gen=8.876, contrastive_loss=0, total=3962.21, n_correct=2774.23, ppl=3.68, accuracy=70.017, wps=12695.6, ups=1.6, wpb=7924.4, bsz=282.1, num_updates=46600, lr=6.55122e-05, gnorm=0.579, clip=0, loss_scale=32, train_wall=62, gb_free=15.7, wall=34303
2023-09-05 20:17:28 | INFO | train_inner | epoch 026:   1070 / 1826 loss=1.856, trans_loss=4.666, nll_loss=1.87, w2v_ctc_loss=0.642, task_loss=7.517, task_loss_gen=8.749, contrastive_loss=0, total=3950, n_correct=2773.67, ppl=3.65, accuracy=70.219, wps=12746.6, ups=1.61, wpb=7900, bsz=280.1, num_updates=46700, lr=6.5442e-05, gnorm=0.557, clip=0, loss_scale=64, train_wall=61, gb_free=15.3, wall=34365
2023-09-05 20:18:29 | INFO | train_inner | epoch 026:   1170 / 1826 loss=1.855, trans_loss=4.675, nll_loss=1.881, w2v_ctc_loss=0.639, task_loss=6.623, task_loss_gen=9.088, contrastive_loss=0, total=4004.58, n_correct=2807.37, ppl=3.68, accuracy=70.104, wps=13031.9, ups=1.63, wpb=8009.2, bsz=291, num_updates=46800, lr=6.5372e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=61, gb_free=14.4, wall=34427
2023-09-05 20:19:31 | INFO | train_inner | epoch 026:   1270 / 1826 loss=1.859, trans_loss=4.674, nll_loss=1.88, w2v_ctc_loss=0.645, task_loss=5.751, task_loss_gen=9.278, contrastive_loss=0, total=3997.09, n_correct=2802.06, ppl=3.68, accuracy=70.102, wps=12881.1, ups=1.61, wpb=7994.2, bsz=290.7, num_updates=46900, lr=6.53023e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=61, gb_free=16.7, wall=34489
2023-09-05 20:20:34 | INFO | train_inner | epoch 026:   1370 / 1826 loss=1.863, trans_loss=4.672, nll_loss=1.877, w2v_ctc_loss=0.655, task_loss=5.932, task_loss_gen=10.039, contrastive_loss=0, total=3931.19, n_correct=2754.5, ppl=3.67, accuracy=70.068, wps=12607.9, ups=1.6, wpb=7862.4, bsz=274.9, num_updates=47000, lr=6.52328e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=61, gb_free=14.6, wall=34551
2023-09-05 20:21:36 | INFO | train_inner | epoch 026:   1470 / 1826 loss=1.86, trans_loss=4.669, nll_loss=1.873, w2v_ctc_loss=0.651, task_loss=6.322, task_loss_gen=10.682, contrastive_loss=0, total=3924.9, n_correct=2750.33, ppl=3.66, accuracy=70.074, wps=12647, ups=1.61, wpb=7849.8, bsz=278.5, num_updates=47100, lr=6.51635e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=61, gb_free=13.7, wall=34613
2023-09-05 20:22:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 20:22:38 | INFO | train_inner | epoch 026:   1571 / 1826 loss=1.863, trans_loss=4.669, nll_loss=1.874, w2v_ctc_loss=0.657, task_loss=6.719, task_loss_gen=10.338, contrastive_loss=0, total=3880.7, n_correct=2721.29, ppl=3.66, accuracy=70.124, wps=12437.6, ups=1.6, wpb=7761.4, bsz=263.5, num_updates=47200, lr=6.50945e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=62, gb_free=16.9, wall=34676
2023-09-05 20:23:41 | INFO | train_inner | epoch 026:   1671 / 1826 loss=1.856, trans_loss=4.672, nll_loss=1.878, w2v_ctc_loss=0.644, task_loss=7.333, task_loss_gen=9.072, contrastive_loss=0, total=3968.2, n_correct=2783.62, ppl=3.68, accuracy=70.148, wps=12759.3, ups=1.61, wpb=7936.4, bsz=286.2, num_updates=47300, lr=6.50256e-05, gnorm=0.593, clip=0, loss_scale=32, train_wall=62, gb_free=16.5, wall=34738
2023-09-05 20:24:42 | INFO | train_inner | epoch 026:   1771 / 1826 loss=1.859, trans_loss=4.679, nll_loss=1.887, w2v_ctc_loss=0.651, task_loss=7.372, task_loss_gen=8.529, contrastive_loss=0, total=4007.68, n_correct=2806.25, ppl=3.7, accuracy=70.022, wps=12945.1, ups=1.62, wpb=8015.4, bsz=297.5, num_updates=47400, lr=6.4957e-05, gnorm=0.584, clip=0, loss_scale=32, train_wall=61, gb_free=12.8, wall=34800
2023-09-05 20:25:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1115.2211, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(676.3088, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1008.5214, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(595.3483, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1684.9390, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(889.3510, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1586.7803, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(926.3365, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1187.2083, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(676.7301, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1666.8011, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(1008.8809, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1759.7390, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(1027.7913, device='cuda:3', grad_fn=<MulBackward0>)
tensor([0.1215], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.1215], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 20:25:55 | INFO | dev_st | epoch 026 | valid on 'dev_st' subset | loss 3.799 | trans_loss 4.982 | nll_loss 2.226 | w2v_ctc_loss 1.419 | task_loss 82.755 | task_loss_gen 31.481 | contrastive_loss 0 | total 3505.91 | n_correct 2431.36 | ppl 4.68 | accuracy 69.35 | uer 17.114 | wer 18.907 | raw_wer 18.907 | bleu 31.11 | wps 1192.5 | wpb 3505.9 | bsz 119.3 | num_updates 47455 | best_bleu 31.44
2023-09-05 20:25:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 47455 updates
2023-09-05 20:25:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.1108.pt
2023-09-05 20:25:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.1108.pt
2023-09-05 20:26:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.1108.pt (epoch 26 @ 47455 updates, score 31.11) (writing took 7.653382853954099 seconds)
2023-09-05 20:26:03 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)
2023-09-05 20:26:03 | INFO | train | epoch 026 | loss 1.856 | trans_loss 4.671 | nll_loss 1.876 | w2v_ctc_loss 0.643 | task_loss 6.98 | task_loss_gen 9.15 | contrastive_loss 0 | total 3956.22 | n_correct 2774.51 | ppl 3.67 | accuracy 70.13 | wps 11722.1 | ups 1.48 | wpb 7912.4 | bsz 284.8 | num_updates 47455 | lr 6.49193e-05 | gnorm 0.571 | clip 0 | loss_scale 32 | train_wall 1112 | gb_free 15.2 | wall 34880
2023-09-05 20:26:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 20:26:03 | INFO | fairseq.trainer | begin training epoch 27
2023-09-05 20:26:03 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 20:26:38 | INFO | train_inner | epoch 027:     45 / 1826 loss=1.847, trans_loss=4.662, nll_loss=1.864, w2v_ctc_loss=0.632, task_loss=8.23, task_loss_gen=9.295, contrastive_loss=0, total=3903.86, n_correct=2747.97, ppl=3.64, accuracy=70.391, wps=6743.8, ups=0.86, wpb=7807.7, bsz=282.2, num_updates=47500, lr=6.48886e-05, gnorm=0.607, clip=0, loss_scale=32, train_wall=60, gb_free=14.5, wall=34915
2023-09-05 20:27:40 | INFO | train_inner | epoch 027:    145 / 1826 loss=1.846, trans_loss=4.655, nll_loss=1.855, w2v_ctc_loss=0.633, task_loss=7.516, task_loss_gen=9.242, contrastive_loss=0, total=3934.64, n_correct=2777.26, ppl=3.62, accuracy=70.585, wps=12841.1, ups=1.63, wpb=7869.3, bsz=281.8, num_updates=47600, lr=6.48204e-05, gnorm=0.586, clip=0, loss_scale=32, train_wall=61, gb_free=16.6, wall=34977
2023-09-05 20:28:41 | INFO | train_inner | epoch 027:    245 / 1826 loss=1.851, trans_loss=4.66, nll_loss=1.862, w2v_ctc_loss=0.639, task_loss=7.456, task_loss_gen=9.031, contrastive_loss=0, total=3939.62, n_correct=2771.42, ppl=3.63, accuracy=70.347, wps=12854.1, ups=1.63, wpb=7879.2, bsz=282.5, num_updates=47700, lr=6.47524e-05, gnorm=0.585, clip=0, loss_scale=32, train_wall=61, gb_free=16.2, wall=35038
2023-09-05 20:29:42 | INFO | train_inner | epoch 027:    345 / 1826 loss=1.847, trans_loss=4.663, nll_loss=1.866, w2v_ctc_loss=0.636, task_loss=6.526, task_loss_gen=8.213, contrastive_loss=0, total=3977.4, n_correct=2799.41, ppl=3.65, accuracy=70.383, wps=12919, ups=1.62, wpb=7954.8, bsz=300.8, num_updates=47800, lr=6.46846e-05, gnorm=0.567, clip=0, loss_scale=32, train_wall=61, gb_free=17, wall=35100
2023-09-05 20:30:45 | INFO | train_inner | epoch 027:    445 / 1826 loss=1.844, trans_loss=4.652, nll_loss=1.851, w2v_ctc_loss=0.627, task_loss=7.455, task_loss_gen=9.101, contrastive_loss=0, total=3893.64, n_correct=2749.7, ppl=3.61, accuracy=70.62, wps=12398.4, ups=1.59, wpb=7787.3, bsz=273.4, num_updates=47900, lr=6.46171e-05, gnorm=0.581, clip=0, loss_scale=32, train_wall=62, gb_free=16.3, wall=35162
2023-09-05 20:31:47 | INFO | train_inner | epoch 027:    545 / 1826 loss=1.846, trans_loss=4.66, nll_loss=1.862, w2v_ctc_loss=0.635, task_loss=6.481, task_loss_gen=8.371, contrastive_loss=0, total=4046.71, n_correct=2850.47, ppl=3.64, accuracy=70.439, wps=13068.1, ups=1.61, wpb=8093.4, bsz=302.4, num_updates=48000, lr=6.45497e-05, gnorm=0.564, clip=0, loss_scale=32, train_wall=61, gb_free=16.8, wall=35224
2023-09-05 20:31:47 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([1.7314], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([1.7314], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 20:32:27 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 3.792 | trans_loss 4.985 | nll_loss 2.227 | w2v_ctc_loss 1.39 | task_loss 130.47 | task_loss_gen 45.156 | contrastive_loss 0 | total 3505.91 | n_correct 2426.45 | ppl 4.68 | accuracy 69.21 | uer 17.232 | wer 19.177 | raw_wer 19.177 | bleu 30.76 | wps 1182.9 | wpb 3505.9 | bsz 119.3 | num_updates 48000 | best_bleu 31.44
2023-09-05 20:32:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 48000 updates
2023-09-05 20:32:27 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_27_48000.pt
2023-09-05 20:32:29 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_27_48000.pt
2023-09-05 20:32:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_27_48000.pt (epoch 27 @ 48000 updates, score 30.76) (writing took 7.775703441002406 seconds)
--Backword ST Loss tensor(1140.1975, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(607.9748, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 20:33:36 | INFO | train_inner | epoch 027:    645 / 1826 loss=1.862, trans_loss=4.675, nll_loss=1.88, w2v_ctc_loss=0.646, task_loss=7.065, task_loss_gen=9.351, contrastive_loss=0, total=3871.95, n_correct=2708.81, ppl=3.68, accuracy=69.96, wps=7132.8, ups=0.92, wpb=7743.9, bsz=269.2, num_updates=48100, lr=6.44826e-05, gnorm=0.581, clip=0, loss_scale=32, train_wall=60, gb_free=15.1, wall=35333
2023-09-05 20:34:38 | INFO | train_inner | epoch 027:    745 / 1826 loss=1.847, trans_loss=4.658, nll_loss=1.859, w2v_ctc_loss=0.632, task_loss=6.939, task_loss_gen=9.263, contrastive_loss=0, total=3917, n_correct=2754.73, ppl=3.63, accuracy=70.328, wps=12584.3, ups=1.61, wpb=7834, bsz=285.4, num_updates=48200, lr=6.44157e-05, gnorm=0.577, clip=0, loss_scale=32, train_wall=61, gb_free=15.7, wall=35395
2023-09-05 20:35:41 | INFO | train_inner | epoch 027:    845 / 1826 loss=1.847, trans_loss=4.663, nll_loss=1.866, w2v_ctc_loss=0.628, task_loss=6.04, task_loss_gen=8.7, contrastive_loss=0, total=4056.19, n_correct=2852.47, ppl=3.65, accuracy=70.324, wps=12960, ups=1.6, wpb=8112.4, bsz=301.1, num_updates=48300, lr=6.43489e-05, gnorm=0.557, clip=0, loss_scale=32, train_wall=62, gb_free=15.6, wall=35458
2023-09-05 20:36:43 | INFO | train_inner | epoch 027:    945 / 1826 loss=1.857, trans_loss=4.67, nll_loss=1.875, w2v_ctc_loss=0.642, task_loss=6.954, task_loss_gen=9.038, contrastive_loss=0, total=3970.38, n_correct=2785.4, ppl=3.67, accuracy=70.154, wps=12746.5, ups=1.61, wpb=7940.8, bsz=277.5, num_updates=48400, lr=6.42824e-05, gnorm=0.571, clip=0, loss_scale=32, train_wall=62, gb_free=15.3, wall=35520
2023-09-05 20:37:44 | INFO | train_inner | epoch 027:   1045 / 1826 loss=1.852, trans_loss=4.664, nll_loss=1.867, w2v_ctc_loss=0.638, task_loss=7.087, task_loss_gen=9.188, contrastive_loss=0, total=3958.52, n_correct=2781.76, ppl=3.65, accuracy=70.273, wps=12883.6, ups=1.63, wpb=7917, bsz=277.2, num_updates=48500, lr=6.42161e-05, gnorm=0.57, clip=0, loss_scale=32, train_wall=61, gb_free=15.8, wall=35582
2023-09-05 20:38:46 | INFO | train_inner | epoch 027:   1145 / 1826 loss=1.856, trans_loss=4.669, nll_loss=1.873, w2v_ctc_loss=0.644, task_loss=7.279, task_loss_gen=8.92, contrastive_loss=0, total=3953.77, n_correct=2777.23, ppl=3.66, accuracy=70.243, wps=12886.8, ups=1.63, wpb=7907.5, bsz=281.5, num_updates=48600, lr=6.415e-05, gnorm=0.569, clip=0, loss_scale=32, train_wall=61, gb_free=16.5, wall=35643
2023-09-05 20:39:47 | INFO | train_inner | epoch 027:   1245 / 1826 loss=1.853, trans_loss=4.667, nll_loss=1.871, w2v_ctc_loss=0.639, task_loss=8.333, task_loss_gen=9.233, contrastive_loss=0, total=3954.68, n_correct=2776.5, ppl=3.66, accuracy=70.208, wps=12863.2, ups=1.63, wpb=7909.4, bsz=284.8, num_updates=48700, lr=6.40841e-05, gnorm=0.601, clip=0, loss_scale=32, train_wall=61, gb_free=16.8, wall=35704
2023-09-05 20:40:49 | INFO | train_inner | epoch 027:   1345 / 1826 loss=1.848, trans_loss=4.654, nll_loss=1.853, w2v_ctc_loss=0.638, task_loss=7.397, task_loss_gen=8.909, contrastive_loss=0, total=3968.7, n_correct=2797.56, ppl=3.61, accuracy=70.491, wps=12844.4, ups=1.62, wpb=7937.4, bsz=283.4, num_updates=48800, lr=6.40184e-05, gnorm=0.572, clip=0, loss_scale=32, train_wall=61, gb_free=17, wall=35766
2023-09-05 20:41:51 | INFO | train_inner | epoch 027:   1445 / 1826 loss=1.858, trans_loss=4.67, nll_loss=1.874, w2v_ctc_loss=0.645, task_loss=7.698, task_loss_gen=9.327, contrastive_loss=0, total=3968.12, n_correct=2781.82, ppl=3.67, accuracy=70.104, wps=12785.9, ups=1.61, wpb=7936.2, bsz=277.9, num_updates=48900, lr=6.39529e-05, gnorm=0.582, clip=0, loss_scale=32, train_wall=61, gb_free=15.7, wall=35828
2023-09-05 20:42:53 | INFO | train_inner | epoch 027:   1545 / 1826 loss=1.856, trans_loss=4.664, nll_loss=1.866, w2v_ctc_loss=0.647, task_loss=7.513, task_loss_gen=9.238, contrastive_loss=0, total=3890.4, n_correct=2734.13, ppl=3.64, accuracy=70.279, wps=12539.5, ups=1.61, wpb=7780.8, bsz=271.6, num_updates=49000, lr=6.38877e-05, gnorm=0.588, clip=0, loss_scale=32, train_wall=61, gb_free=16.6, wall=35890
2023-09-05 20:43:54 | INFO | train_inner | epoch 027:   1645 / 1826 loss=1.853, trans_loss=4.668, nll_loss=1.872, w2v_ctc_loss=0.634, task_loss=6.948, task_loss_gen=8.752, contrastive_loss=0, total=3956.13, n_correct=2777.47, ppl=3.66, accuracy=70.207, wps=12896.4, ups=1.63, wpb=7912.3, bsz=287.4, num_updates=49100, lr=6.38226e-05, gnorm=0.576, clip=0, loss_scale=32, train_wall=61, gb_free=16.5, wall=35952
2023-09-05 20:44:56 | INFO | train_inner | epoch 027:   1745 / 1826 loss=1.854, trans_loss=4.678, nll_loss=1.885, w2v_ctc_loss=0.636, task_loss=6.695, task_loss_gen=8.445, contrastive_loss=0, total=3999.06, n_correct=2803.93, ppl=3.69, accuracy=70.115, wps=13061.1, ups=1.63, wpb=7998.1, bsz=295.4, num_updates=49200, lr=6.37577e-05, gnorm=0.566, clip=0, loss_scale=32, train_wall=60, gb_free=16.6, wall=36013
2023-09-05 20:45:46 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1181.0250, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(716.0396, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1111.2700, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(662.9637, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(765.6337, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(410.5071, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1923.5964, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(1181.8989, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(885.6444, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(514.3302, device='cuda:3', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1136.9237, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(738.1470, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1559.3302, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(924.1948, device='cuda:1', grad_fn=<MulBackward0>)
tensor([1.1611], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([1.1611], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 20:46:25 | INFO | dev_st | epoch 027 | valid on 'dev_st' subset | loss 3.79 | trans_loss 4.977 | nll_loss 2.22 | w2v_ctc_loss 1.401 | task_loss 76.676 | task_loss_gen 30.092 | contrastive_loss 0 | total 3505.91 | n_correct 2433.36 | ppl 4.66 | accuracy 69.407 | uer 16.998 | wer 18.986 | raw_wer 18.986 | bleu 30.66 | wps 1186.8 | wpb 3505.9 | bsz 119.3 | num_updates 49281 | best_bleu 31.44
2023-09-05 20:46:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 49281 updates
2023-09-05 20:46:25 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt
2023-09-05 20:46:33 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt
2023-09-05 20:46:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt (epoch 27 @ 49281 updates, score 30.66) (writing took 7.478734912001528 seconds)
2023-09-05 20:46:33 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)
2023-09-05 20:46:33 | INFO | train | epoch 027 | loss 1.851 | trans_loss 4.663 | nll_loss 1.866 | w2v_ctc_loss 0.638 | task_loss 7.114 | task_loss_gen 8.937 | contrastive_loss 0 | total 3956.37 | n_correct 2781.81 | ppl 3.65 | accuracy 70.312 | wps 11745.8 | ups 1.48 | wpb 7912.7 | bsz 284.8 | num_updates 49281 | lr 6.37053e-05 | gnorm 0.576 | clip 0 | loss_scale 64 | train_wall 1114 | gb_free 16.9 | wall 36110
2023-09-05 20:46:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 20:46:33 | INFO | fairseq.trainer | begin training epoch 28
2023-09-05 20:46:33 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 20:46:52 | INFO | train_inner | epoch 028:     19 / 1826 loss=1.842, trans_loss=4.655, nll_loss=1.856, w2v_ctc_loss=0.633, task_loss=5.78, task_loss_gen=8.198, contrastive_loss=0, total=3970.06, n_correct=2802.81, ppl=3.62, accuracy=70.599, wps=6816.5, ups=0.86, wpb=7940.1, bsz=298.5, num_updates=49300, lr=6.3693e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=61, gb_free=16.4, wall=36129
2023-09-05 20:47:54 | INFO | train_inner | epoch 028:    119 / 1826 loss=1.837, trans_loss=4.643, nll_loss=1.839, w2v_ctc_loss=0.626, task_loss=5.511, task_loss_gen=9.507, contrastive_loss=0, total=3923.02, n_correct=2780.23, ppl=3.58, accuracy=70.87, wps=12753, ups=1.63, wpb=7846, bsz=281.1, num_updates=49400, lr=6.36285e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=61, gb_free=12.1, wall=36191
2023-09-05 20:48:56 | INFO | train_inner | epoch 028:    219 / 1826 loss=1.844, trans_loss=4.65, nll_loss=1.849, w2v_ctc_loss=0.632, task_loss=5.908, task_loss_gen=9.737, contrastive_loss=0, total=3980.33, n_correct=2811.9, ppl=3.6, accuracy=70.645, wps=12724.7, ups=1.6, wpb=7960.7, bsz=282.9, num_updates=49500, lr=6.35642e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=62, gb_free=16.5, wall=36254
2023-09-05 20:49:58 | INFO | train_inner | epoch 028:    319 / 1826 loss=1.845, trans_loss=4.652, nll_loss=1.851, w2v_ctc_loss=0.635, task_loss=6.168, task_loss_gen=9.366, contrastive_loss=0, total=3980.3, n_correct=2806.01, ppl=3.61, accuracy=70.497, wps=12984.5, ups=1.63, wpb=7960.6, bsz=289.5, num_updates=49600, lr=6.35001e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=60, gb_free=17.3, wall=36315
2023-09-05 20:50:59 | INFO | train_inner | epoch 028:    419 / 1826 loss=1.841, trans_loss=4.651, nll_loss=1.851, w2v_ctc_loss=0.63, task_loss=5.434, task_loss_gen=8.722, contrastive_loss=0, total=4002.09, n_correct=2827.31, ppl=3.61, accuracy=70.646, wps=13050.8, ups=1.63, wpb=8004.2, bsz=299, num_updates=49700, lr=6.34361e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=61, gb_free=15.9, wall=36376
2023-09-05 20:52:01 | INFO | train_inner | epoch 028:    519 / 1826 loss=1.854, trans_loss=4.664, nll_loss=1.867, w2v_ctc_loss=0.641, task_loss=6.175, task_loss_gen=10.432, contrastive_loss=0, total=3959.05, n_correct=2783.83, ppl=3.65, accuracy=70.316, wps=12761.4, ups=1.61, wpb=7918.1, bsz=278.9, num_updates=49800, lr=6.33724e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=61, gb_free=16.1, wall=36438
2023-09-05 20:53:03 | INFO | train_inner | epoch 028:    619 / 1826 loss=1.843, trans_loss=4.65, nll_loss=1.849, w2v_ctc_loss=0.629, task_loss=5.422, task_loss_gen=9.564, contrastive_loss=0, total=3930.51, n_correct=2773.59, ppl=3.6, accuracy=70.566, wps=12643.6, ups=1.61, wpb=7861, bsz=285.6, num_updates=49900, lr=6.33089e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=61, gb_free=14.3, wall=36500
2023-09-05 20:54:05 | INFO | train_inner | epoch 028:    719 / 1826 loss=1.851, trans_loss=4.664, nll_loss=1.868, w2v_ctc_loss=0.634, task_loss=4.962, task_loss_gen=10.632, contrastive_loss=0, total=3937.24, n_correct=2770.71, ppl=3.65, accuracy=70.372, wps=12742.5, ups=1.62, wpb=7874.5, bsz=280.3, num_updates=50000, lr=6.32456e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=61, gb_free=16.6, wall=36562
2023-09-05 20:54:05 | INFO | fairseq_cli.train | Stopping training due to num_updates: 50000 >= max_update: 50000
2023-09-05 20:54:05 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.1718], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.1718], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 20:54:45 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.778 | trans_loss 4.975 | nll_loss 2.216 | w2v_ctc_loss 1.369 | task_loss 69.576 | task_loss_gen 28.471 | contrastive_loss 0 | total 3505.91 | n_correct 2441.55 | ppl 4.65 | accuracy 69.641 | uer 16.974 | wer 19.065 | raw_wer 19.065 | bleu 31.49 | wps 1173.2 | wpb 3505.9 | bsz 119.3 | num_updates 50000 | best_bleu 31.49
2023-09-05 20:54:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 50000 updates
2023-09-05 20:54:45 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_28_50000.pt
2023-09-05 20:54:47 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_28_50000.pt
2023-09-05 20:55:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_28_50000.pt (epoch 28 @ 50000 updates, score 31.49) (writing took 15.092550802975893 seconds)
2023-09-05 20:55:00 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-09-05 20:55:00 | INFO | train | epoch 028 | loss 1.845 | trans_loss 4.653 | nll_loss 1.853 | w2v_ctc_loss 0.632 | task_loss 5.65 | task_loss_gen 9.636 | contrastive_loss 0 | total 3959 | n_correct 2794.05 | ppl 3.61 | accuracy 70.574 | wps 11214.1 | ups 1.42 | wpb 7918 | bsz 286 | num_updates 50000 | lr 6.32456e-05 | gnorm 0.545 | clip 0 | loss_scale 64 | train_wall 439 | gb_free 16.6 | wall 36618
2023-09-05 20:55:00 | INFO | fairseq_cli.train | done training in 36565.6 seconds
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 1280 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
2023-09-05 22:51:10 | INFO | fairseq.distributed.utils | distributed init (rank 6): tcp://localhost:15318
2023-09-05 22:51:10 | INFO | fairseq.distributed.utils | distributed init (rank 7): tcp://localhost:15318
2023-09-05 22:51:10 | INFO | fairseq.distributed.utils | distributed init (rank 4): tcp://localhost:15318
2023-09-05 22:51:10 | INFO | fairseq.distributed.utils | distributed init (rank 0): tcp://localhost:15318
2023-09-05 22:51:10 | INFO | fairseq.distributed.utils | distributed init (rank 3): tcp://localhost:15318
2023-09-05 22:51:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 3
2023-09-05 22:51:10 | INFO | fairseq.distributed.utils | distributed init (rank 1): tcp://localhost:15318
2023-09-05 22:51:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 1
2023-09-05 22:51:10 | INFO | fairseq.distributed.utils | distributed init (rank 5): tcp://localhost:15318
2023-09-05 22:51:10 | INFO | fairseq.distributed.utils | distributed init (rank 2): tcp://localhost:15318
2023-09-05 22:51:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 5
2023-09-05 22:51:10 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 2
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 6
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 7
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 4
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:1 to store for rank: 0
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 22:51:11 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 0
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Rank 5: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 22:51:11 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 3
2023-09-05 22:51:11 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 5
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 22:51:11 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 2
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Rank 7: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 22:51:11 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 7
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Rank 4: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Rank 6: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 22:51:11 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 6
2023-09-05 22:51:11 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 4
2023-09-05 22:51:11 | INFO | torch.distributed.distributed_c10d | Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 8 nodes.
2023-09-05 22:51:11 | INFO | fairseq.distributed.utils | initialized host localhost.localdomain as rank 1
2023-09-05 22:51:15 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': True, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': './checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': 'sentencepiece', 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 8, 'distributed_num_procs': 8, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': 'tcp://localhost:15318', 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'no_c10d', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 8, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 2, 'skip_invalid_size_inputs_valid_test': True, 'max_tokens': 15000, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train_st,train_asr', 'valid_subset': 'dev_st', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 15000, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 100, 'max_update': 60000, 'stop_time_hours': 0.0, 'clip_norm': 10.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.0002], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': './checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 2000, 'keep_interval_updates': 5, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 2, 'keep_best_checkpoints': 10, 'no_save': False, 'no_epoch_checkpoints': True, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'bleu', 'maximize_best_checkpoint_metric': True, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 8}, 'generation': {'_name': None, 'beam': 5, 'beam_mt': 0, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'max_len_a_mt': 0.0, 'max_len_b_mt': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'lenpen_mt': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='s2t_joint', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'task': Namespace(_name='joint_triple_pretraining_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'criterion': Namespace(_name='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', activation_dropout=0.1, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adapter_dim=4096, adapter_dropout=0.0, adapter_layers=0, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, add_position_embed=True, add_position_embed_after_ctc=True, add_proj_norm=False, adversarial_training=True, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, apply_mask=True, arch='s2t_joint', at_adapte_win=False, at_level='sentence', at_low_pos=False, at_nomute=False, at_nopad=True, at_scale=3.5, attention_dropout=0.1, avg_shrink=False, azureml_logging=False, ban_cl_step=-1, batch_size=None, batch_size_valid=None, best_checkpoint_metric='bleu', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=10.0, cluster_embed_path='', cnn_module_kernel=31, combine_valid_subsets=None, config_yaml='config_st.yaml', continue_once=None, contrastive_alpha=0.0, contrastive_beta=1.0, contrastive_temperature=0.1, conv_channels=512, conv_kernel_sizes='5,5', cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy_with_w2v_ctc_shrink_joint_AT_merge', ctc_weight=0.3, curriculum=0, data='data_all_enes_lcrm', data_buffer_size=10, dataset_impl=None, ddp_backend='no_c10d', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline', decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0.0, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=True, decoder_output_dim=512, decrease_step=0, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=8, distributed_port=-1, distributed_rank=0, distributed_world_size=8, dropout=0.1, dropout_input=0.0, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, embed_path='', embedding_l2norm=False, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_normalize_before=True, eos=2, eval_bleu=True, eval_bleu_args='{}', eval_bleu_detok='space', eval_bleu_detok_args='{}', eval_bleu_remove_bpe='sentencepiece', eval_tokenized_bleu=True, fast_stat_sync=False, feature_grad_mult=0.0, final_dropout=0.1, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, freeze_finetune_updates=3000, gen_subset='test', get_similarity=False, gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=False, is_shrink='uniq', keep_best_checkpoints=10, keep_interval_updates=5, keep_interval_updates_pattern=-1, keep_last_epochs=2, keep_mt_task=True, label_smoothing=0.2, latent_temp=(1, 0.1, 0.999995), layerdrop=0.0, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lookback=False, lr=[0.0002], lr_scheduler='inverse_sqrt', macaron_style=False, mask_channel_length=6, mask_channel_other=0, mask_channel_prob=0.25, mask_channel_selection='static', mask_length=10, mask_other=0, mask_prob=0.5, mask_selection='static', max_epoch=100, max_position_ctc=0, max_source_positions=6000, max_target_positions=1024, max_tokens=15000, max_tokens_valid=15000, max_update=60000, max_valid_steps=None, maximize_best_checkpoint_metric=True, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_mt_st=True, min_loss_scale=0.0001, mix_tag=0.5, mixup_change_id=True, mixup_for_whole_model=False, mixup_rate=-0.1, model_parallel_size=1, mt_model_args=None, mt_model_filter_size=0, mt_model_path='/mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt', no_epoch_checkpoints=True, no_last_checkpoints=False, no_mask_channel_overlap=False, no_mask_overlap=False, no_progress_bar=True, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, normalize=False, not_fsdp_flatten_parameters=False, nprocs_per_node=8, num_shards=1, num_workers=2, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', position_unit_size=0, post_process='sentencepiece', profile=False, quant_noise_pq=0, quantization_config_path=None, rel_pos_type='legacy', relative_attn=False, report_accuracy=True, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=False, reset_logging=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', save_interval=1, save_interval_updates=2000, scoring='bleu', sead_layers=6, seed=1, sentence_avg=False, shard_id=0, share_ctc_embed=True, share_decoder_input_output_embed=True, share_two_encoders=True, skip_invalid_size_inputs_valid_test=True, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, task='joint_triple_pretraining_merge', tensorboard_logdir='./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5', text_conv_kernel=0, threshold_loss_scale=None, tokenizer=None, tpu=False, train_config='/mnt/zhangyh/fairseq-AT/egs/pretrain-all/conf/train_shrink_AT_zyh.yaml', train_st_without_ctc=False, train_subset='train_st,train_asr', transfer_proj=False, unk=3, update_epoch_batch_itr=False, update_freq=[1], update_ordered_indices_seed=False, use_bmuf=False, use_cnn_module=True, use_ctc_cluster=False, use_ctc_loss=True, use_ctc_shrink=True, use_double_ctc=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, use_token_contrastive=False, use_two_contrastive=False, use_w2v_ctc=True, user_dir=None, valid_subset='dev_st', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, w2v_args=None, w2v_path='/mnt/zhangyh/pretrain/hubert_base_ls960.pt', wandb_project=None, warmup_init_lr=1e-07, warmup_updates=5000, weight_decay=0.0, weight_steps=5000, word_align=False, write_checkpoints_asynchronously=False, zero_infinity=True, zero_sharding='none', zero_triu=False), 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9,0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0002]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 5000, 'warmup_init_lr': 1e-07, 'lr': [0.0002]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}}
2023-09-05 22:51:15 | INFO | fairseq.tasks.joint_triple_pretraining_merge | dictionary size (dict.wrd.txt): 10,000
2023-09-05 22:51:15 | INFO | fairseq.tasks.joint_triple_pretraining_merge | asr dictionary size (dict.wrd.txt): 10,000
2023-09-05 22:51:15 | INFO | fairseq.tasks.joint_triple_pretraining_merge | cluster dictionary size (kmeans100.convert.txt): 5,224
2023-09-05 22:51:15 | INFO | fairseq.tasks.joint_triple_pretraining_merge | Initial task weight: asr 1.0: mt 0.5
2023-09-05 22:51:15 | INFO | root | load pretrained embeddings: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline
2023-09-05 22:51:19 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/zhangyh/fairseq-AT/egs/pretrain-all
2023-09-05 22:51:19 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': 'data_all_enes_lcrm', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50.0, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}
2023-09-05 22:51:19 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50.0, 'extractor_mode': default, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'layer_type': transformer, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.1, 'encoder_layerdrop': 0.0, 'dropout_input': 0.0, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.0, 'mask_length': 10, 'mask_prob': 0.5, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 6, 'mask_channel_prob': 0.25, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False, 'checkpoint_activations': False, 'required_seq_len_multiple': 2, 'depthwise_conv_kernel_size': 31, 'attn_type': '', 'pos_enc_type': 'abs', 'fp16': True}
2023-09-05 22:51:21 | INFO | root | load pretrained hubert
2023-09-05 22:51:28 | INFO | root | load pretrained embedding as ctc proj: /mnt/zhangyh/fairseq-AT/egs/machine_translation/pretrain_embeddings_wmt_enes_baseline
2023-09-05 22:51:32 | INFO | root | load pretrained encoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt
2023-09-05 22:51:39 | INFO | root | load pretrained decoder: /mnt/zhangyh/fairseq-AT/egs/machine_translation/checkpoints/wmt-en2es/enes-baseline/last8.ensemble.pt
2023-09-05 22:51:39 | INFO | root | share the sematic adapter and textual encoder
2023-09-05 22:51:39 | INFO | fairseq_cli.train | S2TJoint(
  (acoustic_encoder): AcousticEncoder(
    (compress_ffn): Linear(in_features=768, out_features=512, bias=True)
    (proj): Linear(in_features=512, out_features=10000, bias=False)
    (w2v_model): HubertModel(
      (feature_extractor): ConvFeatureExtractionModel(
        (conv_layers): ModuleList(
          (0): Sequential(
            (0): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): Fp32GroupNorm(512, 512, eps=1e-05, affine=True)
            (3): GELU(approximate='none')
          )
          (1-4): 4 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
          (5-6): 2 x Sequential(
            (0): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)
            (1): Dropout(p=0.0, inplace=False)
            (2): GELU(approximate='none')
          )
        )
      )
      (post_extract_proj): Linear(in_features=512, out_features=768, bias=True)
      (dropout_input): Dropout(p=0.0, inplace=False)
      (dropout_features): Dropout(p=0.1, inplace=False)
      (encoder): TransformerEncoder(
        (pos_conv): Sequential(
          (0): Conv1d(768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16)
          (1): SamePad()
          (2): GELU(approximate='none')
        )
        (layers): ModuleList(
          (0-11): 12 x TransformerSentenceEncoderLayer(
            (self_attn): MultiheadAttention(
              (dropout_module): FairseqDropout()
              (k_proj): Linear(in_features=768, out_features=768, bias=True)
              (v_proj): Linear(in_features=768, out_features=768, bias=True)
              (q_proj): Linear(in_features=768, out_features=768, bias=True)
              (out_proj): Linear(in_features=768, out_features=768, bias=True)
            )
            (dropout1): Dropout(p=0.1, inplace=False)
            (dropout2): Dropout(p=0.1, inplace=False)
            (dropout3): Dropout(p=0.1, inplace=False)
            (self_attn_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (final_layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
          )
        )
        (layer_norm): FusedLayerNorm(torch.Size([768]), eps=1e-05, elementwise_affine=True)
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      (final_proj): None
    )
    (noad): NormalAdapter(
      (subsample): Conv1dSubsampler(
        (conv_layers): ModuleList(
          (0): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))
          (1): Conv1d(256, 1024, kernel_size=(5,), stride=(2,), padding=(2,))
        )
      )
      (layers): ModuleList()
    )
    (sead): SemanticAdapter(
      (embed_positions): SinusoidalPositionalEmbedding()
      (layers): ModuleList(
        (0-5): 6 x TransformerEncoderLayerBase(
          (self_attn): MultiheadAttention(
            (dropout_module): FairseqDropout()
            (k_proj): Linear(in_features=512, out_features=512, bias=True)
            (v_proj): Linear(in_features=512, out_features=512, bias=True)
            (q_proj): Linear(in_features=512, out_features=512, bias=True)
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
          (dropout_module): FairseqDropout()
          (activation_dropout_module): FairseqDropout()
          (fc1): Linear(in_features=512, out_features=2048, bias=True)
          (fc2): Linear(in_features=2048, out_features=512, bias=True)
          (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    )
    (final_dropout): Dropout(p=0.1, inplace=False)
    (shrink_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (textual_encoder): MTModelEncoder(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerEncoderLayerBase(
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (dropout_module): FairseqDropout()
        (activation_dropout_module): FairseqDropout()
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
  )
  (decoder): TransformerDecoderScriptable(
    (dropout_module): FairseqDropout()
    (embed_tokens): Embedding(10000, 512, padding_idx=1)
    (embed_positions): SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-5): 6 x TransformerDecoderLayerBase(
        (dropout_module): FairseqDropout()
        (self_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (activation_dropout_module): FairseqDropout()
        (self_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (encoder_attn): MultiheadAttention(
          (dropout_module): FairseqDropout()
          (k_proj): Linear(in_features=512, out_features=512, bias=True)
          (v_proj): Linear(in_features=512, out_features=512, bias=True)
          (q_proj): Linear(in_features=512, out_features=512, bias=True)
          (out_proj): Linear(in_features=512, out_features=512, bias=True)
        )
        (encoder_attn_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (final_layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
      )
    )
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (output_projection): Linear(in_features=512, out_features=10000, bias=False)
  )
  (task_net): TaskNetwork(
    (layer_norm): FusedLayerNorm(torch.Size([512]), eps=1e-05, elementwise_affine=True)
    (down_proj): Linear(in_features=512, out_features=256, bias=True)
    (up_proj): Linear(in_features=256, out_features=512, bias=True)
    (dropout_module): FairseqDropout()
    (task_proj): Linear(in_features=512, out_features=1, bias=False)
  )
)
2023-09-05 22:51:39 | INFO | fairseq_cli.train | task: JointTriplePretrainingMergeTask
2023-09-05 22:51:39 | INFO | fairseq_cli.train | model: S2TJoint
2023-09-05 22:51:39 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropywithW2vCtcShrinkJointATMerge
2023-09-05 22:51:39 | INFO | fairseq_cli.train | num. shared model params: 147,044,480 (num. trained: 147,044,480)
2023-09-05 22:51:39 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)
2023-09-05 22:51:39 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 22:51:39 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-05 22:51:39 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-05 22:51:39 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="dev_st", n_samples=1312, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 22:51:54 | INFO | torch.distributed.distributed_c10d | Added key: store_based_barrier_key:2 to store for rank: 0
2023-09-05 22:51:55 | INFO | torch.distributed.distributed_c10d | Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 8 nodes.
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- textual_encoder.embed_tokens.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.embed_tokens.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.weight <- decoder.output_projection.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.0.0.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.1.0.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.2.0.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.3.0.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.4.0.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.5.0.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- acoustic_encoder.w2v_model.feature_extractor.conv_layers.6.0.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- decoder.output_projection.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.proj.bias <- task_net.task_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.weight <- textual_encoder.layers.0.self_attn.k_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.k_proj.bias <- textual_encoder.layers.0.self_attn.k_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.weight <- textual_encoder.layers.0.self_attn.v_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.v_proj.bias <- textual_encoder.layers.0.self_attn.v_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.weight <- textual_encoder.layers.0.self_attn.q_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.q_proj.bias <- textual_encoder.layers.0.self_attn.q_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.weight <- textual_encoder.layers.0.self_attn.out_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn.out_proj.bias <- textual_encoder.layers.0.self_attn.out_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.weight <- textual_encoder.layers.0.self_attn_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.self_attn_layer_norm.bias <- textual_encoder.layers.0.self_attn_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.weight <- textual_encoder.layers.0.fc1.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc1.bias <- textual_encoder.layers.0.fc1.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.weight <- textual_encoder.layers.0.fc2.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.fc2.bias <- textual_encoder.layers.0.fc2.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.weight <- textual_encoder.layers.0.final_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.0.final_layer_norm.bias <- textual_encoder.layers.0.final_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.weight <- textual_encoder.layers.1.self_attn.k_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.k_proj.bias <- textual_encoder.layers.1.self_attn.k_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.weight <- textual_encoder.layers.1.self_attn.v_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.v_proj.bias <- textual_encoder.layers.1.self_attn.v_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.weight <- textual_encoder.layers.1.self_attn.q_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.q_proj.bias <- textual_encoder.layers.1.self_attn.q_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.weight <- textual_encoder.layers.1.self_attn.out_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn.out_proj.bias <- textual_encoder.layers.1.self_attn.out_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.weight <- textual_encoder.layers.1.self_attn_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.self_attn_layer_norm.bias <- textual_encoder.layers.1.self_attn_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.weight <- textual_encoder.layers.1.fc1.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc1.bias <- textual_encoder.layers.1.fc1.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.weight <- textual_encoder.layers.1.fc2.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.fc2.bias <- textual_encoder.layers.1.fc2.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.weight <- textual_encoder.layers.1.final_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.1.final_layer_norm.bias <- textual_encoder.layers.1.final_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.weight <- textual_encoder.layers.2.self_attn.k_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.k_proj.bias <- textual_encoder.layers.2.self_attn.k_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.weight <- textual_encoder.layers.2.self_attn.v_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.v_proj.bias <- textual_encoder.layers.2.self_attn.v_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.weight <- textual_encoder.layers.2.self_attn.q_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.q_proj.bias <- textual_encoder.layers.2.self_attn.q_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.weight <- textual_encoder.layers.2.self_attn.out_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn.out_proj.bias <- textual_encoder.layers.2.self_attn.out_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.weight <- textual_encoder.layers.2.self_attn_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.self_attn_layer_norm.bias <- textual_encoder.layers.2.self_attn_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.weight <- textual_encoder.layers.2.fc1.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc1.bias <- textual_encoder.layers.2.fc1.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.weight <- textual_encoder.layers.2.fc2.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.fc2.bias <- textual_encoder.layers.2.fc2.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.weight <- textual_encoder.layers.2.final_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.2.final_layer_norm.bias <- textual_encoder.layers.2.final_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.weight <- textual_encoder.layers.3.self_attn.k_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.k_proj.bias <- textual_encoder.layers.3.self_attn.k_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.weight <- textual_encoder.layers.3.self_attn.v_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.v_proj.bias <- textual_encoder.layers.3.self_attn.v_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.weight <- textual_encoder.layers.3.self_attn.q_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.q_proj.bias <- textual_encoder.layers.3.self_attn.q_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.weight <- textual_encoder.layers.3.self_attn.out_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn.out_proj.bias <- textual_encoder.layers.3.self_attn.out_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.weight <- textual_encoder.layers.3.self_attn_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.self_attn_layer_norm.bias <- textual_encoder.layers.3.self_attn_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.weight <- textual_encoder.layers.3.fc1.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc1.bias <- textual_encoder.layers.3.fc1.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.weight <- textual_encoder.layers.3.fc2.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.fc2.bias <- textual_encoder.layers.3.fc2.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.weight <- textual_encoder.layers.3.final_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.3.final_layer_norm.bias <- textual_encoder.layers.3.final_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.weight <- textual_encoder.layers.4.self_attn.k_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.k_proj.bias <- textual_encoder.layers.4.self_attn.k_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.weight <- textual_encoder.layers.4.self_attn.v_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.v_proj.bias <- textual_encoder.layers.4.self_attn.v_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.weight <- textual_encoder.layers.4.self_attn.q_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.q_proj.bias <- textual_encoder.layers.4.self_attn.q_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.weight <- textual_encoder.layers.4.self_attn.out_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn.out_proj.bias <- textual_encoder.layers.4.self_attn.out_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.weight <- textual_encoder.layers.4.self_attn_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.self_attn_layer_norm.bias <- textual_encoder.layers.4.self_attn_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.weight <- textual_encoder.layers.4.fc1.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc1.bias <- textual_encoder.layers.4.fc1.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.weight <- textual_encoder.layers.4.fc2.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.fc2.bias <- textual_encoder.layers.4.fc2.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.weight <- textual_encoder.layers.4.final_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.4.final_layer_norm.bias <- textual_encoder.layers.4.final_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.weight <- textual_encoder.layers.5.self_attn.k_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.k_proj.bias <- textual_encoder.layers.5.self_attn.k_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.weight <- textual_encoder.layers.5.self_attn.v_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.v_proj.bias <- textual_encoder.layers.5.self_attn.v_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.weight <- textual_encoder.layers.5.self_attn.q_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.q_proj.bias <- textual_encoder.layers.5.self_attn.q_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.weight <- textual_encoder.layers.5.self_attn.out_proj.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn.out_proj.bias <- textual_encoder.layers.5.self_attn.out_proj.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.weight <- textual_encoder.layers.5.self_attn_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.self_attn_layer_norm.bias <- textual_encoder.layers.5.self_attn_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.weight <- textual_encoder.layers.5.fc1.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc1.bias <- textual_encoder.layers.5.fc1.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.weight <- textual_encoder.layers.5.fc2.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.fc2.bias <- textual_encoder.layers.5.fc2.bias
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.weight <- textual_encoder.layers.5.final_layer_norm.weight
2023-09-05 22:51:55 | INFO | fairseq.trainer | detected shared parameter: acoustic_encoder.sead.layers.5.final_layer_norm.bias <- textual_encoder.layers.5.final_layer_norm.bias
2023-09-05 22:51:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 22:51:55 | INFO | fairseq.utils | rank   0: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 22:51:55 | INFO | fairseq.utils | rank   1: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 22:51:55 | INFO | fairseq.utils | rank   2: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 22:51:55 | INFO | fairseq.utils | rank   3: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 22:51:55 | INFO | fairseq.utils | rank   4: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 22:51:55 | INFO | fairseq.utils | rank   5: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 22:51:55 | INFO | fairseq.utils | rank   6: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 22:51:55 | INFO | fairseq.utils | rank   7: capabilities =  8.6  ; total memory = 23.700 GB ; name = NVIDIA GeForce RTX 3090                 
2023-09-05 22:51:55 | INFO | fairseq.utils | ***********************CUDA enviroments for all 8 workers***********************
2023-09-05 22:51:55 | INFO | fairseq_cli.train | training on 8 devices (GPUs/TPUs)
2023-09-05 22:51:55 | INFO | fairseq_cli.train | max tokens per device = 15000 and max sentences per device = None
2023-09-05 22:51:55 | INFO | fairseq.trainer | Preparing to load checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt
2023-09-05 22:51:57 | INFO | fairseq.trainer | load the task parameters
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
2023-09-05 22:52:15 | INFO | fairseq.optim.adam | using FusedAdam
2023-09-05 22:52:16 | INFO | fairseq.trainer | Loaded checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_last.pt (epoch 28 @ 50000 updates)
2023-09-05 22:52:17 | INFO | fairseq.trainer | loading train data for epoch 28
2023-09-05 22:52:17 | INFO | fairseq.tasks.speech_to_text | pre-tokenizer: {'tokenizer': None}
2023-09-05 22:52:17 | INFO | fairseq.tasks.speech_to_text | tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-05 22:52:17 | INFO | fairseq.tasks.joint_triple_pretraining_merge | src tokenizer: {'bpe': 'sentencepiece', 'sentencepiece_model': '/mnt/zhangyh/fairseq-AT/egs/pretrain-all/data_all_enes_lcrm/sentencepiece.bpe.model'}
2023-09-05 22:52:19 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_asr", n_samples=260049, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 22:52:21 | INFO | fairseq.data.audio.triple_dataset | TripleDataset(split="train_st", n_samples=260049, prepend_tgt_lang_tag=False, shuffle=False, transforms=None)
2023-09-05 22:53:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 22:53:09 | INFO | fairseq.trainer | begin training epoch 28
2023-09-05 22:53:09 | INFO | fairseq_cli.train | Start iterating over samples
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
--Backword ST Loss tensor(1246.3834, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(695.3604, device='cuda:0', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
2023-09-05 22:54:25 | INFO | train_inner | epoch 028:    819 / 1826 loss=1.851, trans_loss=4.66, nll_loss=1.862, w2v_ctc_loss=0.636, task_loss=4.76, task_loss_gen=11.307, contrastive_loss=0, total=3885.71, n_correct=2737.17, ppl=3.64, accuracy=70.442, wps=4598.9, ups=0.59, wpb=7771.4, bsz=271.7, num_updates=50100, lr=6.31824e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=69, gb_free=15.1, wall=0
2023-09-05 22:55:28 | INFO | train_inner | epoch 028:    919 / 1826 loss=1.849, trans_loss=4.652, nll_loss=1.852, w2v_ctc_loss=0.639, task_loss=3.871, task_loss_gen=10.278, contrastive_loss=0, total=3937.09, n_correct=2776.63, ppl=3.61, accuracy=70.525, wps=12504.1, ups=1.59, wpb=7874.2, bsz=276.1, num_updates=50200, lr=6.31194e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=62, gb_free=17.1, wall=0
2023-09-05 22:56:32 | INFO | train_inner | epoch 028:   1019 / 1826 loss=1.849, trans_loss=4.656, nll_loss=1.858, w2v_ctc_loss=0.642, task_loss=3.718, task_loss_gen=9.736, contrastive_loss=0, total=4013.87, n_correct=2827.94, ppl=3.62, accuracy=70.454, wps=12644.9, ups=1.58, wpb=8027.7, bsz=290.7, num_updates=50300, lr=6.30567e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=63, gb_free=12.5, wall=0
2023-09-05 22:57:35 | INFO | train_inner | epoch 028:   1119 / 1826 loss=1.841, trans_loss=4.649, nll_loss=1.848, w2v_ctc_loss=0.626, task_loss=3.756, task_loss_gen=9.355, contrastive_loss=0, total=3914.21, n_correct=2766.62, ppl=3.6, accuracy=70.681, wps=12452.1, ups=1.59, wpb=7828.4, bsz=283, num_updates=50400, lr=6.29941e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=62, gb_free=16.9, wall=0
2023-09-05 22:58:37 | INFO | train_inner | epoch 028:   1219 / 1826 loss=1.845, trans_loss=4.65, nll_loss=1.849, w2v_ctc_loss=0.639, task_loss=3.424, task_loss_gen=9.817, contrastive_loss=0, total=3941.66, n_correct=2779.86, ppl=3.6, accuracy=70.525, wps=12731.7, ups=1.62, wpb=7883.3, bsz=285.1, num_updates=50500, lr=6.29317e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=61, gb_free=16.7, wall=0
2023-09-05 22:59:40 | INFO | train_inner | epoch 028:   1319 / 1826 loss=1.85, trans_loss=4.663, nll_loss=1.866, w2v_ctc_loss=0.633, task_loss=3.722, task_loss_gen=9.912, contrastive_loss=0, total=4018.66, n_correct=2825.31, ppl=3.65, accuracy=70.305, wps=12770.5, ups=1.59, wpb=8037.3, bsz=289.3, num_updates=50600, lr=6.28695e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=62, gb_free=14, wall=0
2023-09-05 23:00:42 | INFO | train_inner | epoch 028:   1419 / 1826 loss=1.844, trans_loss=4.664, nll_loss=1.868, w2v_ctc_loss=0.628, task_loss=3.578, task_loss_gen=9.021, contrastive_loss=0, total=4024.28, n_correct=2835.64, ppl=3.65, accuracy=70.463, wps=12854.8, ups=1.6, wpb=8048.6, bsz=303.8, num_updates=50700, lr=6.28074e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=62, gb_free=16.7, wall=0
2023-09-05 23:01:45 | INFO | train_inner | epoch 028:   1519 / 1826 loss=1.842, trans_loss=4.651, nll_loss=1.851, w2v_ctc_loss=0.627, task_loss=3.853, task_loss_gen=10.049, contrastive_loss=0, total=3969.79, n_correct=2800.8, ppl=3.61, accuracy=70.553, wps=12675.6, ups=1.6, wpb=7939.6, bsz=285.8, num_updates=50800, lr=6.27456e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=62, gb_free=15.6, wall=0
2023-09-05 23:02:48 | INFO | train_inner | epoch 028:   1619 / 1826 loss=1.852, trans_loss=4.654, nll_loss=1.853, w2v_ctc_loss=0.64, task_loss=4.372, task_loss_gen=11.074, contrastive_loss=0, total=3907.56, n_correct=2752.81, ppl=3.61, accuracy=70.448, wps=12365.4, ups=1.58, wpb=7815.1, bsz=268.6, num_updates=50900, lr=6.26839e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=63, gb_free=13, wall=0
2023-09-05 23:03:51 | INFO | train_inner | epoch 028:   1719 / 1826 loss=1.855, trans_loss=4.673, nll_loss=1.879, w2v_ctc_loss=0.636, task_loss=4.542, task_loss_gen=10.251, contrastive_loss=0, total=3932.69, n_correct=2756.89, ppl=3.68, accuracy=70.102, wps=12612.2, ups=1.6, wpb=7865.4, bsz=275.2, num_updates=51000, lr=6.26224e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=62, gb_free=13.1, wall=0
2023-09-05 23:04:53 | INFO | train_inner | epoch 028:   1819 / 1826 loss=1.836, trans_loss=4.644, nll_loss=1.842, w2v_ctc_loss=0.624, task_loss=3.968, task_loss_gen=9.813, contrastive_loss=0, total=3962.76, n_correct=2804.94, ppl=3.58, accuracy=70.782, wps=12614.5, ups=1.59, wpb=7925.5, bsz=292.9, num_updates=51100, lr=6.25611e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=62, gb_free=16.9, wall=0
2023-09-05 23:04:58 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1493.8555, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(869.3645, device='cuda:3', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
--Backword ST Loss tensor(1559.2399, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(862.1732, device='cuda:7', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
--Backword ST Loss tensor(937.6494, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(502.3387, device='cuda:6', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
--Backword ST Loss tensor(1059.5314, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(641.6208, device='cuda:5', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
--Backword ST Loss tensor(1266.5747, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(750.9343, device='cuda:1', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
--Backword ST Loss tensor(686.4268, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(409.0401, device='cuda:2', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
--Backword ST Loss tensor(1068.4685, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(588.6600, device='cuda:4', grad_fn=<MulBackward0>)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
tensor([0.3359], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.3359], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 23:05:36 | INFO | dev_st | epoch 028 | valid on 'dev_st' subset | loss 3.794 | trans_loss 4.979 | nll_loss 2.218 | w2v_ctc_loss 1.411 | task_loss 55.86 | task_loss_gen 32.413 | contrastive_loss 0 | total 3505.91 | n_correct 2433.55 | ppl 4.65 | accuracy 69.413 | uer 16.998 | wer 18.978 | raw_wer 18.978 | bleu 31.51 | wps 1197.9 | wpb 3505.9 | bsz 119.3 | num_updates 51107 | best_bleu 31.51
2023-09-05 23:05:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 51107 updates
2023-09-05 23:05:36 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 23:05:44 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-05 23:05:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 28 @ 51107 updates, score 31.51) (writing took 13.185509682982229 seconds)
2023-09-05 23:05:50 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)
2023-09-05 23:05:50 | INFO | train | epoch 028 | loss 1.846 | trans_loss 4.655 | nll_loss 1.855 | w2v_ctc_loss 0.633 | task_loss 4.616 | task_loss_gen 9.866 | contrastive_loss 0 | total 3956.37 | n_correct 2790.07 | ppl 3.62 | accuracy 70.521 | wps 11068.1 | ups 1.4 | wpb 7912.7 | bsz 284.8 | num_updates 51107 | lr 6.25568e-05 | gnorm 0.547 | clip 0 | loss_scale 64 | train_wall 1133 | gb_free 15.1 | wall 0
2023-09-05 23:05:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 23:05:50 | INFO | fairseq.trainer | begin training epoch 29
2023-09-05 23:05:50 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 23:06:56 | INFO | train_inner | epoch 029:     93 / 1826 loss=1.835, trans_loss=4.649, nll_loss=1.848, w2v_ctc_loss=0.613, task_loss=3.716, task_loss_gen=9.546, contrastive_loss=0, total=3961.04, n_correct=2800.87, ppl=3.6, accuracy=70.71, wps=6478.9, ups=0.82, wpb=7922.1, bsz=296.6, num_updates=51200, lr=6.25e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=62, gb_free=17.5, wall=0
2023-09-05 23:07:59 | INFO | train_inner | epoch 029:    193 / 1826 loss=1.834, trans_loss=4.64, nll_loss=1.837, w2v_ctc_loss=0.615, task_loss=4.525, task_loss_gen=10.077, contrastive_loss=0, total=3949.04, n_correct=2798.64, ppl=3.57, accuracy=70.869, wps=12545.1, ups=1.59, wpb=7898.1, bsz=286, num_updates=51300, lr=6.24391e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=62, gb_free=15.1, wall=0
2023-09-05 23:09:02 | INFO | train_inner | epoch 029:    293 / 1826 loss=1.84, trans_loss=4.645, nll_loss=1.843, w2v_ctc_loss=0.626, task_loss=3.852, task_loss_gen=10.179, contrastive_loss=0, total=3929.73, n_correct=2781.52, ppl=3.59, accuracy=70.781, wps=12479.8, ups=1.59, wpb=7859.5, bsz=276.4, num_updates=51400, lr=6.23783e-05, gnorm=0.549, clip=0, loss_scale=64, train_wall=62, gb_free=16.3, wall=0
2023-09-05 23:10:04 | INFO | train_inner | epoch 029:    393 / 1826 loss=1.839, trans_loss=4.64, nll_loss=1.837, w2v_ctc_loss=0.63, task_loss=3.394, task_loss_gen=9.872, contrastive_loss=0, total=3943.95, n_correct=2791.43, ppl=3.57, accuracy=70.778, wps=12751.5, ups=1.62, wpb=7887.9, bsz=282, num_updates=51500, lr=6.23177e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=61, gb_free=12.1, wall=0
2023-09-05 23:11:06 | INFO | train_inner | epoch 029:    493 / 1826 loss=1.834, trans_loss=4.638, nll_loss=1.833, w2v_ctc_loss=0.616, task_loss=3.374, task_loss_gen=10.384, contrastive_loss=0, total=3943.85, n_correct=2794.68, ppl=3.56, accuracy=70.862, wps=12671.1, ups=1.61, wpb=7887.7, bsz=284, num_updates=51600, lr=6.22573e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=62, gb_free=14.4, wall=0
2023-09-05 23:12:08 | INFO | train_inner | epoch 029:    593 / 1826 loss=1.844, trans_loss=4.637, nll_loss=1.832, w2v_ctc_loss=0.639, task_loss=3.714, task_loss_gen=10.526, contrastive_loss=0, total=3949.13, n_correct=2795.6, ppl=3.56, accuracy=70.79, wps=12705.8, ups=1.61, wpb=7898.3, bsz=275.6, num_updates=51700, lr=6.2197e-05, gnorm=0.552, clip=0, loss_scale=64, train_wall=61, gb_free=12.8, wall=0
2023-09-05 23:13:10 | INFO | train_inner | epoch 029:    693 / 1826 loss=1.835, trans_loss=4.644, nll_loss=1.842, w2v_ctc_loss=0.621, task_loss=3.823, task_loss_gen=9.606, contrastive_loss=0, total=3988.19, n_correct=2822.44, ppl=3.58, accuracy=70.77, wps=12795, ups=1.6, wpb=7976.4, bsz=295.6, num_updates=51800, lr=6.2137e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=62, gb_free=16.3, wall=0
2023-09-05 23:14:14 | INFO | train_inner | epoch 029:    793 / 1826 loss=1.85, trans_loss=4.659, nll_loss=1.86, w2v_ctc_loss=0.636, task_loss=4.281, task_loss_gen=10.483, contrastive_loss=0, total=3922.88, n_correct=2758.63, ppl=3.63, accuracy=70.322, wps=12279.4, ups=1.57, wpb=7845.8, bsz=277, num_updates=51900, lr=6.20771e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=63, gb_free=17, wall=0
2023-09-05 23:15:16 | INFO | train_inner | epoch 029:    893 / 1826 loss=1.84, trans_loss=4.645, nll_loss=1.844, w2v_ctc_loss=0.629, task_loss=3.687, task_loss_gen=9.812, contrastive_loss=0, total=3932.2, n_correct=2777.12, ppl=3.59, accuracy=70.625, wps=12680.8, ups=1.61, wpb=7864.4, bsz=287, num_updates=52000, lr=6.20174e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=61, gb_free=15.4, wall=0
2023-09-05 23:15:16 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-0.1285], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.1285], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 23:15:55 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.787 | trans_loss 4.98 | nll_loss 2.22 | w2v_ctc_loss 1.387 | task_loss 87.829 | task_loss_gen 45.31 | contrastive_loss 0 | total 3505.91 | n_correct 2434 | ppl 4.66 | accuracy 69.426 | uer 16.953 | wer 18.783 | raw_wer 18.783 | bleu 31.11 | wps 1189.9 | wpb 3505.9 | bsz 119.3 | num_updates 52000 | best_bleu 31.51
2023-09-05 23:15:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 52000 updates
2023-09-05 23:15:55 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_29_52000.pt
2023-09-05 23:15:57 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_29_52000.pt
2023-09-05 23:16:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_29_52000.pt (epoch 29 @ 52000 updates, score 31.11) (writing took 6.23412182694301 seconds)
--Backword ST Loss tensor(920.5922, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(533.5892, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 23:17:04 | INFO | train_inner | epoch 029:    993 / 1826 loss=1.843, trans_loss=4.652, nll_loss=1.852, w2v_ctc_loss=0.63, task_loss=4.103, task_loss_gen=10.631, contrastive_loss=0, total=3946.1, n_correct=2783.63, ppl=3.61, accuracy=70.541, wps=7333.5, ups=0.93, wpb=7892.2, bsz=282.8, num_updates=52100, lr=6.19578e-05, gnorm=0.541, clip=0, loss_scale=128, train_wall=62, gb_free=15.4, wall=0
2023-09-05 23:18:06 | INFO | train_inner | epoch 029:   1093 / 1826 loss=1.837, trans_loss=4.652, nll_loss=1.852, w2v_ctc_loss=0.621, task_loss=2.958, task_loss_gen=10.05, contrastive_loss=0, total=4005.9, n_correct=2832.45, ppl=3.61, accuracy=70.707, wps=12808.8, ups=1.6, wpb=8011.8, bsz=299.8, num_updates=52200, lr=6.18984e-05, gnorm=0.536, clip=0, loss_scale=128, train_wall=62, gb_free=16.3, wall=0
2023-09-05 23:19:09 | INFO | train_inner | epoch 029:   1193 / 1826 loss=1.843, trans_loss=4.643, nll_loss=1.84, w2v_ctc_loss=0.632, task_loss=2.99, task_loss_gen=12.032, contrastive_loss=0, total=3961.96, n_correct=2799.65, ppl=3.58, accuracy=70.663, wps=12660.9, ups=1.6, wpb=7923.9, bsz=276.3, num_updates=52300, lr=6.18392e-05, gnorm=0.543, clip=0, loss_scale=128, train_wall=62, gb_free=15.8, wall=0
2023-09-05 23:19:11 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-09-05 23:20:13 | INFO | train_inner | epoch 029:   1294 / 1826 loss=1.843, trans_loss=4.652, nll_loss=1.853, w2v_ctc_loss=0.632, task_loss=3.247, task_loss_gen=10.199, contrastive_loss=0, total=3965.11, n_correct=2799.91, ppl=3.61, accuracy=70.614, wps=12429, ups=1.57, wpb=7930.2, bsz=288.9, num_updates=52400, lr=6.17802e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=63, gb_free=11.5, wall=0
2023-09-05 23:21:14 | INFO | train_inner | epoch 029:   1394 / 1826 loss=1.843, trans_loss=4.645, nll_loss=1.843, w2v_ctc_loss=0.631, task_loss=3.961, task_loss_gen=11.406, contrastive_loss=0, total=3864.59, n_correct=2734.68, ppl=3.59, accuracy=70.762, wps=12569.9, ups=1.63, wpb=7729.2, bsz=264, num_updates=52500, lr=6.17213e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=61, gb_free=16.4, wall=0
2023-09-05 23:22:16 | INFO | train_inner | epoch 029:   1494 / 1826 loss=1.849, trans_loss=4.658, nll_loss=1.86, w2v_ctc_loss=0.635, task_loss=3.669, task_loss_gen=10.35, contrastive_loss=0, total=3973.99, n_correct=2799.62, ppl=3.63, accuracy=70.449, wps=12795.9, ups=1.61, wpb=7948, bsz=279.3, num_updates=52600, lr=6.16626e-05, gnorm=0.542, clip=0, loss_scale=64, train_wall=61, gb_free=16.8, wall=0
2023-09-05 23:23:18 | INFO | train_inner | epoch 029:   1594 / 1826 loss=1.84, trans_loss=4.646, nll_loss=1.845, w2v_ctc_loss=0.626, task_loss=3.373, task_loss_gen=9.998, contrastive_loss=0, total=4009.57, n_correct=2834.34, ppl=3.59, accuracy=70.689, wps=12932.8, ups=1.61, wpb=8019.1, bsz=288.2, num_updates=52700, lr=6.16041e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=61, gb_free=12.3, wall=0
2023-09-05 23:24:21 | INFO | train_inner | epoch 029:   1694 / 1826 loss=1.846, trans_loss=4.652, nll_loss=1.852, w2v_ctc_loss=0.633, task_loss=3.68, task_loss_gen=10.162, contrastive_loss=0, total=3987.04, n_correct=2812.92, ppl=3.61, accuracy=70.552, wps=12793.7, ups=1.6, wpb=7974.1, bsz=285.2, num_updates=52800, lr=6.15457e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=62, gb_free=15.8, wall=0
2023-09-05 23:25:22 | INFO | train_inner | epoch 029:   1794 / 1826 loss=1.839, trans_loss=4.656, nll_loss=1.858, w2v_ctc_loss=0.624, task_loss=3.546, task_loss_gen=9.29, contrastive_loss=0, total=3971.65, n_correct=2804.94, ppl=3.62, accuracy=70.624, wps=12862.5, ups=1.62, wpb=7943.3, bsz=297.1, num_updates=52900, lr=6.14875e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=61, gb_free=16.7, wall=0
2023-09-05 23:25:42 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1335.3074, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(781.4482, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1029.0211, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(593.8595, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1618.2673, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(887.3991, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1273.9331, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(717.8433, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1498.9626, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(870.4244, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1405.0724, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(765.8078, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1725.9415, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(1036.0996, device='cuda:3', grad_fn=<MulBackward0>)
tensor([0.2534], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.2534], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 23:26:22 | INFO | dev_st | epoch 029 | valid on 'dev_st' subset | loss 3.776 | trans_loss 4.979 | nll_loss 2.22 | w2v_ctc_loss 1.351 | task_loss 63.01 | task_loss_gen 34.813 | contrastive_loss 0 | total 3505.91 | n_correct 2440.82 | ppl 4.66 | accuracy 69.62 | uer 16.896 | wer 18.9 | raw_wer 18.9 | bleu 31.22 | wps 1166.1 | wpb 3505.9 | bsz 119.3 | num_updates 52932 | best_bleu 31.51
2023-09-05 23:26:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 52932 updates
2023-09-05 23:26:22 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.2203.pt
2023-09-05 23:26:24 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.2203.pt
2023-09-05 23:26:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.2203.pt (epoch 29 @ 52932 updates, score 31.22) (writing took 6.475310879060999 seconds)
2023-09-05 23:26:28 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)
2023-09-05 23:26:28 | INFO | train | epoch 029 | loss 1.841 | trans_loss 4.648 | nll_loss 1.846 | w2v_ctc_loss 0.627 | task_loss 3.663 | task_loss_gen 10.243 | contrastive_loss 0 | total 3956.19 | n_correct 2795.72 | ppl 3.6 | accuracy 70.667 | wps 11660.2 | ups 1.47 | wpb 7912.4 | bsz 284.8 | num_updates 52932 | lr 6.1469e-05 | gnorm 0.546 | clip 0 | loss_scale 64 | train_wall 1127 | gb_free 16.6 | wall 0
2023-09-05 23:26:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 23:26:29 | INFO | fairseq.trainer | begin training epoch 30
2023-09-05 23:26:29 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 23:27:19 | INFO | train_inner | epoch 030:     68 / 1826 loss=1.84, trans_loss=4.646, nll_loss=1.844, w2v_ctc_loss=0.623, task_loss=3.924, task_loss_gen=10.89, contrastive_loss=0, total=3922.21, n_correct=2772.04, ppl=3.59, accuracy=70.675, wps=6738.1, ups=0.86, wpb=7844.4, bsz=276.4, num_updates=53000, lr=6.14295e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=62, gb_free=16.5, wall=0
2023-09-05 23:28:20 | INFO | train_inner | epoch 030:    168 / 1826 loss=1.832, trans_loss=4.637, nll_loss=1.832, w2v_ctc_loss=0.617, task_loss=3.196, task_loss_gen=9.721, contrastive_loss=0, total=3958.99, n_correct=2807.56, ppl=3.56, accuracy=70.916, wps=12897.7, ups=1.63, wpb=7918, bsz=293.9, num_updates=53100, lr=6.13716e-05, gnorm=0.539, clip=0, loss_scale=64, train_wall=61, gb_free=16.1, wall=0
2023-09-05 23:29:22 | INFO | train_inner | epoch 030:    268 / 1826 loss=1.831, trans_loss=4.638, nll_loss=1.834, w2v_ctc_loss=0.617, task_loss=3.174, task_loss_gen=9.613, contrastive_loss=0, total=4008.53, n_correct=2841.02, ppl=3.57, accuracy=70.874, wps=12928.5, ups=1.61, wpb=8017.1, bsz=295.9, num_updates=53200, lr=6.13139e-05, gnorm=0.537, clip=0, loss_scale=64, train_wall=61, gb_free=15, wall=0
2023-09-05 23:30:24 | INFO | train_inner | epoch 030:    368 / 1826 loss=1.836, trans_loss=4.632, nll_loss=1.826, w2v_ctc_loss=0.63, task_loss=3.015, task_loss_gen=10.409, contrastive_loss=0, total=3980.06, n_correct=2824.57, ppl=3.55, accuracy=70.968, wps=12927.9, ups=1.62, wpb=7960.1, bsz=288.7, num_updates=53300, lr=6.12564e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=61, gb_free=12.4, wall=0
2023-09-05 23:31:26 | INFO | train_inner | epoch 030:    468 / 1826 loss=1.836, trans_loss=4.646, nll_loss=1.844, w2v_ctc_loss=0.62, task_loss=3.318, task_loss_gen=10.577, contrastive_loss=0, total=3968.16, n_correct=2807.7, ppl=3.59, accuracy=70.756, wps=12831.5, ups=1.62, wpb=7936.3, bsz=287.5, num_updates=53400, lr=6.1199e-05, gnorm=0.541, clip=0, loss_scale=64, train_wall=61, gb_free=12.2, wall=0
2023-09-05 23:32:29 | INFO | train_inner | epoch 030:    568 / 1826 loss=1.843, trans_loss=4.643, nll_loss=1.839, w2v_ctc_loss=0.633, task_loss=3.62, task_loss_gen=10.889, contrastive_loss=0, total=3932.69, n_correct=2779.67, ppl=3.58, accuracy=70.681, wps=12467.6, ups=1.59, wpb=7865.4, bsz=271.5, num_updates=53500, lr=6.11418e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=62, gb_free=16.6, wall=0
2023-09-05 23:33:31 | INFO | train_inner | epoch 030:    668 / 1826 loss=1.837, trans_loss=4.635, nll_loss=1.83, w2v_ctc_loss=0.626, task_loss=4.498, task_loss_gen=11.173, contrastive_loss=0, total=3870.24, n_correct=2744.63, ppl=3.55, accuracy=70.916, wps=12434, ups=1.61, wpb=7740.5, bsz=267.8, num_updates=53600, lr=6.10847e-05, gnorm=0.563, clip=0, loss_scale=64, train_wall=62, gb_free=14.4, wall=0
2023-09-05 23:34:33 | INFO | train_inner | epoch 030:    768 / 1826 loss=1.831, trans_loss=4.629, nll_loss=1.822, w2v_ctc_loss=0.619, task_loss=4.34, task_loss_gen=10.234, contrastive_loss=0, total=3965.15, n_correct=2816.59, ppl=3.53, accuracy=71.034, wps=12808.4, ups=1.62, wpb=7930.3, bsz=284.7, num_updates=53700, lr=6.10278e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=61, gb_free=17.2, wall=0
2023-09-05 23:35:35 | INFO | train_inner | epoch 030:    868 / 1826 loss=1.83, trans_loss=4.64, nll_loss=1.837, w2v_ctc_loss=0.611, task_loss=4.316, task_loss_gen=9.95, contrastive_loss=0, total=4001.74, n_correct=2834.18, ppl=3.57, accuracy=70.824, wps=12891.2, ups=1.61, wpb=8003.5, bsz=296.6, num_updates=53800, lr=6.09711e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=61, gb_free=16, wall=0
2023-09-05 23:36:37 | INFO | train_inner | epoch 030:    968 / 1826 loss=1.84, trans_loss=4.642, nll_loss=1.838, w2v_ctc_loss=0.629, task_loss=3.918, task_loss_gen=10.846, contrastive_loss=0, total=3967.05, n_correct=2809.87, ppl=3.58, accuracy=70.83, wps=12896.2, ups=1.63, wpb=7934.1, bsz=278.7, num_updates=53900, lr=6.09145e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=61, gb_free=15.5, wall=0
2023-09-05 23:37:39 | INFO | train_inner | epoch 030:   1068 / 1826 loss=1.838, trans_loss=4.649, nll_loss=1.849, w2v_ctc_loss=0.626, task_loss=3.247, task_loss_gen=8.956, contrastive_loss=0, total=4031.6, n_correct=2848.21, ppl=3.6, accuracy=70.647, wps=12979.6, ups=1.61, wpb=8063.2, bsz=317.4, num_updates=54000, lr=6.08581e-05, gnorm=0.538, clip=0, loss_scale=64, train_wall=61, gb_free=17.5, wall=0
2023-09-05 23:37:39 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.9136], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.9136], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 23:38:18 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.767 | trans_loss 4.972 | nll_loss 2.213 | w2v_ctc_loss 1.337 | task_loss 57.029 | task_loss_gen 32.883 | contrastive_loss 0 | total 3505.91 | n_correct 2439.09 | ppl 4.64 | accuracy 69.571 | uer 16.634 | wer 18.633 | raw_wer 18.633 | bleu 31.52 | wps 1172.9 | wpb 3505.9 | bsz 119.3 | num_updates 54000 | best_bleu 31.52
2023-09-05 23:38:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 54000 updates
2023-09-05 23:38:19 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_30_54000.pt
2023-09-05 23:38:21 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_30_54000.pt
2023-09-05 23:38:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_30_54000.pt (epoch 30 @ 54000 updates, score 31.52) (writing took 11.87896191701293 seconds)
--Backword ST Loss tensor(990.4844, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(572.4625, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-05 23:39:32 | INFO | train_inner | epoch 030:   1168 / 1826 loss=1.841, trans_loss=4.642, nll_loss=1.838, w2v_ctc_loss=0.628, task_loss=3.863, task_loss_gen=10.99, contrastive_loss=0, total=3903.6, n_correct=2764.24, ppl=3.57, accuracy=70.813, wps=6870.4, ups=0.88, wpb=7807.2, bsz=270.8, num_updates=54100, lr=6.08018e-05, gnorm=0.559, clip=0, loss_scale=64, train_wall=61, gb_free=12.4, wall=0
2023-09-05 23:40:35 | INFO | train_inner | epoch 030:   1268 / 1826 loss=1.843, trans_loss=4.648, nll_loss=1.846, w2v_ctc_loss=0.631, task_loss=4.323, task_loss_gen=10.173, contrastive_loss=0, total=3932.64, n_correct=2778.96, ppl=3.59, accuracy=70.664, wps=12563.1, ups=1.6, wpb=7865.3, bsz=276.3, num_updates=54200, lr=6.07457e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=62, gb_free=15.7, wall=0
2023-09-05 23:41:38 | INFO | train_inner | epoch 030:   1368 / 1826 loss=1.84, trans_loss=4.649, nll_loss=1.848, w2v_ctc_loss=0.622, task_loss=4.046, task_loss_gen=9.732, contrastive_loss=0, total=3977.54, n_correct=2809.75, ppl=3.6, accuracy=70.64, wps=12722.7, ups=1.6, wpb=7955.1, bsz=284.6, num_updates=54300, lr=6.06897e-05, gnorm=0.547, clip=0, loss_scale=64, train_wall=62, gb_free=14, wall=0
2023-09-05 23:42:39 | INFO | train_inner | epoch 030:   1468 / 1826 loss=1.837, trans_loss=4.641, nll_loss=1.838, w2v_ctc_loss=0.624, task_loss=4.174, task_loss_gen=9.666, contrastive_loss=0, total=3980.42, n_correct=2818.15, ppl=3.58, accuracy=70.8, wps=12883.9, ups=1.62, wpb=7960.8, bsz=289.5, num_updates=54400, lr=6.06339e-05, gnorm=0.543, clip=0, loss_scale=128, train_wall=61, gb_free=16.3, wall=0
2023-09-05 23:43:42 | INFO | train_inner | epoch 030:   1568 / 1826 loss=1.843, trans_loss=4.641, nll_loss=1.837, w2v_ctc_loss=0.632, task_loss=3.744, task_loss_gen=12.094, contrastive_loss=0, total=3869.45, n_correct=2734.57, ppl=3.57, accuracy=70.671, wps=12425.4, ups=1.61, wpb=7738.9, bsz=262.9, num_updates=54500, lr=6.05783e-05, gnorm=0.555, clip=0, loss_scale=128, train_wall=62, gb_free=16.7, wall=0
2023-09-05 23:44:45 | INFO | train_inner | epoch 030:   1668 / 1826 loss=1.828, trans_loss=4.637, nll_loss=1.833, w2v_ctc_loss=0.611, task_loss=2.739, task_loss_gen=11.098, contrastive_loss=0, total=3955.58, n_correct=2805.75, ppl=3.56, accuracy=70.931, wps=12476, ups=1.58, wpb=7911.2, bsz=291, num_updates=54600, lr=6.05228e-05, gnorm=0.54, clip=0, loss_scale=128, train_wall=63, gb_free=11.7, wall=0
2023-09-05 23:45:47 | INFO | train_inner | epoch 030:   1768 / 1826 loss=1.832, trans_loss=4.643, nll_loss=1.841, w2v_ctc_loss=0.616, task_loss=2.758, task_loss_gen=10.758, contrastive_loss=0, total=4024.22, n_correct=2852.63, ppl=3.58, accuracy=70.887, wps=13011.7, ups=1.62, wpb=8048.4, bsz=305, num_updates=54700, lr=6.04674e-05, gnorm=0.54, clip=0, loss_scale=128, train_wall=61, gb_free=11.8, wall=0
2023-09-05 23:46:22 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(1273.1288, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(709.2595, device='cuda:3', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1150.8021, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(663.3151, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1675.0874, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(1077.0272, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1452.7429, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(800.7577, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1240.6467, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(781.2517, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1020.9138, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(554.0748, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1173.3491, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(717.4301, device='cuda:1', grad_fn=<MulBackward0>)
tensor([-0.8521], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.8521], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-05 23:47:02 | INFO | dev_st | epoch 030 | valid on 'dev_st' subset | loss 3.797 | trans_loss 4.973 | nll_loss 2.21 | w2v_ctc_loss 1.434 | task_loss 55.837 | task_loss_gen 33.3 | contrastive_loss 0 | total 3505.91 | n_correct 2441.64 | ppl 4.63 | accuracy 69.643 | uer 17.036 | wer 18.963 | raw_wer 18.963 | bleu 31.48 | wps 1184.7 | wpb 3505.9 | bsz 119.3 | num_updates 54758 | best_bleu 31.52
2023-09-05 23:47:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 54758 updates
2023-09-05 23:47:02 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.4800.pt
2023-09-05 23:47:04 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.4800.pt
2023-09-05 23:47:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint.best_bleu_31.4800.pt (epoch 30 @ 54758 updates, score 31.48) (writing took 6.979673902038485 seconds)
2023-09-05 23:47:09 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)
2023-09-05 23:47:09 | INFO | train | epoch 030 | loss 1.836 | trans_loss 4.641 | nll_loss 1.837 | w2v_ctc_loss 0.623 | task_loss 3.63 | task_loss_gen 10.497 | contrastive_loss 0 | total 3956.37 | n_correct 2801.65 | ppl 3.57 | accuracy 70.814 | wps 11647.9 | ups 1.47 | wpb 7912.7 | bsz 284.8 | num_updates 54758 | lr 6.04354e-05 | gnorm 0.546 | clip 0 | loss_scale 128 | train_wall 1123 | gb_free 16.1 | wall 0
2023-09-05 23:47:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-05 23:47:09 | INFO | fairseq.trainer | begin training epoch 31
2023-09-05 23:47:09 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-05 23:47:43 | INFO | train_inner | epoch 031:     42 / 1826 loss=1.828, trans_loss=4.632, nll_loss=1.826, w2v_ctc_loss=0.611, task_loss=2.769, task_loss_gen=12.64, contrastive_loss=0, total=3909.69, n_correct=2777.13, ppl=3.54, accuracy=71.032, wps=6762.8, ups=0.86, wpb=7819.4, bsz=278, num_updates=54800, lr=6.04122e-05, gnorm=0.553, clip=0, loss_scale=128, train_wall=61, gb_free=16.6, wall=0
2023-09-05 23:48:44 | INFO | train_inner | epoch 031:    142 / 1826 loss=1.826, trans_loss=4.625, nll_loss=1.818, w2v_ctc_loss=0.608, task_loss=2.751, task_loss_gen=12.294, contrastive_loss=0, total=3973.55, n_correct=2825.16, ppl=3.53, accuracy=71.099, wps=12849.4, ups=1.62, wpb=7947.1, bsz=284, num_updates=54900, lr=6.03572e-05, gnorm=0.54, clip=0, loss_scale=128, train_wall=61, gb_free=16.4, wall=0
2023-09-05 23:49:46 | INFO | train_inner | epoch 031:    242 / 1826 loss=1.833, trans_loss=4.619, nll_loss=1.808, w2v_ctc_loss=0.625, task_loss=3.01, task_loss_gen=13.458, contrastive_loss=0, total=3849.36, n_correct=2737.96, ppl=3.5, accuracy=71.128, wps=12488.8, ups=1.62, wpb=7698.7, bsz=257.3, num_updates=55000, lr=6.03023e-05, gnorm=0.551, clip=0, loss_scale=128, train_wall=61, gb_free=17.1, wall=0
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
2023-09-05 23:50:49 | INFO | train_inner | epoch 031:    342 / 1826 loss=1.83, trans_loss=4.629, nll_loss=1.822, w2v_ctc_loss=0.611, task_loss=2.961, task_loss_gen=12.07, contrastive_loss=0, total=3929.06, n_correct=2791.28, ppl=3.54, accuracy=71.042, wps=12543.8, ups=1.6, wpb=7858.1, bsz=271.4, num_updates=55100, lr=6.02475e-05, gnorm=0.535, clip=0, loss_scale=128, train_wall=62, gb_free=10.7, wall=0
2023-09-05 23:51:50 | INFO | train_inner | epoch 031:    442 / 1826 loss=1.839, trans_loss=4.634, nll_loss=1.829, w2v_ctc_loss=0.632, task_loss=2.879, task_loss_gen=12.357, contrastive_loss=0, total=3892.26, n_correct=2758.31, ppl=3.55, accuracy=70.867, wps=12623.3, ups=1.62, wpb=7784.5, bsz=272.8, num_updates=55200, lr=6.01929e-05, gnorm=0.548, clip=0, loss_scale=128, train_wall=61, gb_free=16.1, wall=0
2023-09-05 23:52:43 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0
2023-09-05 23:52:53 | INFO | train_inner | epoch 031:    543 / 1826 loss=1.834, trans_loss=4.639, nll_loss=1.835, w2v_ctc_loss=0.623, task_loss=2.701, task_loss_gen=11.324, contrastive_loss=0, total=3999.23, n_correct=2834.09, ppl=3.57, accuracy=70.866, wps=12823.3, ups=1.6, wpb=7998.5, bsz=294.5, num_updates=55300, lr=6.01385e-05, gnorm=0.535, clip=0, loss_scale=64, train_wall=62, gb_free=15, wall=0
2023-09-05 23:53:55 | INFO | train_inner | epoch 031:    643 / 1826 loss=1.836, trans_loss=4.637, nll_loss=1.832, w2v_ctc_loss=0.625, task_loss=3.861, task_loss_gen=10.957, contrastive_loss=0, total=3892.37, n_correct=2756.01, ppl=3.56, accuracy=70.805, wps=12549.1, ups=1.61, wpb=7784.7, bsz=279.2, num_updates=55400, lr=6.00842e-05, gnorm=0.557, clip=0, loss_scale=64, train_wall=61, gb_free=15.6, wall=0
2023-09-05 23:54:57 | INFO | train_inner | epoch 031:    743 / 1826 loss=1.825, trans_loss=4.626, nll_loss=1.819, w2v_ctc_loss=0.609, task_loss=3.608, task_loss_gen=10.383, contrastive_loss=0, total=3985.42, n_correct=2837.5, ppl=3.53, accuracy=71.197, wps=12862.3, ups=1.61, wpb=7970.8, bsz=288.1, num_updates=55500, lr=6.003e-05, gnorm=0.543, clip=0, loss_scale=64, train_wall=61, gb_free=16.5, wall=0
2023-09-05 23:55:58 | INFO | train_inner | epoch 031:    843 / 1826 loss=1.833, trans_loss=4.634, nll_loss=1.828, w2v_ctc_loss=0.614, task_loss=4.761, task_loss_gen=11.936, contrastive_loss=0, total=3876.97, n_correct=2751.42, ppl=3.55, accuracy=70.968, wps=12608.4, ups=1.63, wpb=7753.9, bsz=261.9, num_updates=55600, lr=5.9976e-05, gnorm=0.564, clip=0, loss_scale=64, train_wall=61, gb_free=16, wall=0
2023-09-05 23:57:01 | INFO | train_inner | epoch 031:    943 / 1826 loss=1.83, trans_loss=4.639, nll_loss=1.836, w2v_ctc_loss=0.618, task_loss=3.481, task_loss_gen=9.584, contrastive_loss=0, total=4031.75, n_correct=2862.14, ppl=3.57, accuracy=70.99, wps=12820.7, ups=1.59, wpb=8063.5, bsz=305.8, num_updates=55700, lr=5.99222e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=62, gb_free=15.5, wall=0
2023-09-05 23:58:03 | INFO | train_inner | epoch 031:   1043 / 1826 loss=1.827, trans_loss=4.625, nll_loss=1.818, w2v_ctc_loss=0.617, task_loss=4.369, task_loss_gen=10.342, contrastive_loss=0, total=3954.47, n_correct=2811.48, ppl=3.53, accuracy=71.096, wps=12808.3, ups=1.62, wpb=7908.9, bsz=284.7, num_updates=55800, lr=5.98684e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=61, gb_free=15.9, wall=0
2023-09-05 23:58:05 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0
2023-09-05 23:59:06 | INFO | train_inner | epoch 031:   1144 / 1826 loss=1.835, trans_loss=4.647, nll_loss=1.846, w2v_ctc_loss=0.613, task_loss=4.831, task_loss_gen=9.692, contrastive_loss=0, total=3991.57, n_correct=2821.44, ppl=3.6, accuracy=70.685, wps=12643.8, ups=1.58, wpb=7983.1, bsz=288.5, num_updates=55900, lr=5.98149e-05, gnorm=0.6, clip=0, loss_scale=32, train_wall=63, gb_free=15.7, wall=0
2023-09-06 00:00:08 | INFO | train_inner | epoch 031:   1244 / 1826 loss=1.833, trans_loss=4.628, nll_loss=1.821, w2v_ctc_loss=0.623, task_loss=5.244, task_loss_gen=9.988, contrastive_loss=0, total=3933.43, n_correct=2793.8, ppl=3.53, accuracy=71.027, wps=12796.6, ups=1.63, wpb=7866.9, bsz=275.4, num_updates=56000, lr=5.97614e-05, gnorm=0.608, clip=0, loss_scale=32, train_wall=61, gb_free=15.8, wall=0
2023-09-06 00:00:08 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
mt_weight tensor(0.5000)
asr_weight tensor(0.2090)
tensor([-0.3542], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.3542], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-06 00:00:47 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 3.762 | trans_loss 4.973 | nll_loss 2.215 | w2v_ctc_loss 1.316 | task_loss 74.202 | task_loss_gen 40.131 | contrastive_loss 0 | total 3505.91 | n_correct 2438.36 | ppl 4.64 | accuracy 69.55 | uer 16.872 | wer 18.828 | raw_wer 18.828 | bleu 31.19 | wps 1189.6 | wpb 3505.9 | bsz 119.3 | num_updates 56000 | best_bleu 31.52
2023-09-06 00:00:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 56000 updates
2023-09-06 00:00:47 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_31_56000.pt
2023-09-06 00:00:49 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_31_56000.pt
2023-09-06 00:00:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_31_56000.pt (epoch 31 @ 56000 updates, score 31.19) (writing took 7.726558275986463 seconds)
--Backword ST Loss tensor(870.8610, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(462.4943, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-06 00:01:56 | INFO | train_inner | epoch 031:   1344 / 1826 loss=1.831, trans_loss=4.637, nll_loss=1.832, w2v_ctc_loss=0.62, task_loss=4.561, task_loss_gen=8.752, contrastive_loss=0, total=3957.42, n_correct=2807.68, ppl=3.56, accuracy=70.947, wps=7291.4, ups=0.92, wpb=7914.8, bsz=299.6, num_updates=56100, lr=5.97081e-05, gnorm=0.589, clip=0, loss_scale=32, train_wall=61, gb_free=15.1, wall=0
2023-09-06 00:02:59 | INFO | train_inner | epoch 031:   1444 / 1826 loss=1.83, trans_loss=4.636, nll_loss=1.832, w2v_ctc_loss=0.614, task_loss=5.103, task_loss_gen=9.108, contrastive_loss=0, total=4000.25, n_correct=2837.6, ppl=3.56, accuracy=70.936, wps=12683.5, ups=1.59, wpb=8000.5, bsz=294, num_updates=56200, lr=5.9655e-05, gnorm=0.605, clip=0, loss_scale=32, train_wall=62, gb_free=17.5, wall=0
2023-09-06 00:04:01 | INFO | train_inner | epoch 031:   1544 / 1826 loss=1.836, trans_loss=4.639, nll_loss=1.836, w2v_ctc_loss=0.624, task_loss=5.043, task_loss_gen=9.763, contrastive_loss=0, total=3946.83, n_correct=2801.45, ppl=3.57, accuracy=70.98, wps=12754.9, ups=1.62, wpb=7893.7, bsz=278.1, num_updates=56300, lr=5.9602e-05, gnorm=0.6, clip=0, loss_scale=32, train_wall=61, gb_free=16.8, wall=0
2023-09-06 00:05:02 | INFO | train_inner | epoch 031:   1644 / 1826 loss=1.825, trans_loss=4.637, nll_loss=1.834, w2v_ctc_loss=0.613, task_loss=4.197, task_loss_gen=8.022, contrastive_loss=0, total=4131.64, n_correct=2935.13, ppl=3.56, accuracy=71.04, wps=13489, ups=1.63, wpb=8263.3, bsz=318.8, num_updates=56400, lr=5.95491e-05, gnorm=0.571, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=0
2023-09-06 00:06:05 | INFO | train_inner | epoch 031:   1744 / 1826 loss=1.836, trans_loss=4.644, nll_loss=1.842, w2v_ctc_loss=0.618, task_loss=4.808, task_loss_gen=9.295, contrastive_loss=0, total=3988.83, n_correct=2822.1, ppl=3.58, accuracy=70.75, wps=12814, ups=1.61, wpb=7977.7, bsz=295.2, num_updates=56500, lr=5.94964e-05, gnorm=0.601, clip=0, loss_scale=32, train_wall=62, gb_free=15, wall=0
2023-09-06 00:06:55 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(713.8190, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(442.7451, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(952.2001, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(542.6160, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1078.0083, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(621.7098, device='cuda:3', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1604.7430, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(859.3116, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(967.0574, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(501.7798, device='cuda:4', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1506.3879, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(918.4606, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1060.9855, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(648.3058, device='cuda:5', grad_fn=<MulBackward0>)
tensor([-0.9604], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.9604], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-06 00:07:34 | INFO | dev_st | epoch 031 | valid on 'dev_st' subset | loss 3.78 | trans_loss 4.975 | nll_loss 2.216 | w2v_ctc_loss 1.374 | task_loss 62.816 | task_loss_gen 35.051 | contrastive_loss 0 | total 3505.91 | n_correct 2437.36 | ppl 4.65 | accuracy 69.522 | uer 16.969 | wer 18.948 | raw_wer 18.948 | bleu 31.53 | wps 1186.1 | wpb 3505.9 | bsz 119.3 | num_updates 56582 | best_bleu 31.53
2023-09-06 00:07:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 56582 updates
2023-09-06 00:07:34 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-06 00:07:40 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-06 00:07:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 31 @ 56582 updates, score 31.53) (writing took 11.039456723956391 seconds)
2023-09-06 00:07:46 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)
2023-09-06 00:07:46 | INFO | train | epoch 031 | loss 1.832 | trans_loss 4.634 | nll_loss 1.829 | w2v_ctc_loss 0.618 | task_loss 4.035 | task_loss_gen 10.498 | contrastive_loss 0 | total 3955.86 | n_correct 2807.26 | ppl 3.55 | accuracy 70.965 | wps 11667.5 | ups 1.47 | wpb 7911.7 | bsz 284.7 | num_updates 56582 | lr 5.94533e-05 | gnorm 0.569 | clip 0 | loss_scale 32 | train_wall 1120 | gb_free 16.3 | wall 0
2023-09-06 00:07:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-06 00:07:46 | INFO | fairseq.trainer | begin training epoch 32
2023-09-06 00:07:46 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-06 00:08:04 | INFO | train_inner | epoch 032:     18 / 1826 loss=1.838, trans_loss=4.64, nll_loss=1.836, w2v_ctc_loss=0.624, task_loss=4.847, task_loss_gen=9.897, contrastive_loss=0, total=3872.43, n_correct=2742.25, ppl=3.57, accuracy=70.815, wps=6468.3, ups=0.84, wpb=7744.9, bsz=272, num_updates=56600, lr=5.94438e-05, gnorm=0.609, clip=0, loss_scale=32, train_wall=61, gb_free=17.2, wall=0
2023-09-06 00:09:07 | INFO | train_inner | epoch 032:    118 / 1826 loss=1.819, trans_loss=4.619, nll_loss=1.809, w2v_ctc_loss=0.603, task_loss=4.67, task_loss_gen=9.188, contrastive_loss=0, total=3997.19, n_correct=2851.59, ppl=3.5, accuracy=71.34, wps=12835, ups=1.61, wpb=7994.4, bsz=292.9, num_updates=56700, lr=5.93914e-05, gnorm=0.585, clip=0, loss_scale=32, train_wall=62, gb_free=14.3, wall=0
2023-09-06 00:10:09 | INFO | train_inner | epoch 032:    218 / 1826 loss=1.819, trans_loss=4.62, nll_loss=1.811, w2v_ctc_loss=0.601, task_loss=5.127, task_loss_gen=8.998, contrastive_loss=0, total=3932.5, n_correct=2804.03, ppl=3.51, accuracy=71.304, wps=12546.3, ups=1.6, wpb=7865, bsz=283.1, num_updates=56800, lr=5.93391e-05, gnorm=0.599, clip=0, loss_scale=32, train_wall=62, gb_free=16.9, wall=0
2023-09-06 00:11:11 | INFO | train_inner | epoch 032:    318 / 1826 loss=1.823, trans_loss=4.622, nll_loss=1.814, w2v_ctc_loss=0.609, task_loss=5.328, task_loss_gen=9.78, contrastive_loss=0, total=3989.26, n_correct=2841.37, ppl=3.52, accuracy=71.225, wps=12963.9, ups=1.62, wpb=7978.5, bsz=293, num_updates=56900, lr=5.92869e-05, gnorm=0.619, clip=0, loss_scale=32, train_wall=61, gb_free=11.7, wall=0
2023-09-06 00:12:13 | INFO | train_inner | epoch 032:    418 / 1826 loss=1.828, trans_loss=4.628, nll_loss=1.82, w2v_ctc_loss=0.614, task_loss=5.387, task_loss_gen=9.419, contrastive_loss=0, total=4039.33, n_correct=2872.05, ppl=3.53, accuracy=71.102, wps=12959.8, ups=1.6, wpb=8078.7, bsz=293.4, num_updates=57000, lr=5.92349e-05, gnorm=0.621, clip=0, loss_scale=32, train_wall=62, gb_free=14.7, wall=0
2023-09-06 00:13:15 | INFO | train_inner | epoch 032:    518 / 1826 loss=1.825, trans_loss=4.625, nll_loss=1.816, w2v_ctc_loss=0.61, task_loss=4.585, task_loss_gen=9.106, contrastive_loss=0, total=3971.17, n_correct=2828.58, ppl=3.52, accuracy=71.228, wps=12904, ups=1.62, wpb=7942.3, bsz=286.9, num_updates=57100, lr=5.9183e-05, gnorm=0.579, clip=0, loss_scale=32, train_wall=61, gb_free=16.3, wall=0
2023-09-06 00:14:16 | INFO | train_inner | epoch 032:    618 / 1826 loss=1.823, trans_loss=4.622, nll_loss=1.813, w2v_ctc_loss=0.613, task_loss=5.153, task_loss_gen=9.422, contrastive_loss=0, total=3991.73, n_correct=2843.12, ppl=3.51, accuracy=71.225, wps=13070.9, ups=1.64, wpb=7983.5, bsz=292.8, num_updates=57200, lr=5.91312e-05, gnorm=0.62, clip=0, loss_scale=32, train_wall=61, gb_free=16.4, wall=0
2023-09-06 00:15:19 | INFO | train_inner | epoch 032:    718 / 1826 loss=1.818, trans_loss=4.622, nll_loss=1.814, w2v_ctc_loss=0.597, task_loss=4.871, task_loss_gen=8.883, contrastive_loss=0, total=3966.74, n_correct=2828.69, ppl=3.52, accuracy=71.31, wps=12568.1, ups=1.58, wpb=7933.5, bsz=288, num_updates=57300, lr=5.90796e-05, gnorm=0.594, clip=0, loss_scale=32, train_wall=63, gb_free=15, wall=0
2023-09-06 00:16:21 | INFO | train_inner | epoch 032:    818 / 1826 loss=1.825, trans_loss=4.629, nll_loss=1.822, w2v_ctc_loss=0.603, task_loss=5.589, task_loss_gen=9.281, contrastive_loss=0, total=3957.69, n_correct=2809.64, ppl=3.54, accuracy=70.992, wps=12804.5, ups=1.62, wpb=7915.4, bsz=289.8, num_updates=57400, lr=5.90281e-05, gnorm=0.607, clip=0, loss_scale=32, train_wall=61, gb_free=15.1, wall=0
2023-09-06 00:17:22 | INFO | train_inner | epoch 032:    918 / 1826 loss=1.832, trans_loss=4.636, nll_loss=1.832, w2v_ctc_loss=0.619, task_loss=5.373, task_loss_gen=9.026, contrastive_loss=0, total=3945.75, n_correct=2800.01, ppl=3.56, accuracy=70.963, wps=12784.6, ups=1.62, wpb=7891.5, bsz=284.3, num_updates=57500, lr=5.89768e-05, gnorm=0.597, clip=0, loss_scale=32, train_wall=61, gb_free=14.6, wall=0
2023-09-06 00:18:24 | INFO | train_inner | epoch 032:   1018 / 1826 loss=1.835, trans_loss=4.63, nll_loss=1.823, w2v_ctc_loss=0.625, task_loss=5.487, task_loss_gen=9.737, contrastive_loss=0, total=3864.68, n_correct=2743.08, ppl=3.54, accuracy=70.978, wps=12534.3, ups=1.62, wpb=7729.4, bsz=266.8, num_updates=57600, lr=5.89256e-05, gnorm=0.586, clip=0, loss_scale=32, train_wall=61, gb_free=15.9, wall=0
2023-09-06 00:19:26 | INFO | train_inner | epoch 032:   1118 / 1826 loss=1.834, trans_loss=4.64, nll_loss=1.836, w2v_ctc_loss=0.62, task_loss=4.717, task_loss_gen=8.67, contrastive_loss=0, total=3993.51, n_correct=2834.48, ppl=3.57, accuracy=70.977, wps=12987.1, ups=1.63, wpb=7987, bsz=285.9, num_updates=57700, lr=5.88745e-05, gnorm=0.568, clip=0, loss_scale=32, train_wall=61, gb_free=14.9, wall=0
2023-09-06 00:20:27 | INFO | train_inner | epoch 032:   1218 / 1826 loss=1.831, trans_loss=4.631, nll_loss=1.825, w2v_ctc_loss=0.62, task_loss=4.931, task_loss_gen=8.998, contrastive_loss=0, total=3971.71, n_correct=2821.7, ppl=3.54, accuracy=71.045, wps=12850.3, ups=1.62, wpb=7943.4, bsz=283.5, num_updates=57800, lr=5.88235e-05, gnorm=0.581, clip=0, loss_scale=32, train_wall=61, gb_free=16, wall=0
2023-09-06 00:21:29 | INFO | train_inner | epoch 032:   1318 / 1826 loss=1.832, trans_loss=4.637, nll_loss=1.833, w2v_ctc_loss=0.609, task_loss=5.293, task_loss_gen=9.584, contrastive_loss=0, total=3947.13, n_correct=2796.56, ppl=3.56, accuracy=70.85, wps=12754, ups=1.62, wpb=7894.3, bsz=274.4, num_updates=57900, lr=5.87727e-05, gnorm=0.572, clip=0, loss_scale=64, train_wall=61, gb_free=15.5, wall=0
2023-09-06 00:22:31 | INFO | train_inner | epoch 032:   1418 / 1826 loss=1.846, trans_loss=4.641, nll_loss=1.837, w2v_ctc_loss=0.636, task_loss=4.571, task_loss_gen=10.752, contrastive_loss=0, total=3859.3, n_correct=2727.53, ppl=3.57, accuracy=70.674, wps=12501.8, ups=1.62, wpb=7718.6, bsz=260.4, num_updates=58000, lr=5.8722e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=61, gb_free=12.3, wall=0
2023-09-06 00:22:31 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([-1.4258], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-1.4258], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-06 00:23:10 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.774 | trans_loss 4.972 | nll_loss 2.212 | w2v_ctc_loss 1.361 | task_loss 69.717 | task_loss_gen 38.277 | contrastive_loss 0 | total 3505.91 | n_correct 2441.55 | ppl 4.63 | accuracy 69.641 | uer 16.875 | wer 18.776 | raw_wer 18.776 | bleu 31.25 | wps 1186.4 | wpb 3505.9 | bsz 119.3 | num_updates 58000 | best_bleu 31.53
2023-09-06 00:23:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 58000 updates
2023-09-06 00:23:10 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_32_58000.pt
2023-09-06 00:23:14 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_32_58000.pt
2023-09-06 00:23:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_32_58000.pt (epoch 32 @ 58000 updates, score 31.25) (writing took 8.529073728015646 seconds)
--Backword ST Loss tensor(1489.3854, device='cuda:0', grad_fn=<AddBackward0>) 	MT Loss tensor(884.6728, device='cuda:0', grad_fn=<MulBackward0>)
2023-09-06 00:24:22 | INFO | train_inner | epoch 032:   1518 / 1826 loss=1.833, trans_loss=4.644, nll_loss=1.842, w2v_ctc_loss=0.61, task_loss=4.276, task_loss_gen=9.198, contrastive_loss=0, total=4006.24, n_correct=2837.51, ppl=3.59, accuracy=70.827, wps=7241.7, ups=0.9, wpb=8012.5, bsz=296.5, num_updates=58100, lr=5.86715e-05, gnorm=0.545, clip=0, loss_scale=64, train_wall=62, gb_free=11.8, wall=0
2023-09-06 00:25:23 | INFO | train_inner | epoch 032:   1618 / 1826 loss=1.832, trans_loss=4.629, nll_loss=1.822, w2v_ctc_loss=0.622, task_loss=4.316, task_loss_gen=10.119, contrastive_loss=0, total=3919.32, n_correct=2786.14, ppl=3.54, accuracy=71.087, wps=12826.3, ups=1.64, wpb=7838.6, bsz=276.1, num_updates=58200, lr=5.8621e-05, gnorm=0.558, clip=0, loss_scale=64, train_wall=61, gb_free=15.3, wall=0
2023-09-06 00:26:25 | INFO | train_inner | epoch 032:   1718 / 1826 loss=1.826, trans_loss=4.624, nll_loss=1.815, w2v_ctc_loss=0.612, task_loss=5.524, task_loss_gen=10.627, contrastive_loss=0, total=3904.21, n_correct=2779.88, ppl=3.52, accuracy=71.202, wps=12674.7, ups=1.62, wpb=7808.4, bsz=278.1, num_updates=58300, lr=5.85707e-05, gnorm=0.565, clip=0, loss_scale=64, train_wall=61, gb_free=16.2, wall=0
2023-09-06 00:27:27 | INFO | train_inner | epoch 032:   1818 / 1826 loss=1.831, trans_loss=4.636, nll_loss=1.832, w2v_ctc_loss=0.615, task_loss=4.114, task_loss_gen=8.782, contrastive_loss=0, total=3983.47, n_correct=2822.95, ppl=3.56, accuracy=70.867, wps=12761, ups=1.6, wpb=7966.9, bsz=302.2, num_updates=58400, lr=5.85206e-05, gnorm=0.548, clip=0, loss_scale=64, train_wall=62, gb_free=16.5, wall=0
2023-09-06 00:27:32 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
--Backword ST Loss tensor(960.9020, device='cuda:3', grad_fn=<AddBackward0>) 	MT Loss tensor(523.2554, device='cuda:3', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1289.1136, device='cuda:7', grad_fn=<AddBackward0>) 	MT Loss tensor(694.2788, device='cuda:7', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1417.0057, device='cuda:6', grad_fn=<AddBackward0>) 	MT Loss tensor(835.7301, device='cuda:6', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(701.0512, device='cuda:1', grad_fn=<AddBackward0>) 	MT Loss tensor(375.3833, device='cuda:1', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1606.0938, device='cuda:5', grad_fn=<AddBackward0>) 	MT Loss tensor(917.4380, device='cuda:5', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1557.8618, device='cuda:2', grad_fn=<AddBackward0>) 	MT Loss tensor(943.7380, device='cuda:2', grad_fn=<MulBackward0>)
--Backword ST Loss tensor(1335.2626, device='cuda:4', grad_fn=<AddBackward0>) 	MT Loss tensor(703.1639, device='cuda:4', grad_fn=<MulBackward0>)
tensor([-0.2374], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([-0.2374], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-06 00:28:11 | INFO | dev_st | epoch 032 | valid on 'dev_st' subset | loss 3.789 | trans_loss 4.976 | nll_loss 2.215 | w2v_ctc_loss 1.401 | task_loss 75.032 | task_loss_gen 39.63 | contrastive_loss 0 | total 3505.91 | n_correct 2439.82 | ppl 4.64 | accuracy 69.592 | uer 17.031 | wer 18.997 | raw_wer 18.997 | bleu 31.74 | wps 1191.6 | wpb 3505.9 | bsz 119.3 | num_updates 58408 | best_bleu 31.74
2023-09-06 00:28:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 58408 updates
2023-09-06 00:28:11 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-06 00:28:19 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt
2023-09-06 00:28:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_best.pt (epoch 32 @ 58408 updates, score 31.74) (writing took 15.319910328020342 seconds)
2023-09-06 00:28:26 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)
2023-09-06 00:28:26 | INFO | train | epoch 032 | loss 1.828 | trans_loss 4.63 | nll_loss 1.823 | w2v_ctc_loss 0.613 | task_loss 4.947 | task_loss_gen 9.406 | contrastive_loss 0 | total 3956.37 | n_correct 2811.75 | ppl 3.54 | accuracy 71.069 | wps 11644.9 | ups 1.47 | wpb 7912.7 | bsz 284.8 | num_updates 58408 | lr 5.85166e-05 | gnorm 0.584 | clip 0 | loss_scale 64 | train_wall 1119 | gb_free 16.7 | wall 0
2023-09-06 00:28:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 1826
2023-09-06 00:28:27 | INFO | fairseq.trainer | begin training epoch 33
2023-09-06 00:28:27 | INFO | fairseq_cli.train | Start iterating over samples
2023-09-06 00:29:31 | INFO | train_inner | epoch 033:     92 / 1826 loss=1.812, trans_loss=4.602, nll_loss=1.788, w2v_ctc_loss=0.603, task_loss=3.809, task_loss_gen=9.299, contrastive_loss=0, total=3879.62, n_correct=2777.66, ppl=3.45, accuracy=71.596, wps=6278.5, ups=0.81, wpb=7759.2, bsz=283.6, num_updates=58500, lr=5.84705e-05, gnorm=0.562, clip=0, loss_scale=64, train_wall=61, gb_free=17.2, wall=0
2023-09-06 00:30:32 | INFO | train_inner | epoch 033:    192 / 1826 loss=1.82, trans_loss=4.616, nll_loss=1.805, w2v_ctc_loss=0.601, task_loss=3.785, task_loss_gen=10.319, contrastive_loss=0, total=3959.9, n_correct=2827.24, ppl=3.49, accuracy=71.397, wps=12781.5, ups=1.61, wpb=7919.8, bsz=279.4, num_updates=58600, lr=5.84206e-05, gnorm=0.546, clip=0, loss_scale=64, train_wall=61, gb_free=15.2, wall=0
2023-09-06 00:31:34 | INFO | train_inner | epoch 033:    292 / 1826 loss=1.818, trans_loss=4.617, nll_loss=1.807, w2v_ctc_loss=0.602, task_loss=4.164, task_loss_gen=9.876, contrastive_loss=0, total=3946.19, n_correct=2814.25, ppl=3.5, accuracy=71.316, wps=12873.5, ups=1.63, wpb=7892.4, bsz=287.5, num_updates=58700, lr=5.83708e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=61, gb_free=15.8, wall=0
2023-09-06 00:32:36 | INFO | train_inner | epoch 033:    392 / 1826 loss=1.831, trans_loss=4.624, nll_loss=1.816, w2v_ctc_loss=0.615, task_loss=4.399, task_loss_gen=10.441, contrastive_loss=0, total=3939.86, n_correct=2802.04, ppl=3.52, accuracy=71.12, wps=12754.8, ups=1.62, wpb=7879.7, bsz=272, num_updates=58800, lr=5.83212e-05, gnorm=0.555, clip=0, loss_scale=64, train_wall=61, gb_free=16.6, wall=0
2023-09-06 00:33:38 | INFO | train_inner | epoch 033:    492 / 1826 loss=1.834, trans_loss=4.631, nll_loss=1.825, w2v_ctc_loss=0.628, task_loss=4.132, task_loss_gen=9.685, contrastive_loss=0, total=3950.46, n_correct=2803.57, ppl=3.54, accuracy=70.968, wps=12632.3, ups=1.6, wpb=7900.9, bsz=284.9, num_updates=58900, lr=5.82717e-05, gnorm=0.562, clip=0, loss_scale=64, train_wall=62, gb_free=16.9, wall=0
2023-09-06 00:34:40 | INFO | train_inner | epoch 033:    592 / 1826 loss=1.815, trans_loss=4.619, nll_loss=1.81, w2v_ctc_loss=0.594, task_loss=3.562, task_loss_gen=9.027, contrastive_loss=0, total=4019.94, n_correct=2866.74, ppl=3.51, accuracy=71.313, wps=13005.4, ups=1.62, wpb=8039.9, bsz=309.7, num_updates=59000, lr=5.82223e-05, gnorm=0.536, clip=0, loss_scale=64, train_wall=61, gb_free=15.5, wall=0
2023-09-06 00:35:42 | INFO | train_inner | epoch 033:    692 / 1826 loss=1.823, trans_loss=4.624, nll_loss=1.816, w2v_ctc_loss=0.606, task_loss=3.68, task_loss_gen=9.834, contrastive_loss=0, total=4009.9, n_correct=2855.37, ppl=3.52, accuracy=71.208, wps=12946.7, ups=1.61, wpb=8019.8, bsz=286.5, num_updates=59100, lr=5.8173e-05, gnorm=0.544, clip=0, loss_scale=64, train_wall=61, gb_free=14.8, wall=0
2023-09-06 00:36:43 | INFO | train_inner | epoch 033:    792 / 1826 loss=1.813, trans_loss=4.606, nll_loss=1.793, w2v_ctc_loss=0.602, task_loss=3.26, task_loss_gen=8.975, contrastive_loss=0, total=4020.93, n_correct=2877.75, ppl=3.46, accuracy=71.569, wps=13138.3, ups=1.63, wpb=8041.9, bsz=301.4, num_updates=59200, lr=5.81238e-05, gnorm=0.54, clip=0, loss_scale=64, train_wall=61, gb_free=11.3, wall=0
2023-09-06 00:37:45 | INFO | train_inner | epoch 033:    892 / 1826 loss=1.822, trans_loss=4.617, nll_loss=1.807, w2v_ctc_loss=0.607, task_loss=3.899, task_loss_gen=10.451, contrastive_loss=0, total=3938.41, n_correct=2809.74, ppl=3.5, accuracy=71.342, wps=12706.4, ups=1.61, wpb=7876.8, bsz=277.6, num_updates=59300, lr=5.80748e-05, gnorm=0.556, clip=0, loss_scale=64, train_wall=61, gb_free=16.4, wall=0
2023-09-06 00:38:47 | INFO | train_inner | epoch 033:    992 / 1826 loss=1.828, trans_loss=4.628, nll_loss=1.821, w2v_ctc_loss=0.61, task_loss=4.031, task_loss_gen=10.066, contrastive_loss=0, total=3941.94, n_correct=2802.75, ppl=3.53, accuracy=71.101, wps=12672.9, ups=1.61, wpb=7883.9, bsz=279.4, num_updates=59400, lr=5.80259e-05, gnorm=0.554, clip=0, loss_scale=64, train_wall=62, gb_free=13, wall=0
2023-09-06 00:39:50 | INFO | train_inner | epoch 033:   1092 / 1826 loss=1.828, trans_loss=4.63, nll_loss=1.823, w2v_ctc_loss=0.609, task_loss=4.471, task_loss_gen=11.047, contrastive_loss=0, total=3934.01, n_correct=2798.17, ppl=3.54, accuracy=71.128, wps=12650.8, ups=1.61, wpb=7868, bsz=268.3, num_updates=59500, lr=5.79771e-05, gnorm=0.555, clip=0, loss_scale=64, train_wall=62, gb_free=12.5, wall=0
2023-09-06 00:40:51 | INFO | train_inner | epoch 033:   1192 / 1826 loss=1.821, trans_loss=4.622, nll_loss=1.814, w2v_ctc_loss=0.61, task_loss=3.764, task_loss_gen=9.67, contrastive_loss=0, total=4000.02, n_correct=2854.57, ppl=3.52, accuracy=71.364, wps=13055.8, ups=1.63, wpb=8000, bsz=298.5, num_updates=59600, lr=5.79284e-05, gnorm=0.553, clip=0, loss_scale=64, train_wall=61, gb_free=16, wall=0
2023-09-06 00:41:53 | INFO | train_inner | epoch 033:   1292 / 1826 loss=1.826, trans_loss=4.617, nll_loss=1.807, w2v_ctc_loss=0.616, task_loss=3.681, task_loss_gen=10.439, contrastive_loss=0, total=3899.95, n_correct=2776.67, ppl=3.5, accuracy=71.198, wps=12510.2, ups=1.6, wpb=7799.9, bsz=276.3, num_updates=59700, lr=5.78799e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=62, gb_free=15.6, wall=0
2023-09-06 00:42:55 | INFO | train_inner | epoch 033:   1392 / 1826 loss=1.827, trans_loss=4.63, nll_loss=1.823, w2v_ctc_loss=0.61, task_loss=3.746, task_loss_gen=9.963, contrastive_loss=0, total=3995.61, n_correct=2838.06, ppl=3.54, accuracy=71.029, wps=13021.2, ups=1.63, wpb=7991.2, bsz=290.4, num_updates=59800, lr=5.78315e-05, gnorm=0.55, clip=0, loss_scale=64, train_wall=61, gb_free=16.6, wall=0
2023-09-06 00:43:56 | INFO | train_inner | epoch 033:   1492 / 1826 loss=1.833, trans_loss=4.63, nll_loss=1.824, w2v_ctc_loss=0.624, task_loss=4.655, task_loss_gen=10.783, contrastive_loss=0, total=3910.24, n_correct=2778.26, ppl=3.54, accuracy=71.051, wps=12655.9, ups=1.62, wpb=7820.5, bsz=273.3, num_updates=59900, lr=5.77832e-05, gnorm=0.561, clip=0, loss_scale=128, train_wall=61, gb_free=16.8, wall=0
2023-09-06 00:44:59 | INFO | train_inner | epoch 033:   1592 / 1826 loss=1.824, trans_loss=4.623, nll_loss=1.814, w2v_ctc_loss=0.608, task_loss=3.564, task_loss_gen=10.417, contrastive_loss=0, total=3922.79, n_correct=2795.5, ppl=3.52, accuracy=71.263, wps=12441.7, ups=1.59, wpb=7845.6, bsz=277.6, num_updates=60000, lr=5.7735e-05, gnorm=0.543, clip=0, loss_scale=128, train_wall=63, gb_free=15.7, wall=0
2023-09-06 00:44:59 | INFO | fairseq_cli.train | Stopping training due to num_updates: 60000 >= max_update: 60000
2023-09-06 00:44:59 | INFO | fairseq_cli.train | begin validation on "dev_st" subset
tensor([0.2749], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
tensor([0.2749], device='cuda:0', dtype=torch.float16) torch.Size([1]) 1
2023-09-06 00:45:39 | INFO | dev_st | epoch 033 | valid on 'dev_st' subset | loss 3.788 | trans_loss 4.974 | nll_loss 2.215 | w2v_ctc_loss 1.403 | task_loss 78.686 | task_loss_gen 41.424 | contrastive_loss 0 | total 3505.91 | n_correct 2442.36 | ppl 4.64 | accuracy 69.664 | uer 16.974 | wer 19.106 | raw_wer 19.106 | bleu 31.66 | wps 1182.2 | wpb 3505.9 | bsz 119.3 | num_updates 60000 | best_bleu 31.74
2023-09-06 00:45:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 60000 updates
2023-09-06 00:45:39 | INFO | fairseq.trainer | Saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_33_60000.pt
2023-09-06 00:45:42 | INFO | fairseq.trainer | Finished saving checkpoint to /mnt/zhangyh/fairseq-AT/egs/pretrain-all/checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_33_60000.pt
2023-09-06 00:45:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint ./checkpoints/mustc/st/enes_v4_merge_wmt_0905_shrink_soft_noCL_AT_sentence_mixup_changeid_scale3.5_alpha0_mt0.5/checkpoint_33_60000.pt (epoch 33 @ 60000 updates, score 31.66) (writing took 9.291692459024489 seconds)
2023-09-06 00:45:49 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)
2023-09-06 00:45:49 | INFO | train | epoch 033 | loss 1.824 | trans_loss 4.621 | nll_loss 1.812 | w2v_ctc_loss 0.609 | task_loss 3.905 | task_loss_gen 10.013 | contrastive_loss 0 | total 3955.49 | n_correct 2818.12 | ppl 3.51 | accuracy 71.246 | wps 12083.5 | ups 1.53 | wpb 7911 | bsz 284 | num_updates 60000 | lr 5.7735e-05 | gnorm 0.55 | clip 0 | loss_scale 128 | train_wall 976 | gb_free 15.7 | wall 0
2023-09-06 00:45:49 | INFO | fairseq_cli.train | done training in 6759.8 seconds
/mnt/zhangyh/miniconda3/envs/at/lib/python3.8/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 256 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '
